[
  {
    "id": 203,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "-bo7oVejgRM",
    "title": "How Giphy uses CDN to serve a billion GIFs every day",
    "description": "Giphy is the world's most popular GIF website and it serves 10 billion media content every single day, and we can guess it would be using CDN to do that, but is that it?\n\nIn this video, we dive deep into how Giphy uses different features of CDNs to solve different kinds of problems; and while going through it we will also look at a super-interesting internal implementation detail of CDN.",
    "img": "https://i.ytimg.com/vi/-bo7oVejgRM/mqdefault.jpg",
    "gif": null,
    "duration": "16:33",
    "view_count": 34,
    "like_count": 3,
    "comment_count": 0,
    "released_at": "2022-10-17",
    "gist": "",
    "notes_gd": "https://drive.google.com/file/d/1M2Id3sJb9ABbMGSU2WFqpexDzbrd2FEH/view?usp=sharing",
    "slug": "how-giphy-uses-cdn-to-serve-a-billion-gifs-every-day"
  },
  {
    "id": 202,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "FczWm6kx0Kg",
    "title": "How Dropbox efficiently serves and renders a large number of thumbnails",
    "description": "A classic challenge that comes while building Instagram or Google Photos is about quickly and efficiently serving and rendering a large number of thumbnails.\n\nIn this video, we take a look at an ultimate hack that Dropbox used to very efficiently serve a large number of preview thumbnails by streaming the response from the servers using Chunked Transfer Encoding.\n\nOutline:\n\n00:00 Agenda\n02:28 Design of the Photos App\n04:39 Why is this even a problem?\n07:11 Drobox's Solution: Batching\n11:00 Chunked Transfer Encoding",
    "img": "https://i.ytimg.com/vi/FczWm6kx0Kg/mqdefault.jpg",
    "gif": null,
    "duration": "19:3",
    "view_count": 1315,
    "like_count": 89,
    "comment_count": 13,
    "released_at": "2022-10-07",
    "gist": "",
    "notes_gd": "https://drive.google.com/file/d/1l8gl197gxPaCh0mama3LjtiRxNaWgt3v/view?usp=sharing",
    "slug": "how-dropbox-efficiently-serves-and-renders-a-large-number-of-thumbnails"
  },
  {
    "id": 201,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "VNvTEin2liY",
    "title": "Aggregating DynamoDB data in realtime to list restaurants at Deliveroo",
    "description": "DynamoDB is an extremely powerful, scalable, and fast KV store but it lacks Aggregations.\n\nIn this video, we design a usecase that is very common for food delivery startups like Deliveroo and Swiggy that would require us to do real-time aggregation of the DynamoDB data in an extremely cost-efficient way.\n\nOutline:\n\n00:00 Agenda\n02:28 Marking Restaurants as Favourite\n04:49 Realtime aggregating DynamoDB data using Streams\n11:10 Why Serverless?",
    "img": "https://i.ytimg.com/vi/VNvTEin2liY/mqdefault.jpg",
    "gif": null,
    "duration": "15:52",
    "view_count": 1261,
    "like_count": 58,
    "comment_count": 13,
    "released_at": "2022-10-03",
    "gist": "",
    "notes_gd": "https://drive.google.com/file/d/1EuEW8z07r7mJ0dXO8yWIJ9xJpc_8LQFW/view?usp=sharing",
    "slug": "aggregating-dynamodb-data-in-realtime-to-list-restaurants-at-deliveroo"
  },
  {
    "id": 200,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "DQwlmTvs6xA",
    "title": "How Razorpay scaled their notification system",
    "description": "Notifications are extremely crucial for Fintech companies as it is a way to notify a user about an incoming transaction. Hence it becomes extremely important for companies like Razorpay to ensure that notification is delivered to a user within a certain amount of time.\n\nIn this video, we take a detailed look into how Razorpay scaled their notifications systems, and look at their high-level architecture and key design decisions they made along the way to ensure they always meet the SLA.\n\nOutline:\n\n00:00 Agenda\n02:37 Existing notification system\n08:54 Re-architecting notification system",
    "img": "https://i.ytimg.com/vi/DQwlmTvs6xA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/MCQYzMKw8xOsKDprkZ/giphy.gif",
    "duration": "17:32",
    "view_count": 3065,
    "like_count": 170,
    "comment_count": 43,
    "released_at": "2022-09-30",
    "gist": "Delivery of notifications is critical for a FinTech company like Razorpay because it is a way to notify customers about their transactions and connect with external systems through Webhooks.\n\nSo, how did they design their notification system? let's find out\n\n## Key requirement\n\n1. maintaining an SLA is very important\n2. guaranteed delivery via SMS, Email, Push, and Webhooks\n\n## Existing Setup\n\nUpon every transaction, an event/message was sent to SQS (message broker) which was consumed by a worker that then fanned out the notification through different channels.\n\nBecause we want to guarantee delivery, a state was maintained in the database that tells if the notification was successfully sent or not (esp via Webhook).\n\nHence, there is a component called Scheduler that pulls the unsent notifications from this database and re-queues it in SQS; thus guaranteeing the delivery.\n\n### Challenges at scale\n\n1. huge load on this database\n2. scaling workers were limited by the IPOS on the database\n3. surges, during festive seasons, affected transactional notifications\n\n## New architecture\n\n1. Prioritising incoming load\n\nIn order to ensure that one type of notification does not affect others, every notification type is classified with some priority and depending on which they are pushed to the corresponding SQS queue.\n\nThis ensures that a huge marketing campaign does not affect transactional messages.\n\n2. Rate Limiting\n\nTo ensure mass notification from one customer does not affect others, we add Rate Limiter that would limit the notifications per customer and per type ensuring that critical notifications always meet the SLA.\n\n3. Reducing DB bottleneck\n\nWe could not scale workers because of high DB load, and hence instead of doing a sync write to the database, the notifications that are unsent and need to be retried are pushed in a sync way to the database.\n\nBecause of this async write, we ensure that we write to the database in a staggered way and not put unnecessary load on it.\n\n### Observability\n\nTo ensure we are maintaining our SLA, we have to exhaustively monitor the entire infra for any anomaly; the metrics like - health of the infra, success rate of delivery, and SLA.",
    "notes_gd": "https://drive.google.com/file/d/1423Wn1CrO0goeiYuQo8DpUhrWbfR6KDs/view?usp=sharing",
    "slug": "how-razorpay-scaled-their-notification-system"
  },
  {
    "id": 199,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "NcNCty7_3kc",
    "title": "How Flipkart made their type ahead search hyper personalized",
    "description": "Search is one of the most used features in any e-commerce and to give a better user experience it is almost customary to provide type-ahead search suggestions.\n\nIn this video, we will talk about how Flipkart made their type-ahead search hyper-personalized and dive deep into their high-level architecture and key design decisions that make it extra special.\n\nOutline:\n\n00:00 Agenda\n02:28 Introduction and need for type-ahead search\n04:23 Parameters of Ranking\n07:52 Key design decisions\n14:37 High-level Architecture",
    "img": "https://i.ytimg.com/vi/NcNCty7_3kc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/qQRfz2VfUbDeebczif/giphy.gif",
    "duration": "19:",
    "view_count": 1607,
    "like_count": 131,
    "comment_count": 5,
    "released_at": "2022-09-28",
    "gist": "To help us search quicker and better, Flipkart suggests queries as we type. These suggestions are not only popular suggestions, instead, but they are also hyper-personalized. Let's take a look at how they designed this system.\n\nFor a given prefix, say \"sh\", we should rank the query suggestions - \"shoes\", \"shirts\", and \"shorts\" - in the context of the user.\n\n## Parameters of ranking\n\n1. Quality of the suggestion\n\n- popularity: how popular the search term is?\n- performance: does this term has enough results?\n- grammar quality: is the term grammatically correct?\n\n2. User-actions\n\n- past few search terms of the user\n- past purchase history of the user\n\n## Personalizing the suggestions\n\nA naive way of doing this would be to create cohorts of the users and show all of them the same suggestions for the given prefix. But we wanted to show the suggestions that are relevant as per the recently fired queries.\n\nFor example: if a user searched - shoes, red shoes, Nike shoes and then typed \"a\" - we should be showing \"Adidas shoes\" and not \"apple iPhone\".\n\n### Understanding the intent\n\nFlipkart has a taxonomy of the product categories they sell and lists on the platform. The taxonomy holds categories like Fashion and Electronics, and within Fashion, we have Clothing and Footwear, etc.\n\nWe first associate the given search query with this taxonomy. If two terms are mapped to close nodes in the taxonomy, they would be contextually relevant and similar.\n\n### Evaluate Category Similarity\n\nWe need to determine the probability that the current search term/prefix would belong to the same category as the past search terms.\n\nFor example: \"computer monitor\", \"mou\" -> \"computer mouse\"\n\n### Evaluate Reformulation\n\nWe need to determine the probability that the current search term/prefix is being written to reformulate the existing context.\n\nFor example: \"shoes\", \"red shoes\", \"n...\" -> \"Nike Shoes\"\n\n## Traning the model\n\nA machine learning model needs to be trained on all viewed items on suggestions, all clicked suggestions, and all unclicked suggestions.\n\nOur model should try to maximize the likelihood of the person clicking the suggestion.\n\nThe feature relationships were modeled and ingested in Xgboost ( decision trees ) and the importance of each feature needs to be quantified and evaluated.\n\n## Architecture\n\nThere will be an \"autosuggestion\" service whose job is to serve the suggestions to the user, given the search term.\n\nThis service will have a small cache that would hold the data for the most popular search term prefix to serve non-personalized suggestions.\n\nThe autosuggestion service will talk to Solr to serve the ML-ranked query suggestions for the given term. The relevance model to be configured in Solr will be \"Learning to Rank\"\n\nA huge amount of data, through the data pipelines, will be ingested into Solr and the ML model will be explicitly trained on xgboost and ingested through a different component.",
    "notes_gd": "https://drive.google.com/file/d/1i6w0I8I1H0TxFRoT0Cs0IkcpL6MO6Rbb/view?usp=sharing",
    "slug": "how-flipkart-made-their-type-ahead-search-hyper-personalized"
  },
  {
    "id": 198,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "tZPTpa3JcKA",
    "title": "The Architecture of Pinterest's Time Series Database - Goku",
    "description": "It is extremely critical to continuously monitor the health of the services and infrastructure. We use Time Series Databases to hold the key vitals like CPU, RAM, Disk, Requests, Network IO, etc. Pinterest generates millions of data points every second and the existing Time Series Databases were not performant enough to meet their needs, hence they built one in-house.\n\nIn this video, we take a detailed look into the architecture and key design decisions that Pinterest took while designing their own in-house time series database named Goku.\n\nOutline:\n\n00:00 Agenda\n02:44 Need for Time Series Database\n03:49 Time Series Data Model\n06:53 Challenges and Key Decisions\n10:45 Architecture of Goku",
    "img": "https://i.ytimg.com/vi/tZPTpa3JcKA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/pO4UHglOY2vII/giphy.gif",
    "duration": "17:53",
    "view_count": 1632,
    "like_count": 84,
    "comment_count": 8,
    "released_at": "2022-09-26",
    "gist": "Pinterest built their own time-series database named Goku because existing databases did not fit their requirements, here is the architecture of it.\n\n## Existing OpenTSDB setup\n\nPinterest used OpenTSDB to hold their time-series data but it didn't work very well at scale. The two key aspects that hurt them were\n\n- Long GC Pauses\n- Frequent process crash\n\n## Challenges\n\n1. OpenTSDB disk-based scans are inefficient\n2. The data stored in OpenTSDB does not have a good compression\n3. OpenTSDB is JSON based and hence very inefficient\n4. OpenTSDB is distributed and for a query that spans multiple shards, it first collects the data in one node and then evaluates the query.\n\n## Key Decisions\n\n1. To make the scan efficient, Goku stores data in-memory\n2. Goku uses Gorilla engine which gives 12x compression\n3. Goku computes a partial response at each shard and then sends it to the proxy; thus doing a minimal data transfer.\n4. Goku uses Thrift Binary protocol, much more efficient than JSON\n\n## Architecture\n\nGoku stores 24 hours' worth of data in-memory with a configured periodic flush to the disk. The most recent query is fired to this in-memory store for quick evaluation.\n\nGoku is a shared time-series database and each shard may contain data from multiple time series. Each Goku instance holds Bucket Map within which the time-series data resides. Each bucket holds data for a 2-hour window.\n\nThe writes on each time-series data go to the bucket map, within which it writes to a mutable buffer. Once the window is done, the buffer becomes immutable.\n\nDuring a query, the request comes to a Goku Proxy which, if required, fans it out to all the involved shards. Each shard does the computation on its share of data and sends the response back to the coordinator/proxy node.\n\nThe coordinator/proxy node aggregates the response and sends it back to the client, thus completing the operation.",
    "notes_gd": "https://drive.google.com/file/d/1AqR4FuiCZbjuHl5v5H4cVFi8CPJwcFWX/view?usp=sharing",
    "slug": "the-architecture-of-pinterest-s-time-series-database-goku"
  },
  {
    "id": 197,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "7xKgQmqkfD0",
    "title": "The Architecture of Airbnb's Knowledge Graph",
    "description": "When a product is catering to a large audience, providing contextual information becomes important and at the scale of Airbnb, it becomes non-optional. Hence the engineering team at Airbnb collated all the data and structured it into a Knowledge Graph that today powers its Search, Discovery, and Trip Planner services.\n\nIn this video, we take a detailed look into how Airbnb designed its Knowledge Graph, some key components of it, and some key decisions it took while architecting it.\n\nOutline:\n\n00:00 Agenda\n02:41 The need for a Knowledge Graph\n05:49 Architecture of the Knowledge Graph",
    "img": "https://i.ytimg.com/vi/7xKgQmqkfD0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/orUC71iuzC3rtVWyPn/giphy.gif",
    "duration": "16:46",
    "view_count": 1617,
    "like_count": 77,
    "comment_count": 25,
    "released_at": "2022-09-23",
    "gist": "Google and Facebook are known to have humungous knowledge graphs; and so does Airbnb. So, what are they? and how are they designed?\n\nKnowledge Graphs are just super-structured information collated from all different data sources. At Airbnb, they power Search, Discovery, and Trip Planner products.\n\nKnowledge Graphs power sophisticated queries like\n\n- find cities that host football matches in July and August and are known for para-gliding\n- find a neighborhood in LA where Huts or Private Islands are available in the upcoming summer.\n\n## Key Components\n\nThe knowledge graph system has three critical components: Storage, API, and Storage Mutator.\n\n### Graph Storage\n\nAirbnb collates all the structured information and stores it in a relational database having one table to store all the nodes and another table to store all the edges.\n\nEach node in the graph could have a different schema. For example, the location could have a name and GPS coordinates, while the Event would have a title, data, and venue.\n\nEach edge in the graph will hold the types of nodes it can connect to which ensures strong data integrity. For example, an edge \"landmark-in-a-city\" can connect a landmark and a city.\n\nAirbnb chose to not use GraphDB because of its operational overhead. The team had much higher confidence in relational databases and their capability in managing them.\n\n### Graph API\n\nQuery API layer exposes a JSON-based query structure that can be used by clients to interact with the Knowledge Graph.\n\nThe JSON query is converted to SQL and fired on the database to get the desired information.\n\n### Storage Mutator\n\nWe may think the best way for the Knowledge Graph to remain updated with any changes happening in other systems is to expose an updated API. But that would be too slow and expensive.\n\nHence, a better way to design this is to ingest bulk updates through Kafka. Updates coming from other systems are put in the Kafka and then the mutator updates the knowledge graph by consuming the events.\n\n## Offline Processing\n\nNot every service wants to synchronously query the graph, for example, the search might want to run an offline ranking job. Querying the graph every time will be inefficient and hence there is a periodic job that exports the entire graph database.\n\nThis export is then consumed by the services for offline processing.",
    "notes_gd": "https://drive.google.com/file/d/17Az1C3sESXA0jGPqmcJWjk2GOwSi12Kf/view?usp=sharing",
    "slug": "the-architecture-of-airbnb-s-knowledge-graph"
  },
  {
    "id": 196,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "W8LDyEOPaPY",
    "title": "How Swiggy designed and scaled its chatbot",
    "description": "We all love food but are lazy to go to a restaurant and pick it ourselves and this gave rise to Food Delivery Startups like Swiggy and Zomato. Chatbots are essential for them as they resolve most of the complaints that customers have without spending a lot of money on customer service representatives.\n\nIn this video, we take a look at how Swiggy designed their chatbots to achieve business efficiency at scale; and dive deep into their tech architecture and key components that we need to consider while designing it.\n\nOutline:\n\n00:00 Agenda\n02:36 What are Chatbots\n03:30 Designing Chatbots\n07:00 Architecture of a chatbot",
    "img": "https://i.ytimg.com/vi/W8LDyEOPaPY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3orif627F2MRFcnpeg/giphy.gif",
    "duration": "15:11",
    "view_count": 1922,
    "like_count": 114,
    "comment_count": 9,
    "released_at": "2022-09-21",
    "gist": "Swiggy has automated its customer support with Chatbots; so what's their architecture? Let's find out today.\n\nChatbots are essentially a Decision Tree where nodes are the states the conversation could be in and the edges are conditional statements. Depending on which statement (edge) meets the criteria, the conversation moves forward in that direction.\n\nAt each step, Swiggy shows the customer the options which are the child nodes of the current node until all the information is gathered; and the flow stops upon reaching the terminal state.\n\n## Key components and architecture\n\nThe decision tree is stored in a relational database. It can be generated using historical customer support data and can be updated by product and business teams.\n\nThe core chatbot service needs to access data from other services/databases like Payments, Orders, and Notifications. Accessing peripheral information will provide a rich experience to the users.\n\nThe most important part chatbot is to interact with the Fraud Detection system. Given that the entire flow is automated and needs no manual intervention, the chances of Fraud shoot up.\n\nFraud Detection System, in real-time, would see if the customer is trying to commit fraud. This system would interact with historical information about the customer, past refunds, image processing, and much more.\n\nThere may be a possibility that the customer still needs to talk to an executive hence the customer support executive needs to get all of this information, including fraud probability, in an easy-to-use dashboard.\n\nHence, a real-time pipeline would be needed to ingest and move data across all of these systems. Apache Spark is an excellent candidate for building this.\n\n## Business Continuity Plan\n\nIf the Chatbot is down, then Swiggy switches to a simple chat interface that connects directly to an executive, continuing to service the end users.\n\n## Key Metric\n\nA metric that Swiggy chases is Bot Efficacy which is nothing but the percentage of requests handled by the bot vs executive. Swiggy wants to handle as many requests as possible through the bots as it would help them remain efficient at scale.",
    "notes_gd": "https://drive.google.com/file/d/1_C7mUXohjvTR1OAfYL0UWov7-w9B_8zJ/view?usp=sharing",
    "slug": "how-swiggy-designed-and-scaled-its-chatbot"
  },
  {
    "id": 195,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "CA2_0ZhVW2g",
    "title": "How Instagram efficiently serves HashTags ordered by count",
    "description": "Instagram has millions of HashTags and millions of people tag their photos every single day. To discover, Instagram allows us to search for a Hash Tag.\n\nIn this video, we will take a detailed look into how Instagram efficiently searches for HashTags and serves them ordered by count; while doing so we will dive deep into a super-interesting Database optimization called Partial Indexes.\n\nOutline:\n\n00:00 Agenda\n02:27 HashTag Ordering Problem and Instagram Database\n05:41 Naive Query\n07:17 Optimized Solution with Partial Indexes",
    "img": "https://i.ytimg.com/vi/CA2_0ZhVW2g/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT5LMvdq41931HThDO/giphy.gif",
    "duration": "12:18",
    "view_count": 2083,
    "like_count": 174,
    "comment_count": 26,
    "released_at": "2022-09-19",
    "gist": "Instagram has millions of photos, and each has tens of hashtags, so how does Instagram efficiently serves hashtags for a search query?\n\nInstagram uses Postgres as its primary transactions database. For each hashtag, it stores the `media_count` in a table. This allows us to render the Hashtag and some meta information on the search page.\n\nNote: Instagram could also use a dedicated search engine for this usecase, but this usecase is trivial to use something as sophisticated as ElasticSearch. We take a look at how they do it with Postgres.\n\nA naive query to get hashtags for a given prefix ordered by count would look something like this\n\n```\nSELECT * from hashtags\nWHERE name LIKE 'snow%'\nORDER BY media_count\nDESC LIMIT 10;\n```\n\nExecuting this query would require the database engine to filter out the hashtags starting with `snow` and then sort. Sorting is an expensive operation and this query required the engine to sort 15000 rows.\n\nUnder high load, this sorting becomes a pain. So, what's the way out?\n\nThe key insight here is the fact that hashtags have a long-tail distribution i.e. most hashtags will have far fewer media counts and while serving we are always ordering by `media_count`.\n\nHence, instead of indexing everything, we can simply index the hashtags having `media_count > 100` because other hashtags are highly unlikely to be surfaced.\n\n## Partial Indexes\n\nPostgres database has this exact same capability and it is called Partial Indexing. With partial indexes, we can keep only a subset of data in the index.\n\nWe can create a partial index on our hashtags table like\n\n```\nCREATE INDEX CONCURRENTLY ON hashtags WHERE media_count > 100;\n```\n\nWith this index, the query we fire would have to scan a very limited number of index entries to spit out the result.\n\nThe SQL query to get hashtags with the prefix `snow%` would look something like this\n\n```\nSELECT * from hashtags\nWHERE name LIKE 'snow%'\nAND media_count > 100\nORDER BY media_count\nDESC LIMIT 10;\n```\n\nThe above query required it to sort only 169 rows, completing its evaluation superfast. This is a classic usecase where we all can utilize Partial Indexes.",
    "notes_gd": "https://drive.google.com/file/d/1C0uGvqYFcBCvqDfkDyJr1RJnTDOq2c6T/view?usp=sharing",
    "slug": "how-instagram-efficiently-serves-hashtags-ordered-by-count"
  },
  {
    "id": 146,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "9kjUG_yvVqM",
    "title": "An in-depth introduction to Rolling Deployments",
    "description": "One of the simplest deployment strategies that make deployment a breeze is Rolling Deployment. It is the most widely adopted deployment strategy purely because of its simplicity and cost-effectiveness. Most of the deployment tool has this as their default deployment strategy.\n\nIn this video, we take an in-depth look into what Rolling deployment is, how they are implemented, how to tune it, some key challenges we face during adoption, and conclude with an understanding of the pros and the cons of adopting it.\n\nOutline:\n\n00:00 Agenda\n02:37 Introduction to Rolling Deployments\n05:43 How to implement Rolling Deployment\n11:32 Tuning Rolling Deployments\n15:59 Pros of Rolling Deployment\n18:37 Cons of Rolling Deployment",
    "img": "https://i.ytimg.com/vi/9kjUG_yvVqM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Kxn4rf7eceDpGBGOZY/giphy.gif",
    "duration": "21:41",
    "view_count": 1024,
    "like_count": 49,
    "comment_count": 1,
    "released_at": "2022-05-27",
    "gist": "Rolling Deployment is a deployment strategy that slowly replaces the previous version of the application with the new one by replacing the underlying infrastructure.\n\nSay we have 9 servers and each one is running version 1 of the code. With rolling deployment, we roll out our changes i.e. version 2 of the code to one server at a time eventually covering all 9 servers. This is the core idea of the rolling deployment, however, the implementation could vary a bit.\n\nA key thing to note here is the fact that deployment is incremental in nature which means during the deployment there would be a few servers that are serving the old version of the code and the remaining servers serving the newer version. Hence the changes we push should be both backward and forward compatible.\n\n## Implementing Rolling Deployment\n\nRolling deployments are always gradual and graceful, and we typically happen through the below steps\n\n1. Pick a server for deployment\n2. Stop the incoming traffic by removing it from the load balancer\n3. Wait for the existing requests to be completed\n4. if we are not replacing the infra, pull the latest code and reload\n5. if we are replacing the infra, delete the server and launch a new one with the new code\n6. attach the server behind the load balancer and start serving the requests\n\n## Tuning Rolling Deployment\n\n### Concurrent Servers\n\nInstead of deploying to one server at a time, we can deploy changes to `n` servers concurrently. This would complete the entire deployment quicker.\n\nChoosing the appropriate `n` is critical as a small value would mean the deployment takes ages to complete and a large one would affect the availability during deployment.\n\n### Double-Half Deployment\n\nAn interesting way to implement rolling deployment is to double the infrastructure and then delete the older half.\n\nSay, we have 4 servers with version 1 of our code so in order to deploy the changes we add 4 new servers with the new version of the code to the infra taking our total count to 8, and then delete the 4 older servers. This way, what remains are the 4 servers with the newer version of the code.\n\n## Pros of Rolling Deployments\n\n- Cost efficient\n- Rollouts are gradual\n- Rollbacks are simple\n- Deployment incurs zero downtime\n- Much faster than Blue-Green Deployment\n- Any hiccup during deployment affects only a fraction of the users\n\n## Cons of Rolling Deployments\n\n- No environment isolation\n- Naive deployment takes a long time to complete\n- Stateful applications are affected during deployment\n- Changes we rollout should be backward and forward compatible",
    "notes_gd": "https://drive.google.com/file/d/1Mtox_ulRNSajmbVXXyLyJrhV3GZouOiv/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-rolling-deployments"
  },
  {
    "id": 145,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "9iAJjtvBwyI",
    "title": "Implementing Vertical Sharding",
    "description": "Sharding is super-important when you want to handle the traffic that cannot be handled through one server. Sharding comes in two flavors - Horizontal and Vertical. In horizontal sharding, we split the table by rows and keep them on separate servers. In vertical sharding, we distribute the tables across multiple database servers.\n\nFor example, keeping all the payments-related tables in one database server, and all the auth-related tables in another. Vertical sharding comes in super handy when we are moving from monolith to microservices. All this sounds simple yet awesome theoretically, but would we actually implement it?\n\nIn this video, we take an in-depth look, not at the theoretical side of vertical sharding, but at the implementation side of it. We will see how Vertical Sharding is implemented with minimal downtime and what are the exact steps to do it.\n\nOutline:\n\n00:00 Agenda\n03:17 Introduction to Vertical Sharding\n05:23 Implementing Vertical Sharding\n05:55 Picking a configuration store\n10:34 Moving a table from one server to another\n18:58 Summarizing the overall flow",
    "img": "https://i.ytimg.com/vi/9iAJjtvBwyI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif",
    "duration": "24:41",
    "view_count": 1317,
    "like_count": 84,
    "comment_count": 20,
    "released_at": "2022-05-25",
    "gist": "Vertical sharding is fine, but how can we actually implement it? \ud83e\udd14\n\n## Vertical Sharding\n\nVertical sharding is splitting a database by the tables. Shards will hold a subset of tables. For example, all payments-related tables go to one shard, while all auth-related tables go to another.\n\nSo, how to implement it?\n\n## Need for a configuration store\n\nFor our API servers to talk to the correct database we would need a configuration store that holds the information for all the tables mapped to the database server that holds it.\n\nFor example, the Users table is present on DB1 while Transactions on DB2\n\nWhenever the request comes, the API servers first check the config to find which DB holds the table and then fire the SQL query to that specific database for the table.\n\n### Reactive update\n\nAll API servers will cache the configuration to avoid an expensive network call to get the database ensuring we get a solid boost to the performance.\n\nWhen a table is moved from one database server to another, the configuration will be updated and hence the changes would need to be reactively propagated to all the API servers. Hence our config store needs to support reactive communication.\n\nThis is where we choose Zookeeper which is resilient and battle-tested to achieve this.\n\n## Moving tables\n\nSay, we are moving table `T2` from database server DB1 to DB2. Moving the table from one server to another is done in 4 simple steps.\n\n### Dump the table `T2`\n\nWe first dump the table `T2` from DB1 transactionally using the utility `mysqldump` that not only dumps the table data but also records the position in the `binlog`. This is like taking a point-in-time snapshot of the table.\n\n### Restore the dump\n\nWe now restore the dump to database DB2. This way we will have a database server with the table `T2` containing data till a certain point in time.\n\n### Sync table `T2` on DB1 and DB2\n\nWe now setup the replication from DB1 to DB2 specifically for sync changes happening on table `T2`. It is done through a custom job that will use the recorded `binlog` position and start syncing from it.\n\n### Cutover\n\nOnce the table `T2` is synced with almost 0 replication lag on DB1 and DB2 we cutover. We first rename the table to `T2_bak` and update the config in Zookeeper.\n\nAs we rename the table any queries going to DB1 for table `T2` will start throwing \"Table not found\" errors, but as Zookeeper will propagate the changes to all API servers they would use DB2 to fire any query on table `T2`, thus completing the table movement.\n\nThis is how you can implement vertical sharding.",
    "notes_gd": "https://drive.google.com/file/d/1AxijzqfIksP_QOKc9eKhuXba7apUugCT/view?usp=sharing",
    "slug": "implementing-vertical-sharding"
  },
  {
    "id": 142,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "W6HANd8c9t4",
    "title": "An in-depth introduction to Blue Green Deployments",
    "description": "Deployments are a pain if we are unsure about our release changes. But sometimes even if we know our changes well, something weird could happen in the infra that would fail your deployment and put your infra in an inconsistent state.\n\nSo, is there a way to address this? What if we have a place to deploy our changes and validate them before they hit production; and have a quick way to roll back if something goes wrong?\n\nThis is the core idea behind a Blue-Green Deployment\n\nIn this video, we take an in-depth look into a blue-green deployment pattern, understand why we need them, what problem it addresses, learn how they are implemented, talk about its benefits and challenges, and conclude with some points to remember when you adopt this deployment pattern.\n\nOutline:\n\n00:00 Agenda\n03:13 An introduction to Blue-Green Deployments\n06:36 Why do we need Blue-Green Deployments\n09:12 How Blue-Green Deployment is implemented?\n13:26 Pros of having a Blue-Green Deployment\n19:49 Challenges in having a Blue-Green Deployment\n25:36 When to use Blue Green Deployments\n26:39 Key points to remember while adopting Blue Green Deployment",
    "img": "https://i.ytimg.com/vi/W6HANd8c9t4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oGRFnT4RyU4ayoUFi/giphy.gif",
    "duration": "28:55",
    "view_count": 1387,
    "like_count": 63,
    "comment_count": 12,
    "released_at": "2022-05-18",
    "gist": "Blue Green Deployment is a deployment pattern that reduces the downtime during deployment by running two identical production setups called - Blue and Green.\n\nDuring deployment when we reboot the API servers there are chances that the incoming request fail because the server is unresponsive for a short period. Also, it might happen that the release had a major bug and we need a quick rollback.\n\nHow can we achieve both of them in one shot? The answer is Blue Green Deployment.\n\n## Implemention\n\nBlue Green deployment is implemented by having a separate fleet of infrastructure for the old version - Blue and the new version - Green. The new infrastructure is identical to the old one.\n\nThe deployment flow\n\n1. the new deployment artifact is tested and kept ready to be deployed\n2. a parallel infrastructure is set up identical to the existing\n3. the new version is deployed on the new fleet - Green\n4. the correctness of the setup is validated\n5. the proxy is re-configured to now forward 100% of traffic from the Blue (old) setup to the Green (new) setup\n6. a final sanity test is run on the new fleet\n7. the blue fleet is now shut down\n\n## Pros of Blue Green Deployment\n\n1. rollbacks are just a config change and hence quick\n2. downtime during deployment is minimal\n3. deployment is just a flip of a switch\n4. disaster recovery is simple given we already have the automation to build a parallel setup\n5. deployments can now happen during the working hours\n6. debugging a failed deployment is simple as we have the infrastructure with the debug information handy\n\n## Possible challenges\n\n1. during the deployment the infrastructure cost shoots 2x\n2. the stateful application would need to rebuild the state on new servers\n3. the database would have to be shared between the fleets\n4. any schema migration on the database needs to be backward and forward compatible\n5. the API responses have to be forward and backward compatible\n6. setting up this deployment strategy for the first time is difficult\n\n## When to use Blue Green Deployment?\n\n- when you need zero downtime deployment\n- your infrastructure can tolerate 100% traffic switch\n- you can bear the 2x cost of infrastructure during deployment\n\n## Points to remember\n\n- have a solid automation test suite to validate the correctness\n- ensure forward and backward compatibility of API and schema changes\n- infra cost will shoot up hence minimizing the time for which you are running 2x infra",
    "notes_gd": "https://drive.google.com/file/d/1jSowz0IW8kD4Fjrv2fsE-ygHVTaZto1d/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-blue-green-deployments"
  },
  {
    "id": 141,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "nnseeKxovaM",
    "title": "An in-depth introduction to Canary Deployments",
    "description": "Deployments are stressful; what if something goes wrong? What if you forgot to handle an edge case that was also missed during the unit test, integration test, or an internal QA iteration.\n\nPutting such code into production can take down your entire infrastructure and could cause a massive outage. In order or handle such a situation gracefully and provide us with an early warning about something's wrong we have Canary Deployment.\n\nIn this video, we take an in-depth look into canary deployments, learn why canary deployments are called canary deployments, and understand how they are actually implemented, talk about the pros and cons of this deployment pattern, and conclude with a one really solid use case where you absolutely need them.\n\nOutline:\n\n00:00 Agenda\n03:05 Introduction to Canary Deployment\n06:06 Why Canary Deployments are called Canary Deployments?\n08:04 How to implement Canary Deployments?\n10:03 Pros of having Canary Deployments\n16:21 How to pick servers and users for a rollout?\n19:08 Cons of having Canary Deployments\n21:25 When we absolutely need Canary Deployment",
    "img": "https://i.ytimg.com/vi/nnseeKxovaM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NAVQDibk6Sesg/giphy.gif",
    "duration": "23:53",
    "view_count": 2081,
    "like_count": 124,
    "comment_count": 15,
    "released_at": "2022-05-16",
    "gist": "Canary Deployments is a deployment pattern that rolls out the changes to a limited set of users before doing it for 100%.\n\nWe compare the vitals side-by-side from the old setup and the canary servers to ensure everything is as expected. If all is okay, then we incrementally roll out to a wider audience. If not, we immediately roll back our changes from the canaries.\n\nCanary Deployment thus acts as an Early Warning Indicator to prevent a potential outage.\n\n## Why canary deployment is named canary deployment?\n\nIn 1920, coal miners used to carry caged canaries with them. If the gases in the mines were highly toxic the canaries would die and that alerted the miners to evacuate immediately, thus saving their lives.\n\nIn canary deployment, the canary servers are the caged canaries that alert us when anything goes wrong.\n\n## Implementing canary deployment\n\nCanary deployments are implemented through a setup where a few servers serve the newer version while the reset serves the old version.\n\nA router (load balancer / API gateway) is placed in front of the setup and it routes some traffic to the new fleet while the other requests continue to go to the old one.\n\n## Pros of Canary Deployment\n\n- we test our changes on real traffic\n- rollbacks are much faster\n- if something's wrong only a fraction of users are affected\n- zero downtime deployments\n- we can gradually roll out the changes to users\n- we can power A/B Testing\n\n## Cons of Canary Deployment\n\n- engineers will get habituated to testing things in production\n- a little complex setup\n- a parallel monitoring setup is required to compare vitals side-by-side\n\n## Selecting users/servers for canary deployment?\n\nThe selection is use-case specific, but the common strategies are:\n\n- geographical selection to power regional roll-out\n- create specific user cohorts eg: beta users\n- random selection\n\n## When we absolutely need Canary Deployments\n\nSay you own the Auth service that is written in Java and you chose to re-write it in - Golang. When taking it to production, you would NOT want to make a direct 100% roll-out given that the new codebase might have a lot of bugs.\n\nThis is where canary is super-helpful when we a fraction of servers serving requests from Golang server while others from the existing setup. We now forward 5% traffic to the new ones and observe how it reacts.\n\nOnce we have enough confidence in the newer setup, we increase the roll-out fraction to 15%, 50%, 75%, and eventually 100%. Canary setup thus gives us a seamless transition from our old server to a newer one.",
    "notes_gd": "https://drive.google.com/file/d/1JJD_Pa9AkUvhaZ7Dzwd4sQiGdC8nPn5t/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-canary-deployments"
  },
  {
    "id": 135,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "oiZH5U_a0pg",
    "title": "Introduction to Serverless Computing and Architecture",
    "description": "Serverless Computing is one of the hottest topics of discussion today, but the term \"serverless\" is slightly misleading and it does not mean that your code will not need a server to run. We have to be extremely cautious while deciding on adopting serverless for our use case, but it is not something that fits all the use cases.\n\nIn this video, we talk about what serverless computing is, see why it was built in the first place, learn about 5 real-world use-cases that become super-efficient with serverless architecture, understand the advantages and more importantly, the disadvantages of adopting it, and conclude with acknowledging when to use and when not to use this computation pattern.\n\nOutline:\n\n00:00 Agenda\n03:01 Need and the idea of the Serverless Computing\n11:16 Usecase 1: Chatbots\n13:57 Usecase 2: Online Judge\n16:27 Usecase 3: Vending Machines\n18:00 Usecase 4: CRON Jobs\n19:54 Usecase 5: Batch and Stream Processing\n22:16 Advantages of Serverless Computing and Architecture\n26:21 Disadvantages of Serverless Computing and Architecture\n31:37 When NOT to use Serverless\n34:00 When to use Serverless",
    "img": "https://i.ytimg.com/vi/oiZH5U_a0pg/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/5k1VkABjw5wQq0PNEU/giphy.gif",
    "duration": "36:23",
    "view_count": 2254,
    "like_count": 135,
    "comment_count": 17,
    "released_at": "2022-05-02",
    "gist": "Serverless is a cost-efficient way to host your APIs and it forms the crux of systems like Chatbots and Online Judge.\n\nServerless does not mean that your code will not run on the server; it means that you do not manage, maintain, access, or scale the server your code is running on.\n\nThe traditional way to host APIs is by spinning up a server with some RAM, and CPU. Say the resources make your server handle 1000 RPS, but you are getting 1000 RPS only 1% of the time which means for the other 99% you are overprovisioned.\n\nSo, what if there was an Infrastructure that\n\n- scales up and down as per the traffic\n- is billed per execution\n- is self-managed maintained and fault-tolerant\n\nThese requirements gave rise to Serverless Computing.\n\n## Real-world applications\n\n### Chatbot\n\nSay, we build a Slack chatbot that responds with the Holiday list when someone messages `holidays` . The traffic for this utility is going to be insignificant, and keeping a server running the whole time is a waste. This is best modeled on Serverless which is invoked on receiving a message.\n\n### Online Judge\n\nEvery submission can be evaluated on a serverless function and results can be updated in a database. Serverless gives you isolation out of the box and keeps the cost to a bare minimum. It would also seamlessly handle the surge in submissions.\n\n### Vending Machine\n\nUpon purchase, the Vending machine would need to update the main database, and the APIs for that could be hosted on Serverless. Given the traffic is low and bursty, Serverless would help us keep the cost down.\n\n### Scheduled DB Backups\n\nSchedule daily DB backups on the Serverless function instead of running a separate crontab server just to trigger the backup.\n\n### Batch and Stream Processing\n\nUse serverless and invoke the function every time a message is pushed on the broker making the system reactive instead of poll-based.\n\n## Advantages\n\n- No need to manage and scale the infra\n- The cost is 0 when you do not get any traffic\n- Scale is out of the box; so no capacity planning is needed\n\n## Disadvantages\n\n- Takes time to serve the first request as the underlying infra might boot up\n- The execution has a max timeout, so your job should complete within the limit\n- Debugging is a challenge\n- You are locked in on the vendor you chose\n\n## When NOT to use Serverless\n\n- Load, usage, and traffic pattern is consistent\n- Execution will go beyond the max timeout\n- You need multi-tenancy\n\n## When to use Serverless\n\n- Quick build, prototype, and deploy the changes\n- Usecase is lightweight\n- Traffic is bursty",
    "notes_gd": "https://drive.google.com/file/d/1sZShE0r41XcFa2gEPW1RS_YTaR3tC-zH/view?usp=sharing",
    "slug": "introduction-to-serverless-computing-and-architecture"
  },
  {
    "id": 131,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "wXvljefXyEo",
    "title": "Database Sharding and Partitioning",
    "description": "Sharding and partitioning come in very handy when we want to scale our systems. These concepts operate on the database and help us improve the overall throughput and availability of the system.\n\nIn this video, we take a detailed look into how a database is scaled and evolved through different stages, what sharding and partitioning are, understand the difference between them, see at which stage should we introduce this complexity, and a few advantages and disadvantages of adopting them.\n\nOutline:\n\n00:00 Introduction and Agenda\n03:05 How a database is progressively scaled?\n08:10 Scaling beyond the limit of vertical scaling\n11:57 Sharding vs Partitioning\n12:43 Example of Data Partitioning\n17:15 Sharding and Partitioning together\n20:20 Advantages and Disadvantages of Sharding and Partitioning",
    "img": "https://i.ytimg.com/vi/wXvljefXyEo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif",
    "duration": "23:53",
    "view_count": 4113,
    "like_count": 271,
    "comment_count": 23,
    "released_at": "2022-04-25",
    "gist": "Sharding and partitioning come in very handy when we want to scale our systems. Let's talk about these concepts in detail.\n\n## How is the database scaled?\n\nA database server is just a database process (like MySQL, MongoDB) running on a virtual server like EC2. Now when we put our database in production it starts getting from real good traction, say 100 writes per second (WPS).\n\n### Steady user growth\n\nSay, your product started getting some traction, and we find that the database is not able to handle the load, we scale it up by adding more CPU, RAM, and Disk to the server. This way we are now handling 200 WPS.\n\n### More read traffic  \n\nIf we see nor reads then can also choose to add a Read Replica and divert some of the read traffic to this node, while the master node can take in 200 WPS.\n\n### Viral Growth\n\nSay, your product went viral and you now got 5x more load which means now you have to handle 1000 WPS. To achieve this you again scale it up vertically and handle the desired load.\n\n### Insane growth\n\nSay, you now cracked the PMF and are getting some really solid traction and need to handle 1500 WPS, and when you visit the database console you found out that it is not possible to vertically scale your database any further, so how do you handle 1500 WPS?\n\nThis is where the horizontal scaling comes into the picture.\n\n## Scaling the database horizontally\n\nWe know one database server can handle 1000 WPS, but we need to handle 1500 WPS, so we split the data into half and split it across two databases such that each database owns half of the data and all the writes for that data goes to that particular instance.\n\nThis way each server will get 750 WPS, which it can very easily handle, and owns 50% of the data. Thus by adding more database servers we handled 1500 WPS (more than what a single machine could handle)\n\n## Sharding and Partitioning\n\nEach database server in the above architecture is called a Shard while the data is said to be partitioned. Overall, a database is sharded and the data is partitioned.\n\n### Partitioned data on shards\n\nIt is possible to have more partitions and fewer shards and in that case, each shard will own multiple partitions. Say, we have 100GB of data and it is split into 5 partitions and we have 2 shards. One shard will be responsible for 3 partitions while the other for 2.\n\n## Advantages and Disadvantages\n\n### Advantages of Sharding\n\n- handle more reads and writes\n- increases overall storage capacity\n- overall high availability\n\n### Disadvantages of Sharding\n\n- sharding is operationally complex\n- cross-shard queries are super-expensive",
    "notes_gd": "https://drive.google.com/file/d/14RqKYjN2pgqYTaVB1DYlH4WjZ0A8XQ02/view?usp=sharing",
    "slug": "database-sharding-and-partitioning"
  },
  {
    "id": 106,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "CW4gVlU0xtU",
    "title": "Why, where, and when should we throttle or rate limit?",
    "description": "It is a common belief that a rate limiter is always external and is designed to prevent our systems from being abused by the external world, but this is not true. In this video, we understand what throttling is, why we need it in the first place and 5 use cases where external and internal rate limiters are super useful.\n\nOutline:\n00:00 Introduction\n02:56 What is Throttling?\n03:37 What rate limiter does when it gets a surge of requests?\n06:39 Why do we need a rate limiter?\n10:45 Usecase 1: Preventing catastrophic DDoS Attack\n12:20 Usecase 2: Gracefully handling a surge in legitimate users\n13:46 Usecase 3: Multi-tiered limits\n15:42 Usecase 4: Not overusing an expensive vendor\n16:48 Usecase 5: Streamlining deletes to protect an unprotected database",
    "img": "https://i.ytimg.com/vi/CW4gVlU0xtU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT5LMuVtaVYI03uXsc/giphy.gif",
    "duration": "19:5",
    "view_count": 2092,
    "like_count": 127,
    "comment_count": 16,
    "released_at": "2022-04-18",
    "gist": "## What is throttling?\n\nThrottling is a technique that ensures that the flow of the data or the requests being sent at the target machine/service/sub-system can be consumed at an acceptable rate.\n\nIt is a defensive measure and 3 possible reactions could be\n\n- slowing down the incoming requests\n- rejecting the surplus requests\n- ignoring the surplus requests\n\n## Why do we need throttling in the first place?\n\n- to prevent system abuse\n- to allow the amount of traffic we could handle\n- control the consumption cost\n- prevent cascading failures leading to a massive outage\n\n## Real-world use-cases for throttling\n\n### To prevent catastrophic DDoS attack\n\nWhen your service is under a DDoS attack the rate limiter acts as your first line of defense that could prevent the surplus request from reaching your system. It would only allow the requests to go through at the configured rate.\n\n### To gracefully handle a surge of users\n\nIt is possible that your product goes viral and now you are seeing a genuine surge in users. Upon getting a genuine surge in users, the stateful components like databases and caches crash which takes down the entire site.\n\nRate limiter in this case will help in preventing the entire site from going down; although some users would see some error, like 429- Too many requests- your product will continue to seamlessly work for the other set of users.\n\n### Multi-tiered limits\n\nSay, you are running a CICD company and offer 3 tiers of pricing- Tier 1 offers 200 minutes of build time, Tier 2 offers 1000 mins while Tier 3 offers unlimited build time. An internal rate limiter can keep track of the build times consumed by a customer and reject the requests once the limit is hit.\n\n### Ensure you are not over-consuming\n\nSay, we are consuming a super expensive third-party API and we want to ensure that we are not using it beyond a certain number otherwise the cost will shoot up. An internal rate limiter can keep a check on this to ensure the surplus request does not go through.\n\n### Not overwhelming an unprotected system\n\nHard deleting from a database is an expensive operation. If we are deleting a huge number of rows from the DB it may severely affect the performance of the DB and hence it is best done in a staggered way. An internal rate limiter can help us streamline the writing by spreading them uniformly across time.",
    "notes_gd": "https://drive.google.com/file/d/11lTCiIbRk0aRlEaebjSI1mU6g_hTAway/view?usp=sharing",
    "slug": "why-where-and-when-should-we-throttle-or-rate-limit"
  },
  {
    "id": 105,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "1r9bPisYaOQ",
    "title": "How to approach System Design?",
    "description": "System Design is tricky but it does not have to be difficult - be it a technical discussion at your workplace or your next big interview. In this video, I share the two approaches that I have been using to design scalable systems in the last 10 years of my career. I will also share the 3 key pointers to remember while designing any system that would help you keep your discussion crisp and focused.\n\nOutline:\n00:00 Introduction\n02:41 What is System Design?\n06:02 The Spiral Approach to System Design\n07:27 The Incremental MVP Approach to System Design\n09:47 Key Pointers to remember during System Design",
    "img": "https://i.ytimg.com/vi/1r9bPisYaOQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/kQ3FSVoJrkYWk/giphy.gif",
    "duration": "13:55",
    "view_count": 3886,
    "like_count": 139,
    "comment_count": 10,
    "released_at": "2022-04-13",
    "gist": "System Design is tricky but it does not have to be difficult- be it a technical discussion at your workplace or your next big interview. Let's talk about how to approach System Design. Something I compiled from 10 years of my career.\n\n## What is System Design?\n\nSystem Design is all about translating and solving customer needs and business requirements into something tangible. The output system could be an application, a microservice, a library, or even hardware.\n\nThere are a couple of approaches that we can use to design any system out there. Picking one over the other depends on the company you work for and the flexibility it provides.\n\n## The Spiral Approach\n\nThe Spiral Approach pans like a spiral in which you start with some core that you are comfortable with (database, communication protocol, queue. etc) and build your system around it. Every single component you add to the design is something that you are pretty confident about and can proceed with the added complexities.\n\nFor example:\n\n- start with the database\n- then add LB and more servers\n- then add a queue for async processing\n- then other services\n- then add synchronous HTTP based communication between them\n\n## The Incremental MVP Approach\n\nIn the Incremental MVP-based approach we with a Day 0 design and then see how each component behaves at scale by dry-running it; after identifying the bottlenecks you fix them and re-iterate. You stop the iteration once you are happy with the final product. This kind of approach is typically seen in startups where they do not want to invest in architecture and quickly roll out features.\n\nFor example:\n\n- start with Day 0 architecture of users, API servers, and DB\n- then you add LB and more API servers\n- then you add Read Replica on DB to support more reads\n- then you split the service into a couple of microservices\n- then you partition the DB to handle more scale\n\n## 3 key pointers while designing systems\n\n- Every system is infinitely buildable, hence fence it well\n- Seek clarifications from your seniors\n- Ask critical questions that challenge the design decisions\n",
    "notes_gd": "https://drive.google.com/file/d/185a6688TxLDLXlrDfn2l4ODLSR2m1xLL/view?usp=sharing",
    "slug": "how-to-approach-system-design"
  },
  {
    "id": 102,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "m6DtqSb1BDM",
    "title": "Implementing Idempotence in a Payments Microservice",
    "description": "Idempotence is an extremely critical property that we must consider while implementing an API or designing a microservice. The situation becomes even more critical when the money is involved - ensuring no matter how many times the user or internal service retries, the amount is transferred just once between the two users in one transaction.\n\nThis video looks at idempotence, why there is even a need for it, and, more importantly, one common implementation approach commonly observed in payments services.\n\nOutline:\n\n00:00 What is Idempotence?\n02:32 Examples where Idempotence is relevant\n04:06 Why do we even need to retry?\n07:18 Implementation Approach 1: Do not retry\n09:45 Implementation Approach 1: Check and Update",
    "img": "https://i.ytimg.com/vi/m6DtqSb1BDM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/j3x5hjUoXIesM/giphy.gif",
    "duration": "16:36",
    "view_count": 3884,
    "like_count": 219,
    "comment_count": 19,
    "released_at": "2022-04-06",
    "gist": "Idempotence is executing the same action multiple times, but the result is as if the operation was applied just once.\n\nExample: Double tapping a post multiple times on Instagram does not increase the like count. It just increases the first time and by 1.\n\nIdempotence becomes extremely critical when money is involved. If A wants to transfer money to B, the transfer should happen just once. If due for any reason, the payment is implicitly retried, the funds will be deducted twice, which is unacceptable.\n\n### Why would a transaction repeat?\n\nBefore we talk about idempotence, it is important to understand why it would repeat in the first place.\n\nConsider a situation where the payments service initiated a payment with a Payment Gateway, the money got deducted, but the payments service did not get the response. This would make the Payments service retry the API call, which would lead to a double deduction.\n\n## Implementing idempotence\n\nCheck and Update: Weave everything with a single ID.\n\nThe idea is to retry only after checking if the payment is processed or not. But how do we do this? The implementation is pretty simple- a global payment ID that weaves all the services and parties together.\n\nThe flow is:\n\n1. Payments service calls the PG and generates a unique Payment ID\n2. Payments service passes this ID to the end-user and all involved services\n3. Payments service initiates the payment with Payment Gateway with this ID specifying the transfer between A and B\n4. If there are any failures, the Payment service retries and in that request specifies the Payment ID\n5. Using the payment ID, the payment gateway checks if the transfer was indeed done or not and would transfer only when it was not done\n\nAlthough we talked about the Payments service here, this approach of implementing idempotence is pretty common across all the use cases. The core idea is to have a single ID (acting as the Idempotence Key) weaving all the involved services and parties together.",
    "notes_gd": "https://drive.google.com/file/d/1Zyt8qN11IiAZJKrdan4wi1c5J6n_eAyU/view?usp=sharing",
    "slug": "implementing-idempotence-in-a-payments-microservice"
  },
  {
    "id": 103,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "8S4k7k_f9Sk",
    "title": "Sending payload in an HTTP GET request",
    "description": "Can we send data in an HTTP GET request? Most people think, No. The truth is, we can send the data in the request payload of an HTTP GET request so long as our webserver can understand it.\n\nIn this video, we go through the HTTP 1.1 specification and see what it says about the GET requests, write a simple Flask application to see that we can indeed process the payload of a GET request if we want to, and, more importantly, go through a real-world example where it was essential to send data in the request payload.\n\nOutline:\n\n00:00 HTTP GET Request\n01:53 What does HTTP 1.1 specification say?\n05:38 Request payload in Python Flask\n07:18 ElasticSearch using request payload for search\n10:40 When to use HTTP request payload in a GET request",
    "img": "https://i.ytimg.com/vi/8S4k7k_f9Sk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l2Je53k7wnV8sQZOg/giphy.gif",
    "duration": "12:",
    "view_count": 4474,
    "like_count": 157,
    "comment_count": 22,
    "released_at": "2022-04-04",
    "gist": "It is a common myth that we could not pass the request body in the HTTP GET request. HTTP 1.1 specification neither enforces nor suggests this behavior.\n\nThis means it is up to implementing the application web servers- Flask, uWSGI, etc.- to see if it parses the request body in the HTTP GET request. To do this, just check the request object you would be getting in your favorite framework.\n\n### What can we do with this information?\n\nSay you are modeling an analytics service like Google Analytics in which you are exposing an endpoint that returns you the data point depending on the requirements. The requirements specified here could be a large, complex JSON.\n\nPassing this query in the URL of the GET request as a query param is not convenient as it would require you to serialize and escape the JSON string before passing.\n\nThis is a perfect use case where the complex JSON query can be passed as a request body in the HTTP GET request, giving a good user experience.\n\n### So, does any popular tool uses this convention?\n\nYes. ElasticSearch- one of the most popular search utilities, uses this convention.\n\nThe search endpoint of ElasticSearch is a GET endpoint where the complex search queries in JSON format are sent in the request payload.\n",
    "notes_gd": "https://drive.google.com/file/d/1JwVEh9EG0ZGts-VePXNlIE1e8kivdHbM/view?usp=sharing",
    "slug": "sending-payload-in-an-http-get-request"
  },
  {
    "id": 101,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "dTPh3KcKPYo",
    "title": "Scaling Taxonomy Service and Database",
    "description": "In this video, we do the high-level design of Udemy's Taxonomy Service and see how to scale databases in general.\n\nOutline:\n\n0:00 Recap Taxonomy DB Design\n0:57 Bird's eye view\n2:30 Within the Taxonomy Service\n3:50 Scaling the relational database\n5:32 High-Level Architecture\n7:20 Join my System Design Course\n7:57 Like, Share, and Subscribe",
    "img": "https://i.ytimg.com/vi/dTPh3KcKPYo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/YxlUxrYGw2w9y/giphy.gif",
    "duration": "8:38",
    "view_count": 1051,
    "like_count": 30,
    "comment_count": 1,
    "released_at": "2021-05-02",
    "gist": "",
    "notes_gd": "",
    "slug": "scaling-taxonomy-service-and-database"
  },
  {
    "id": 100,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "4_jlmX_oB94",
    "title": "Designing Taxonomy on SQL",
    "description": "In this video, we design the Taxonomy of Udemy on top of a SQL-based Relational DB. We learn how to model any taxonomy on relational DB, why to choose SQL over NoSQL, designing the schema, deciding Indexes, and writing very interesting SQL queries.\n\nLink to the article: https://arpitbhayani.me/blogs/udemy-sql-taxonomy\n\nChapters:\n0:00 Udemy's Taxonomy\n1:40 A bad Database Design for Taxonomy\n3:29 Good Database Design for Taxonomy\n7:34 Is Relational DB a good choice to store taxonomy?\n13:04 Deciding Indexes\n15:26 Get topic by ID\n15:56 Get topic path\n23:00 Get child-topics\n25:05 Get Top Taxonomy in Single Query\n31:05 Summarizing Indexes\n31:52 Fun exercise to explore SQL\n33:55 What next in System Design",
    "img": "https://i.ytimg.com/vi/4_jlmX_oB94/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/6XuA2WMKsgqS4/giphy.gif",
    "duration": "35:15",
    "view_count": 2436,
    "like_count": 89,
    "comment_count": 8,
    "released_at": "2021-04-19",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-taxonomy-on-sql"
  },
  {
    "id": 104,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "m_7xSIhxZL8",
    "title": "End-to-end Message Encryption",
    "description": "In this video, we find how to implement a very very simple version of end-to-end WhatsApp-like message encryption. This is not even 1% of how WhatsApp does it but it gives a fair amount of idea on how we could do it.\n\nHow do WhatsApp, Facebook, Signal actually do it? Answer: The Signal Protocol\n\nWe do it with a very simple Public Key Cryptography and a Digital Signature.",
    "img": "https://i.ytimg.com/vi/m_7xSIhxZL8/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/pYfYNJbEftHOfIpoBC/giphy.gif",
    "duration": "15:57",
    "view_count": 6047,
    "like_count": 244,
    "comment_count": 17,
    "released_at": "2021-04-05",
    "gist": "",
    "notes_gd": "",
    "slug": "end-to-end-message-encryption"
  },
  {
    "id": 99,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "kIP8L-CSl2Y",
    "title": "Designing Notifications Service for Instagram",
    "description": "This video is a snippet from my System Design Masterclass and in it, we are discussing How Instagram Scales its notification systems. The primary challenge of such a system is doing a real fast fanout and we discuss how to do this very efficiently.",
    "img": "https://i.ytimg.com/vi/kIP8L-CSl2Y/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/huyZxIJvtqVeRp7QcS/giphy.gif",
    "duration": "37:18",
    "view_count": 25863,
    "like_count": 608,
    "comment_count": 56,
    "released_at": "2021-04-01",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-notifications-service-for-instagram"
  }
]