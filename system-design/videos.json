[
  {
    "id": 216,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "8sTuCPh3s0s",
    "title": "Thundering Herd Problem and How not to do API retries",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nWhen the network is unreliable the clients retry the APIs to ensure completion. This approach works when there are fewer clients; but what happens when there are millions of them?\n\nThey all will keep on bombarding the server with their requests and this problem is called the Thundering Herd problem and in this video, we understand what this problem is, why it occurs, and how to solve it.",
    "img": "https://i.ytimg.com/vi/8sTuCPh3s0s/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l49K0BrLj9SYNLK0M/giphy.gif",
    "duration": "10:44",
    "view_count": 9899,
    "like_count": 354,
    "comment_count": 24,
    "released_at": "2023-01-31",
    "gist": "When a user makes an API call to the backend and encounters a network issue causing the call to fail, one common solution is to retry the call assuming that the APIs are idempotent.\n\nHowever, when multiple clients simultaneously retry their calls, it can cause a \"Thundering Herd\" problem, overwhelming the server and making it difficult to recover. In this article, we will discuss how to address this issue by implementing Exponential Backoff and Jitter.\n\n## Retrying is brutal\n\nThe simplest way to retry failed API calls is to repeat the call immediately in a loop for a specified number of attempts. However, this approach can make the problem worse at scale.\n\nImagine the server is already overwhelmed with requests, and each client retries every single connected API call that has failed. This creates more pressure on the server and leaves no room for recovery.\n\n## Exponential Backoff\n\nExponential Backoff is a retry strategy that introduces a delay between retries, increasing the delay exponentially with each retry attempt.\n\nThis approach gives servers a breathing space and allows them time to recover. Instead of repeating the API call back-to-back, we introduce a delay that increases with each retry attempt.\n\nAdding exponential backoff reduces immediate retries but there is still a chance that the retries repeating at the same time will continue to bombard the server concurrently.\n\n## Jitter to the rescue\n\nTo further improve this approach, we can add a jitter to the retry intervals. Jitter refers to adding some random delay to the retry interval to avoid retries coinciding.\n\nThis approach helps to distribute the retries and prevent them from adding to the problem. By introducing randomness, we can ensure that the retries are distributed across a wider time interval, reducing the chances of coincidences during retries.\n\n### Caveats\n\nWhen implementing Exponential Backoff and Jitter, there are two key things to keep in mind. First, it is essential to add random jitter to the retry interval to avoid coincidences during retries. Second, ensure that the retries are exponentially spaced, so that the time interval between retries increases with each attempt.\n",
    "notes_gd": "https://drive.google.com/file/d/1YeEfBgoVpFBVjVJqd2v9sRK3owBLaTpc/view?usp=share_link",
    "slug": "thundering-herd-problem-and-how-not-to-do-api-retries"
  },
  {
    "id": 215,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "J2IcD9FZvZU",
    "title": "Designing Idempotent API Endpoints for Payments at Stripe",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nSay, you are writing a payment service and you wrote an API that transfers money from one account to another. Because of a network glitch, the client retried the API call, and this led to the deduction of money twice. To handle this situation we need our APIs to be Idempotent.\n\nIn this video, we take a look at what idempotent APIs are, how to write them, and how they form the heart and crux of any payment service.",
    "img": "https://i.ytimg.com/vi/J2IcD9FZvZU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xTiTnqUxyWbsAXq7Ju/giphy.gif",
    "duration": "14:26",
    "view_count": 8794,
    "like_count": 406,
    "comment_count": 64,
    "released_at": "2023-01-29",
    "gist": "In the world of payments, reliability and consistency are crucial. Customers expect their transactions to be processed accurately and efficiently, without any errors or duplicates. However, in a distributed system, failures can occur, and the same request can be sent multiple times.\n\nFor example, consider a payment service API that transfers money from one user to another. No matter where the failure occurs, the system needs to ensure that one request is processed exactly once; leading us to build idempotent APIs.\n\n## Idempotent APIs\n\nAn idempotent API is an API that can be called multiple times, but the result will always be the same. If a request is made twice or more, the API will only perform the operation once, ensuring that there are no duplicates or errors.\n\nHowever, designing an idempotent API is not a trivial task. There are many things to consider, such as handling errors, passing idempotency keys, and maintaining consistency.\n\n## Core idea implementation\n\nThe core idea behind item potency keys is straightforward. Before making an API call, the client talks to the server to generate a random ID, which serves as the idempotency key.\n\nThe client then passes this key in all future requests to the server. The server stores the key and the purpose of the request in a backend database.\n\nWhen the server receives a request, it extracts the idempotency key and checks if it has already processed the request. If it has, the server ignores the request. If it hasn't, the server processes the request and deletes the idempotency key.\n\nThis gives us the much-needed exactly-once processing.\n\n### Handling Errors\n\nWhen designing an idempotent API, we need to consider what to do in case of failure. Depending on the application usecase and the transaction state, we can choose to\n\n- ignore the failure,\n- pass the error to the user, or\n- retry the request.\n\nRetrying the request is often preferred because it provides a better user experience. However, we need to be careful when retrying requests, as we could end up processing the same payment multiple times.\n\n\n## Passing the keys\n\nThere are several ways to pass the idempotency key to the server, such as using request headers or query parameters. Stripe, a popular payment processing company, requires clients to pass idempotency keys in request headers named `Idempotency-Key`.\n\n## Database decisions\n\nIt is recommended to use a separate database or table to store idempotency keys to ensure that the server can quickly validate the processing status and load isolation.\n\n## Conclusion\n\nIn conclusion, designing an idempotent API for payments is crucial for ensuring reliability and consistency. By using idempotency keys, we can avoid processing the same payment multiple times.\n\nThis approach can significantly improve the reliability and user experience of your API, making it a must-have for any critical API that needs to be executed exactly once, no matter what.",
    "notes_gd": "https://drive.google.com/file/d/12mM4QJvkgVdFn7CKSLshQuzBlbEn0kQU/view?usp=share_link",
    "slug": "designing-idempotent-api-endpoints-for-payments-at-stripe"
  },
  {
    "id": 214,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "BwxU9EnCFXA",
    "title": "How Slack efficiently classifies emails at scale with an eventually consistent system",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nAlmost all engineers start working on a feature thinking it is a simple and a no-brainer but when we start thinking of the implementation details we realize how complicated things actually are.\n\nIn this video, we go deep into Email Classification Service at Slack whose sole job is to classify an email as internal or external. The feature seems a cakewalk but when we start jotting down the implementation specifics, things turn out a little more complicated than what we anticipated.",
    "img": "https://i.ytimg.com/vi/BwxU9EnCFXA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/1XgSqCWMonbLLpxNev/giphy.gif",
    "duration": "23:19",
    "view_count": 3767,
    "like_count": 119,
    "comment_count": 11,
    "released_at": "2023-01-13",
    "gist": "One of the features of Slack is the ability to invite people to join a workspace by email. However, not all email addresses belong to the same organization, and it is a challenge to distinguish between internal and external invites.\n\nIn this article, we will discuss the technical details of how Slack classifies emails to provide a smoother product onboarding experience.\n\n## Need for classification\n\nOnboarding employees and external vendors into a Slack workspace can be a tedious process. Depending on who is being invited, an internal or an external onboarding flow will be triggered. This can be as simple as showing a different dialogue.\n\nA solution to this problem is an email classification service that classifies every email into an internal, or external category. This service kicks in when a user types in an email address during the invitation process.\n\n## Heuristics\n\nSlack's email classification service attempts to classify emails by using heuristics. For example,\n\n- if the user's domain is one of the allowed ones, then the user is internal\n- if the domain of the inviter matches the domain of the invitee, the invitee is classified as per the inviter's class.\n\n###But heuristics fail\n\nThe above two approaches are simple and effective for Slack workspaces that are homogeneous (having one or few email domains). But, things become interesting when the number of email domains is many and there is no one rule that will fit.\n\nSlack solves this classification problem by keeping track of all domains part of a workspace and using that aggregated count to determine the class.\n\n## Implementation Details\n\nTo implement this classification, Slack maintains a table called `domains` which stores the aggregation (count) per `(team, domain, role)`. This table effectively answers how many users of a workspace have a particular domain and a specific role.\n\nFor example workspace, `A` has `68` members having the role `member`, while `6` members with the role `admin`.\n\nThe count is maintained with eventual consistency. User-created, updated, and deleted events are sent to Kafka, and consumed by a set of consumers to update the `domains` table.\n\n### Classifying Threshold\n\nSlack operates with a threshold of 10%, which implies that a domain is considered internal if there are at least 10% or more employees in the organization with a given domain, otherwise, it is classified as external.\n\n### Adding a new user\n\nWhile processing the `user_created` event, Slack does `upsert` instead of an `update` as upserts allow us to not check for the existence of the row with matching constraints.\n\n```\nUPSERT count = count + 1 WHERE\nteam_id = 1 AND domain = \"bar.com\" AND role = \"member\";\n```\n\n### Changing the role\n\nIf a user's role changes from `member` to `admin`, the count for the member role is decremented, and the count for the admin role is incremented transactional.\n\n```\nUPSERT count = count + 1 WHERE\nteam_id = 1 AND domain = \"bar.com\" AND role = \"admin\";\n\nUPSERT count = count - 1 WHERE\nteam_id = 1 AND domain = \"bar.com\" AND role = \"member\";\n```\n\n### Reprocessing the message\n\nOne challenge is that user events can be processed twice causing the numbers to drift. To overcome this challenge, Slack uses a healing mechanism that automatically corrects drifts.\n\nThe healer process goes through all the users, until a certain timestamp, and extracts their email domains to perform the count in memory. It then computes the drift from numbers stored in the `domains` table.\n\nInstead of updating the database with the observed count, it pushes the drift as a separate message `+N / -N`. This ensures that numbers in the `domain` table always converge to the correct value and are eventually consistent.\n\n## Conclusion\n\nSlack's email classification service is a critical feature that helps give a smoother invitation experience to users. Classifying emails accurately is a challenging problem, but Slack has implemented an effective solution by using a table to maintain counts and eventual consistency.",
    "notes_gd": "https://drive.google.com/file/d/1Mbz40vZdj5Cet-qFwOBlLQUnSnqfRGGX/view?usp=share_link",
    "slug": "how-slack-efficiently-classifies-emails-at-scale-with-an-eventually-consistent-system"
  },
  {
    "id": 213,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "7v-wrJjcg4k",
    "title": "How @ShopifyEngineering avoids hot shards by moving data across databases without any downtime",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nA truly scalable system is one that can be scaled horizontally. A database is typically scaled by splitting the data across multiple shards. But what happens when a particular shard becomes hot due to excessive load hitting it, while others are underutilized? A classic way to address this is by moving a fragment of data from one node to another. But how?\n\nIn this video, we look at how Shopify re-balances the shard by moving a fragment of data from one database to another without incurring any downtime.",
    "img": "https://i.ytimg.com/vi/7v-wrJjcg4k/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/2A3DG83yvN8uaBiaNR/giphy.gif",
    "duration": "20:58",
    "view_count": 5433,
    "like_count": 197,
    "comment_count": 29,
    "released_at": "2023-01-08",
    "gist": "Shopify is an e-commerce platform that enables individuals to create their online stores. Shopify uses MySQL database to hold their transactional data and each table has a column called `shop_id` that enables easy identification of rows belonging to a specific shop.\n\nShopify uses a distributed architecture to handle a large number of shops. A set of shops is grouped in a logical entity called Pod and all of them share the same database. Thus Shopify has multiple pods and each pod has multiple shops sharing the same database.\n\nAs the platform grows and more shops sign up, there arises a need to balance the load on different pods by moving the data across databases without downtime.\n\nLet's discuss how they do it in detail.\n\n## Routing Layer\n\nThe routing module uses NGINX and is the front-facing entity in the architecture. It routes requests to the pod that is supposed to handle them.\n\n## Distribution of Shops in Shards\n\nDistribution based on the number of shops is not a good idea because two 'heavy shops' may end up on one shard, risking failure due to over-utilization and inconsistent database utilization.\n\nThe decision of which shop lives in which shard depends on the 'heuristics' applied by the Shopify data science team. They consider historical database utilization, historical traffic on the shop, and forecasted load.\n\n## Moving Shops Without Downtime\n\nWhen moving a shop from one pod to another, Shopify ensures that there is no downtime or data loss. Shopify follows three high-level phases to move a shop from one pod to another.\n\n### Phase 1: Batch Copy and Tail Binlog\n\nIn the first phase, Shopify uses an internal tool named ghostferry to batch and pick the rows with a particular shop id from multiple tables of the source database and write them to another database present in another pod.\n\nWhile the batch copy is happening, the newer changes are consumed through `Binlog` and pushed into a queue after filtering out irrelevant events.\n\n### Phase 2: Prepare for Cutover\n\nOnce the batch copy is complete, the newer changes are consumed from the queue and applied to the new database until the 'lag' is down to seconds.\n\nWhen newer events are almost immediately consumed, the writes to the source database are stopped. The source DB's binlog coordinates are recorded, and as soon as the target DB reaches that point, we say replication is done.\n\n### Phase 3: Cutover and Updating the Routing\n\nAt this stage, there are no new writes to the source DB, and the source DB is equal to the target DB.\n\nThe routing table is then updated and traffic is switched on. The requests for the shop now go to the new pod. After doing a few sanity checks, we mark the shop migration as complete.\n\n## Conclusion\n\nShopify moves shops from one pod to another to balance shards. Shopify uses an internal tool named ghostferry to move a shop's data from one pod to another. Shopify ensures that there is no downtime or data loss while moving a shop from one pod to another. This article discussed how Shopify balances shards without downtime.",
    "notes_gd": "https://drive.google.com/file/d/1W5AM_NXAZ7CjpmjcRzUKzq8Fpq0KpQ1p/view?usp=sharing",
    "slug": "how-shopifyengineering-avoids-hot-shards-by-moving-data-across-databases-without-any-downtime"
  },
  {
    "id": 212,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "KeV4erIm47o",
    "title": "The architecture of Grab's data layer that serves millions of orders every day",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nGrab processes millions of Food and Mart orders every single day, and the most critical, but brittle part of the infrastructure is the database. Given how critical orders are to Grab, they have to ensure very high availability of the database while ensuring a very fast query time. So, how do they achieve this?\n\nIn this video, we take a look at the high-level architecture of the data layer of Grab's Order Platform and see the practices they follow to ensure high availability, stability, and performance at scale.",
    "img": "https://i.ytimg.com/vi/KeV4erIm47o/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3vAa3t5Df4tmspUNtT/giphy.gif",
    "duration": "22:33",
    "view_count": 10880,
    "like_count": 448,
    "comment_count": 57,
    "released_at": "2023-01-01",
    "gist": "Grab stores and processes millions of orders every day, here is the design of the systems that powers it \u26a1\n\nThe architecture of Grab's order platform is divided into two main databases, one for transactional queries and the other for analytical queries.\n\nThe transactional database holds transactional data about the orders that serve as the single source of truth for ongoing orders. In contrast, the analytical database holds historical and statistical data and stores it for longer periods. This database is defined to be more efficient for read-heavy analytical queries.\n\n## Design Goals\n\nGiven the critical role that databases play in any system, the most important criteria for a good design are stability, availability, and consistency.\n\nStability is critical as the database must handle high volumes of queries per second. Availability is important because the database stores orders and any downtime could lead to revenue loss. Consistency ensures that users receive updated information when performing transactional queries. Therefore, strong consistency is necessary for transactional queries, while eventual consistency is sufficient for analytical queries.\n\n## Architecture\n\n### DynamoDB as Transactional Database\n\nGrab uses DynamoDB as its primary data store. DynamoDB offers strong consistency, scalability, and high availability, making it an ideal choice for transactional queries.\n\nGrab uses key-value queries such as get, create, and update orders, and DynamoDB's internal partition balancing feature to handle spikey traffic loads and hotkeys.\n\nThe orders table in DynamoDB contains several key attributes, including order ID, the state of the order (ongoing, completed, or canceled), the time the order was created, and the user ID.\n\nTo optimize performance, a global secondary index was created on the user ID, which allows for quick retrieval of ongoing orders for a particular user. When an order is completed, it is automatically removed from the index to keep it lean.\n\nGrab has also designed its schema to keep its global secondary index (GSI) lean and performant by removing entries from the index after three months of inactivity.\n\n### MySQL as Analytical Database\n\nFor analytical queries, Grab uses MySQL as its Analytical database. It is partitioned by the created_at, ensuring minimal cross-partition queries, and it drops old partitions to keep the database lean and consistent.\n\n### Sync between databases\n\nGrab uses Kafka to handle asynchronous batch ingestion from DynamoDB to MySQL. All events from the order service are sent to Kafka, which then updates the analytics database.\n\n## Conclusion\n\nIn conclusion, Grab's order platform is an example of a complex system that balances consistency, availability, and cost-effectiveness to process millions of orders every day. By separating transactional and analytical queries into two separate databases, using specialized processing units, and leveraging modern data storage technologies, Grab has built a system that is highly scalable, available, and cost-effective.",
    "notes_gd": "https://drive.google.com/file/d/1L2CKh6YEC5oQoc8LUsT_IswlOkKygREW/view?usp=share_link",
    "slug": "the-architecture-of-grab-s-data-layer-that-serves-millions-of-orders-every-day"
  },
  {
    "id": 211,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "dOyCq_mMtdI",
    "title": "How @twitter keeps its Search systems up and stable at scale",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nSearch is one of the most important services for any product and at the Twitter scale, it becomes ultra important. But what does it take to maintain Search at scale, how can we ensure that the search service continues to function no matter what kind of load hits the service?\n\nIn this video, we dive deep into how Twitter built tooling around their Elasticsearch that helps handle massive surges in the search traffic, do real-time ingestion, and 100s of terabytes of back-fill.",
    "img": "https://i.ytimg.com/vi/dOyCq_mMtdI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xThuWi50rS1VZG3QTS/giphy.gif",
    "duration": "15:",
    "view_count": 5322,
    "like_count": 197,
    "comment_count": 37,
    "released_at": "2022-12-25",
    "gist": "Managing massive, talking hundreds of terabytes here, Search clusters is no joke, especially at @Twitter's scale.\n\nTo manage them efficiently, Twitter built a bunch of toolings, here's a quick gist about it \ud83e\uddf5\ud83d\udc47\n\nTwitter uses ES to power the search of tweets, users, and DMs. ES gives them the necessary speed, performance, and horizontal scalability.\n\nGiven massive adoption, they needed to ensure the efficiency, and stability of these clusters and provide some standardized way of access.\n\n## Elasticsearch Proxy\n\nThe Twitter team built a simple proxy for Elasticsearch that transparently sits in front of the Elasticsearch cluster.\n\nThe proxy is an extremely simple and lightweight TCP and HTTP-based relay that...\n\nin a standard way, captures all critical metrics like - cluster health, latency, success, and failure rates here; along with this we can also\n\n- throttle when some client abuses\n- apply security practices\n- route to a specific node\n- authenticate\n\n## Ingestion Service\n\nES performance degrades when there is a massive surge in traffic. We typically see an\n\n- increased indexing latencies\n- increased query latencies\n\nBut it is a common usecase for Twitter to ingest massive data (tweets) every now and then, hence they tweaked the ingestion...\n\nThe write requests that come to the ES proxy are sent to Kafka. Consumers read from Kafka and relay them to the ES cluster.\n\nDoing it asynchronously allows us to\n\n- do batch writes\n- and retry if the ES down\n- consume at a comfortable pace\n- slow down if ES is overwhelmed\n\n## Backfill Service\n\nTwitter has a constant need of ingesting 100s of TBs of data in the Elasticsearch clusters.\n\nDoing massive ingestion through Map Reduce jobs directly on ES will take down the entire cluster and doing it through Kafka makes it unnecessarily granular;\n\nhence a backfill service ...\n\nThe backfill indexing requests are dumped on an HDFS.\n\nThe requests are partitioned and read using distributed jobs and indexed in Elasticsearch.\n\nA separate orchestrator computes the number of workers required to consume the indexing requests.",
    "notes_gd": "https://drive.google.com/file/d/10Q3u6wvppumrooEZODudwRKp_sW_6M5q/view?usp=share_link",
    "slug": "how-twitter-keeps-its-search-systems-up-and-stable-at-scale"
  },
  {
    "id": 210,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "JL9x9N6YSUc",
    "title": "How Zomato improved its search by identifying intent through NLP",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nSearch is one of the most important services, especially for Food Aggregators like Zomato. An interesting challenge comes when the search query is not homogeneous, instead it contains multiple entities.\n\nFor example, \"Best Dominos Pizza Near Me\". This query contains a restaurant, a dish, and a location. Getting relevant information from the search engine from this query is an interesting problem to solve.\n\nIn this video, we dive deep into how Zomato identifies the intent of the search query to better its Search Experience and make it more conversational using NLP.",
    "img": "https://i.ytimg.com/vi/JL9x9N6YSUc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/dsW2rI1Vu0PntiyIOl/giphy.gif",
    "duration": "26:45",
    "view_count": 5130,
    "like_count": 228,
    "comment_count": 9,
    "released_at": "2022-12-11",
    "gist": "Search is one of the most interesting problems to attempt and Zomato has made their search understand natural language; here's a quick gist about this system \ud83e\uddf5\n\nA simple search engine that just does a weighted search on name and description is easy to game. For example: \"Best Coffee Cafe\" would rank restaurants having the word \"best\" in their names higher.\n\nBut the actual intent of the user is to get the list of best coffee cafes near its current location.\n\nHandling such queries requires natural language understanding. On Zomato, the search queries can be classified into 3 categories\n\n1. Dish + Dish - chai and samosa\n2. Restaurant + Dish - mcd burger\n3. Restaurant/Dish + near me/best/some text - pizza near me\n\n## Training the model\n\nWe train a Neural Network with domain data that helps us understand the different entities present in the search query; and for this, we leverage\n\n- Word2Vec\n- Byte-pair Encoding, and\n- LSTMs.\n\n### Word2Vec\n\nWord2Vec helps in generating word embeddings i.e. vector representation of a word such that the weights in the vector mean something as per the corpus.\n\nDocuments are tokenized and passed as inputs to Word2Vec. So a restaurant name \"Domino's Pizza\" should be passed as tokens \"Dominos\" and \"Pizza\". But how do we tokenize?\n\n### Byte-pair Encoding\n\nTokenizing the document on simple spaces won't work well because, in the Food domain, we see some words appear together more frequently than others. Ex: \"Cheeze Pizza\".\n\nTo train Word2Vec better, we would prefer, \"Cheeze Pizza\" to be considered as one token instead of \"Cheese\" and \"pizza\" as two; because in the end, these will be our entities.\n\nThis requires us to do a supervised tokenization and we leverage an algorithm called Byte-pair Encoding. It is a really simple supervised algorithm that does a great job at tokenizing the text as per the corpus.\n\nThe algorithm just works by merging the most frequent subtokens and creating new amalgamated tokens.\n\nFor example, BPE enables us to tokenize \"Friedrice\" as \"Fried\" and \"Rice\" which would not be possible if we just split by space.\n\n### Sequence Tagging\n\nThe tokens extracted using Byte-pair Encoding are used as a vocabulary to generate word embeddings which are used to train a Neural Network to understand Named Entities using Bidirectional LSTM.\n\nWith this network, we could process the text \"Jack's Aaloo Tikki Burger\" and get\n\n- Jack's is a Restaurant\n- Aaloo Tikki Burger is a Dish\n\n## Architecture\n\nThe data about Restaurants, Food, and Locations is ingested to train the model. The model is loaded in a lightweight API server and served through an API Gateway.\n\nThe Search service upon getting the search request makes a call to this API that responds with extracted - Dish, Restaurant, and Intent.\n\nThe information is then used to formulate an Elasticsearch Query to get the search results. These results are then streamed back to the user and rendered on their applications.",
    "notes_gd": "https://drive.google.com/file/d/1n688D9Wa6JgwbqmeRuxTAOOyyxAyYZo1/view?usp=share_link",
    "slug": "how-zomato-improved-its-search-by-identifying-intent-through-nlp"
  },
  {
    "id": 209,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "gpzGpPiRoCo",
    "title": "Designing Uber's highly available Emergency SOS Service",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nAn emergency button in a ride-hailing app like Uber can be life-saving but what happens when you click it? What kind of information does it capture? how does it notify the nearest police station?\n\nIn this video, we dive deep into the architecture of Uber's emergency SOS service and look at key design decisions that make it so reliable and highly available.",
    "img": "https://i.ytimg.com/vi/gpzGpPiRoCo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT5LMJDD7nQeWkhvva/giphy.gif",
    "duration": "20:27",
    "view_count": 4548,
    "like_count": 166,
    "comment_count": 10,
    "released_at": "2022-11-28",
    "gist": "The emergency button on the Uber app is life-saving; and the service powering it needs to be available 24 x 7, no matter what!\n\nSo, how did Uber design such a service? How did they make it highly available; here's a quick gist about its system design \ud83d\udc47\u200d\n\n## Information Gathering\n\nWhen the emergency button is pressed, we first need to gather all the critical information and send it to the server. The critical information could be\n\n- Current Location\n- Vehicle and Trip Details\n- Rider and Driver Details\n\n## Capturing Location\n\nWhen the emergency button is pressed, we do not just need to fetch the location at that moment, instead we continuously capture the location and keep sending it to the backend.\n\nThis would help us notify the nearby police and help them keep an eye on the movement.\n\n### Lat-long are not enough\n\nTo make tracing effective, we cannot just share the lat and long because they are incomprehensible; hence we try to deduce the address from the lat-long.\n\nThis process of deducing address from lat-long is called Reverse Geocoding.\n\n## Notifying the police\n\nUber uses a 3rd party service named RapidSOS to notify nearby local authorities.\n\nRapidSOS provides APIs to register an emergency and send live updates about the emergency. It takes care of notifying the local authorities and providing them with the necessary information.\n\n## Notifying others\n\nUber not only notifies the police through RapidSOS, but it also notifies\n\n- emergency contacts\n- internal support staff for close follow-ups\n\nThe notification to all channels happens in parallel to increase the probability of someone getting notified.\n\n## Reliability and Availability\n\nGiven that the emergency service deals with events that are urgent, important, and critical; it is extremely crucial that the service is reliable and highly available; which means\n\n- no events loss\n- persistence and retries\n- fallback to every single component \n\n## Key Decisions\n\n1. Location getting ingested in Kafka\n2. Kafka guarantees persistence and reprocessing if required\n3. RapidSOS can be down and hence apply retries on consumers\n4. Emergency service notifies emergency and internal staff async",
    "notes_gd": "https://drive.google.com/file/d/1RMjNyqQoDK0z3OmzhICOg7WHRhAEIMG1/view?usp=share_link",
    "slug": "designing-uber-s-highly-available-emergency-sos-service"
  },
  {
    "id": 208,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "BFyWl9MNDjY",
    "title": "How Booking com designed and scaled their highly available and performant User Review System",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nDesigning a highly available and performant service is really difficult but booking.com does it really well. The rating and review service is one of the most critical services for Booking and in this video, we dive deep into how they designed and scaled it to ensure they seamlessly handle peak traffic of more than 10,000 requests per second.",
    "img": "https://i.ytimg.com/vi/BFyWl9MNDjY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6Mb480rSmySjt7AA/giphy.gif",
    "duration": "21:46",
    "view_count": 8791,
    "like_count": 336,
    "comment_count": 35,
    "released_at": "2022-11-21",
    "gist": "The review system for Booking is one critical service as it drives business and hence making it highly available becomes extremely crucial.\n\nHere's a thread about building a highly available system and the architecture of Booking's Review Service \ud83d\udc47\u200d\n\n## Importance\n\nReview shown on Booking.com are authentic and it helps people make a decision and thus bring in revenue.\n\nThe review system is also a high-throughput system as people can land on it from Search Engines or internal navigation.\n\n## Review API Service\n\nThe core Review Service will be a simple REST-based API that exposes endpoints to create, read, update, and delete reviews.\n\nReview Service of Booking handles 10,000 requests per second at peak with p99 of 50ms.\n\nGiven that the latncy requirement is too stringent, Booking serves most of the review from a centralized remote Cache and has done a bunch of optimizations on the Database through materialized views.\n\n## Configuring storage\n\nBooking.com has 250 million reviews and given the amount of info a review holds, they store these reviews in a shared datastore. \n\nGiven Booking needs storage to be highly available, they configured each database to have multiple replicas and that too across regions to tolerate regional outages.\n\n## Auto-scaling Storage\n\nGiven that the traffic for a travel business surges during the holiday season, the database needs to be scaled up to handle the load.\n\nBut keeping the database scaled up throughout the year without much load is very inefficient and hence Booking.com required some so of storage autoscaling.\n\nScaling up and down a sharded database requires us to add and remove nodes; but doing this naively requires the data to be re-partitioned.\n\nTo keep the movement to a bare minimum during scaling, Booking.com chose Consistent Hashing for determining data ownership.\n\n## Architecture\n\n1. Review Service is a simple REST service\n2. Centralised cache and materialized views for quicker response\n3. Database is sharded to handle larger loads\n4. Data ownership is determined by Consistent Hashing\n5. Database is replicated across within and across regions for HA",
    "notes_gd": "https://drive.google.com/file/d/1oxo-BsAUbqCGnfCjYTqw82kpLU1OZ_Cw/view?usp=share_link",
    "slug": "how-booking-com-designed-and-scaled-their-highly-available-and-performant-user-review-system"
  },
  {
    "id": 207,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "yGpEzO32lU4",
    "title": "Overview of Discord's data platform that daily processes petabytes of data and trillion points",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nWhen a company scales, they adopt microservices and each service typically gets its own independent database. With data being distributed across so many databases, there emerges a need to unify them to extract deep product insights, make strategic business decisions, and train ML, models.\n\nIn this video, we take a super-detailed look into how Discord built its unified Data Platform that processes trillions of data points and petabytes of data every single day.",
    "img": "https://i.ytimg.com/vi/yGpEzO32lU4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3orifg8kPOKtK8tKxO/giphy.gif",
    "duration": "22:",
    "view_count": 3236,
    "like_count": 125,
    "comment_count": 26,
    "released_at": "2022-11-14",
    "gist": "To handle trillions of data points and petabytes of data every single day, Discord needs a simple yet robust Data Platform.\n\nHere's a quick overview of their arch and key design decisions \ud83d\udc47\u200d\n\n## What is a Data Platform?\n\nA data platform comprises a set of services that ensures data is replicated from various databases, put in central storage, and making it available for consumption by internal teams, and services.\n\nDiscord needs to analyze the data to\n\n- make strategic business decisions\n- power their machine learning models\n- understand how people are using their product\n\n### Why replicate data in one place?\n\n- data is split across microservices\n- firing queries that span multiple databases infeasible\n- each microservice has its own flavor of database (SQL and NoSQL)\n\nFor example, firing queries across orders (MongoDB) and payments (MySQL) to get the items that generated the most revenue.\n\n## Discrod's Data Platform - Derived\n\nDiscord uses Google's BigQuery as its Data Warehouse (a place to keep and query large volumes of data). The data is stored, processed, and consumed across 3 layers\n\n1. Transactional Layer\n2. Core Tables\n3. Derived Tables\n\nLet's understand each layer in detail.\n\n### Transactional Layer\n\nThe transactional layer comprises the transactional databases used in powering the microservices. These databases typically act as a source of truth for the services.\n\nMicroservices are free to choose the flavor of the database - SQL or NoSQL to power their usecase.\n\n### Core Tables\n\nThe core layer holds the series of tables in BigQuery that are populated using the transactional layer.\n\nData pipelines replicate the data from various transactional databases, like MongoDB, MySQL, etc into a set of structured core tables, and become the input for the subsequent Derived Layer.\n\n### Derived Tables\n\nDerived Tables are the actual consumable tables created from a set of core tables. Each team can create its own set of derived tables by joining a set of core tables as per their need.\n\nEach derived table is essentially a SQL query on core tables. The specified SQL query is fired periodically to join and replicate the unprocessed data into a derived table.\n\nEach derived table has its own configuration file that holds\n\n- columns of the derived table\n- schedule and window\n- partition key, cluster key,\n- dataset and SQL query\n\nA replication strategy is also specified in the YAML file that implies if the output of the SQL query should append to, merge with, or replace the existing derived data.\n\nA separate K8S pod is run for each derived table that ensures an isolated continuous data replication to the derived tables.\n\nThus, each team can define its own set of derived tables using just a SQL query, enabling teams to make data-driven decisions.",
    "notes_gd": "https://drive.google.com/file/d/1rnmNSk5GB9OSdMdxk7U-9TwMvwn3IRla/view?usp=share_link",
    "slug": "overview-of-discord-s-data-platform-that-daily-processes-petabytes-of-data-and-trillion-points"
  },
  {
    "id": 206,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "5FIPtC3xJSQ",
    "title": "How Airbnb designed and scaled its central authorization system - Himeji",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nAuthorization plays a critical role in ensuring that the platform is not abused. For example, Instagram ensures that if an account is made private, only the people allowed can see the posts from it. Such granular fine-grained access control requires a very robust and flexible authorization system.\n\nIn this video, we dive deep into how Airbnb achieves this through its in-house service named Himeji and explore its architecture and key design decisions that ensure robustness, extensibility, availability, and ability to scale to millions of users.",
    "img": "https://i.ytimg.com/vi/5FIPtC3xJSQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3WdUt1PX5mlSo/giphy.gif",
    "duration": "29:56",
    "view_count": 5248,
    "like_count": 176,
    "comment_count": 14,
    "released_at": "2022-11-07",
    "gist": "Building a central, robust, extensible and highly available authorization service is no joke\n\nand @Airbnb does it beautifully\n\nhere's a thread about its architecture and key design decisions... \ud83e\uddf5\ud83d\udc47\n\n## What is authorization?\n\nIt is all about managing fine-grained control over different entities, for example:\n\n- can user A edit comment C?\n- can user B access hotels in region R?\n- can user C read a file in folder F shared with group G?\n\nwhy do we need a service for this?\n\n## Why a central service?\n\nIf each microservice handles its own authorization, then there would be\n\n- duplication of auth logic\n- inter-service calls to check for auth\n\nhence, it is beneficial to create a central auth service. Let's see how it is modeled.\n\n## Auth model\n\n- Principal: user or service that needs to be tested\n- Entity: on which we are checking the access\n- Relation: between entity and principal\n\ncan user A edit comment C?\n\nA - is the principal,\nC - is the entity, and\nedit - is the relation\n\nThe tuple is represented and (optionally stored) in the database as\n\n```\n<entity> # <relation> @ <principal>\n```\n\nif user A has owner privileges on comment C, it will be represented (and optionally stored) as\n\n```\nC # OWNER @ A\n```\n\nStoring one entry for each entity and relation will make the data explode, For example:\n\nif A owns a comment C, then he/she can read and write to it as well. This would make us have 3 entries in the database\n\n- `C # READ @ A`\n- `C # WRITE @ A`\n- `C # OWNER @ A`\n\nhence...\n\nwe need a way to define relations between relations and entities to reduce the size of the data.\n\nA simple YAML-based config would look like this\n\n```\nLISTING:\n\n  # WRITE:\n    union:\n      - # WRITE\n      - # OWNER\n\n  # READ:\n    union:\n      - # READ\n      - # WRITE\n```\n\nThe above configuration implies,\n\n1. WRITE relation is a union of WRITE and OWNER\n2. READ relation is a union of READ and WRITE\n\nAnyone with the write and owner relation can write and anyone with read and write (and transitively owner) relations can read the listing on Airbnb.\n\nTo check if user A can read listing L, we hit\n\n```\ncheck (listing:L, READ, user:A)\n```\n\nIt evaluates as\n\n- LISTING:L # READ @ user:A\n- LISTING:L # WRITE @ user:A\n- LISTING:L # OWNER @ user:A\n\nbecause of `union`, if anyone of these exists in DB, `check` evaluates to True.\n\n## Architecture\n\nHimeji (authorization) service is consist of 3 layers\n\n1. Data Layer\n2. Caching Layer\n3. Orchestration Layer\n\nLet's take a detailed look at each in detail.\n\n### Data Layer\n\nThe data layer of the Himeji service consists of\n\n- persistent relational database\n- data is logically shared within the same instance\n- any mutation in the data is read through CDC, streamed through Kafka, and used in invalidating the cache\n\n### Caching Layer\n\nThe caching layer of the Himeji service is super-critical for performance as it ensures low response time at scale.\n\n- ensures 98% hit ratio\n- cache cluster is sharded\n- consistent hashing determines the data ownership across the cluster\n\n### Orchestration Layer\n\nThe orchestration layer is used by clients and internal jobs to interact with the service. The layer\n\n- forwards reads to caching\n- forwards the writes to the data layer\n- computes response as per the config\n\nThis design is taken from @Airbnb's Engineering Blog and it is linked in the description of the video attached.",
    "notes_gd": "https://drive.google.com/file/d/1zouO57YG9nc1ORJY0uhG-1IPr5yUTBB8/view?usp=share_link",
    "slug": "how-airbnb-designed-and-scaled-its-central-authorization-system-himeji"
  },
  {
    "id": 205,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "nEEaSZZ8R4I",
    "title": "How Gojek masks and keeps users' phone numbers secure at scale?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nDo hyperlocal companies like Uber, Ola, Swiggy, Gojek, Zomato, etc share our phone numbers with the delivery people or theirs with us? If they do then that would be a massive breach of privacy. Hence, they have a system in place that instead of sharing the actual phone numbers, share a masked version of it.\n\nIn this video, we talk about how these hyper-local companies work with telecom operators to generate virtual phone numbers and keep our privacy intact.",
    "img": "https://i.ytimg.com/vi/nEEaSZZ8R4I/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT3i18VruGjUKGTys8/giphy.gif",
    "duration": "16:21",
    "view_count": 4456,
    "like_count": 243,
    "comment_count": 25,
    "released_at": "2022-10-31",
    "gist": "Phone numbers can be misused if gone into the wrong hands. But sometimes delivery agents need to reach out to us for directions.\n\nSo, do delivery agents get our phone numbers?\n\nHere's how Gojek ensures seamless communication without sharing actual phone numbers.\n\n## Problem Statement\n\nSay, customer A ordered something from Gojek and a delivery agent D gets assigned to this order.\n\nWe want to\n\n- enable A to contact D\n- enable D to contact A\n\nbut neither should have the other's phone numbers.\n\nand we achieve this using...\n\n## Virtual Phone Numbers\n\nInstead of sharing the actual phone numbers of the users, we create virtual phone numbers which are\n\n- temporary but functional\n- can be assigned to any user at will\n\nTelecom operators and providers like Twilio and Exotel provide these services.\n\n### Is it for everyone?\n\nIf we assign a fixed VN to every user in the system\n\n- does not improve the security posture\n- we would need millions of virtual numbers\n\nIf VN remains the same for a user, then abusers can keep track, and attack the user; breaching their privacy.\n\n## Constraints\n\nHence, the virtual numbers we assign to the users should be\n\n- assigned on demand\n- bound to a transaction/order\n\nOnce the transaction is over, or the order is delivered, we assign the VN to some other user.\n\nso, how do we assign?\n\n## Fetching Virtual Numbers\n\nInstead of trying to get Virtual Numbers on the fly from the telecom operators and providers, we fetch them periodically and keep them handy in our database.\n\nSay, we call this service VN service.\n\n### Assigning Numbers\n\nWhen a delivery agent is assigned to an order,\n\n- we hit the VN database,\n- fetch a couple of unused VN,\n- and make an entry into the orders DB.\n\nwe show the assigned numbers on the app to the customer and the delivery agent.\n\nIf a user (customer/delivery agent) has multiple active orders, he/she will be assigned one VN for each active transaction.\n\nHence, if a delivery agent is delivering two orders at the same time, he/she will be assigned two VNs, ensuring privacy.\n\n## Flow\n\nA user places an order and the event is sent to Kafka, to be consumed by the consumers which ensure we have enough VNs available.\n\nIf not, more VNs are fetched from the telecom operators and providers.\n\nOnce the delivery agent is assigned to the order, the Kafka consumers fetch the VNs from the database and update the mapping in Orders DB.\n\nThis entry is used to render the phone number of the other party on the app.\n\nso, what happens when a user calls the delivery agent's VN?\n\n### Calling a VN\n\nSay customer A wants to call the delivery agent D. Say, DDD is the virtual number of D that A has.\n\nWhen A makes a call on the number DDD, the telecom provider gets the call and it needs the actual number to connect to.\n\nHence, it makes a call to the VN service. The VN service then checks\n\n- who is calling\n- to whom the call is made\n- the existence of a valid transaction between A and D\n\nonce everything is validated, the VN service responds with A's VN and actual number against DDD.\n\nThe telecom provider then bridges the call that was initiated from A to the actual phone number of D but it sets the source phone number to VN of A.\n\nThis way, when the D receives the call, it does not see A's actual number, instead, it sees the VN of A.\n\nThis is how the customer and the delivery agent can connect over the phone call, while neither gets the actual phone number of the other.\n\nThis is exactly what Gojek does. The link to their blog is in the description of the attached video.",
    "notes_gd": "https://drive.google.com/file/d/1c5p7d7rT9Fi2qoedmtZx9zo0dqCPvbBc/view?usp=share_link",
    "slug": "how-gojek-masks-and-keeps-users-phone-numbers-secure-at-scale"
  },
  {
    "id": 204,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "i8MweKYoG1U",
    "title": "The architecture of Yelp's in-house Search Engine - nrtSearch",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nElasticsearch is a great search engine, but Yelp was not happy with its performance, so they built their own HTTP layer on top of Lucene. In this video, we take an in-depth look into the architecture and critical decisions Yelp took while building its own search engine on top of Lucene.",
    "img": "https://i.ytimg.com/vi/i8MweKYoG1U/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6ZtekN7LuGnulfAQ/giphy.gif",
    "duration": "24:47",
    "view_count": 3842,
    "like_count": 130,
    "comment_count": 27,
    "released_at": "2022-10-24",
    "gist": "Elasticsearch is a great search engine, but Yelp was not happy with its performance, so they built their own HTTP layer on top of Lucene. Here's the architecture of it...\n\n## Why not Elasticsearch?\n\n1. ES replication is naive. The operations that happen on the master are re-done on replicas; hence scaling is not cheap.\n2. ES suffers from the Hot Node problem and requires manual moving of the shards across nodes to balance the load.\n3. ES autoscaling is time-consuming and hence we always provision it for the peak load.\n\n## Key features of Lucene\n\n### Near Realtime Indexes\n\nIn Lucene, the data of an index is stored in an immutable structure called Segment. Once the segment is written it can be quickly copied to a replica, in near-real-time. Thus, replication does not require the Replicas to re-index the documents, making it super-efficient to scale out.\n\n### Concurrent Search on Segments\n\nSearch in Lucene is parallelized on segments. Hence for a given search query, Lucene can make parallel searches across segments using multiple threads and compute the most relevant results. It helps in leveraging all the cores of the underlying hardware.\n\nElasticsearch lacks the above features and is expensive in most cases, and hence Yelp thought of writing their own performant HTTP layer on top of Lucene, similar to Elasticsearch; and they call it nrtSearch.\n\n## Implementation Details\n\n### gRPC and Protobuf\n\nInstead of using standard JSON-based endpoints for communication, nrtSearch uses gRPC/protobuf-based endpoints, saving a ton of time spent in serialization and de-serialization of the data.\n\nTo support existing systems that understand JSON, nrtSearch also provides a REST server built using gRPC-gateway. This gives clients an option to talk over gRPC or REST.\n\n### Quick Failovers\n\nFailovers are painful with Lucene. A standard flow is to backup the index segments on S3 and when a new node spins up, download the required segments and start serving the request.\n\nTo make failover quicker, Yelp used EBS by AWS which is like pluggable storage. The index segments are stored on these EBS volumes.\n\nWhen a node goes down, and a new node spins up, instead of downloading the segments from S3, the unaffected EBS volume is attached to the new node. This makes the recovery happen in seconds instead of minutes.\n\n### Replication\n\nWhen the new segments are created on the master node, the master notifies all the replicas about it. The replicas then fetch the newly created segments from the Master over gRPC and maintain an eventual sync.\n\n## Performance Improvements\n\n### Virtual Sharding\n\nSearch in Lucene happens over segments, and the worst we can do is spin one thread for searching over each segment. This is sub-optimal and hence, the segments are virtually sharded.\n\nSearch Threads are capped and the segments are grouped greedily into slices. One search thread is allotted to each slice, thus searching over multiple segments in one go.\n\nThis logical abstraction helps in maintaining consistent utilization of search threads for a given search request.\n\n### Other improvements\n\n1. Fetching document fields in parallel\n2. Segment-level search timeout to maintain consistent SLA\n\n## Migration from ES to nrtSearch\n\n### Dark Launch\n\nInstead of directly launching the nrtSearch to the public, it is important to test the correctness of the response on production data. Hence, the first phase of the launch is a Dark launch.\n\nThe idea is to serve the entire 100% of requests from Elasticsearch, but send 5% of the requests to the new nrtSearch. The responses of this 5% of the request are compared with the legacy system.\n\nOnce the difference is analyzed and confidence is built on the correctness, the percentage is increased to 10, 15, 20, and 50%.\n\n### Phased Rollout\n\nOnce there was 100% confidence in nrtSearch, the changes were rolled out to the general users in a phased manner and this time the requests we exclusively bifurcated across Elasticsearch and nrtSearch.\n\nThe initial percentage was 5%, eventually increasing it to 100% and then plugging out the legacy Elasticsearch.",
    "notes_gd": "https://drive.google.com/file/d/1J5gDekL7mK52vmZjgA8qO-fQFcPzNiYb/view?usp=sharing",
    "slug": "the-architecture-of-yelp-s-in-house-search-engine-nrtsearch"
  },
  {
    "id": 203,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "-bo7oVejgRM",
    "title": "How Giphy uses CDN to serve a billion GIFs every day",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nGiphy is the world's most popular GIF website and it serves 10 billion media content every single day, and we can guess it would be using CDN to do that, but is that it?\n\nIn this video, we dive deep into how Giphy uses different features of CDNs to solve different kinds of problems; and while going through it we will also look at a super-interesting internal implementation detail of CDN.",
    "img": "https://i.ytimg.com/vi/-bo7oVejgRM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/hklv9aNS7Gcda/giphy.gif",
    "duration": "16:33",
    "view_count": 4931,
    "like_count": 203,
    "comment_count": 18,
    "released_at": "2022-10-17",
    "gist": "GIPHY serves 10 billion GIFs every day, here's how it beautifully uses different features of CDN.\n\n## What is CDN\n\nThink of CDN as a geographically distributed cache; and just like any regular cache, it sits between the user and the origin.\n\nFor any request, if it has the data, it serves the response. If not, it hits the origin to grab the data, cache it, and then responds.\n\n### Geographical Nearness\n\nA key highlight of using a CDN is geographical nearness. Because the CDN servers are distributed worldwide, the request from a user is served from the nearest edge server giving an excellent UX.\n\n## CDN for media content\n\nThis is a no-brainer application of CDN. Giphy serves all the media content like images and videos through CDN that sits transparently between the user and the origin (eg: S3).\n\n## CDN for API responses\n\nApart from the media content, Giphy uses CDN to cache API responses of Search and Discover APIs like\n\n- /v1/gifs/trending\n- /v1/search?q=funny\n\nIt serves these APIs from CDN because the responses of these APIs do not change often; hence using CDN for this reduces the load on API servers.\n\n## Route-specific TTL\n\nNot all APIs or media objects need to be cached on CDN for the same amount of time. Hence Giphy configures different expirations for different types of APIs.\n\nMedia object endpoints are cached longer while trending API is cached for a shorter duration.\n\n## Response-driven TTL\n\nSometimes, it is the backend server that should dictate for how long the response should be cached.\n\nHence, Giphy, in the HTTP response from the origin server provides `max-age` headers that tell CDN the TTL for the specific response. This gives finer control over key expiration.\n\n## Cache invalidation by grouping\n\nGiphy uses Surrogate Keys (tags) while caching endpoints on CDN. It helps in smarter cache invalidation, eg:\n\n- invalidate API responses that contain a specific GIF\n- invalidate API responses from an API key\n- invalidate API responses where the query contains a particular query",
    "notes_gd": "https://drive.google.com/file/d/1M2Id3sJb9ABbMGSU2WFqpexDzbrd2FEH/view?usp=sharing",
    "slug": "how-giphy-uses-cdn-to-serve-a-billion-gifs-every-day"
  },
  {
    "id": 202,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "FczWm6kx0Kg",
    "title": "How Dropbox efficiently serves and renders a large number of thumbnails",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nA classic challenge that comes while building Instagram or Google Photos is about quickly and efficiently serving and rendering a large number of thumbnails.\n\nIn this video, we take a look at an ultimate hack that Dropbox used to very efficiently serve a large number of preview thumbnails by streaming the response from the servers using Chunked Transfer Encoding.\n\nOutline:\n\n00:00 Agenda\n02:28 Design of the Photos App\n04:39 Why is this even a problem?\n07:11 Drobox's Solution: Batching\n11:00 Chunked Transfer Encoding",
    "img": "https://i.ytimg.com/vi/FczWm6kx0Kg/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3osxYzIQRqN4DOEddC/giphy.gif",
    "duration": "19:3",
    "view_count": 2717,
    "like_count": 145,
    "comment_count": 16,
    "released_at": "2022-10-07",
    "gist": "Dropbox has to render a lot of thumbnails when we browse a folder containing a bunch of images. Here's how they serve them efficiently at scale.\n\n## Simple approach\n\nThumbnail is created for each image present in a Dropbox folder and uploaded to a blob store like s3 or hdfs. Each thumbnail thus has a path using which it can be fetched on the client and shown to the end user.\n\nA simple approach to serve thumbnails is to send thumbnail URLs in an API response to the client and as the user scrolls through them, we keep fetching the images and render them.\n\nThis sounds good in theory but gives a very sluggish experience in practice; because of the browser limitation. Every browser caps the maximum number of concurrent connections it can make to a domain.\n\nThis limit is 6 for chrome and 8 for firefox, which means at max the browser will be able to fetch 6 images at a time (at best). When there are many photos to render, the experience becomes sluggish as the user will need to scroll and wait for the photos to load.\n\n## Dropbox's Solution\n\n> Note: This is HTTP/1.1 based solution; you can do a lot more if you are using HTTP/2\n\n### Batching Requests\n\nExpose an endpoint (GET) that accepts multiple thumbnail paths as query parameters. \n\n```\nGET https://photos.dropbox.com/tbatch?paths=1.png,2.png\n```\n\nThe server upon receiving this request goes to the blob store and grabs the thumbnails. It then converts the thumbnails in Base64 and puts them in a text response.\n\nThe server then sends this response, containing base64 encoded image data of multiple images, back to the client. The client then identifies the image and its base64 data and renders it.\n\n### Chunked Transfer Encoding\n\nThe server need not wait to get image data of all the requested images before it can send the response; instead, it uses chunked transfer encoding to stream a partial response to the client.\n\nThe server gets the request and it initiates the fetching of thumbnails in parallel. As and when it receives image data, it creates a chunked response and sends it to the client.\n\nA chunked response is a text file that looks like this\n\n```\n0: data:image/jpeg;base64, <image data>\n2: data:image/jpeg;base64, <image data>\n11: data:image/png;base64, <image data>\n```\n\nOnce it has completed sending the response for all the images, it sends the terminating response informing the client that the entire response is sent, and closes the connection.",
    "notes_gd": "https://drive.google.com/file/d/1l8gl197gxPaCh0mama3LjtiRxNaWgt3v/view?usp=sharing",
    "slug": "how-dropbox-efficiently-serves-and-renders-a-large-number-of-thumbnails"
  },
  {
    "id": 201,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "VNvTEin2liY",
    "title": "Aggregating DynamoDB data in realtime to list restaurants at Deliveroo",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nDynamoDB is an extremely powerful, scalable, and fast KV store but it lacks Aggregations.\n\nIn this video, we design a usecase that is very common for food delivery startups like Deliveroo and Swiggy that would require us to do real-time aggregation of the DynamoDB data in an extremely cost-efficient way.\n\nOutline:\n\n00:00 Agenda\n02:28 Marking Restaurants as Favourite\n04:49 Realtime aggregating DynamoDB data using Streams\n11:10 Why Serverless?",
    "img": "https://i.ytimg.com/vi/VNvTEin2liY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3ofSB8L3vLqWZ3wlaM/giphy.gif",
    "duration": "15:52",
    "view_count": 2560,
    "like_count": 117,
    "comment_count": 22,
    "released_at": "2022-10-03",
    "gist": "DynamoDB does not support aggregation queries, but we need it for a use case; let's build a real-time DDB aggregation today...\n\nDeliveroo, a food delivery startup had a similar problem. On their app, people can mark a restaurant as \"favorite\" and now they wanted to render restaurants ordered by most favorite first.\n\n## Data Model\n\nWe have a `favorites` table in which we store users and their favorite restaurants. The table has `restaurant_id_user_id` as their primary key and `created_at`, and `user_id`, as other attributes.\n\nWith the above data model, getting if a user marked a restaurant as a favorite is an O(1) lookup and so are the marking and unmarking activites.\n\n## Getting restaurants ordered by favorite count\n\nWith this data model, it becomes near impossible to get restaurants ordered by their favorite count, purely because DynamoDB does not support aggregations.\n\n### Core idea\n\nMaintain a separate table having aggregated favorite count as one of the attributes and use it to get tables ordered by favorite count.\n\n### Data Model\n\nIntroducing a new table `aggregated_favourites` having the following schema\n\n- `rastaurant_id` as the primary key\n- `time_window` as the sort key\n- `favourite_count`, `updated_at` as other attributes.\n\n### Data Flow\n\nWe set up a DynamoDB stream that would contain all the events happening on the `favorites` table. This stream will be consumed by an AWS lambda function.\n\nThe lambda function will transactional do `count++` upon every creation and `count--` on deletion.\n\nThis way, we maintain the aggregated favorite count for each restaurant in near-realtime without doing any fancy code changes.\n\n### Advantages\n\n- extremely cost coefficient for Deliveroo scale\n- count updation is asynchronous, hence API response time unaffected\n- better than running a daily cron job or doing a sync write to the aggregated table.",
    "notes_gd": "https://drive.google.com/file/d/1EuEW8z07r7mJ0dXO8yWIJ9xJpc_8LQFW/view?usp=sharing",
    "slug": "aggregating-dynamodb-data-in-realtime-to-list-restaurants-at-deliveroo"
  },
  {
    "id": 200,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "DQwlmTvs6xA",
    "title": "How Razorpay scaled their notification system",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nNotifications are extremely crucial for Fintech companies as it is a way to notify a user about an incoming transaction. Hence it becomes extremely important for companies like Razorpay to ensure that notification is delivered to a user within a certain amount of time.\n\nIn this video, we take a detailed look into how Razorpay scaled their notifications systems, and look at their high-level architecture and key design decisions they made along the way to ensure they always meet the SLA.\n\nOutline:\n\n00:00 Agenda\n02:37 Existing notification system\n08:54 Re-architecting notification system",
    "img": "https://i.ytimg.com/vi/DQwlmTvs6xA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/MCQYzMKw8xOsKDprkZ/giphy.gif",
    "duration": "17:32",
    "view_count": 7249,
    "like_count": 374,
    "comment_count": 54,
    "released_at": "2022-09-30",
    "gist": "Delivery of notifications is critical for a FinTech company like Razorpay because it is a way to notify customers about their transactions and connect with external systems through Webhooks.\n\nSo, how did they design their notification system? let's find out\n\n## Key requirement\n\n1. maintaining an SLA is very important\n2. guaranteed delivery via SMS, Email, Push, and Webhooks\n\n## Existing Setup\n\nUpon every transaction, an event/message was sent to SQS (message broker) which was consumed by a worker that then fanned out the notification through different channels.\n\nBecause we want to guarantee delivery, a state was maintained in the database that tells if the notification was successfully sent or not (esp via Webhook).\n\nHence, there is a component called Scheduler that pulls the unsent notifications from this database and re-queues it in SQS; thus guaranteeing the delivery.\n\n### Challenges at scale\n\n1. huge load on this database\n2. scaling workers were limited by the IPOS on the database\n3. surges, during festive seasons, affected transactional notifications\n\n## New architecture\n\n1. Prioritising incoming load\n\nIn order to ensure that one type of notification does not affect others, every notification type is classified with some priority and depending on which they are pushed to the corresponding SQS queue.\n\nThis ensures that a huge marketing campaign does not affect transactional messages.\n\n2. Rate Limiting\n\nTo ensure mass notification from one customer does not affect others, we add Rate Limiter that would limit the notifications per customer and per type ensuring that critical notifications always meet the SLA.\n\n3. Reducing DB bottleneck\n\nWe could not scale workers because of high DB load, and hence instead of doing a sync write to the database, the notifications that are unsent and need to be retried are pushed in a sync way to the database.\n\nBecause of this async write, we ensure that we write to the database in a staggered way and not put unnecessary load on it.\n\n### Observability\n\nTo ensure we are maintaining our SLA, we have to exhaustively monitor the entire infra for any anomaly; the metrics like - health of the infra, success rate of delivery, and SLA.",
    "notes_gd": "https://drive.google.com/file/d/1423Wn1CrO0goeiYuQo8DpUhrWbfR6KDs/view?usp=sharing",
    "slug": "how-razorpay-scaled-their-notification-system"
  },
  {
    "id": 199,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "NcNCty7_3kc",
    "title": "How Flipkart made their type ahead search hyper personalized",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nSearch is one of the most used features in any e-commerce and to give a better user experience it is almost customary to provide type-ahead search suggestions.\n\nIn this video, we will talk about how Flipkart made their type-ahead search hyper-personalized and dive deep into their high-level architecture and key design decisions that make it extra special.\n\nOutline:\n\n00:00 Agenda\n02:28 Introduction and need for type-ahead search\n04:23 Parameters of Ranking\n07:52 Key design decisions\n14:37 High-level Architecture",
    "img": "https://i.ytimg.com/vi/NcNCty7_3kc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/qQRfz2VfUbDeebczif/giphy.gif",
    "duration": "19:",
    "view_count": 4811,
    "like_count": 290,
    "comment_count": 10,
    "released_at": "2022-09-28",
    "gist": "To help us search quicker and better, Flipkart suggests queries as we type. These suggestions are not only popular suggestions, instead, but they are also hyper-personalized. Let's take a look at how they designed this system.\n\nFor a given prefix, say \"sh\", we should rank the query suggestions - \"shoes\", \"shirts\", and \"shorts\" - in the context of the user.\n\n## Parameters of ranking\n\n1. Quality of the suggestion\n\n- popularity: how popular the search term is?\n- performance: does this term has enough results?\n- grammar quality: is the term grammatically correct?\n\n2. User-actions\n\n- past few search terms of the user\n- past purchase history of the user\n\n## Personalizing the suggestions\n\nA naive way of doing this would be to create cohorts of the users and show all of them the same suggestions for the given prefix. But we wanted to show suggestions that are relevant as per the recently fired queries.\n\nFor example: if a user searched - shoes, red shoes, Nike shoes and then typed \"a\" - we should be showing \"Adidas shoes\" and not \"apple iPhone\".\n\n### Understanding the intent\n\nFlipkart has a taxonomy of the product categories they sell and lists on the platform. The taxonomy holds categories like Fashion and Electronics, and within Fashion, we have Clothing and Footwear, etc.\n\nWe first associate the given search query with this taxonomy. If two terms are mapped to close nodes in the taxonomy, they would be contextually relevant and similar.\n\n### Evaluate Category Similarity\n\nWe need to determine the probability that the current search term/prefix would belong to the same category as the past search terms.\n\nFor example: \"computer monitor\", \"mou\" -> \"computer mouse\"\n\n### Evaluate Reformulation\n\nWe need to determine the probability that the current search term/prefix is being written to reformulate the existing context.\n\nFor example: \"shoes\", \"red shoes\", \"n...\" -> \"Nike Shoes\"\n\n## Training the model\n\nA machine learning model needs to be trained on all viewed items on suggestions, all clicked suggestions, and all unclicked suggestions.\n\nOur model should try to maximize the likelihood of the person clicking the suggestion.\n\nThe feature relationships were modeled and ingested in Xgboost ( decision trees ) and the importance of each feature needs to be quantified and evaluated.\n\n## Architecture\n\nThere will be an \"autosuggestion\" service whose job is to serve the suggestions to the user, given the search term.\n\nThis service will have a small cache that would hold the data for the most popular search term prefix to serve non-personalized suggestions.\n\nThe autosuggestion service will talk to Solr to serve the ML-ranked query suggestions for the given term. The relevance model to be configured in Solr will be \"Learning to Rank\"\n\nA huge amount of data, through the data pipelines, will be ingested into Solr and the ML model will be explicitly trained on xgboost and ingested through a different component.",
    "notes_gd": "https://drive.google.com/file/d/1i6w0I8I1H0TxFRoT0Cs0IkcpL6MO6Rbb/view?usp=sharing",
    "slug": "how-flipkart-made-their-type-ahead-search-hyper-personalized"
  },
  {
    "id": 198,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "tZPTpa3JcKA",
    "title": "The Architecture of Pinterest's Time Series Database - Goku",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nIt is extremely critical to continuously monitor the health of the services and infrastructure. We use Time Series Databases to hold the key vitals like CPU, RAM, Disk, Requests, Network IO, etc. Pinterest generates millions of data points every second and the existing Time Series Databases were not performant enough to meet their needs, hence they built one in-house.\n\nIn this video, we take a detailed look into the architecture and key design decisions that Pinterest took while designing their own in-house time series database named Goku.\n\nOutline:\n\n00:00 Agenda\n02:44 Need for Time Series Database\n03:49 Time Series Data Model\n06:53 Challenges and Key Decisions\n10:45 Architecture of Goku",
    "img": "https://i.ytimg.com/vi/tZPTpa3JcKA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/pO4UHglOY2vII/giphy.gif",
    "duration": "17:53",
    "view_count": 3027,
    "like_count": 134,
    "comment_count": 9,
    "released_at": "2022-09-26",
    "gist": "Pinterest built their own time-series database named Goku because existing databases did not fit their requirements, here is the architecture of it.\n\n## Existing OpenTSDB setup\n\nPinterest used OpenTSDB to hold their time-series data but it didn't work very well at scale. The two key aspects that hurt them were\n\n- Long GC Pauses\n- Frequent process crash\n\n## Challenges\n\n1. OpenTSDB disk-based scans are inefficient\n2. The data stored in OpenTSDB does not have a good compression\n3. OpenTSDB is JSON based and hence very inefficient\n4. OpenTSDB is distributed and for a query that spans multiple shards, it first collects the data in one node and then evaluates the query.\n\n## Key Decisions\n\n1. To make the scan efficient, Goku stores data in-memory\n2. Goku uses Gorilla engine which gives 12x compression\n3. Goku computes a partial response at each shard and then sends it to the proxy; thus doing a minimal data transfer.\n4. Goku uses Thrift Binary protocol, much more efficient than JSON\n\n## Architecture\n\nGoku stores 24 hours' worth of data in-memory with a configured periodic flush to the disk. The most recent query is fired to this in-memory store for quick evaluation.\n\nGoku is a shared time-series database and each shard may contain data from multiple time series. Each Goku instance holds Bucket Map within which the time-series data resides. Each bucket holds data for a 2-hour window.\n\nThe writes on each time-series data go to the bucket map, within which it writes to a mutable buffer. Once the window is done, the buffer becomes immutable.\n\nDuring a query, the request comes to a Goku Proxy which, if required, fans it out to all the involved shards. Each shard does the computation on its share of data and sends the response back to the coordinator/proxy node.\n\nThe coordinator/proxy node aggregates the response and sends it back to the client, thus completing the operation.",
    "notes_gd": "https://drive.google.com/file/d/1AqR4FuiCZbjuHl5v5H4cVFi8CPJwcFWX/view?usp=sharing",
    "slug": "the-architecture-of-pinterest-s-time-series-database-goku"
  },
  {
    "id": 197,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "7xKgQmqkfD0",
    "title": "The Architecture of Airbnb's Knowledge Graph",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nWhen a product is catering to a large audience, providing contextual information becomes important and at the scale of Airbnb, it becomes non-optional. Hence the engineering team at Airbnb collated all the data and structured it into a Knowledge Graph that today powers its Search, Discovery, and Trip Planner services.\n\nIn this video, we take a detailed look into how Airbnb designed its Knowledge Graph, some key components of it, and some key decisions it took while architecting it.\n\nOutline:\n\n00:00 Agenda\n02:41 The need for a Knowledge Graph\n05:49 Architecture of the Knowledge Graph",
    "img": "https://i.ytimg.com/vi/7xKgQmqkfD0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/orUC71iuzC3rtVWyPn/giphy.gif",
    "duration": "16:46",
    "view_count": 2975,
    "like_count": 123,
    "comment_count": 25,
    "released_at": "2022-09-23",
    "gist": "Google and Facebook are known to have humungous knowledge graphs; and so does Airbnb. So, what are they? and how are they designed?\n\nKnowledge Graphs are just super-structured information collated from all different data sources. At Airbnb, they power Search, Discovery, and Trip Planner products.\n\nKnowledge Graphs power sophisticated queries like\n\n- find cities that host football matches in July and August and are known for para-gliding\n- find a neighborhood in LA where Huts or Private Islands are available in the upcoming summer.\n\n## Key Components\n\nThe knowledge graph system has three critical components: Storage, API, and Storage Mutator.\n\n### Graph Storage\n\nAirbnb collates all the structured information and stores it in a relational database having one table to store all the nodes and another table to store all the edges.\n\nEach node in the graph could have a different schema. For example, the location could have a name and GPS coordinates, while the Event would have a title, data, and venue.\n\nEach edge in the graph will hold the types of nodes it can connect to which ensures strong data integrity. For example, an edge \"landmark-in-a-city\" can connect a landmark and a city.\n\nAirbnb chose to not use GraphDB because of its operational overhead. The team had much higher confidence in relational databases and their capability in managing them.\n\n### Graph API\n\nQuery API layer exposes a JSON-based query structure that can be used by clients to interact with the Knowledge Graph.\n\nThe JSON query is converted to SQL and fired on the database to get the desired information.\n\n### Storage Mutator\n\nWe may think the best way for the Knowledge Graph to remain updated with any changes happening in other systems is to expose an updated API. But that would be too slow and expensive.\n\nHence, a better way to design this is to ingest bulk updates through Kafka. Updates coming from other systems are put in the Kafka and then the mutator updates the knowledge graph by consuming the events.\n\n## Offline Processing\n\nNot every service wants to synchronously query the graph, for example, the search might want to run an offline ranking job. Querying the graph every time will be inefficient and hence there is a periodic job that exports the entire graph database.\n\nThis export is then consumed by the services for offline processing.",
    "notes_gd": "https://drive.google.com/file/d/17Az1C3sESXA0jGPqmcJWjk2GOwSi12Kf/view?usp=sharing",
    "slug": "the-architecture-of-airbnb-s-knowledge-graph"
  },
  {
    "id": 196,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "W8LDyEOPaPY",
    "title": "How Swiggy designed and scaled its chatbot",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nWe all love food but are lazy to go to a restaurant and pick it ourselves and this gave rise to Food Delivery Startups like Swiggy and Zomato. Chatbots are essential for them as they resolve most of the complaints that customers have without spending a lot of money on customer service representatives.\n\nIn this video, we take a look at how Swiggy designed their chatbots to achieve business efficiency at scale; and dive deep into their tech architecture and key components that we need to consider while designing it.\n\nOutline:\n\n00:00 Agenda\n02:36 What are Chatbots\n03:30 Designing Chatbots\n07:00 Architecture of a chatbot",
    "img": "https://i.ytimg.com/vi/W8LDyEOPaPY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3orif627F2MRFcnpeg/giphy.gif",
    "duration": "15:11",
    "view_count": 4425,
    "like_count": 198,
    "comment_count": 9,
    "released_at": "2022-09-21",
    "gist": "Swiggy has automated its customer support with Chatbots; so what's their architecture? Let's find out today.\n\nChatbots are essentially a Decision Tree where nodes are the states the conversation could be in and the edges are conditional statements. Depending on which statement (edge) meets the criteria, the conversation moves forward in that direction.\n\nAt each step, Swiggy shows the customer the options which are the child nodes of the current node until all the information is gathered; and the flow stops upon reaching the terminal state.\n\n## Key components and architecture\n\nThe decision tree is stored in a relational database. It can be generated using historical customer support data and can be updated by product and business teams.\n\nThe core chatbot service needs to access data from other services/databases like Payments, Orders, and Notifications. Accessing peripheral information will provide a rich experience to the users.\n\nThe most important part chatbot is to interact with the Fraud Detection system. Given that the entire flow is automated and needs no manual intervention, the chances of Fraud shoot up.\n\nFraud Detection System, in real-time, would see if the customer is trying to commit fraud. This system would interact with historical information about the customer, past refunds, image processing, and much more.\n\nThere may be a possibility that the customer still needs to talk to an executive hence the customer support executive needs to get all of this information, including fraud probability, in an easy-to-use dashboard.\n\nHence, a real-time pipeline would be needed to ingest and move data across all of these systems. Apache Spark is an excellent candidate for building this.\n\n## Business Continuity Plan\n\nIf the Chatbot is down, then Swiggy switches to a simple chat interface that connects directly to an executive, continuing to service the end users.\n\n## Key Metric\n\nA metric that Swiggy chases is Bot Efficacy which is nothing but the percentage of requests handled by the bot vs executive. Swiggy wants to handle as many requests as possible through the bots as it would help them remain efficient at scale.",
    "notes_gd": "https://drive.google.com/file/d/1_C7mUXohjvTR1OAfYL0UWov7-w9B_8zJ/view?usp=sharing",
    "slug": "how-swiggy-designed-and-scaled-its-chatbot"
  },
  {
    "id": 195,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "CA2_0ZhVW2g",
    "title": "How Instagram efficiently serves HashTags ordered by count",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nInstagram has millions of HashTags and millions of people tag their photos every single day. To discover, Instagram allows us to search for a Hash Tag.\n\nIn this video, we will take a detailed look into how Instagram efficiently searches for HashTags and serves them ordered by count; while doing so we will dive deep into a super-interesting Database optimization called Partial Indexes.\n\nOutline:\n\n00:00 Agenda\n02:27 HashTag Ordering Problem and Instagram Database\n05:41 Naive Query\n07:17 Optimized Solution with Partial Indexes",
    "img": "https://i.ytimg.com/vi/CA2_0ZhVW2g/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT5LMvdq41931HThDO/giphy.gif",
    "duration": "12:18",
    "view_count": 7705,
    "like_count": 474,
    "comment_count": 41,
    "released_at": "2022-09-19",
    "gist": "Instagram has millions of photos, and each has tens of hashtags, so how does Instagram efficiently serves hashtags for a search query?\n\nInstagram uses Postgres as its primary transactions database. For each hashtag, it stores the `media_count` in a table. This allows us to render the Hashtag and some meta information on the search page.\n\nNote: Instagram could also use a dedicated search engine for this usecase, but this usecase is trivial to use something as sophisticated as ElasticSearch. We take a look at how they do it with Postgres.\n\nA naive query to get hashtags for a given prefix ordered by count would look something like this\n\n```\nSELECT * from hashtags\nWHERE name LIKE 'snow%'\nORDER BY media_count\nDESC LIMIT 10;\n```\n\nExecuting this query would require the database engine to filter out the hashtags starting with `snow` and then sort. Sorting is an expensive operation and this query required the engine to sort 15000 rows.\n\nUnder high load, this sorting becomes a pain. So, what's the way out?\n\nThe key insight here is the fact that hashtags have a long-tail distribution i.e. most hashtags will have far fewer media counts and while serving we are always ordering by `media_count`.\n\nHence, instead of indexing everything, we can simply index the hashtags having `media_count > 100` because other hashtags are highly unlikely to be surfaced.\n\n## Partial Indexes\n\nPostgres database has this exact same capability and it is called Partial Indexing. With partial indexes, we can keep only a subset of data in the index.\n\nWe can create a partial index on our hashtags table like\n\n```\nCREATE INDEX CONCURRENTLY ON hashtags WHERE media_count > 100;\n```\n\nWith this index, the query we fire would have to scan a very limited number of index entries to spit out the result.\n\nThe SQL query to get hashtags with the prefix `snow%` would look something like this\n\n```\nSELECT * from hashtags\nWHERE name LIKE 'snow%'\nAND media_count > 100\nORDER BY media_count\nDESC LIMIT 10;\n```\n\nThe above query required it to sort only 169 rows, completing its evaluation superfast. This is a classic usecase where we all can utilize Partial Indexes.",
    "notes_gd": "https://drive.google.com/file/d/1C0uGvqYFcBCvqDfkDyJr1RJnTDOq2c6T/view?usp=sharing",
    "slug": "how-instagram-efficiently-serves-hashtags-ordered-by-count"
  },
  {
    "id": 146,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "9kjUG_yvVqM",
    "title": "An in-depth introduction to Rolling Deployments",
    "description": "One of the simplest deployment strategies that make deployment a breeze is Rolling Deployment. It is the most widely adopted deployment strategy purely because of its simplicity and cost-effectiveness. Most of the deployment tool has this as their default deployment strategy.\n\nIn this video, we take an in-depth look into what Rolling deployment is, how they are implemented, how to tune it, some key challenges we face during adoption, and conclude with an understanding of the pros and the cons of adopting it.\n\nOutline:\n\n00:00 Agenda\n02:37 Introduction to Rolling Deployments\n05:43 How to implement Rolling Deployment\n11:32 Tuning Rolling Deployments\n15:59 Pros of Rolling Deployment\n18:37 Cons of Rolling Deployment",
    "img": "https://i.ytimg.com/vi/9kjUG_yvVqM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Kxn4rf7eceDpGBGOZY/giphy.gif",
    "duration": "21:41",
    "view_count": 1024,
    "like_count": 49,
    "comment_count": 1,
    "released_at": "2022-05-27",
    "gist": "Rolling Deployment is a deployment strategy that slowly replaces the previous version of the application with the new one by replacing the underlying infrastructure.\n\nSay we have 9 servers and each one is running version 1 of the code. With rolling deployment, we roll out our changes i.e. version 2 of the code to one server at a time eventually covering all 9 servers. This is the core idea of the rolling deployment, however, the implementation could vary a bit.\n\nA key thing to note here is the fact that deployment is incremental in nature which means during the deployment there would be a few servers that are serving the old version of the code and the remaining servers serving the newer version. Hence the changes we push should be both backward and forward compatible.\n\n## Implementing Rolling Deployment\n\nRolling deployments are always gradual and graceful, and we typically happen through the below steps\n\n1. Pick a server for deployment\n2. Stop the incoming traffic by removing it from the load balancer\n3. Wait for the existing requests to be completed\n4. if we are not replacing the infra, pull the latest code and reload\n5. if we are replacing the infra, delete the server and launch a new one with the new code\n6. attach the server behind the load balancer and start serving the requests\n\n## Tuning Rolling Deployment\n\n### Concurrent Servers\n\nInstead of deploying to one server at a time, we can deploy changes to `n` servers concurrently. This would complete the entire deployment quicker.\n\nChoosing the appropriate `n` is critical as a small value would mean the deployment takes ages to complete and a large one would affect the availability during deployment.\n\n### Double-Half Deployment\n\nAn interesting way to implement rolling deployment is to double the infrastructure and then delete the older half.\n\nSay, we have 4 servers with version 1 of our code so in order to deploy the changes we add 4 new servers with the new version of the code to the infra taking our total count to 8, and then delete the 4 older servers. This way, what remains are the 4 servers with the newer version of the code.\n\n## Pros of Rolling Deployments\n\n- Cost efficient\n- Rollouts are gradual\n- Rollbacks are simple\n- Deployment incurs zero downtime\n- Much faster than Blue-Green Deployment\n- Any hiccup during deployment affects only a fraction of the users\n\n## Cons of Rolling Deployments\n\n- No environment isolation\n- Naive deployment takes a long time to complete\n- Stateful applications are affected during deployment\n- Changes we rollout should be backward and forward compatible",
    "notes_gd": "https://drive.google.com/file/d/1Mtox_ulRNSajmbVXXyLyJrhV3GZouOiv/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-rolling-deployments"
  },
  {
    "id": 145,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "9iAJjtvBwyI",
    "title": "Implementing Vertical Sharding",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nSharding is super-important when you want to handle the traffic that cannot be handled through one server. Sharding comes in two flavors - Horizontal and Vertical. In horizontal sharding, we split the table by rows and keep them on separate servers. In vertical sharding, we distribute the tables across multiple database servers.\n\nFor example, keeping all the payments-related tables in one database server, and all the auth-related tables in another. Vertical sharding comes in super handy when we are moving from monolith to microservices. All this sounds simple yet awesome theoretically, but would we actually implement it?\n\nIn this video, we take an in-depth look, not at the theoretical side of vertical sharding, but at the implementation side of it. We will see how Vertical Sharding is implemented with minimal downtime and what are the exact steps to do it.\n\nOutline:\n\n00:00 Agenda\n03:17 Introduction to Vertical Sharding\n05:23 Implementing Vertical Sharding\n05:55 Picking a configuration store\n10:34 Moving a table from one server to another\n18:58 Summarizing the overall flow",
    "img": "https://i.ytimg.com/vi/9iAJjtvBwyI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif",
    "duration": "24:41",
    "view_count": 3734,
    "like_count": 184,
    "comment_count": 34,
    "released_at": "2022-05-25",
    "gist": "Vertical sharding is fine, but how can we actually implement it? \ud83e\udd14\n\n## Vertical Sharding\n\nVertical sharding is splitting a database by the tables. Shards will hold a subset of tables. For example, all payments-related tables go to one shard, while all auth-related tables go to another.\n\nSo, how to implement it?\n\n## Need for a configuration store\n\nFor our API servers to talk to the correct database we would need a configuration store that holds the information for all the tables mapped to the database server that holds it.\n\nFor example, the Users table is present on DB1 while Transactions on DB2\n\nWhenever the request comes, the API servers first check the config to find which DB holds the table and then fire the SQL query to that specific database for the table.\n\n### Reactive update\n\nAll API servers will cache the configuration to avoid an expensive network call to get the database ensuring we get a solid boost to the performance.\n\nWhen a table is moved from one database server to another, the configuration will be updated and hence the changes would need to be reactively propagated to all the API servers. Hence our config store needs to support reactive communication.\n\nThis is where we choose Zookeeper which is resilient and battle-tested to achieve this.\n\n## Moving tables\n\nSay, we are moving table `T2` from database server DB1 to DB2. Moving the table from one server to another is done in 4 simple steps.\n\n### Dump the table `T2`\n\nWe first dump the table `T2` from DB1 transactionally using the utility `mysqldump` that not only dumps the table data but also records the position in the `binlog`. This is like taking a point-in-time snapshot of the table.\n\n### Restore the dump\n\nWe now restore the dump to database DB2. This way we will have a database server with the table `T2` containing data till a certain point in time.\n\n### Sync table `T2` on DB1 and DB2\n\nWe now setup the replication from DB1 to DB2 specifically for sync changes happening on table `T2`. It is done through a custom job that will use the recorded `binlog` position and start syncing from it.\n\n### Cutover\n\nOnce the table `T2` is synced with almost 0 replication lag on DB1 and DB2 we cutover. We first rename the table to `T2_bak` and update the config in Zookeeper.\n\nAs we rename the table any queries going to DB1 for table `T2` will start throwing \"Table not found\" errors, but as Zookeeper will propagate the changes to all API servers they would use DB2 to fire any query on table `T2`, thus completing the table movement.\n\nThis is how you can implement vertical sharding.",
    "notes_gd": "https://drive.google.com/file/d/1AxijzqfIksP_QOKc9eKhuXba7apUugCT/view?usp=sharing",
    "slug": "implementing-vertical-sharding"
  },
  {
    "id": 142,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "W6HANd8c9t4",
    "title": "An in-depth introduction to Blue Green Deployments",
    "description": "Deployments are a pain if we are unsure about our release changes. But sometimes even if we know our changes well, something weird could happen in the infra that would fail your deployment and put your infra in an inconsistent state.\n\nSo, is there a way to address this? What if we have a place to deploy our changes and validate them before they hit production; and have a quick way to roll back if something goes wrong?\n\nThis is the core idea behind a Blue-Green Deployment\n\nIn this video, we take an in-depth look into a blue-green deployment pattern, understand why we need them, what problem it addresses, learn how they are implemented, talk about its benefits and challenges, and conclude with some points to remember when you adopt this deployment pattern.\n\nOutline:\n\n00:00 Agenda\n03:13 An introduction to Blue-Green Deployments\n06:36 Why do we need Blue-Green Deployments\n09:12 How Blue-Green Deployment is implemented?\n13:26 Pros of having a Blue-Green Deployment\n19:49 Challenges in having a Blue-Green Deployment\n25:36 When to use Blue Green Deployments\n26:39 Key points to remember while adopting Blue Green Deployment",
    "img": "https://i.ytimg.com/vi/W6HANd8c9t4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oGRFnT4RyU4ayoUFi/giphy.gif",
    "duration": "28:55",
    "view_count": 1387,
    "like_count": 63,
    "comment_count": 12,
    "released_at": "2022-05-18",
    "gist": "Blue Green Deployment is a deployment pattern that reduces the downtime during deployment by running two identical production setups called - Blue and Green.\n\nDuring deployment when we reboot the API servers there are chances that the incoming request fail because the server is unresponsive for a short period. Also, it might happen that the release had a major bug and we need a quick rollback.\n\nHow can we achieve both of them in one shot? The answer is Blue Green Deployment.\n\n## Implemention\n\nBlue Green deployment is implemented by having a separate fleet of infrastructure for the old version - Blue and the new version - Green. The new infrastructure is identical to the old one.\n\nThe deployment flow\n\n1. the new deployment artifact is tested and kept ready to be deployed\n2. a parallel infrastructure is set up identical to the existing\n3. the new version is deployed on the new fleet - Green\n4. the correctness of the setup is validated\n5. the proxy is re-configured to now forward 100% of traffic from the Blue (old) setup to the Green (new) setup\n6. a final sanity test is run on the new fleet\n7. the blue fleet is now shut down\n\n## Pros of Blue Green Deployment\n\n1. rollbacks are just a config change and hence quick\n2. downtime during deployment is minimal\n3. deployment is just a flip of a switch\n4. disaster recovery is simple given we already have the automation to build a parallel setup\n5. deployments can now happen during the working hours\n6. debugging a failed deployment is simple as we have the infrastructure with the debug information handy\n\n## Possible challenges\n\n1. during the deployment the infrastructure cost shoots 2x\n2. the stateful application would need to rebuild the state on new servers\n3. the database would have to be shared between the fleets\n4. any schema migration on the database needs to be backward and forward compatible\n5. the API responses have to be forward and backward compatible\n6. setting up this deployment strategy for the first time is difficult\n\n## When to use Blue Green Deployment?\n\n- when you need zero downtime deployment\n- your infrastructure can tolerate 100% traffic switch\n- you can bear the 2x cost of infrastructure during deployment\n\n## Points to remember\n\n- have a solid automation test suite to validate the correctness\n- ensure forward and backward compatibility of API and schema changes\n- infra cost will shoot up hence minimizing the time for which you are running 2x infra",
    "notes_gd": "https://drive.google.com/file/d/1jSowz0IW8kD4Fjrv2fsE-ygHVTaZto1d/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-blue-green-deployments"
  },
  {
    "id": 141,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "nnseeKxovaM",
    "title": "An in-depth introduction to Canary Deployments",
    "description": "Deployments are stressful; what if something goes wrong? What if you forgot to handle an edge case that was also missed during the unit test, integration test, or an internal QA iteration.\n\nPutting such code into production can take down your entire infrastructure and could cause a massive outage. In order or handle such a situation gracefully and provide us with an early warning about something's wrong we have Canary Deployment.\n\nIn this video, we take an in-depth look into canary deployments, learn why canary deployments are called canary deployments, and understand how they are actually implemented, talk about the pros and cons of this deployment pattern, and conclude with a one really solid use case where you absolutely need them.\n\nOutline:\n\n00:00 Agenda\n03:05 Introduction to Canary Deployment\n06:06 Why Canary Deployments are called Canary Deployments?\n08:04 How to implement Canary Deployments?\n10:03 Pros of having Canary Deployments\n16:21 How to pick servers and users for a rollout?\n19:08 Cons of having Canary Deployments\n21:25 When we absolutely need Canary Deployment",
    "img": "https://i.ytimg.com/vi/nnseeKxovaM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NAVQDibk6Sesg/giphy.gif",
    "duration": "23:53",
    "view_count": 2081,
    "like_count": 124,
    "comment_count": 15,
    "released_at": "2022-05-16",
    "gist": "Canary Deployments is a deployment pattern that rolls out the changes to a limited set of users before doing it for 100%.\n\nWe compare the vitals side-by-side from the old setup and the canary servers to ensure everything is as expected. If all is okay, then we incrementally roll out to a wider audience. If not, we immediately roll back our changes from the canaries.\n\nCanary Deployment thus acts as an Early Warning Indicator to prevent a potential outage.\n\n## Why canary deployment is named canary deployment?\n\nIn 1920, coal miners used to carry caged canaries with them. If the gases in the mines were highly toxic the canaries would die and that alerted the miners to evacuate immediately, thus saving their lives.\n\nIn canary deployment, the canary servers are the caged canaries that alert us when anything goes wrong.\n\n## Implementing canary deployment\n\nCanary deployments are implemented through a setup where a few servers serve the newer version while the reset serves the old version.\n\nA router (load balancer / API gateway) is placed in front of the setup and it routes some traffic to the new fleet while the other requests continue to go to the old one.\n\n## Pros of Canary Deployment\n\n- we test our changes on real traffic\n- rollbacks are much faster\n- if something's wrong only a fraction of users are affected\n- zero downtime deployments\n- we can gradually roll out the changes to users\n- we can power A/B Testing\n\n## Cons of Canary Deployment\n\n- engineers will get habituated to testing things in production\n- a little complex setup\n- a parallel monitoring setup is required to compare vitals side-by-side\n\n## Selecting users/servers for canary deployment?\n\nThe selection is use-case specific, but the common strategies are:\n\n- geographical selection to power regional roll-out\n- create specific user cohorts eg: beta users\n- random selection\n\n## When we absolutely need Canary Deployments\n\nSay you own the Auth service that is written in Java and you chose to re-write it in - Golang. When taking it to production, you would NOT want to make a direct 100% roll-out given that the new codebase might have a lot of bugs.\n\nThis is where canary is super-helpful when we a fraction of servers serving requests from Golang server while others from the existing setup. We now forward 5% traffic to the new ones and observe how it reacts.\n\nOnce we have enough confidence in the newer setup, we increase the roll-out fraction to 15%, 50%, 75%, and eventually 100%. Canary setup thus gives us a seamless transition from our old server to a newer one.",
    "notes_gd": "https://drive.google.com/file/d/1JJD_Pa9AkUvhaZ7Dzwd4sQiGdC8nPn5t/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-canary-deployments"
  },
  {
    "id": 135,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "oiZH5U_a0pg",
    "title": "Introduction to Serverless Computing and Architecture",
    "description": "Serverless Computing is one of the hottest topics of discussion today, but the term \"serverless\" is slightly misleading and it does not mean that your code will not need a server to run. We have to be extremely cautious while deciding on adopting serverless for our use case, but it is not something that fits all the use cases.\n\nIn this video, we talk about what serverless computing is, see why it was built in the first place, learn about 5 real-world use-cases that become super-efficient with serverless architecture, understand the advantages and more importantly, the disadvantages of adopting it, and conclude with acknowledging when to use and when not to use this computation pattern.\n\nOutline:\n\n00:00 Agenda\n03:01 Need and the idea of the Serverless Computing\n11:16 Usecase 1: Chatbots\n13:57 Usecase 2: Online Judge\n16:27 Usecase 3: Vending Machines\n18:00 Usecase 4: CRON Jobs\n19:54 Usecase 5: Batch and Stream Processing\n22:16 Advantages of Serverless Computing and Architecture\n26:21 Disadvantages of Serverless Computing and Architecture\n31:37 When NOT to use Serverless\n34:00 When to use Serverless",
    "img": "https://i.ytimg.com/vi/oiZH5U_a0pg/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/5k1VkABjw5wQq0PNEU/giphy.gif",
    "duration": "36:23",
    "view_count": 2254,
    "like_count": 135,
    "comment_count": 17,
    "released_at": "2022-05-02",
    "gist": "Serverless is a cost-efficient way to host your APIs and it forms the crux of systems like Chatbots and Online Judge.\n\nServerless does not mean that your code will not run on the server; it means that you do not manage, maintain, access, or scale the server your code is running on.\n\nThe traditional way to host APIs is by spinning up a server with some RAM, and CPU. Say the resources make your server handle 1000 RPS, but you are getting 1000 RPS only 1% of the time which means for the other 99% you are overprovisioned.\n\nSo, what if there was an Infrastructure that\n\n- scales up and down as per the traffic\n- is billed per execution\n- is self-managed maintained and fault-tolerant\n\nThese requirements gave rise to Serverless Computing.\n\n## Real-world applications\n\n### Chatbot\n\nSay, we build a Slack chatbot that responds with the Holiday list when someone messages `holidays` . The traffic for this utility is going to be insignificant, and keeping a server running the whole time is a waste. This is best modeled on Serverless which is invoked on receiving a message.\n\n### Online Judge\n\nEvery submission can be evaluated on a serverless function and results can be updated in a database. Serverless gives you isolation out of the box and keeps the cost to a bare minimum. It would also seamlessly handle the surge in submissions.\n\n### Vending Machine\n\nUpon purchase, the Vending machine would need to update the main database, and the APIs for that could be hosted on Serverless. Given the traffic is low and bursty, Serverless would help us keep the cost down.\n\n### Scheduled DB Backups\n\nSchedule daily DB backups on the Serverless function instead of running a separate crontab server just to trigger the backup.\n\n### Batch and Stream Processing\n\nUse serverless and invoke the function every time a message is pushed on the broker making the system reactive instead of poll-based.\n\n## Advantages\n\n- No need to manage and scale the infra\n- The cost is 0 when you do not get any traffic\n- Scale is out of the box; so no capacity planning is needed\n\n## Disadvantages\n\n- Takes time to serve the first request as the underlying infra might boot up\n- The execution has a max timeout, so your job should complete within the limit\n- Debugging is a challenge\n- You are locked in on the vendor you chose\n\n## When NOT to use Serverless\n\n- Load, usage, and traffic pattern is consistent\n- Execution will go beyond the max timeout\n- You need multi-tenancy\n\n## When to use Serverless\n\n- Quick build, prototype, and deploy the changes\n- Usecase is lightweight\n- Traffic is bursty",
    "notes_gd": "https://drive.google.com/file/d/1sZShE0r41XcFa2gEPW1RS_YTaR3tC-zH/view?usp=sharing",
    "slug": "introduction-to-serverless-computing-and-architecture"
  },
  {
    "id": 131,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "wXvljefXyEo",
    "title": "Database Sharding and Partitioning",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nSharding and partitioning come in very handy when we want to scale our systems. These concepts operate on the database and help us improve the overall throughput and availability of the system.\n\nIn this video, we take a detailed look into how a database is scaled and evolved through different stages, what sharding and partitioning are, understand the difference between them, see at which stage should we introduce this complexity, and a few advantages and disadvantages of adopting them.\n\nOutline:\n\n00:00 Introduction and Agenda\n03:05 How a database is progressively scaled?\n08:10 Scaling beyond the limit of vertical scaling\n11:57 Sharding vs Partitioning\n12:43 Example of Data Partitioning\n17:15 Sharding and Partitioning together\n20:20 Advantages and Disadvantages of Sharding and Partitioning",
    "img": "https://i.ytimg.com/vi/wXvljefXyEo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif",
    "duration": "23:53",
    "view_count": 15272,
    "like_count": 724,
    "comment_count": 56,
    "released_at": "2022-04-25",
    "gist": "Sharding and partitioning come in very handy when we want to scale our systems. Let's talk about these concepts in detail.\n\n## How is the database scaled?\n\nA database server is just a database process (like MySQL, MongoDB) running on a virtual server like EC2. Now when we put our database in production it starts getting from real good traction, say 100 writes per second (WPS).\n\n### Steady user growth\n\nSay, your product started getting some traction, and we find that the database is not able to handle the load, we scale it up by adding more CPU, RAM, and Disk to the server. This way we are now handling 200 WPS.\n\n### More read traffic  \n\nIf we see nor reads then can also choose to add a Read Replica and divert some of the read traffic to this node, while the master node can take in 200 WPS.\n\n### Viral Growth\n\nSay, your product went viral and you now got 5x more load which means now you have to handle 1000 WPS. To achieve this you again scale it up vertically and handle the desired load.\n\n### Insane growth\n\nSay, you now cracked the PMF and are getting some really solid traction and need to handle 1500 WPS, and when you visit the database console you found out that it is not possible to vertically scale your database any further, so how do you handle 1500 WPS?\n\nThis is where the horizontal scaling comes into the picture.\n\n## Scaling the database horizontally\n\nWe know one database server can handle 1000 WPS, but we need to handle 1500 WPS, so we split the data into half and split it across two databases such that each database owns half of the data and all the writes for that data goes to that particular instance.\n\nThis way each server will get 750 WPS, which it can very easily handle, and owns 50% of the data. Thus by adding more database servers we handled 1500 WPS (more than what a single machine could handle)\n\n## Sharding and Partitioning\n\nEach database server in the above architecture is called a Shard while the data is said to be partitioned. Overall, a database is sharded and the data is partitioned.\n\n### Partitioned data on shards\n\nIt is possible to have more partitions and fewer shards and in that case, each shard will own multiple partitions. Say, we have 100GB of data and it is split into 5 partitions and we have 2 shards. One shard will be responsible for 3 partitions while the other for 2.\n\n## Advantages and Disadvantages\n\n### Advantages of Sharding\n\n- handle more reads and writes\n- increases overall storage capacity\n- overall high availability\n\n### Disadvantages of Sharding\n\n- sharding is operationally complex\n- cross-shard queries are super-expensive",
    "notes_gd": "https://drive.google.com/file/d/14RqKYjN2pgqYTaVB1DYlH4WjZ0A8XQ02/view?usp=sharing",
    "slug": "database-sharding-and-partitioning"
  },
  {
    "id": 106,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "CW4gVlU0xtU",
    "title": "Why, where, and when should we throttle or rate limit?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nIt is a common belief that a rate limiter is always external and is designed to prevent our systems from being abused by the external world, but this is not true. In this video, we understand what throttling is, why we need it in the first place and 5 use cases where external and internal rate limiters are super useful.\n\nOutline:\n00:00 Introduction\n02:56 What is Throttling?\n03:37 What rate limiter does when it gets a surge of requests?\n06:39 Why do we need a rate limiter?\n10:45 Usecase 1: Preventing catastrophic DDoS Attack\n12:20 Usecase 2: Gracefully handling a surge in legitimate users\n13:46 Usecase 3: Multi-tiered limits\n15:42 Usecase 4: Not overusing an expensive vendor\n16:48 Usecase 5: Streamlining deletes to protect an unprotected database",
    "img": "https://i.ytimg.com/vi/CW4gVlU0xtU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT5LMuVtaVYI03uXsc/giphy.gif",
    "duration": "19:5",
    "view_count": 5587,
    "like_count": 245,
    "comment_count": 20,
    "released_at": "2022-04-18",
    "gist": "## What is throttling?\n\nThrottling is a technique that ensures that the flow of the data or the requests being sent at the target machine/service/sub-system can be consumed at an acceptable rate.\n\nIt is a defensive measure and 3 possible reactions could be\n\n- slowing down the incoming requests\n- rejecting the surplus requests\n- ignoring the surplus requests\n\n## Why do we need throttling in the first place?\n\n- to prevent system abuse\n- to allow the amount of traffic we could handle\n- control the consumption cost\n- prevent cascading failures leading to a massive outage\n\n## Real-world use-cases for throttling\n\n### To prevent catastrophic DDoS attack\n\nWhen your service is under a DDoS attack the rate limiter acts as your first line of defense that could prevent the surplus request from reaching your system. It would only allow the requests to go through at the configured rate.\n\n### To gracefully handle a surge of users\n\nIt is possible that your product goes viral and now you are seeing a genuine surge in users. Upon getting a genuine surge in users, the stateful components like databases and caches crash which takes down the entire site.\n\nRate limiter in this case will help in preventing the entire site from going down; although some users would see some error, like 429- Too many requests- your product will continue to seamlessly work for the other set of users.\n\n### Multi-tiered limits\n\nSay, you are running a CICD company and offer 3 tiers of pricing- Tier 1 offers 200 minutes of build time, Tier 2 offers 1000 mins while Tier 3 offers unlimited build time. An internal rate limiter can keep track of the build times consumed by a customer and reject the requests once the limit is hit.\n\n### Ensure you are not over-consuming\n\nSay, we are consuming a super expensive third-party API and we want to ensure that we are not using it beyond a certain number otherwise the cost will shoot up. An internal rate limiter can keep a check on this to ensure the surplus request does not go through.\n\n### Not overwhelming an unprotected system\n\nHard deleting from a database is an expensive operation. If we are deleting a huge number of rows from the DB it may severely affect the performance of the DB and hence it is best done in a staggered way. An internal rate limiter can help us streamline the writing by spreading them uniformly across time.",
    "notes_gd": "https://drive.google.com/file/d/11lTCiIbRk0aRlEaebjSI1mU6g_hTAway/view?usp=sharing",
    "slug": "why-where-and-when-should-we-throttle-or-rate-limit"
  },
  {
    "id": 105,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "1r9bPisYaOQ",
    "title": "How to approach System Design?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nSystem Design is tricky but it does not have to be difficult - be it a technical discussion at your workplace or your next big interview. In this video, I share the two approaches that I have been using to design scalable systems in the last 10 years of my career. I will also share the 3 key pointers to remember while designing any system that would help you keep your discussion crisp and focused.\n\nOutline:\n00:00 Introduction\n02:41 What is System Design?\n06:02 The Spiral Approach to System Design\n07:27 The Incremental MVP Approach to System Design\n09:47 Key Pointers to remember during System Design",
    "img": "https://i.ytimg.com/vi/1r9bPisYaOQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/kQ3FSVoJrkYWk/giphy.gif",
    "duration": "13:55",
    "view_count": 14801,
    "like_count": 402,
    "comment_count": 15,
    "released_at": "2022-04-13",
    "gist": "System Design is tricky but it does not have to be difficult- be it a technical discussion at your workplace or your next big interview. Let's talk about how to approach System Design. Something I compiled from 10 years of my career.\n\n## What is System Design?\n\nSystem Design is all about translating and solving customer needs and business requirements into something tangible. The output system could be an application, a microservice, a library, or even hardware.\n\nThere are a couple of approaches that we can use to design any system out there. Picking one over the other depends on the company you work for and the flexibility it provides.\n\n## The Spiral Approach\n\nThe Spiral Approach pans like a spiral in which you start with some core that you are comfortable with (database, communication protocol, queue. etc) and build your system around it. Every single component you add to the design is something that you are pretty confident about and can proceed with the added complexities.\n\nFor example:\n\n- start with the database\n- then add LB and more servers\n- then add a queue for async processing\n- then other services\n- then add synchronous HTTP based communication between them\n\n## The Incremental MVP Approach\n\nIn the Incremental MVP-based approach we with a Day 0 design and then see how each component behaves at scale by dry-running it; after identifying the bottlenecks you fix them and re-iterate. You stop the iteration once you are happy with the final product. This kind of approach is typically seen in startups where they do not want to invest in architecture and quickly roll out features.\n\nFor example:\n\n- start with Day 0 architecture of users, API servers, and DB\n- then you add LB and more API servers\n- then you add Read Replica on DB to support more reads\n- then you split the service into a couple of microservices\n- then you partition the DB to handle more scale\n\n## 3 key pointers while designing systems\n\n- Every system is infinitely buildable, hence fence it well\n- Seek clarifications from your seniors\n- Ask critical questions that challenge the design decisions\n",
    "notes_gd": "https://drive.google.com/file/d/185a6688TxLDLXlrDfn2l4ODLSR2m1xLL/view?usp=sharing",
    "slug": "how-to-approach-system-design"
  },
  {
    "id": 102,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "m6DtqSb1BDM",
    "title": "Implementing Idempotence in a Payments Microservice",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nIdempotence is an extremely critical property that we must consider while implementing an API or designing a microservice. The situation becomes even more critical when the money is involved - ensuring no matter how many times the user or internal service retries, the amount is transferred just once between the two users in one transaction.\n\nThis video looks at idempotence, why there is even a need for it, and, more importantly, one common implementation approach commonly observed in payments services.\n\nOutline:\n\n00:00 What is Idempotence?\n02:32 Examples where Idempotence is relevant\n04:06 Why do we even need to retry?\n07:18 Implementation Approach 1: Do not retry\n09:45 Implementation Approach 1: Check and Update",
    "img": "https://i.ytimg.com/vi/m6DtqSb1BDM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/j3x5hjUoXIesM/giphy.gif",
    "duration": "16:36",
    "view_count": 9633,
    "like_count": 453,
    "comment_count": 27,
    "released_at": "2022-04-06",
    "gist": "Idempotence is executing the same action multiple times, but the result is as if the operation was applied just once.\n\nExample: Double tapping a post multiple times on Instagram does not increase the like count. It just increases the first time and by 1.\n\nIdempotence becomes extremely critical when money is involved. If A wants to transfer money to B, the transfer should happen just once. If due for any reason, the payment is implicitly retried, the funds will be deducted twice, which is unacceptable.\n\n### Why would a transaction repeat?\n\nBefore we talk about idempotence, it is important to understand why it would repeat in the first place.\n\nConsider a situation where the payments service initiated a payment with a Payment Gateway, the money got deducted, but the payments service did not get the response. This would make the Payments service retry the API call, which would lead to a double deduction.\n\n## Implementing idempotence\n\nCheck and Update: Weave everything with a single ID.\n\nThe idea is to retry only after checking if the payment is processed or not. But how do we do this? The implementation is pretty simple- a global payment ID that weaves all the services and parties together.\n\nThe flow is:\n\n1. Payments service calls the PG and generates a unique Payment ID\n2. Payments service passes this ID to the end-user and all involved services\n3. Payments service initiates the payment with Payment Gateway with this ID specifying the transfer between A and B\n4. If there are any failures, the Payment service retries and in that request specifies the Payment ID\n5. Using the payment ID, the payment gateway checks if the transfer was indeed done or not and would transfer only when it was not done\n\nAlthough we talked about the Payments service here, this approach of implementing idempotence is pretty common across all the use cases. The core idea is to have a single ID (acting as the Idempotence Key) weaving all the involved services and parties together.",
    "notes_gd": "https://drive.google.com/file/d/1Zyt8qN11IiAZJKrdan4wi1c5J6n_eAyU/view?usp=sharing",
    "slug": "implementing-idempotence-in-a-payments-microservice"
  },
  {
    "id": 103,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "8S4k7k_f9Sk",
    "title": "Sending payload in an HTTP GET request",
    "description": "Can we send data in an HTTP GET request? Most people think, No. The truth is, we can send the data in the request payload of an HTTP GET request so long as our webserver can understand it.\n\nIn this video, we go through the HTTP 1.1 specification and see what it says about the GET requests, write a simple Flask application to see that we can indeed process the payload of a GET request if we want to, and, more importantly, go through a real-world example where it was essential to send data in the request payload.\n\nOutline:\n\n00:00 HTTP GET Request\n01:53 What does HTTP 1.1 specification say?\n05:38 Request payload in Python Flask\n07:18 ElasticSearch using request payload for search\n10:40 When to use HTTP request payload in a GET request",
    "img": "https://i.ytimg.com/vi/8S4k7k_f9Sk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l2Je53k7wnV8sQZOg/giphy.gif",
    "duration": "12:",
    "view_count": 4474,
    "like_count": 157,
    "comment_count": 22,
    "released_at": "2022-04-04",
    "gist": "It is a common myth that we could not pass the request body in the HTTP GET request. HTTP 1.1 specification neither enforces nor suggests this behavior.\n\nThis means it is up to implementing the application web servers- Flask, uWSGI, etc.- to see if it parses the request body in the HTTP GET request. To do this, just check the request object you would be getting in your favorite framework.\n\n### What can we do with this information?\n\nSay you are modeling an analytics service like Google Analytics in which you are exposing an endpoint that returns you the data point depending on the requirements. The requirements specified here could be a large, complex JSON.\n\nPassing this query in the URL of the GET request as a query param is not convenient as it would require you to serialize and escape the JSON string before passing.\n\nThis is a perfect use case where the complex JSON query can be passed as a request body in the HTTP GET request, giving a good user experience.\n\n### So, does any popular tool uses this convention?\n\nYes. ElasticSearch- one of the most popular search utilities, uses this convention.\n\nThe search endpoint of ElasticSearch is a GET endpoint where the complex search queries in JSON format are sent in the request payload.\n",
    "notes_gd": "https://drive.google.com/file/d/1JwVEh9EG0ZGts-VePXNlIE1e8kivdHbM/view?usp=sharing",
    "slug": "sending-payload-in-an-http-get-request"
  },
  {
    "id": 101,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "dTPh3KcKPYo",
    "title": "Scaling Taxonomy Service and Database",
    "description": "In this video, we do the high-level design of Udemy's Taxonomy Service and see how to scale databases in general.\n\nOutline:\n\n0:00 Recap Taxonomy DB Design\n0:57 Bird's eye view\n2:30 Within the Taxonomy Service\n3:50 Scaling the relational database\n5:32 High-Level Architecture\n7:20 Join my System Design Course\n7:57 Like, Share, and Subscribe",
    "img": "https://i.ytimg.com/vi/dTPh3KcKPYo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/YxlUxrYGw2w9y/giphy.gif",
    "duration": "8:38",
    "view_count": 1051,
    "like_count": 30,
    "comment_count": 1,
    "released_at": "2021-05-02",
    "gist": "",
    "notes_gd": "",
    "slug": "scaling-taxonomy-service-and-database"
  },
  {
    "id": 100,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "4_jlmX_oB94",
    "title": "Designing Taxonomy on SQL",
    "description": "In this video, we design the Taxonomy of Udemy on top of a SQL-based Relational DB. We learn how to model any taxonomy on relational DB, why to choose SQL over NoSQL, designing the schema, deciding Indexes, and writing very interesting SQL queries.\n\nLink to the article: https://arpitbhayani.me/blogs/udemy-sql-taxonomy\n\nChapters:\n0:00 Udemy's Taxonomy\n1:40 A bad Database Design for Taxonomy\n3:29 Good Database Design for Taxonomy\n7:34 Is Relational DB a good choice to store taxonomy?\n13:04 Deciding Indexes\n15:26 Get topic by ID\n15:56 Get topic path\n23:00 Get child-topics\n25:05 Get Top Taxonomy in Single Query\n31:05 Summarizing Indexes\n31:52 Fun exercise to explore SQL\n33:55 What next in System Design",
    "img": "https://i.ytimg.com/vi/4_jlmX_oB94/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/6XuA2WMKsgqS4/giphy.gif",
    "duration": "35:15",
    "view_count": 4730,
    "like_count": 157,
    "comment_count": 10,
    "released_at": "2021-04-19",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-taxonomy-on-sql"
  },
  {
    "id": 104,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "m_7xSIhxZL8",
    "title": "End-to-end Message Encryption",
    "description": "In this video, we find how to implement a very very simple version of end-to-end WhatsApp-like message encryption. This is not even 1% of how WhatsApp does it but it gives a fair amount of idea on how we could do it.\n\nHow do WhatsApp, Facebook, Signal actually do it? Answer: The Signal Protocol\n\nWe do it with a very simple Public Key Cryptography and a Digital Signature.",
    "img": "https://i.ytimg.com/vi/m_7xSIhxZL8/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/pYfYNJbEftHOfIpoBC/giphy.gif",
    "duration": "15:57",
    "view_count": 6047,
    "like_count": 244,
    "comment_count": 17,
    "released_at": "2021-04-05",
    "gist": "",
    "notes_gd": "",
    "slug": "end-to-end-message-encryption"
  },
  {
    "id": 99,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "yt_video_id": "kIP8L-CSl2Y",
    "title": "Designing Notifications Service for Instagram",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nThis video is a snippet from my System Design Masterclass and in it, we are discussing How Instagram Scales its notification systems. The primary challenge of such a system is doing a real fast fanout and we discuss how to do this very efficiently.",
    "img": "https://i.ytimg.com/vi/kIP8L-CSl2Y/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/huyZxIJvtqVeRp7QcS/giphy.gif",
    "duration": "37:18",
    "view_count": 39155,
    "like_count": 869,
    "comment_count": 70,
    "released_at": "2021-04-01",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-notifications-service-for-instagram"
  }
]