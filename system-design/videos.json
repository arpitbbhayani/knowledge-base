[
  {
    "id": 142,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "W6HANd8c9t4",
    "title": "An in-depth introduction to Blue Green Deployments",
    "description": "Deployments are a pain if we are unsure about our release changes. But sometimes even if we know our changes well, something weird could happen in the infra that would fail your deployment and put your infra in an inconsistent state.\n\nSo, is there a way to address this? What if we have a place to deploy our changes and validate them before they hit production; and have a quick way to roll back if something goes wrong?\n\nThis is the core idea behind a Blue-Green Deployment\n\nIn this video, we take an in-depth look into a blue-green deployment pattern, understand why we need them, what problem it addresses, learn how they are implemented, talk about its benefits and challenges, and conclude with some points to remember when you adopt this deployment pattern.\n\nOutline:\n\n00:00 Agenda\n03:13 An introduction to Blue-Green Deployments\n06:36 Why do we need Blue-Green Deployments\n09:12 How Blue-Green Deployment is implemented?\n13:26 Pros of having a Blue-Green Deployment\n19:49 Challenges in having a Blue-Green Deployment\n25:36 When to use Blue Green Deployments\n26:39 Key points to remember while adopting Blue Green Deployment",
    "img": "https://i.ytimg.com/vi/W6HANd8c9t4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oGRFnT4RyU4ayoUFi/giphy.gif",
    "duration": "28:55",
    "view_count": 314,
    "like_count": 22,
    "comment_count": 7,
    "released_at": "2022-05-18",
    "gist": "Blue Green Deployment is a deployment pattern that reduces the downtime during deployment by running two identical production setups called - Blue and Green.\n\nDuring deployment when we reboot the API servers there are chances that the incoming request fail because the server is unresponsive for a short period. Also, it might happen that the release had a major bug and we need a quick rollback.\n\nHow can we achieve both of them in one shot? The answer is Blue Green Deployment.\n\n## Implemention\n\nBlue Green deployment is implemented by having a separate fleet of infrastructure for the old version - Blue and the new version - Green. The new infrastructure is identical to the old one.\n\nThe deployment flow\n\n1. the new deployment artifact is tested and kept ready to be deployed\n2. a parallel infrastructure is set up identical to the existing\n3. the new version is deployed on the new fleet - Green\n4. the correctness of the setup is validated\n5. the proxy is re-configured to now forward 100% of traffic from the Blue (old) setup to the Green (new) setup\n6. a final sanity test is run on the new fleet\n7. the blue fleet is now shut down\n\n## Pros of Blue Green Deployment\n\n1. rollbacks are just a config change and hence quick\n2. downtime during deployment is minimal\n3. deployment is just a flip of a switch\n4. disaster recovery is simple given we already have the automation to build a parallel setup\n5. deployments can now happen during the working hours\n6. debugging a failed deployment is simple as we have the infrastructure with the debug information handy\n\n## Possible challenges\n\n1. during the deployment the infrastructure cost shoots 2x\n2. the stateful application would need to rebuild the state on new servers\n3. the database would have to be shared between the fleets\n4. any schema migration on the database needs to be backward and forward compatible\n5. the API responses have to be forward and backward compatible\n6. setting up this deployment strategy for the first time is difficult\n\n## When to use Blue Green Deployment?\n\n- when you need zero downtime deployment\n- your infrastructure can tolerate 100% traffic switch\n- you can bear the 2x cost of infrastructure during deployment\n\n## Points to remember\n\n- have a solid automation test suite to validate the correctness\n- ensure forward and backward compatibility of API and schema changes\n- infra cost will shoot up hence minimizing the time for which you are running 2x infra",
    "notes_gd": "https://drive.google.com/file/d/1jSowz0IW8kD4Fjrv2fsE-ygHVTaZto1d/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-blue-green-deployments"
  },
  {
    "id": 141,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "nnseeKxovaM",
    "title": "An in-depth introduction to Canary Deployments",
    "description": "Deployments are stressful; what if something goes wrong? What if you forgot to handle an edge case that was also missed during the unit test, integration test, or an internal QA iteration.\n\nPutting such code into production can take down your entire infrastructure and could cause a massive outage. In order or handle such a situation gracefully and provide us with an early warning about something's wrong we have Canary Deployment.\n\nIn this video, we take an in-depth look into canary deployments, learn why canary deployments are called canary deployments, and understand how they are actually implemented, talk about the pros and cons of this deployment pattern, and conclude with a one really solid use case where you absolutely need them.\n\nOutline:\n\n00:00 Agenda\n03:05 Introduction to Canary Deployment\n06:06 Why Canary Deployments are called Canary Deployments?\n08:04 How to implement Canary Deployments?\n10:03 Pros of having Canary Deployments\n16:21 How to pick servers and users for a rollout?\n19:08 Cons of having Canary Deployments\n21:25 When we absolutely need Canary Deployment",
    "img": "https://i.ytimg.com/vi/nnseeKxovaM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NAVQDibk6Sesg/giphy.gif",
    "duration": "23:53",
    "view_count": 1274,
    "like_count": 88,
    "comment_count": 14,
    "released_at": "2022-05-16",
    "gist": "Canary Deployments is a deployment pattern that rolls out the changes to a limited set of users before doing it for 100%.\n\nWe compare the vitals side-by-side from the old setup and the canary servers to ensure everything is as expected. If all is okay, then we incrementally roll out to a wider audience. If not, we immediately roll back our changes from the canaries.\n\nCanary Deployment thus acts as an Early Warning Indicator to prevent a potential outage.\n\n## Why canary deployment is named canary deployment?\n\nIn 1920, coal miners used to carry caged canaries with them. If the gases in the mines were highly toxic the canaries would die and that alerted the miners to evacuate immediately, thus saving their lives.\n\nIn canary deployment, the canary servers are the caged canaries that alert us when anything goes wrong.\n\n## Implementing canary deployment\n\nCanary deployments are implemented through a setup where a few servers serve the newer version while the reset serves the old version.\n\nA router (load balancer / API gateway) is placed in front of the setup and it routes some traffic to the new fleet while the other requests continue to go to the old one.\n\n## Pros of Canary Deployment\n\n- we test our changes on real traffic\n- rollbacks are much faster\n- if something's wrong only a fraction of users are affected\n- zero downtime deployments\n- we can gradually roll out the changes to users\n- we can power A/B Testing\n\n## Cons of Canary Deployment\n\n- engineers will get habituated to testing things in production\n- a little complex setup\n- a parallel monitoring setup is required to compare vitals side-by-side\n\n## Selecting users/servers for canary deployment?\n\nThe selection is use-case specific, but the common strategies are:\n\n- geographical selection to power regional roll-out\n- create specific user cohorts eg: beta users\n- random selection\n\n## When we absolutely need Canary Deployments\n\nSay you own the Auth service that is written in Java and you chose to re-write it in - Golang. When taking it to production, you would NOT want to make a direct 100% roll-out given that the new codebase might have a lot of bugs.\n\nThis is where canary is super-helpful when we a fraction of servers serving requests from Golang server while others from the existing setup. We now forward 5% traffic to the new ones and observe how it reacts.\n\nOnce we have enough confidence in the newer setup, we increase the roll-out fraction to 15%, 50%, 75%, and eventually 100%. Canary setup thus gives us a seamless transition from our old server to a newer one.",
    "notes_gd": "https://drive.google.com/file/d/1JJD_Pa9AkUvhaZ7Dzwd4sQiGdC8nPn5t/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-canary-deployments"
  },
  {
    "id": 135,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "oiZH5U_a0pg",
    "title": "Introduction to Serverless Computing and Architecture",
    "description": "Serverless Computing is one of the hottest topics of discussion today, but the term \"serverless\" is slightly misleading and it does not mean that your code will not need a server to run. We have to be extremely cautious while deciding on adopting serverless for our use case, but it is not something that fits all the use cases.\n\nIn this video, we talk about what serverless computing is, see why it was built in the first place, learn about 5 real-world use-cases that become super-efficient with serverless architecture, understand the advantages and more importantly, the disadvantages of adopting it, and conclude with acknowledging when to use and when not to use this computation pattern.\n\nOutline:\n\n00:00 Agenda\n03:01 Need and the idea of the Serverless Computing\n11:16 Usecase 1: Chatbots\n13:57 Usecase 2: Online Judge\n16:27 Usecase 3: Vending Machines\n18:00 Usecase 4: CRON Jobs\n19:54 Usecase 5: Batch and Stream Processing\n22:16 Advantages of Serverless Computing and Architecture\n26:21 Disadvantages of Serverless Computing and Architecture\n31:37 When NOT to use Serverless\n34:00 When to use Serverless",
    "img": "https://i.ytimg.com/vi/oiZH5U_a0pg/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/5k1VkABjw5wQq0PNEU/giphy.gif",
    "duration": "36:23",
    "view_count": 1687,
    "like_count": 100,
    "comment_count": 12,
    "released_at": "2022-05-02",
    "gist": "Serverless is a cost-efficient way to host your APIs and it forms the crux of systems like Chatbots and Online Judge.\n\nServerless does not mean that your code will not run on the server; it means that you do not manage, maintain, access, or scale the server your code is running on.\n\nThe traditional way to host APIs is by spinning up a server with some RAM, and CPU. Say the resources make your server handle 1000 RPS, but you are getting 1000 RPS only 1% of the time which means for the other 99% you are overprovisioned.\n\nSo, what if there was an Infrastructure that\n\n- scales up and down as per the traffic\n- is billed per execution\n- is self-managed maintained and fault-tolerant\n\nThese requirements gave rise to Serverless Computing.\n\n## Real-world applications\n\n### Chatbot\n\nSay, we build a Slack chatbot that responds with the Holiday list when someone messages `holidays` . The traffic for this utility is going to be insignificant, and keeping a server running the whole time is a waste. This is best modeled on Serverless which is invoked on receiving a message.\n\n### Online Judge\n\nEvery submission can be evaluated on a serverless function and results can be updated in a database. Serverless gives you isolation out of the box and keeps the cost to a bare minimum. It would also seamlessly handle the surge in submissions.\n\n### Vending Machine\n\nUpon purchase, the Vending machine would need to update the main database, and the APIs for that could be hosted on Serverless. Given the traffic is low and bursty, Serverless would help us keep the cost down.\n\n### Scheduled DB Backups\n\nSchedule daily DB backups on the Serverless function instead of running a separate crontab server just to trigger the backup.\n\n### Batch and Stream Processing\n\nUse serverless and invoke the function every time a message is pushed on the broker making the system reactive instead of poll-based.\n\n## Advantages\n\n- No need to manage and scale the infra\n- The cost is 0 when you do not get any traffic\n- Scale is out of the box; so no capacity planning is needed\n\n## Disadvantages\n\n- Takes time to serve the first request as the underlying infra might boot up\n- The execution has a max timeout, so your job should complete within the limit\n- Debugging is a challenge\n- You are locked in on the vendor you chose\n\n## When NOT to use Serverless\n\n- Load, usage, and traffic pattern is consistent\n- Execution will go beyond the max timeout\n- You need multi-tenancy\n\n## When to use Serverless\n\n- Quick build, prototype, and deploy the changes\n- Usecase is lightweight\n- Traffic is bursty",
    "notes_gd": "https://drive.google.com/file/d/1sZShE0r41XcFa2gEPW1RS_YTaR3tC-zH/view?usp=sharing",
    "slug": "introduction-to-serverless-computing-and-architecture"
  },
  {
    "id": 131,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "wXvljefXyEo",
    "title": "Database Sharding and Partitioning",
    "description": "Sharding and partitioning come in very handy when we want to scale our systems. These concepts operate on the database and help us improve the overall throughput and availability of the system.\n\nIn this video, we take a detailed look into how a database is scaled and evolved through different stages, what sharding and partitioning are, understand the difference between them, see at which stage should we introduce this complexity, and a few advantages and disadvantages of adopting them.\n\nOutline:\n\n00:00 Introduction and Agenda\n03:05 How a database is progressively scaled?\n08:10 Scaling beyond the limit of vertical scaling\n11:57 Sharding vs Partitioning\n12:43 Example of Data Partitioning\n17:15 Sharding and Partitioning together\n20:20 Advantages and Disadvantages of Sharding and Partitioning",
    "img": "https://i.ytimg.com/vi/wXvljefXyEo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif",
    "duration": "23:53",
    "view_count": 2051,
    "like_count": 125,
    "comment_count": 12,
    "released_at": "2022-04-25",
    "gist": "Sharding and partitioning come in very handy when we want to scale our systems. Let's talk about these concepts in detail.\n\n## How is the database scaled?\n\nA database server is just a database process (like MySQL, MongoDB) running on a virtual server like EC2. Now when we put our database in production it starts getting from real good traction, say 100 writes per second (WPS).\n\n### Steady user growth\n\nSay, your product started getting some traction, and we find that the database is not able to handle the load, we scale it up by adding more CPU, RAM, and Disk to the server. This way we are now handling 200 WPS.\n\n### More read traffic  \n\nIf we see nor reads then can also choose to add a Read Replica and divert some of the read traffic to this node, while the master node can take in 200 WPS.\n\n### Viral Growth\n\nSay, your product went viral and you now got 5x more load which means now you have to handle 1000 WPS. To achieve this you again scale it up vertically and handle the desired load.\n\n### Insane growth\n\nSay, you now cracked the PMF and are getting some really solid traction and need to handle 1500 WPS, and when you visit the database console you found out that it is not possible to vertically scale your database any further, so how do you handle 1500 WPS?\n\nThis is where the horizontal scaling comes into the picture.\n\n## Scaling the database horizontally\n\nWe know one database server can handle 1000 WPS, but we need to handle 1500 WPS, so we split the data into half and split it across two databases such that each database owns half of the data and all the writes for that data goes to that particular instance.\n\nThis way each server will get 750 WPS, which it can very easily handle, and owns 50% of the data. Thus by adding more database servers we handled 1500 WPS (more than what a single machine could handle)\n\n## Sharding and Partitioning\n\nEach database server in the above architecture is called a Shard while the data is said to be partitioned. Overall, a database is sharded and the data is partitioned.\n\n### Partitioned data on shards\n\nIt is possible to have more partitions and fewer shards and in that case, each shard will own multiple partitions. Say, we have 100GB of data and it is split into 5 partitions and we have 2 shards. One shard will be responsible for 3 partitions while the other for 2.\n\n## Advantages and Disadvantages\n\n### Advantages of Sharding\n\n- handle more reads and writes\n- increases overall storage capacity\n- overall high availability\n\n### Disadvantages of Sharding\n\n- sharding is operationally complex\n- cross-shard queries are super-expensive",
    "notes_gd": "https://drive.google.com/file/d/14RqKYjN2pgqYTaVB1DYlH4WjZ0A8XQ02/view?usp=sharing",
    "slug": "database-sharding-and-partitioning"
  },
  {
    "id": 106,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "CW4gVlU0xtU",
    "title": "Why, where, and when should we throttle or rate limit?",
    "description": "It is a common belief that a rate limiter is always external and is designed to prevent our systems from being abused by the external world, but this is not true. In this video, we understand what throttling is, why we need it in the first place and 5 use cases where external and internal rate limiters are super useful.\n\nOutline:\n00:00 Introduction\n02:56 What is Throttling?\n03:37 What rate limiter does when it gets a surge of requests?\n06:39 Why do we need a rate limiter?\n10:45 Usecase 1: Preventing catastrophic DDoS Attack\n12:20 Usecase 2: Gracefully handling a surge in legitimate users\n13:46 Usecase 3: Multi-tiered limits\n15:42 Usecase 4: Not overusing an expensive vendor\n16:48 Usecase 5: Streamlining deletes to protect an unprotected database",
    "img": "https://i.ytimg.com/vi/CW4gVlU0xtU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/4KFjJdWCyLZ743mWIg/giphy.gif",
    "duration": "19:5",
    "view_count": 1106,
    "like_count": 78,
    "comment_count": 12,
    "released_at": "2022-04-18",
    "gist": "## What is throttling?\n\nThrottling is a technique that ensures that the flow of the data or the requests being sent at the target machine/service/sub-system can be consumed at an acceptable rate.\n\nIt is a defensive measure and 3 possible reactions could be\n\n- slowing down the incoming requests\n- rejecting the surplus requests\n- ignoring the surplus requests\n\n## Why do we need throttling in the first place?\n\n- to prevent system abuse\n- to allow the amount of traffic we could handle\n- control the consumption cost\n- prevent cascading failures leading to a massive outage\n\n## Real-world use-cases for throttling\n\n### To prevent catastrophic DDoS attack\n\nWhen your service is under a DDoS attack the rate limiter acts as your first line of defense that could prevent the surplus request from reaching your system. It would only allow the requests to go through at the configured rate.\n\n### To gracefully handle a surge of users\n\nIt is possible that your product goes viral and now you are seeing a genuine surge in users. Upon getting a genuine surge in users, the stateful components like databases and caches crash which takes down the entire site.\n\nRate limiter in this case will help in preventing the entire site from going down; although some users would see some error, like 429- Too many requests- your product will continue to seamlessly work for the other set of users.\n\n### Multi-tiered limits\n\nSay, you are running a CICD company and offer 3 tiers of pricing- Tier 1 offers 200 minutes of build time, Tier 2 offers 1000 mins while Tier 3 offers unlimited build time. An internal rate limiter can keep track of the build times consumed by a customer and reject the requests once the limit is hit.\n\n### Ensure you are not over-consuming\n\nSay, we are consuming a super expensive third-party API and we want to ensure that we are not using it beyond a certain number otherwise the cost will shoot up. An internal rate limiter can keep a check on this to ensure the surplus request does not go through.\n\n### Not overwhelming an unprotected system\n\nHard deleting from a database is an expensive operation. If we are deleting a huge number of rows from the DB it may severely affect the performance of the DB and hence it is best done in a staggered way. An internal rate limiter can help us streamline the writing by spreading them uniformly across time.",
    "notes_gd": "https://drive.google.com/file/d/11lTCiIbRk0aRlEaebjSI1mU6g_hTAway/view?usp=sharing",
    "slug": "why-where-and-when-should-we-throttle-or-rate-limit"
  },
  {
    "id": 105,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "1r9bPisYaOQ",
    "title": "How to approach System Design?",
    "description": "System Design is tricky but it does not have to be difficult - be it a technical discussion at your workplace or your next big interview. In this video, I share the two approaches that I have been using to design scalable systems in the last 10 years of my career. I will also share the 3 key pointers to remember while designing any system that would help you keep your discussion crisp and focused.\n\nOutline:\n00:00 Introduction\n02:41 What is System Design?\n06:02 The Spiral Approach to System Design\n07:27 The Incremental MVP Approach to System Design\n09:47 Key Pointers to remember during System Design",
    "img": "https://i.ytimg.com/vi/1r9bPisYaOQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/kQ3FSVoJrkYWk/giphy.gif",
    "duration": "13:55",
    "view_count": 1724,
    "like_count": 94,
    "comment_count": 5,
    "released_at": "2022-04-13",
    "gist": "System Design is tricky but it does not have to be difficult- be it a technical discussion at your workplace or your next big interview. Let's talk about how to approach System Design. Something I compiled from 10 years of my career.\n\n## What is System Design?\n\nSystem Design is all about translating and solving customer needs and business requirements into something tangible. The output system could be an application, a microservice, a library, or even hardware.\n\nThere are a couple of approaches that we can use to design any system out there. Picking one over the other depends on the company you work for and the flexibility it provides.\n\n## The Spiral Approach\n\nThe Spiral Approach pans like a spiral in which you start with some core that you are comfortable with (database, communication protocol, queue. etc) and build your system around it. Every single component you add to the design is something that you are pretty confident about and can proceed with the added complexities.\n\nFor example:\n\n- start with the database\n- then add LB and more servers\n- then add a queue for async processing\n- then other services\n- then add synchronous HTTP based communication between them\n\n## The Incremental MVP Approach\n\nIn the Incremental MVP-based approach we with a Day 0 design and then see how each component behaves at scale by dry-running it; after identifying the bottlenecks you fix them and re-iterate. You stop the iteration once you are happy with the final product. This kind of approach is typically seen in startups where they do not want to invest in architecture and quickly roll out features.\n\nFor example:\n\n- start with Day 0 architecture of users, API servers, and DB\n- then you add LB and more API servers\n- then you add Read Replica on DB to support more reads\n- then you split the service into a couple of microservices\n- then you partition the DB to handle more scale\n\n## 3 key pointers while designing systems\n\n- Every system is infinitely buildable, hence fence it well\n- Seek clarifications from your seniors\n- Ask critical questions that challenge the design decisions\n",
    "notes_gd": "https://drive.google.com/file/d/185a6688TxLDLXlrDfn2l4ODLSR2m1xLL/view?usp=sharing",
    "slug": "how-to-approach-system-design"
  },
  {
    "id": 102,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "m6DtqSb1BDM",
    "title": "Implementing Idempotence in a Payments Microservice",
    "description": "Idempotence is an extremely critical property that we must consider while implementing an API or designing a microservice. The situation becomes even more critical when the money is involved - ensuring no matter how many times the user or internal service retries, the amount is transferred just once between the two users in one transaction.\n\nThis video looks at idempotence, why there is even a need for it, and, more importantly, one common implementation approach commonly observed in payments services.\n\nOutline:\n\n00:00 What is Idempotence?\n02:32 Examples where Idempotence is relevant\n04:06 Why do we even need to retry?\n07:18 Implementation Approach 1: Do not retry\n09:45 Implementation Approach 1: Check and Update",
    "img": "https://i.ytimg.com/vi/m6DtqSb1BDM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/j3x5hjUoXIesM/giphy.gif",
    "duration": "16:36",
    "view_count": 1822,
    "like_count": 128,
    "comment_count": 19,
    "released_at": "2022-04-06",
    "gist": "Idempotence is executing the same action multiple times, but the result is as if the operation was applied just once.\n\nExample: Double tapping a post multiple times on Instagram does not increase the like count. It just increases the first time and by 1.\n\nIdempotence becomes extremely critical when money is involved. If A wants to transfer money to B, the transfer should happen just once. If due for any reason, the payment is implicitly retried, the funds will be deducted twice, which is unacceptable.\n\n### Why would a transaction repeat?\n\nBefore we talk about idempotence, it is important to understand why it would repeat in the first place.\n\nConsider a situation where the payments service initiated a payment with a Payment Gateway, the money got deducted, but the payments service did not get the response. This would make the Payments service retry the API call, which would lead to a double deduction.\n\n## Implementing idempotence\n\nCheck and Update: Weave everything with a single ID.\n\nThe idea is to retry only after checking if the payment is processed or not. But how do we do this? The implementation is pretty simple- a global payment ID that weaves all the services and parties together.\n\nThe flow is:\n\n1. Payments service calls the PG and generates a unique Payment ID\n2. Payments service passes this ID to the end-user and all involved services\n3. Payments service initiates the payment with Payment Gateway with this ID specifying the transfer between A and B\n4. If there are any failures, the Payment service retries and in that request specifies the Payment ID\n5. Using the payment ID, the payment gateway checks if the transfer was indeed done or not and would transfer only when it was not done\n\nAlthough we talked about the Payments service here, this approach of implementing idempotence is pretty common across all the use cases. The core idea is to have a single ID (acting as the Idempotence Key) weaving all the involved services and parties together.",
    "notes_gd": "https://drive.google.com/file/d/1Zyt8qN11IiAZJKrdan4wi1c5J6n_eAyU/view?usp=sharing",
    "slug": "implementing-idempotence-in-a-payments-microservice"
  },
  {
    "id": 103,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "8S4k7k_f9Sk",
    "title": "Sending payload in an HTTP GET request",
    "description": "Can we send data in an HTTP GET request? Most people think, No. The truth is, we can send the data in the request payload of an HTTP GET request so long as our webserver can understand it.\n\nIn this video, we go through the HTTP 1.1 specification and see what it says about the GET requests, write a simple Flask application to see that we can indeed process the payload of a GET request if we want to, and, more importantly, go through a real-world example where it was essential to send data in the request payload.\n\nOutline:\n\n00:00 HTTP GET Request\n01:53 What does HTTP 1.1 specification say?\n05:38 Request payload in Python Flask\n07:18 ElasticSearch using request payload for search\n10:40 When to use HTTP request payload in a GET request",
    "img": "https://i.ytimg.com/vi/8S4k7k_f9Sk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l2Je53k7wnV8sQZOg/giphy.gif",
    "duration": "12:",
    "view_count": 2920,
    "like_count": 129,
    "comment_count": 23,
    "released_at": "2022-04-04",
    "gist": "It is a common myth that we could not pass the request body in the HTTP GET request. HTTP 1.1 specification neither enforces nor suggests this behavior.\n\nThis means it is up to implementing the application web servers- Flask, uWSGI, etc.- to see if it parses the request body in the HTTP GET request. To do this, just check the request object you would be getting in your favorite framework.\n\n### What can we do with this information?\n\nSay you are modeling an analytics service like Google Analytics in which you are exposing an endpoint that returns you the data point depending on the requirements. The requirements specified here could be a large, complex JSON.\n\nPassing this query in the URL of the GET request as a query param is not convenient as it would require you to serialize and escape the JSON string before passing.\n\nThis is a perfect use case where the complex JSON query can be passed as a request body in the HTTP GET request, giving a good user experience.\n\n### So, does any popular tool uses this convention?\n\nYes. ElasticSearch- one of the most popular search utilities, uses this convention.\n\nThe search endpoint of ElasticSearch is a GET endpoint where the complex search queries in JSON format are sent in the request payload.\n",
    "notes_gd": "https://drive.google.com/file/d/1JwVEh9EG0ZGts-VePXNlIE1e8kivdHbM/view?usp=sharing",
    "slug": "sending-payload-in-an-http-get-request"
  },
  {
    "id": 101,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "dTPh3KcKPYo",
    "title": "Scaling Taxonomy Service and Database",
    "description": "In this video, we do the high-level design of Udemy's Taxonomy Service and see how to scale databases in general.\n\nOutline:\n\n0:00 Recap Taxonomy DB Design\n0:57 Bird's eye view\n2:30 Within the Taxonomy Service\n3:50 Scaling the relational database\n5:32 High-Level Architecture\n7:20 Join my System Design Course\n7:57 Like, Share, and Subscribe",
    "img": "https://i.ytimg.com/vi/dTPh3KcKPYo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/YxlUxrYGw2w9y/giphy.gif",
    "duration": "8:38",
    "view_count": 898,
    "like_count": 27,
    "comment_count": 1,
    "released_at": "2021-05-02",
    "gist": "",
    "notes_gd": "",
    "slug": "scaling-taxonomy-service-and-database"
  },
  {
    "id": 100,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "4_jlmX_oB94",
    "title": "Designing Taxonomy on SQL",
    "description": "In this video, we design the Taxonomy of Udemy on top of a SQL-based Relational DB. We learn how to model any taxonomy on relational DB, why to choose SQL over NoSQL, designing the schema, deciding Indexes, and writing very interesting SQL queries.\n\nLink to the article: https://arpitbhayani.me/blogs/udemy-sql-taxonomy\n\nChapters:\n0:00 Udemy's Taxonomy\n1:40 A bad Database Design for Taxonomy\n3:29 Good Database Design for Taxonomy\n7:34 Is Relational DB a good choice to store taxonomy?\n13:04 Deciding Indexes\n15:26 Get topic by ID\n15:56 Get topic path\n23:00 Get child-topics\n25:05 Get Top Taxonomy in Single Query\n31:05 Summarizing Indexes\n31:52 Fun exercise to explore SQL\n33:55 What next in System Design",
    "img": "https://i.ytimg.com/vi/4_jlmX_oB94/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/6XuA2WMKsgqS4/giphy.gif",
    "duration": "35:15",
    "view_count": 1730,
    "like_count": 68,
    "comment_count": 8,
    "released_at": "2021-04-19",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-taxonomy-on-sql"
  },
  {
    "id": 104,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "m_7xSIhxZL8",
    "title": "End-to-end Message Encryption",
    "description": "In this video, we find how to implement a very very simple version of end-to-end WhatsApp-like message encryption. This is not even 1% of how WhatsApp does it but it gives a fair amount of idea on how we could do it.\n\nHow do WhatsApp, Facebook, Signal actually do it? Answer: The Signal Protocol\n\nWe do it with a very simple Public Key Cryptography and a Digital Signature.",
    "img": "https://i.ytimg.com/vi/m_7xSIhxZL8/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/pYfYNJbEftHOfIpoBC/giphy.gif",
    "duration": "15:57",
    "view_count": 4097,
    "like_count": 190,
    "comment_count": 12,
    "released_at": "2021-04-05",
    "gist": "",
    "notes_gd": "",
    "slug": "end-to-end-message-encryption"
  },
  {
    "id": 99,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "kIP8L-CSl2Y",
    "title": "Designing Notifications Service for Instagram",
    "description": "This video is a snippet from my System Design Masterclass and in it, we are discussing How Instagram Scales its notification systems. The primary challenge of such a system is doing a real fast fanout and we discuss how to do this very efficiently.",
    "img": "https://i.ytimg.com/vi/kIP8L-CSl2Y/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/huyZxIJvtqVeRp7QcS/giphy.gif",
    "duration": "37:18",
    "view_count": 15837,
    "like_count": 375,
    "comment_count": 36,
    "released_at": "2021-04-01",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-notifications-service-for-instagram"
  }
]