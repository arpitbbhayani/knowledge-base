so it is extremely critical to continuously monitor the health of the services and the infrastructure. we use time series databases to hold the key vitals like cpu, ram, disk request, network, io, etc. pinterest generates millions of data points every second, and the existing time series databases were not performant enough to meet their needs, hence they built one in-house. in this video, we take a detailed look into the architecture and the key design decisions that pinterest took while designing their own in-house time series database named goku. but before we move forward, i'd like to talk to you about a course on system design that i've been running for over a year and a half now. the course is a cohort based course, which means i won't be rambling a solution and it will not be a monologue at all. instead, a small, focused group of 50 to 60 engineers will be brainstorming the systems and designing it together. this way, we build a very solid system and learn from each other's experiences. the course is enrolled by 800 plus engineers spanning 12 cohorts and 12 countries. engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say. the course is focused on building systems the way they are built in the real world. we will be focusing heavily on building the right intuition so that you are ready to build any and every system out there. we will be discussing the trade-offs of every single decision we make, just like how you do in your team. we cover topics ranging from real-time text communication for slack to designing our own toilet balancer, to quick buses, live text commentary to doing impressions counting at scale. in all, we would be covering roughly 28 systems, and the detailed curriculum, split week by week, can be found in the course page linked in the description down below. so if you're looking to learn system design from the first principles, you will love this course. i have two offerings for you. the first one is the live cohort based course and the second one is the recorded offering. the live cover based course happens once every two months and will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to learn and want to binge learn system design, i would recommend going you for the recorded one. otherwise, the live code is where you can participate and discuss the systems and its design life with me and the entire cohort. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page. arpegbani dot me slash master class. i repeat at with many: dot me slash master class and i would highly recommend you to check that out. i've also put the link of this course page in the description down below and i'm looking forward to see you in my next cohort. so great companies run on analytics. they measure anything and everything. the data, like basically metrics, vitals events and whatnot, is stored in a time series database so that you can analyze it. you can see the trend, you can see anomalies, you can see if someone's attacking you and whatnot. everything is constantly being measured. pinterest, one of the most famous companies in the world. they used at that time they used open tsdb as their main time series database to hold all the time series data that they have. tsdb is based on hbase, which is based on hadoop, so they use that as their time, basically as their time series database, and they were ingesting a million points every second. but because open tsdb is based on edge base, it had a lot of garbage collection issues, for example, long gc process. everyone hates them and it crashed very frequently, which is why they built their own in-house time series db called goku. now, in this one, we'll take a look at the architecture of it. insanely good, okay. so first let's understand what time series data model is, which means how are you collecting the data and how are you persisting. so first we'll look at how are you collecting the data and in which format. so every time series data point has two parts. first one is the metric. the metric that you are tracking, for example, cpu of every single machine that you have, might look something like this: tcprogstatcputotal, which is the metric. you would want to keep it verbose, separated by dots so that you may want to do aggregation, use wildcards and whatnot. so typically you create hierarchy of such metrics, but while putting a data point, you would say, hey, this is my metric, and against the metric, you may also optionally specify the tags. now, tags would be, let's say, i have this metric, cpu total, but for which host? so, host, you may want to pass a string that this is the host and which service does this host belong to, maybe authentication service, so that you can find out what is the total cpu consumption of a particular service. right and this becomes your key. key is your metric plus tag and value is the timestamp and the value. so at this point i am registering this value for this metric with this particular tag. the tags are specifically used for filteration. they can't do much, they're just used for filtering. for example, i want to man, i want to see how authentication service only for authentication service. i want to see i can do that. i can apply a filter on the tag so you may do- uh, you may do- exact matching, you may do wildcard matching, you might do like x matching. all of all three of them works right. apart from that, what you can also do with the data of the time series is you can do aggregation, because that is what you would want to find. you don't want to see individual data points, you want to aggregate. you might want to see: hey, what are the total number of requests that i received in last 60 days? you just want to see one number. you don't want to see the trend. if you just want to see one number, you would just do summation of all of them, or rather count of, not real summation, but the count of all of them. if you want to see that, hey, every payment that i received, i'm registering one data point in my time series db and now i just want to sum them so that i can calculate my total revenue, right? so for aggregators, that most time series databases support- which they also support- are some max, min, average, count and deviation. then one very interesting concept is called down sampling. down sampling is all about: let's say, i have 100 data points, but when i'm storing it i don't want to store individual items. let's say the data is a month old. why do i need to store that granular information? i can down sample, which means out of those 100 point, i can find one point that represents them, for example, average of all of them, for example, minimum of all of them, for example, maximum of all of them. right, so i can down sample. now, this down sample is basically having a single point to represent a group of point. this way you typically you do this to save space and do efficient computation at scale. right, this is a time series data model. now let's talk about the challenges and the key decision that pinterest took while designing this database. first of all, scanning open tsdb scans are very, very inefficient, given that it is based on top of hadoop hdfs. so all of these scans would be disk based. if their disk based, they would be slow and it internally creates a lot of bucket, so you do a lot of random reads on the disk, very inefficient. so what goku did? the key design decision that they took is: hey, let's not store anything on the disk, let's put everything in memory, which is why it is very fast. and the way they have structured it is that they are using inverted index, simple hash map. they are using hash maps to store that information, given this metric. these are the number of like, these are all the data points for that in last two hours, for example, and it just stores them in raw hash maps. this way, your scanning is efficiently far is very fast, given that everything, data, every data, is stored in memory. second is data science. so goku uses facebook's in memory time series database. so facebook has their own in memory time series database named gorilla, and there is a brilliant research paper on it, and it gives 12x compression out of the box, given that it is storing data in memory, and it does a lot of optimization. that, given that it knows how the data is and and how the key- the core properties of any time series information. so it gives out 12, uh, it basically gives out 12x compression out of the box. so goku- that's why- uses gorilla as its time series engine. like that one instance, it would run gorillas time series db like not really time series db, but the storage engine of it, right. this way it reduces the size of the data, right, that it would need to process and store in memory. third is computer and aggregation. this is where it's really interesting. so what open? uh? so what open tsdb does is open tsdb does scatter and gather, which means that it basically goes to every single node, like, for example, if you fired a query that requires it to go to three or four different places. it goes to three and four different places, gathers all the raw data, puts it in one server and then does the aggregation. this is extremely expensive, why? because you are doing a lot of network io, because you are fetching the raw data and then doing aggregation, right. so open dsdb has to fetch all the data from all the places and then do aggregation. that's why it's very slow. so what goku does? goku made a very great decision. so what it does is: it says that i would move this computation layer near to my storage, because your storage is in memory. let the first aggregation, the first level of aggregation, happen on the storage node itself. first level of aggregation happens over there and then it is sent to a proxy and that proxy does this final aggregation. for example, if i'm doing summation, and i'm doing summation across ah, summation of the data that is present across three shards, so the query goes in parallel. each of the three shards does its own summation of the necessary query and then sends the request- sorry, sends the response to the proxier. proxier does the summation of the output coming in, the partial output coming in from the three charts, and then sends the response to the user. this is extremely efficient. you are doing a minimal data transfer over the network. that's why it's fast, so you don't need a lot of data movement here and there. and the fourth one is where open tsdb uses json, which is the worst possible choice. right, it's too slow and goku improved it by using thrift binary protocol to serialize the data. this weight requires very minimal storage, even if you would want to persist it on the disk, keeping it in memory using gorilla. that is any way fast. so this makes things much, much, much, much better. now let's look at the high level architecture, and this is very interesting, very interesting high level architecture. so goku uses facebook's gorilla in-memory storage engine. like, obviously, gorilla itself is a time series db, goku does not use it, it just uses the storage engine to power an in-memory time series database of itself on one node, right, and what it does is goku is storing 24 hours worth of data in memory. after that it is flushing to the disk, so it is not storing. so in memory storage is restricted to last 24 hours data only. right, because that is the most frequently used. you want to do anomaly detection. you want to do, uh, you want to basically predict something, scale something out and do something with that. you would always need the most recent data, typically 24 hours, good enough, worst case, three days- historical data. you may put it onto the list, load it in memory whenever you need it and then process it. so now here, what would happen is that architecture looks something like this, where you might have multiple time series, for example, one for cpu, one for ram, one for something, one for payments, authentication, request, network o what not. you have multiple time series information. now, each time series information. as we just discussed, each time series data point has a metric tags and value. so, depending on what the metric is, it would go to one of the shard. for simplicity, assume each shard is one machine and each machine and each goku machine is running a time series db independently, right? that's a simplistic assumption. so each, depending on the metric, it goes to a particular shard. each shard in memory has multiple buckets. for example, if i'm having a 24r for which i'm keeping the data, you might create 2r bucket, so one bucket for each, for each 2r combination, so 0, 1, 0, 1, 2, 3 and so on and so forth, so that you would have 2 r window present in one bucket map. this is called a bucket map. each bucket map, how it is storing the data. it is having a. it is basically storing it in something called as a bucket time series object, simple hash map, not over complicating. a simple hash map. it is storing this information, right? so in a bucket time series object it is storing all of this information, and a bucket map also contains an in-memory- again an in-memory, immutable structure. so the idea here is a bucket map that you have. it basically contains two things. first is a bucket time series object, which is a mutable buffer. basically, any incoming point would go into this, will go into this modifiable buffer because you might get some events out of order, so you might just want to insert it in between and update it because this is modifiable buffer. but now, let's say this buffer, after an hour or after few minutes, becomes immutable, it would still be kept in memory because you are keeping last 24 hours worth of data in memory. this would still be kept in memory in something called as a bucket storage. so bucket time series object is a simple in-memory hash map where you are storing this information and then periodically putting it into a bucket storage again and in memory some data structure, right so last towards data kept in this hash map, then flushed. it's not really a disk plus but a memory memory flush in which you are making it immutable, right last 24 hours still held in memory. now what is this? this? what does this bucket storage look like? bucket storage look like this: for each bucket you would have an immutable, immutable structure which is very optimized for queries because once the data has become immutable you cannot write to it, it is just there for query purposes, right? so the way the data would be structured in this bucket storage will be very optimal for querying it, given that it is in memory. it is typically storing it in an inverted index, normal hash map, right, and what would it have? it would every bucket. it is also in memory, right. so, but periodically. to have the persistence, it would be periodically flushing it to the disk. for example, every five minutes, entire bucket data is plused onto the disk so that you don't lose out on this data. once the bucket data becomes immutable, you also flush it to the disk. so this way, you get full persistence. now, when the when you basically get a query, this is the path that it typically takes and this is how it is very beautifully structured. now let's see what happens when a user queries it. when, let's say, i have an analytics dashboard and it wants to query the time series dp. so the request goes to a goku proxy server. now this goku proxy has connection to all the shards that there are and all the goku instances, and this goku proxy, like, depending on what the query is, it knows which all, basically, which all goku shot should i go to. it would go to each of those particular goku shards, fire that same query, all of the shards would compute the aggregation there and send it to this goku proxy. it would do the final aggregation and send it to the user. this is a classic scattergather, but in memory, very efficient. so here, this is what i was talking about when i said computation is moved towards the storage, because if the data is in memory, each chart itself does a local aggregation and then sends the aggregated response to the user, uh, to the proxy. so this way your proxy does not need large ram where it all the shards send the data to the proxy, then proxy does this aggregation. all the shards independently do their own aggregation. they send this response, then this does this final aggregation and responds back to the user. right, and this is a very beautiful, simple time series db implementation that pinterest actually uses such a beautiful design. no over complication of it when you- and it's so easy to implement. so, so, so easy to implement and, to be honest, hardly to this thing- you can very well write a very quick prototype of this design. right, and that is it. i hope you found this amusing. you saw how a company like pinterest has to like they had to build their own time series db because no other db was able to support their workload, but it was so easy for them to build it, given that they're sitting on the shoulders of giant. they're sitting on top of what facebook did an amazing job with gorilla. so highly recommend you to read the paper of gorilla and see how they're structured and typically this is what the structure is right. goku is just- uh, it's just basically beautifying the approach. so the in-memory gorilla engine is taking care, it's basically taking care of storage, while goku is taking care of the compute part- kind of this, right? so, yeah, that is it for this one. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel that's up. i post three in-depth engineering videos every week and that's in the next one. thanks a ton. [Music] you.