so grab processes millions of foods and Mart orders every single day and the most critical but brittle part of the infrastructure is the database given how critical orders are to grab they have to ensure a very high availability of the database while ensuring a very fast query tank so how do they achieve this in this video we take a look at a very high level architecture of the data layer of grabs order platform and see the practices they follow to ensure High availability stability and performance at scale but before we move forward I'd like to talk to you about a course on system design that I've been running for over a year and a half now the course is a code based course which means I won't be rambling a solution and it will not be a monologue at all instead a small focused group of 50 to 60 Engineers will be brainstorming the systems and designing it together this way we build a very solid system and learn from each other's experiences the course is enrolled by 800 plus Engineers spending 12 Sports and 12 countries Engineers from companies like Google Microsoft GitHub slack Facebook Tesla Yelp Flipkart dream11 and many many many more have taken this course and have some wonderful things to say the course is focused on Building Systems the way they are built in the real world we will be focusing heavily on building the right intuition so that you are ready to build any and every system out there we will be discussing the trade-offs of every single decision we make just like how you do in your team we cover topics ranging from Real Time text communication for slack to designing our own toilet balancer to cricbuzzes live text commentary to doing impressions counting at scale in all we would be covering roughly 28 systems and the detailed curriculum split week by week can be found in the course page linked in the description down below so if you are looking to learn system design from the first principles you will love this course I have two offerings for you the first one is the live covert best course and the second one is the recorded offering the Live code base course happens once every two months and will go on for eight weeks while the recorded course contains the recordings from one of the past cohorts as is if you are in a hurry and want to learn and want to binge learn system design I would recommend going you for the recorded one otherwise the Live code is where you can participate and discuss the systems and its design life with me and the entire cohort the decision is totally up to you the course details prerequisites testimonials can be found on the course page arpitbani dot me slash masterclass I repeat at with many dot me slash masterclass and I would highly recommend you to check that out I've also put the link of this course page in the description down below and I'm looking forward to see you in my next cohort so the first step towards architecting a database layer for any such product any such service any search system like this is to understand the kind of queries that we would fire onto this database right so there are two kinds of queries that we typically fire first is transactional query second are analytical queries now transactional queries are classically basically getting something updating something deleting something right so basically get an order create an order delete an order update an order these are transactional queries second are the analytical queries where you might want to get historical orders of a particular user or some statistic around orders let's say grouping orders by a particular attribute right these are analytical queries which can take some time and it's okay right so we would be leveraging that now with respect to grab one very interesting thing that happens is they have to be very good at handling spikes because the traffic for a platform like grab would not be uniform it would be spiky nature because let's say there is a dinner time you would see a youth search and specifically around a particular time window you would see a massive influx of searches or maybe there is an event and a lot of people order from food and Mart on grab right so which means that handling Pi uh handling spikes is really important right so then let's talk about design goals now this and again this entire thing is taken from gravs engineering blog which I've Linked In the description down below I would highly recommend you to check that out so with respect to grab their design goals were pretty simple and straightforward but really important first one stability now when you are architecting a database layer given how critical database is to any system because if databases on everything is done so given how critical database is the most important part or the most important criteria for a good design has to be stability that the system has to be stable enough which means it needs to handle a very high level of query per second that would come in plus it needs to be available because given the database is the one that is storing the order basically creating and getting the order which means that if this database goes down it is loss of Revenue which means that your data layer has to be highly available then with respect to availability you can also sense that hey in case there are failure because failures are inevitable in case there are some failures here or there it can have degraded performance impact on your system but it should not lead to a complete outage so it's okay for some fragment to be down but some fragment down should not lead to an complete outage this is really important for your business to survive second design goal is around being cost effective it's really important when a company operates at scale a small slow query or a simple looking slow query would increase would shoot your infrastructure cost by a huge margin when you talk in absolute terms which means architecture has to be very cost effective third consistency this is so important like you would not as a user when you're using grab when you create an order when you go to the page to see your historical orders you don't want to array my order is not dead you want strong consistency for transactional but eventual still fast enough but eventual consistency for analytics load so because transactional queries are really important because when you are updating an order it needs to update right there and then you cannot just say yeah I will update it with some DeLay So you need strong consistency for transactional queries and eventual consistency for analytics queries now these are design goals now let's see how they architected the data layer considering these things now first step is because there are two types of queries one transactional one analytical what if we create or we use two types of database one specializing for transactional side one specializing for analytic side now here a key point to note that these analysis is not gigantic analytics you may not need a data warehouse per se but a query that is good with analytics kind of stuff right we will take a look on the on the kind of tech that they choose so transactional database they are called oltp online transactional processing database and analytical database because this is online which means you need almost right there when you fire the query you need immediate response you don't want delayed response so these are o l a p database online analytical processing database so first let's talk let's take a look at oltp database so oltp databases are databases which are meant for transactional purposes which means that they would be the single source of truth no matter what right because that is where your orders would be created updated deleted and whatnot okay now these databases are meant to be transactional in nature which means that any interaction coming from the user will be directly going to this database there would not be any asynchronous processing over here so which means they need to be able to sustain a huge amount of load so these databases are specialized to be transactional in nature right and they may not hold all data because it's okay if your historical data is moved to some other place right so these are key design decisions right we are we would be taking into consideration when you're architecting it then oil AP database now olap database would have same data as transactional database but even larger because we are we may delete data from transactional but not delete from oltp or sorry olap olap would have all data we would use it for statistical processing the data would be stored for a very long duration let's say three years old orders will also be there and these database would be designed or the database that we choose over here would be specialized to make analytics efficient right so different databases for different use cases then obviously the next problem that comes in that how do we keep these data in sync a classic way is to build a data ingestion pipeline whose job is from your order service whenever you need any transactional thing you go to a transactional DB let's say use dynamodb or my you may use dynamodb for this purpose or people use relational databases as well like mongod sorry like MySQL postgres and all or mongodb whatever floats are put any transactional database would work so so long as it provides a strong consistency right but given that you need to have now you are having two databases one transactional one analytics you need to keep these two in sync classic way when your transition thing is done you push a message to Kafka consumed by workers eventually being updated like eventually being persisted in your analytics database redshift bigquery or you may use any other we will we still not convert what to use we will see what we can use over there but this is a standard way to do it right so transactional goes to a transactional DB analytics asynchronously goes to your analytics database right okay third point now how do we choose a database so let's say we we have to make a choice for what type like we want to choose a transactional DB in order to choose a transitional DP we need to understand the kind of queries we would be firing on that and see how it fits on it so here with respect to grab what happened is given the transactional database was going to use for get an order delete an order updated order create an order it's a key value use case so what they went for is they went for dynamodial because dynamodb number one is managed so you get scale and it being highly available out of the box because it's aws's headache on how to do it we don't have to worry because at that business matters not how much are you managing your own infrastructure right so dynamodb is scalable highly available brilliant so we offload that part and given our use case or grabs use cases pretty straightforward we can very well afford to do this second dynamodb because we want strong consistency from our primary database dynamodb provides strong consistency for primary key reads right so for a particular key which is the primary key of the table you fire the query get put create delete it would give you consistent review of it you need it so Dynamic fits that criteria third is really interesting so here what happens is this is beautiful part of dynamodb from not red dynamodb I highly recommend you to do that but in general dynamodb is known to handle spikes and which is what grab also needed so but how do dynamodb do that so what dynamodp does behind the scenes is that it sees because internally dynamodb stores data in multiple partitions right so the data is partitioned inside dynamodb so when that is the case when it gets the load it on the run time it decides where to forward the request to depending on the partition but if a particular partition is hot because let's say few keys are constantly getting requests and that partition becomes hot what dynamodb does it it literally moves the data into some other partition not hot but the data which is infrequently accessed it moves to a shared partition while hot partition gets its own set of full capacity of the underlying infrastructure this way dynamodb internally does that we don't have to do anything there dynamodb internally handles hotkey problem seamlessly right and this is brilliant part of dynamodb which is why grab went for dynamodb and again all of this is taken from graves engineering blog Linked In the description down below highly recommend you to check that out now apart from just using dynamodb tools it also matters how smartly do you use it right so here there is one brilliant Insight which I personally loved when I when I was going through the blog so let's take a look at that schema so given that we are all we want to store is orders we would have orders table in which we have order ID the state of the order which means is it ongoing or completed or canceled something around that created at it means at this time the order was created user ID has been who placed the order and there is one more key called user ID GSI so sorry so order stable has orders ID as a primary key because you want to get an order delete another basically put an order and whatnot right so that is a primary key but you would also want to fetch the ongoing order of a particular user won't you you need that so given that you would need an index on user so you create a global secondary index on user so you do that but now what happens is This Global secondary index is managed by dynamodb we don't have to do anything we just create an index and it powers that but now this index the common query that we are firing on this database would be give me ongoing orders of a particular user right once the order is moved from ongoing to completed we would not be querying on this DB because we have another database to do it so we because we are never going to query on that particular attribute when an order goes from ongoing to completed we would never query that which means that we can remove it from the index but how can we directly remove from the index this is the brilliant part over here so what they do is when so they created this key user ID underscore GSI they didn't create an index on user ID they added a new attribute called user underscore ID underscore GSI and they created Global secondary index on this attribute right okay so now what they do when an order is created and it goes into ongoing set by default the first date is ongoing state it sets user underscore ID underscores GSI is equal to user ID right then when the order is moved from ongoing to done they set this column this attribute to null user ID is still there but user ID underscore GSI this is set to null as soon as this is set to null the Global Security the entry is removed from the global secondary index because this is null that index does not hold anything so the global secondary index would at a given point in time would only hold the entries that are of a of ongoing orders this way the index is very small such a brilliant such a brilliant optimization I loved it when I found it like God this is interesting right so just to reiterate order stable has order ID State created at user ID other meta Fields would be that we don't care about them so uh partition key the entire table is partitioned by order ID they instead of creating a global secondary index on user ID in order to get ongoing orders of a particular user they created another field called user ID GSI it's a special field and they created Global secondary index on this when an order is created user underscore ID underscore GSI is set to user ID right so that you can find ongoing orders of a particular user but as soon as you move your order from ongoing to done then you set this user ID underscore GSI to null which would make it delete from the global secondary index the entry would still be that in the order stable but it would not be there in the index it is basically implementation of partial indexing in dynamodb this way you keep the GSI the the global secondary index size very small it would be equal to the amount of ongoing orders at any given moment which means it would be very lean which means when you fire the query to get the ongoing orders of a particular user it would be lightning fast brilliant optimization right which is how you need to know the database internals to leverage it to the full extent right okay so they do this and obviously given that there is an analytics database in which the same data is already flowing through events you will have TTL set on these rows so that let's say you would want to keep only three months worth of data in the uh in this transactional DB while historical data goes to analytics DB so you typically set TTL to three months as three months ex as three month pass for a particular key the key would be automatically deleted by the dynamody means and you don't have to worry about it right okay fourth which database would we pick for olap now this analytics like now this use cases analytics right so you may choose hey let me go for data warehouse you don't need data warehouse do you still want to fire good chunk of queries but why would you want to Overkill with an analytics database because it's all about only orders it's not humongous data that you need a data warehouse per se you still want to support analytics you still want to support statistics you still want to power basic analytics on this database but data warehouse would be an Overkill right because these are almost almost near real-time queries that you would want to support so what graph did is grab chose MySQL as our analytics database right and what the way they structure this information is such that they minimize the cross shot query that needs to happen on MySQL now this is the overall architecture that is there for uh their entire data layer it's very simple but few very interesting caveats over here so you have order service use dynamodb as your transactional database it offers you strong consistency that you need then all the events from order service decks are sent to Kafka right Kafka gives you an SLA of 99.95 which means there is still chance of Kafka going down so now when what happens when Kafka goes down you still have to handle that because High availability you won't say that hey if Kafka is on I won't push the data over here right no matter what your business should run so but first let's complete the happy path then we'll check for the for the edge cases so once our events are pushed to Kafka a bunch of consumers read them and they update it in MySQL right so consumers consume and ingest the data in MySQL all the events flow near real-time eventual consistency the data flows into my SQL right okay but now given the Kafka has an SLA of 99.95 there is still chance of this going down so what do you do what if Kafka is down that is where because grab is on AWS they have sqs as a backup option so if Kafka is down the order service pushes the same message to sqs and which is then which is then consumed by few consumers here and there and put into MySQL so even if Kafka is down you have a fallback now what if sqs is down if sqs is down AWS offers you a dlq which is a dead letter Q so you configure sqs with the dlq configuration and put it and have the consumer so no matter what your system even if your Kafka is down your excuse would do if sqs is down dlq would work that's AWS guarantee and which means that no matter what your analytics database is getting the data eventually right but the lag is bare minimum right okay few more edge cases so when you are inserting the data in my SQL you always have to do obserts right now why why why why why am I talking about upsets because what if given messages may come out of order you never know messages may be processed out of order so what do you do what if an update event on an order comes before create event you cannot just say hey my data is not there so I'll drop that event right so that is where you do upsets every time that you are getting it this way even if you get out of older events it would not matter right but now what if you get two updates which such that the update 2 comes before update one so in that case you cannot just apply updates every time because then you would be overriding the later update with the previous update wrong right so that is where you will have a check with timestamp field that hey if I am like I will update only when the update is newer one otherwise I won't do that because here you have to be very of the fact that messages may come out of order so that is why two very important design decisions is we always do obsert in the database in order to handle update coming before create and while we are updating we always need to check for the timestamp that no matter what we were never applying a past update on a later version of the data very important when you think about implementation and yeah this is how grab handles millions of orders every single day around food and mod categories that they have such a brilliant such a brilliant architecture the two micro nuances were such brilliant optimizations where one with the partial indexing on dynamodb there a lot even I would have done a created a Global Security index on user ID but this is such a brilliant optimization where they added a new attribute and they created a GSI on that brilliant optimization I hope you also found it as I'm using as I did and second one is how they ensure that hey your Kafka is down they added it with sqs with the dlq configuration and doing upsets and instead of doing raw inserts over there right very interesting very interesting data architecture that we see over here from crap and yeah that is it that is it this is how grab does it I hope you found it interesting hope you found it amusing this entire piece is taken from grabs engineering block which I've Linked In the description down below highly recommend you to check that out and yeah that is it for this one I'll see you in the next one thanks again [Music]