now, transactional queries are classically basically getting something, updating something, deleting something, right?
second are the analytical queries, where you might want to get historical orders of a particular user or some statistic around orders, let's say, grouping orders by a particular attribute, right?
these are analytical queries which can take some time, and it's okay, right?
one very interesting thing that happens is they have to be very good at handling spikes, because the traffic for a platform like grab would not be uniform.
or maybe there is an event and a lot of people order from food and Mart on grab, right.
now, when you are architecting a database layer, given how critical database is to any system- because if databases on everything is done, so given how critical database is, the most important part or the most important criteria for a good design has to be stability, that the system has to be stable enough, which means it needs to handle a very high level of query per second.
it needs to be available because, given the database is the one that is storing the order, basically creating and getting the order, which means that if this database goes down it is loss of Revenue, which means that your data layer has to be highly available.
third, consistency: this is so important, like you would not as a user, when you're using grab, when you create an order, when you go to the page to see your historical orders, you don't want to array.
so because transactional queries are really important, because when you are updating an order, it needs to update right there and then you cannot just say, yeah, I will update it with some DeLay- So you need strong consistency for transactional queries and eventual consistency for analytics queries.
what if we create or we use two types of database: one specializing for transactional side, one specializing for analytic side?
you may not need a data warehouse per se, but a query that is good with analytics kind of stuff.
so transactional database- they are called oltp- online transactional processing database- and analytical database, because this is online, which means you need almost right there when you fire the query.
right, because that is where your orders would be created, updated, deleted and whatnot.
okay, now, these databases are meant to be transactional in nature, which means that any interaction coming from the user will be directly going to this database.
classic way, when your transition thing is done, you push a message to Kafka consumed by workers, eventually being updated, like eventually being persisted in your analytics database.
in order to choose a transitional DP, we need to understand the kind of queries we would be firing on that and see how it fits on it.
so here, with respect to grab, what happened is given the transactional database was going to use for: get an order, delete an order, updated order, create an order.
second, dynamodb because we want strong consistency from our primary database.
dynamodb provides strong consistency for primary key reads, right.
highly recommend you to check that out now, apart from just using dynamodb tools, it also matters, how smartly do you use it right?
so, given that we are, all we want to store is orders, we would have orders table in which we have order ID, the state of the order, which means: is it ongoing or completed or canceled, something around that created at it means at this time the order was created, user ID has been, who placed the order, and there is one more key called user ID GSI.
but now this index, the common query that we are firing on this database would be: give me ongoing orders of a particular user, right.
once the order is moved from ongoing to completed, we would not be querying on this DB because we have another database to do it.
when an order goes from ongoing to completed, we would never query that, which means that we can remove it from the index.
they added a new attribute called user underscore ID, underscore GSI and they created Global secondary index on this attribute, right, okay.
so the global secondary index would, at a given point in time, would only hold the entries that are of a of ongoing orders.
right, so that you can find ongoing orders of a particular user.
but as soon as you move your order from ongoing to done, then you set this user ID underscore GSI to null, which would make it delete from the global secondary index.
it would be equal to the amount of ongoing orders at any given moment, which means it would be very lean, which means when you fire the query to get the ongoing orders of a particular user, it would be lightning, fast, brilliant optimization, right?
and obviously, given that there is an analytics database in which the same data is already flowing through events, you will have TTL set on these rows so that, let's say, you would want to keep only three months worth of data in the uh, in this transactional DB, while historical data goes to analytics DB.
the key would be automatically deleted by the dynamody means and you don't have to worry about it, right, okay, fourth, which database would we pick for?
olap, now this analytics, like, now this use cases analytics, right, so you may choose.
do you still want to fire good chunk of queries, but why would you want to Overkill with an analytics database, because it's all about only orders.
so what graph did is grab, chose MySQL as our analytics database, right, and what?
use dynamodb as your transactional database.
then all the events from order service decks are sent to Kafka, right?
you still have to handle that because High availability- you won't say that, hey, if Kafka is on, I won't push the data over here, right?
so that is why two very important design decisions is: we always do obsert in the database in order to handle update coming before create.