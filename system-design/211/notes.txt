How Twitter scales and keeps their search stable

scalability and stability of search & Twitter Twitter uses Elasticsearch to power search of trees, user, directmessages of < I Elasticsearch? Why distributed nature, and speed, scalability, simple REST APIs Internal customers/services directly talk to ES and get things done Hence, need a of Standardization, and stability performance. Elasticsearch Proxy Querying, Indexing, monitoring, metrics was all manuals to be done separately. Proxy standardized throttling, routing, security, authentication. monitoring-cluster health, success rate, failure rate, latency # c< Proxy < > simple Elasticsearch HTT P

ES collapses Ingestion service If when there Elasticsearch is a massive surge in traffic, r gives up! increased indexing latency increased query latency To handle heavy ingestion gracefully. Twitter built an ingestion service Ingestion service queues indexing dequest in Kayka later to be consumed by workers S o I i Kafka 3 Elasticsearch workers Advantages: Request batching: batch write on ES cluster Backpressure:consuming at its own pace Throttling: slowing down if ES is overwhelmed Retries: if cluster down, we can retry easily

lingesting data in ES) Backfill Service 100s of TB of does not work Directly ingesting massive data using MapReduce syncheonously Indexing happened in sync from MapRed. Elasticsearch cannot "handle this Now the indexing request is dumped huge ingestion in one shot. in temporary storage 2 then processed. workers 3 Kaffic Ocseroxy WRITE > > 3 READ - Y BACKFILL -x 3 Data Partition T > ata partition D) - Data Partition workers #casticsearch HDFS cluster dynamic N allocation workers read index requests from Orchestrator and index data into the cluster.