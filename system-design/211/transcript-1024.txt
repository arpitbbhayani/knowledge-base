so Twitter uses elasticsearch to power search of their tweets, users and direct messages.
what happens is you just need a simple HTTP client to talk to elasticsearch servers, and it makes it super easy to do so.
every team was allowed to spin up their elasticsearch clusters to get their things done.
but because there were many clusters like this, the team owning that cluster had the responsibility to set things up correctly, for example, the, because there are multiple clusters.
so there was a clear need that there had to be a standardized way to set up a cluster, right?
now, this proxy sits between the client and the elasticsearch cluster, right?
now, this proxy is a very simple HTTP client and but because it is a simple HTTP client, it can piggyback a lot of stuff, for example, this proxy, because you know that all the requests will go through this proxy to any elasticsearch cluster.
thirdly, right, you don't want a lot of request coming from a particular client to to take down the elasticsearch cluster- which is where this proxy comes in- which has a rate limiter inbuilt right.
you can dump key metrics out of this proxy, for example, what is the cluster Health, what is the success rate, what is a failure rate, what is latencies and what not?
this increases the stability of the cluster, because not people are not directly interfacing with elasticsearch right.
second thing that can affect elasticsearch is injection, because, given at Twitter scale, there would be huge amount of load coming onto this elasticsearch clustered when a new like, for example, a new tweet, is posted.
where right, rather when a tweet is posted, it needs to be index in elastic search.
right now, elasticsearch is little tricky to handle such spikes and what happens is when there is a sudden Spike, elastic surge does not get time to like.
right, and elasticsearch is bad at handling such searches.
two things that happen typically is the indexing latency shoots up and the query latency shoots up, so it like indexing the data into elasticsearch becomes slower and firing the search query on elasticsearch becomes slower.
instead of letting that thing directly get ingested into elastic surge, they did it in an asynchronous way.
so any right coming to be put into elastic search.
how it used to happen was people used to like directly call a pillar, so the client used to directly call elasticsearch endpoint and used to put the data.
this is what they have configured and the rights are put into Kafka later to be consumed by workers and then indexed it to elasticsearch.
so proxy puts it in Kafka later to be consumed by workers, right?
so there is no direct ingestion happening in elastic search.
when you put in a bulk of indexing request, you can take few at a time and make bulk calls to elasticsearch, reducing the lowest non-elastic search.
in case elastic searches are due to any reason, the right request- the injection is law- is basically queued up in this Kafka and again later be consumed to be put into elastic search.
it ensures that we are streamlining a consumption and ingestion into elastic search, right.
so, for example, you have a job which is running which takes data from multiple data sources and it wants to index them into elasticsearch.
this map reduce job would take data from multiple sources and make calls, like each reducer will call elasticsearch endpoints to get it indexed.
so they already had a service which was a map reduce job whose job was to read data from multiple sources, when it was a backfill to read data from multiple sources and the reducers used to directly write to elasticsearch.
the reducer, instead of writing it directly to elasticsearch, they flush it into an hdfs- hello- distributed file system, S3, any, any compliant file system would work: hdfs, S3, GCS, pick your favorite, right?
ingestion happened, right?
so what they have is: they created a proxy for elasticsearch, right?
so whenever someone wants to talk to elasticsearch, it has to talk through this proxy.
you want to ingest this tweet in real time into elasticsearch.
a HTTP proxy: instead of directly writing to elasticsearch, it writes it, it puts an event into Kafka, from this Kafka workers consume and it puts it into elasticsearch.
right, this way, real time ingestion happens in a staggered way, protecting an elastic search cluster- right so, like how users would- a normally index directly to elasticsearch is now changed via this proxy, which has very similar interfaces: elasticsearch, but now it happens in a staggered way.
and the more important part, around backfill, where they already had mapreduce job running, which was directly putting it into elasticsearch.
now it takes, like the HTTP request that we're making to index the data into elasticsearch.
the only thing like one small change that needs to happen: the map reducer, where the reducer is used to directly put it into elasticsearch.
now they put it into hdfs and a bunch of workers are spun depending on the load or depending on the data that was created, the orchestrator creates those very workers and they consume the data from the https and put it into elasticsearch right, a very seamless way.
especially in a workload like this, if it is a non-transactional use case, you defer it all the time, right?
so, just to give you a gist, read happens synchronously to elasticsearch Via this proxy.
the right through this proxy goes through Kafka, consumed by workers, put into elasticsearch, while the backfill- the job, the map reduce job- that used to directly write it to elasticsearch now flushes it into an hdfs, which is consumed by workers, fund direct, spun dynamically using orchestrator and put into elasticsearch.
this is such a beautiful piece of thing, like how Twitter, at scale of Twitter, how they are ensuring that their elasticsearch clusters don't go down, while ensuring a very nice standardization by adding this proxiliary in front of it.