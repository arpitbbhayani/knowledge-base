[
  {
    "id": 3,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "indexing-on-partitioned-data",
    "title": "Indexing on Partitioned Data",
    "description": "In this essay, we will take a detailed look into how we could index the partitioned data, allowing us to query the data on secondary attributes quickly.",
    "gif": "https://media.giphy.com/media/3o6Mb9cGKe3JzhbqPC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/152698544-c62436c9-8735-4490-90be-4d117902736b.png",
    "released_at": "2022-02-07",
    "total_views": 380,
    "body": "The previous essay looked at two popular ways to [horizontally partition](https://arpitbhayani.me/blogs/data-partitioning-strategies) the data - Range-based Partitioning and Hash-based Partitioning. In this essay, we will take a detailed look into how we could [index](https://en.wikipedia.org/wiki/Database_index) the partitioned data, allowing us to query the data on secondary attributes quickly.\n\n# Partitioning and Querying\n\nIn a [partitioned database](https://arpitbhayani.me/blogs/data-partitioning), the data is split horizontally on the partitioned key. Given that each partition is required to handle a fragment of data, the query that is bound to a single partition is answered very quickly vs the query that requires cross partition execution. But what happens when we want to query the data on any attribute other than the partitioned key; that is where things become very interesting.\n\nSay we have a `movies` collection partitioned on `id` (the movie ID), and each record has the following structure.\n\n```json\n{\n\t\"id\": tt0111161,\n\t\"name\": \"The Shawshank Redemption\",\n\t\"genre\": [\"Drama\"],\n\t\"year\": 1994\n}\n```\n\nGiven that the collection is partitioned on `id`, querying a movie by its `id` will be lightning-quick as it would need to hit just one partition to grab the record as determined by the Hash function.\n\n![Pointed Query in Partitioned Database](https://user-images.githubusercontent.com/4745789/152688735-16e15acf-fcee-491c-9b74-965e3590df9c.png)\n\nWhat if we need to get the list of all the movies belonging to a particular genre? Answering this query is very expensive as we would have to go through every record across all the partitions and see which ones match our criteria, accumulate them, and return them as the response. Given that this process is tedious we leverage indexing to compute the answer quickly.\n\n# Indexing\n\nIndexing is a popular technique to make reads super-efficient, and it does so by creating a query-able mapping between indexed attributes and the identity of the document. An index that maps non-primary key attributes to the record id is called a Secondary Index.\n\nSay, we have the following 6 movie documents, partitioned on `id` (the movie ID) and split across 2 partitions as shown below\n\n```json\n{ \"id\": tt0111161, \"name\": \"The Shawshank Redemption\", \"genre\": [\"Drama\"], \"year\": 1994 }\n{ \"id\": tt0068646, \"name\": \"The Godfather\", \"genre\": [\"Crime\", \"Drama\"], \"year\": 1972 }\n{ \"id\": tt0071562, \"name\": \"The Godfather: Part II\", \"genre\": [\"Crime\", \"Drama\"], \"year\": 1974 }\n\n\n{ \"id\": tt0468569, \"name\": \"The Dark Knight\", \"genre\": [\"Action\", \"Crime\", \"Drama\"], \"year\": 2008 }\n{ \"id\": tt0050083, \"name\": \"12 Angry Men\", \"genre\": [\"Crime\", \"Drama\"], \"year\": 1957 }\n{ \"id\": tt0108052, \"name\": \"Schindler's List\", \"genre\": [\"Biography\", \"Drama\", \"History\"], \"year\": 1993 }\n```\n\n![movies Partitioned across 2 partitions](https://user-images.githubusercontent.com/4745789/152688989-d70c541f-92c9-4f3f-ad54-740f660f7bd0.png)\n\nTo query movies by `genre = Crime`, we will have to index the data on `genre` allowing us to find the relevant documents quickly. Indexes are a little tricky in a partitioned database, and there are two ways to implement them: Local Indexing and Global Indexing. [AWS's DynamoDB](https://aws.amazon.com/dynamodb/) is a partitioned KV store that supports secondary indexes on non-partitioned attributes, and it supports both of these indexing techniques.\n\n## Local Secondary Index\n\nLocal Secondary Indexing creates indexes on a non-partitioned attribute on the data belonging to the partition. Thus, each partition has a secondary index that is built on that data owned by that partition and it knows nothing about the data present in other partitions. Hence, on the example that we have at hand, the Local Secondary Index on attribute `genre` would look like this\n\n![Local Secondary Index - Movies](https://user-images.githubusercontent.com/4745789/152689634-c3235f38-5cf2-4446-af87-ecd58807296d.png)\n\nThe key advantage of having a Local Secondary Index is that whenever a write happens on a partition, the index update happens locally without needing any cross partition communication (mostly a network IO). When the data is fetched from a Local Secondary Index, it is fetched from the partition that holds the index data and the entire record; so execution takes a minimal time.\n\nLocal Secondary Indexes come in handy when we want to query the data in conjunction with the partitioned key. For example, if the movies were partitioned by `genre` (instead of `id`) and we create an index on `year` it will help us efficiently answer the queries like movies of a particular `genre` released in a specific `year`.\n\n### When Local Secondary Indexes suffer?\n\nAlthough Local Secondary Indexes are great, they cannot efficiently answer the queries that require cross partition fetch. For example, if we fire the query to get all `Crime` movies through a Local Secondary Index, we will be getting the records that are local to the partition on which the query executes.\n\nBut, answering the query to fetch all the movies from the `Crime` genre requires us to go through all the partitions and individually execute the query, then gather (accumulate) the results and return. This is an extremely expensive process that is also prone to network delays, partitioning, and unreliability.\n\n![Scatter Gather Local Secondary Index](https://user-images.githubusercontent.com/4745789/152691045-f9958236-b532-4fed-a5d3-902962edd1b0.png)\n\nWe face this limitation because the movies with the `crime` genre are distributed across partitions because there is no way to ensure all movies with the `Crime` genre belong to the same partition when the data partitioning is done on `id`.\n\nHence, it is very important to structure data partitioning and indexing depending on the queries we want to support ensuring that the queries can be answered through just one partition. To address this problem of being able to query the data on an indexed attribute, we create Global Secondary Indexes.\n\n## Global Secondary Index\n\nGlobal Secondary Indexes choose not to be local to a partition's data instead, this indexing technique covers the entire dataset. Global Secondary Index is a kind of re-partitioning of data on a different partition key allowing us to have faster reads and a global view on the indexed attribute.\n\nOn the example that we have at hand, Global Secondary Index on `genre` would look like this.\n\n![Global Secondary Index - Movies example](https://user-images.githubusercontent.com/4745789/152696486-33d94f29-6918-48f6-8644-b6ee809a2a81.png)\n\nThe key advantage of having a Global Secondary Index is that it allows us to query the data on the indexed attribute globally and not limit ourselves to a fragment of the data. Since it literally re-partitions the data on a different attribute, firing query on the indexed attribute requires it to hit just one partition for execution and thus saving fanning out to multiple partitions.\n\n### When Global Secondary Indexes suffer?\n\nThe database takes a performance hit when a Global Secondary Index needs to be synchronously updated as soon as the update happened on the main record, and if the updation happens asynchronously then the readers need to be aware of a possible stale data fetch.\n\nSynchronous updation of a Global Secondary Index is an extremely expensive operation given that every write on primary data will be translated to a number of synchronous updation across partitions for index updation wrapped in a long [Distributed Transaction](https://en.wikipedia.org/wiki/Distributed_transaction) to ensure [Data Consistency](https://en.wikipedia.org/wiki/Data_consistency).\n\n![Global Secondary Index updation](https://user-images.githubusercontent.com/4745789/152697444-4b90d88b-fa18-4456-b8ba-bdde15ddbac4.png)\n\nHence, in practice, most Global Secondary Indexes are updated asynchronously involving a rick of Replication Lag and stale data reads. The readers from these indexes should be okay with reading stale data and the system being eventually consistent. The delay in propagation could vary from a second to a few minutes, depending on the underlying hardware's CPU consumption and network capacity.\n",
    "similar": [
      "data-partitioning",
      "conflict-detection",
      "data-partitioning-strategies",
      "architectures-in-distributed-systems"
    ]
  },
  {
    "id": 4,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "data-partitioning-strategies",
    "title": "Data Partitioning Strategies",
    "description": "In this essay, we take a detailed look into the two common approaches to horizontally partition the data - Hash Based and Range Based Partitioning.",
    "gif": "https://media.giphy.com/media/xTiTnrliW65rlwud8I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/151733223-2f1f5611-7b4a-463d-aadf-fd522b535d67.png",
    "released_at": "2022-01-31",
    "total_views": 302,
    "body": "[Partitioning](https://arpitbhayani.me/blogs/partitioning) plays a vital role in scaling a database beyond a certain scale of reads and writes. This essay takes a detailed look into the two common approaches to horizontally partition the data.\n\n# Partitioning\n\nA database is partitioned when we split it, logically or physically, into mutually exclusive segments. Each partition of the database is a subset that can operate as a smaller independent database on its own.\n\n## Our goal with partitioning\n\nOur primary goal with partitioning is to spread the data across multiple nodes, each responsible for only a fraction of the data allowing us to dodge the limitations with vertical scaling. A database is uniformly partitioned across 5 data nodes; each node will be roughly responsible for a fifth of the reads and writes hitting the cluster, allowing us to handle a greater load seamlessly.\n\n## What if partitioning is skewed?\n\nPartitioning does help in handling the scale only when the load spreads uniformly. Partitions are skewed when few (hot) partitions are responsible for bulk data or query load. This happens when the partitioning logic does not respect the data and access pattern of the use-case at hand.\n\n![Skewed Partitioning](https://user-images.githubusercontent.com/4745789/150775353-358b6183-30a2-4fb4-8291-e3642c668747.png)\n\nIf the partitioning is skewed, the entire architecture will be less effective on performance and cost. Hence, the access and storage pattern of the use-case is heavily considered while deciding on the partitioning attribute, algorithm, and logic.\n\n# Ways of Partitioning Data\n\n## Range-based Partitioning\n\nOne of the most popular ways of partitioning data is by assigning a continuous range of data to each partition, making each partition responsible for the assigned fragment. Every partition, thus, knows its boundaries, making it deterministic to find the partition given the partition key.\n\n![Range-based Partitioning](https://user-images.githubusercontent.com/4745789/150777106-4ee22e27-de48-4dda-999e-f1b286a7d5f5.png)\n\nAn example of range-based partitioning is splitting a Key-Value store over 5 partitions with each partition responsible for a fragment, defined as,\n\n```\npartition 1: [a - e]\npartition 2: [f - k]\npartition 3: [l - q]\npartition 4: [r - v]\npartition 5: [w - z]\n```\n\nEach partition is thus responsible for the set of keys starting with a specific character. This allows us to define how our entire key-space will be distributed across all partition nodes.\n\nGiven that we partition the data to evenly distribute the load across partition nodes, we create the range of the keys that uniformly distributes the load and not the keyspace. Hence in range-based partition, it is not uncommon to see an uneven distribution of key-space. The goal is to optimize the load distribution and not the keyspace.\n\n### When Range-based partitioning fails?\n\nA classic use-case where range-based partitioning fails is when we range-partition the time-series data on timestamp. For example, we create per-day partitions of data coming in from thousands of IoT sensors.\n\nSince IoT sensors will continue to send the latest data, there will always be just one partition that will have to bear the entire ingestion while others will just be sitting idle. When the write-volume for time-series data is very high, it may not be wise to partition the data on time.\n\n## Hash-based Partitioning\n\nAnother popular approach for horizontal partitioning is by hashing the partitioned attribute and determining the partition that will own the record. The hashing function used in partitioning is not cryptographically strong but does a good job evenly distributing values across the given range.\n\nEach partition owns a set of hashes. We hash the partitioned attribute when a record needs to be inserted or looked up. A partition that owns the hash will own and store the record. While fetching the record, we first hash the partition key find the owning partition, and then fire the query to get our record from it.\n\n![Hash-based Partitioning](https://user-images.githubusercontent.com/4745789/150777895-b524d8b2-56f3-4a53-bf8b-27f06b824bc6.png)\n\nHash-based partitioning defers the problem of hot partition to statistics and relies on the randomness of hash-based distribution. But, there is still a slim chance of some partition being hot when many records get hashed to the same partition; this issue is addressed to some extent with the famous [Consistent Hashing](https://arpitbhayani.me/blogs/consistent-hashing).\n\n### When Hash-based partitioning fails?\n\nHash-based partitioning is a very common technique of data partitioning and is quite prevalent across databases. Although the method is good, it suffers from a few major problems.\n\nSince the record is partitioned on an attribute through a hash function, it is difficult to perform a range query on the data. Since the data is unordered and scattered across all partitions, we will have to visit all the partitions, making the entire process inefficient to perform a range query on key.\n\nRange queries are doable when the required range lies on one partition. This is something leveraged by [Amazon's DynamoDB](https://aws.amazon.com/dynamodb/) that asks us to specify Partition Key (Hash Key) and Range Key. The data is stored across multiple partitioned and is partitioned by the Hash Key. The records are ordered by Range Key within each partition, allowing us to fire range queries local to one partition.\n",
    "similar": [
      "indexing-on-partitioned-data",
      "data-partitioning",
      "mistaken-beliefs-of-distributed-systems",
      "conflict-detection"
    ]
  },
  {
    "id": 5,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "data-partitioning",
    "title": "Data Partitioning",
    "description": "In this essay, we take a detailed look into Partitioning basics and understand how it can help us scale our Reads and Writes beyond a single machine.",
    "gif": "https://media.giphy.com/media/l0G17UXa1wk36OIAE/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/150692301-6abb0b95-c91f-4a8f-8d10-6e0f49b4d58d.png",
    "released_at": "2022-01-24",
    "total_views": 435,
    "body": "Partitioning plays a vital role in scaling a database beyond a certain scale of reads and writes. In this essay, we take a detailed look into Partitioning basics and understand how it can help us scale our Reads and Writes beyond a single machine.\n\n# What is Partitioning?\n\nA database is partitioned when we split it, logically or physically, into mutually exclusive segments. Each partition of the database is a subset that can operate as a smaller independent database on its own. A database is always deterministically partitioned on a particular attribute like User ID, Time, Location, etc., allowing all records having the same attribute value to reside in the same partition. This will enable us to fire localized queries on a partitioned attribute.\n\n![Partitioning](https://user-images.githubusercontent.com/4745789/149617510-73d710c4-4ff1-4f6c-8ba7-6f8345847248.png)\n\nSay a database has grown to be 100GB big, and we choose to _partition_ it on User ID into 4 partitions. To decide which record goes in which partition, we can use a [Hash Function](https://en.wikipedia.org/wiki/Hash_function) applied on the User ID to map one record to exactly one partition. Hence to trace where a record of a particular user resides, we pass it through the same Hash Function and find the owning partition.\n\n> We will talk about partitioning strategies in detail in future [essays](https://arpitbhayani.me/blogs), so keep an eye.\n\n# Why do we partition?\n\nWe need to partition a database for several reasons, but load distribution and availability are the primary reasons. Let's dive deeper into each and see how partitioning benefits us.\n\n## Load Distribution\n\nA database is partitioned when it needs to handle more reads or writes than one over-scaled database. Our go-to strategy to handle more reads or more writes is to scale the database vertically. Given that vertical scaling has a limit due to hardware constraints, we have to go horizontal and distribute the load across multiple nodes.\n\n### Scaling Reads\n\nBy partitioning a database into multiple segments, we get a significant boost in the performance of localized queries. Say we have a database with 100M rows split into 4 partitions with roughly 25M rows each. Now, instead of one database supporting querying over 100M rows, we split the read load across 4 databases allowing us to quickly execute the query and serve the results to the users.\n\nIf the read query is localized by partitioned attribute, we need only one (of the four) partitions to execute the query and get the results, thus distributing the read load. For example, in a blogging platform, if our database is partitioned by User ID and we want to find the total number of posts made by a user, this query only needs to be executed on one small partition of data.\n\n![Scaling Reads with Partitioning](https://user-images.githubusercontent.com/4745789/149617513-2dd6bd59-7fea-413a-a73d-313fad080661.png)\n\nSuppose the read queries require us to fetch records from multiple partitions, given that each partition is independent. In that case, we can parallelize the execution and then merge the results before sending them out to the users. In either case, we get a massive performance boost in query execution.\n\n![Scaling Reads with Partitioning - Parallel Reads](https://user-images.githubusercontent.com/4745789/149617508-e62d16d1-bc3e-4aec-9b5c-49785699cff8.png)\n\n### Scaling Writes\n\nIn a traditional [Master-Replica setup](https://arpitbhayani.me/blogs/master-replica-replication), there is one Master node that takes in all the write requests, and to scale reads, this Master has a few configured Replicas. To handle more Write operations in such a setup, one approach is to scale the Master node vertically by adding more CPU and RAM. The second approach is to scale it horizontally by adding multiple nodes acting as independent Multiple Master nodes.\n\nGiven that vertical scaling has a limit, scaling writes that adding multiple independent Master nodes becomes a go-to strategy beyond a certain scale, where Partitioning plays a key role. In a partitioned setup, since one record can is present on one partition, the total write operations are evenly distributed across all the Master nodes, allowing us to scale beyond a single machine.\n\n![Scaling writes with Partitioning](https://user-images.githubusercontent.com/4745789/149632842-1497874e-13a3-4af2-86fd-096c1eb2e1d7.png)\n\n## Improving Availability\n\nBy partitioning a database, we also get a massive improvement in data availability. Since our data is divided across multiple data nodes, even if one of the nodes abruptly crashes and becomes unrecoverable, we only lose a fraction of our data and not the whole of it.\n\nWe can further improve the availability of our data by replicating it across multiple secondary data nodes. Thus each partition resides on multiple data nodes, and in case of them crashes, we can fully recover the lost data from the secondary node, giving our fault tolerance a massive boost.\n\n![Partitioning for High Availability](https://user-images.githubusercontent.com/4745789/149632846-d9be03ca-104b-4628-9d5a-b03f9c6ea690.png)\n\nEach record thus belongs to exactly one partition, but the replicated copy of the record can be stored on other data nodes for fault tolerance. These replicated copies are similar to [Read Replicas](https://arpitbhayani.me/blogs/master-replica-replication) that either synchronously or asynchronously follow the primary copy and keep itself updated.\n\n# Types of partitioning\n\nData can be partitioned in two ways - Horizontal and Vertical. In terms of relational databases, Horizontal Partitioning involves putting different rows into different partitions, and Vertical Partitioning involves putting different columns into separate partitions.\n\nHorizontal partitioning is a very common practice in scaling relational and non-relational databases. It allows us to visit just one partition and get our query answered. It also enables us to split our query load across partitions by making one partition responsible for a particular row/record.\n\nVertical partitioning is seen in action in [Data Warehouses](https://en.wikipedia.org/wiki/Data_warehouse), where we have to crunch a lot of numbers and fire complex aggregation queries. Vertical partitioning is particularly useful when we are not querying all the columns of a particular record and refer to querying a fewer set of columns in each query.\n",
    "similar": [
      "multi-master-replication",
      "indexing-on-partitioned-data",
      "handling-outages-master-replica",
      "data-partitioning-strategies"
    ]
  },
  {
    "id": 6,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "leaderless-replication",
    "title": "Leaderless Replication",
    "description": "In this essay, we take a look into a different way of replication, called Leaderless Replication, that comes in handy in a multi-master setup that demands strong consistency.",
    "gif": "https://media.giphy.com/media/HUkOv6BNWc1HO/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/149646378-0d38d51f-af73-483b-8f05-d5251d6e5b4b.png",
    "released_at": "2022-01-16",
    "total_views": 444,
    "body": "Traditional leader-centric replication strategies revolve around the fact that there will be one Master (leader) node that will acknowledge and accept all the writes operations and then replicate the updates across the replicas (read or master). In this essay, we take a look into a different way of replication, called Leaderless Replication, that comes in handy in a multi-master setup that demands strong consistency.\n\n# Leader-centric Replication\n\nIn a leader-centric replication, there is a Master node that accepts the writes. Upon applying the writes on its copy of data, the database engine sends out the updates across read-replicas or master nodes. Given that all the writes flow through the Master node, the order of the writes is deterministic, slimming down the chances of having a write conflict.\n\n![Leader-centric Replication](https://user-images.githubusercontent.com/4745789/148632480-29a90496-0d65-41f0-a31a-5fb83cf208d0.png)\n\nLeader-centric replication is not fault-tolerant by design because we lose the write operation when the Master node is down. Leaderless replication addresses this concern and ensures our system can handle Write operations even when a subset of nodes are having an outage.\n\n# Leaderless Replication\n\nLeaderless Replication eradicates the need of having a leader accepting the writes; instead, it leverages quorum to ensure strong consistency across multiple nodes and good tolerance to failures. Here's how `WRITE` and `READ` operations happen in such a system.\n\n## Write Operation\n\nSay we have a database cluster of `5` nodes (all Masters). In Leaderless Replication, when a client wants to issue a write operation, it fires this operation on all `5` nodes and waits for `ACK` from at least `3` nodes. Once it receives `ACK` from a majority of the nodes, it marks the write as `OK` and returns; otherwise, it marks the operation as `FAIL`.\n\n![Write Operation in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634243-36259bc6-ee6e-4fd2-b399-c475ad7a405d.png)\n\nEvery record in the database has a monotonically increasing version number. Every successful write updates this version number allowing us and the system to identify the latest value of the record upon conflict.\n\nSay, when the write operation was triggered, it reached just `4` nodes because the fifth node was down; so, when this node comes back up, it gossips with the other `4` nodes and identifies the writes that it missed and then pulls the necessary updates.\n\nOnce the write is `ACK` and confirmed, the nodes gossip internally to propagate the writes to other nodes. There could be a significant delay for the writes to propagate and sync across all nodes; hence we need to ensure that our reading strategy is robust enough to handle this delay while guaranteeing strong consistency.\n\n## Read Operation\n\nGiven that there could be a significant delay in the updates to propagate across all `N` nodes, the Read strategy in Leaderless Replication needs to be robust enough.\n\nLike how the client fanned out the write operation to all the nodes, it also fans out the Read operation to all `N` nodes. The client waits to get responses from at least `N/2 + 1` nodes. Upon receiving the responses from a majority of the nodes, it returns the value having the largest version number.\n\n![Read Operation in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634242-adf09717-a063-455f-b5d2-57497e60ca27.png)\n\nGiven that we mark a write as `OK` only when at least `N/2 + 1` of them `ACK` it and we return our read-only when we get responses from at least `N/2 + 1` nodes, we ensure that there is at least one node that is sending the latest value of the record.\n\nIf we send our read operation to just one node chosen at random, there is a high chance that the value it returns in the response is a stale one, defeating our guarantee of having a strong consistency.\n\n# Generic Leaderless Replication\n\nThe Leaderless Replication system we just discussed is specific because it restricts clients to send write to all `N` nodes and wait for `ACK` from at least `N/2 + 1` nodes. This constraint is generalized in real-world with\n\n - `w`: number of nodes that confirm the writes with an `ACK`\n - `r`: number of nodes we query for the read\n - `n`: total number of nodes\n\nto have a strong consistency i.e. we can expect to get the latest value of any record so long as `w + r > n`, because with this there will be at least one node that has the latest value that will return it. Such reads and writes are called Quorum Reads and Writes.\n\n## Approaches for Leaderless Replication\n\nNow that we understand Leaderless Replication, we look at approaches to implementing it. In the flow discussed above, the approach we saw was the client was sending reads and writes to all the replicated data nodes and, depending on the quorum configuration, decides the correctness. This approach is called *client-driven fan-out* and is very popular.\n\n![Client Driven Fan-out in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634241-245bb1bd-c766-44d0-8f0a-ebd0618c6628.png)\n\nAnother popular approach to implement Leaderless Replication is to have a *Node Coordinator*. The client will make the request to any one node, and it then starts to act as the coordinator for that transaction. This node coordinator will then take care of the fan-out to other nodes and complete the transaction. Upon completion, it returns the response to the client.\n\n![Node Coordinator Driven Fan-out in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634240-0c05e68f-ec35-4ce8-8053-e7334f616dd6.png)\n\nNode coordinator-based replication makes life simpler for clients by offloading all the complications and coordination to the node coordinator. [Apache Cassandra](https://cassandra.apache.org/_/index.html) uses this approach for implementing Leaderless Replication.\n\n# Why Leaderless Replication?\n\nNow that we understand the micro-nuances of Leaderless Replication, let's address the elephant in the room - Why should we even use Leaderless Replication when it is so complex to set up? There are a couple of strong reasons.\n\n## Strong Consistency\n\nLeaderless Replication is strongly consistent by default. In an over-simplified database cluster of `N` nodes, we are fan-out writes to all `N` nodes and wait for the `ACK` from at least `N/2 + 1` nodes; and while reading, we fan-out reads to all `N` nodes and wait for a response from at least `N/2 + 1` nodes, and then return the value that is most recent among all.\n\nGiven that we are playing with the majority here, the set of nodes that handle reads and that handles writes cannot be mutually exclusive and will always have at least `1` overlapping node having the latest value. The assurance of fetching the latest value when Read, no matter how immediate, is how Leaderless Replication ensures a Strong Consistency by design.\n\n> Note: As discussed in previous sections, it is not mandatory to fan-out reads and writes to majority nodes, instead we need a subset of nodes for reads `r`, and another subset for writes `w`, and ensure `r + w > N` for strong consistency.\n\n## Fault tolerance by design\n\nLeaderless Replication is fault-tolerant by design with no [Single Point of Failure](https://en.wikipedia.org/wiki/Single_point_of_failure) in the setup. Given there is no single node acting as a leader, the system allows us to fan-out writes to multiple nodes and wait for an `ACK`; and once we get it from a majority of nodes we are assured that our Write is registered and will never be lost.\n\nSimilarly, the reads do not go to just one node; instead, the reads are also fanned-out to all the nodes, and upon receiving the response from a majority of the nodes, we are bound to get the most recent value in one of those responses given there will be an overlap of at least `1` node where the latest write went and the value was read from.\n",
    "similar": [
      "rum",
      "master-replica-replication",
      "persistent-data-structures-introduction",
      "replication-formats"
    ]
  },
  {
    "id": 7,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "conflict-resolution",
    "title": "Conflict Resolution",
    "description": "In this essay, go in-depth to understand ways to resolve and avoid conflicts in a multi-master setup.",
    "gif": "https://media.giphy.com/media/l2JeiBuUsaFNQIfEk/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/148543014-5b785b5a-fa6c-4ef5-8e8a-656177033e9f.png",
    "released_at": "2022-01-09",
    "total_views": 275,
    "body": "\nEvery multi-master replication setup comes with a side-effect - Conflicts. Conflict happens when two or more database accepts conflicting updates on the same record. We say that the updates are conflicting when we cannot resolve them to one value deterministically. In the previous essay, we took a detailed look into [Conflict Detection](https://arpitbhayani.me/blogs/conflict-detection), and in this one, we go in-depth to understand ways to handle conflicts (resolve and avoid).\n\n# Conflict Resolution\n\nIn the case of a single-master setup, conflicts are avoided by funneling all the writes sequentially. When there are multiple updates on the same field, the last write operation on the record will determine the final value. We can leverage this insight into devising solutions that apply to multi-master setup.\n\nGiven that the writes can hit any Master in a multi-master setup, the challenge is to deterministically find the _order_ of the operations to identify which operation came last. So, the approaches for conflict resolution will all revolve around determining or assigning the order to the operations, somehow.\n\n## Globally unique ID to transaction\n\nOne possible way to determine the order of the write operations spanning multiple masters is to assign globally unique monotonically increasing IDs to each write operation. When conflict is detected, the write operation having the largest ID overwrites everything else.\n\n![Globally Unique ID](https://user-images.githubusercontent.com/4745789/148541564-aafe6b1d-66e8-434e-8879-85180d09be8f.png)\n\nA globally unique, monotonically increasing ID generator has challenges and is an exhausting problem to solve for scale across distributed nodes. Still, it is essential to consider the idea behind the solution and understand the pattern.\n\nThis approach is similar to ordering write on a single master node but without affecting write throughput and concurrent updates. The monotonically increasing globally unique ID gives an implicit ordering to the writes to determine which one came last and hence mimics *Last Write Wins*.\n\n## Precedence of a database\n\nGiven that managing an ID generator at scale could be taxing, another possible solution is to assign the order to the master nodes. Upon conflict, the write from the Master having the highest number wins.\n\n![Database Precedence - Conflict Resolution](https://user-images.githubusercontent.com/4745789/148541568-7f1da590-62ad-4764-9995-a3569fc23e0a.png)\n\nThis approach is very lightweight, given that assigning orders to master nodes is simple and an infrequent activity. This approach will not guarantee the actual ordering of writes, so it is possible that the actual Last Write got overwritten by some write that happened on Master with the higher ID (precedence).\n\n## Track and Resolve\n\nIf, for a use case, it is not possible to resolve the conflicts at the database level, then the best approach in such a scenario is to record the conflict in a data structure designed to preserve all the information. Build a separate job that reads this data structure and resolves the conflict through a custom business logic.\n\n# When to resolve conflict?\n\nThere are two possible places where we can inject our conflict resolution logic (handler); the first one is upon writing, and the second one is upon reading.\n\n## On Write\n\nIn this approach, as soon as a conflict is detected, the custom conflict resolution logic has triggered that resolve the conflict and make the data consistent. This is a more proactive approach to conflict resolution.\n\n## On Reading\n\nThe other approach is to be lazy and resolve conflict when someone tries to read the conflicting data. The custom conflict resolution handler is triggered when the read is triggered on the conflicting data, the database engine realizing it and then invoking the solution handler.\n\nThis lazy approach can be seen in action in scenarios where we have to ensure that the [writes are never rejected](https://arpitbhayani.me/blogs/conflict-detection), no matter what.\n\n# Conflict Avoidance\n\nNow that we have gone through these seemingly complex ways of conflict resolution, it seems better to try to avoid conflicts in the first place. This is indeed the simplest and widely adopted strategy for dealing with conflicts.\n\nA possible way to avoid conflict is by adding stickiness in the system, allowing all writes of a particular record to go to a specific Master node, ensuring sequential operations, and simplifying the core requirement of *Last Write Wins*.\n",
    "similar": [
      "architectures-in-distributed-systems",
      "conflict-detection",
      "mistaken-beliefs-of-distributed-systems",
      "monotonic-reads"
    ]
  },
  {
    "id": 8,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "conflict-detection",
    "title": "Conflict Detection",
    "description": "In this essay, we talk about conflicts and understand what they are, how to detect them in a multi-master setup.",
    "gif": "https://media.giphy.com/media/3o6MbiJPqtMPHykonK/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/143674841-79dc836f-e989-4d25-bfce-ee2939843ad4.png",
    "released_at": "2021-11-28",
    "total_views": 709,
    "body": "Every [multi-master replication](https://arpitbhayani.me/blogs/multi-master-replication) setup comes with a side-effect - Conflicts. Conflict happens when two or more database accepts conflicting updates on the same record. We say that the updates are conflicting when we are unable to resolve them to one value deterministically. In this essay, we talk about conflicts and understand what they are, how to detect them.\n\n# Conflicts\n\nSay we are building an online book store allowing users to purchase books at the click of a button. Like any e-commerce application, even ours has a _Shopping Cart_, which acts as a staging area for everything the user shops for. The user likes a book, adds it to the cart, and proceeds to pay once the shopping is done, proceed to payment. When a user adds a book to the cart, this operation can never be forgotten or rejected - as it will result in loss of revenue and a very poor user experience.\n\nSay the user had books `B1, B2, B3` in the cart already, and this state of the cart is consistent in both the master nodes `Master 1` and `Master 2`. Say the user just added book `B4` to the cart, and this request went to `Master 1` which makes the local state of `Master 1` to be `B1, B2, B3, B4`, while `Master 2` continues to remain at `B1, B2, B3`.\n\nNow, say the user added another book `B5` to the cart, and this request went to `Master 2`. The request reached `Master 2` before the changes from `Master 1` got a chance to be propagated. Since the master node will accept the write the local state of `Master 2` becomes `B1, B2, B3, B5` while state of `Master 1` is `B1, B2, B3, B4`.\n\n![Conflict in real world - shopping cart](https://user-images.githubusercontent.com/4745789/143672208-5be61867-13ba-41dd-bae5-d3f856512d54.png)\n\nThus, we have two versions of the same shopping cart. When the two master nodes syn, they will detect a conflict that needs to be resolved. The resolution to this conflict is not as simple as replacing one with the other because replacement will lead to the loss of information.\n\nThe correct way to address this situation is that we will have to merge the two versions of the cart such similar to the set union. This is a classical case of Conflict Detection and Resolution, and the possible resolution strategy depends on the application and context.\n\nIn the above example, we saw a custom conflict resolution strategy in a real use case, but resolving conflicts is not always possible. For example, when two users book the same seat for the same movie, the requests go to two different Masters, and both successfully acknowledge the user confirming the seat. Thus, in this case, the same seat for the same movie show was allotted to two different users. But, since we already sent the confirmation to the user, there is no way to resolve this conflict without giving one of the users an extremely poor experience.\n\n# Conflict Detection\n\nDetecting conflict is simple when we have a single Master node, given that we can serialize all the writes going through it - the second write waits while the first one executes.\n\nBut when we have a multi-master setup, all the Master nodes can accept the writes and successfully apply them to their copy of data. When the changes [asynchronously](https://arpitbhayani.me/blogs/replication-strategies) propagate to other Master nodes, the conflict is detected. Given that both the writes requests were successfully accepted and applied, there is no way to communicate the conflict to the client.\n\n![Conflict Detection - Async Replication](https://user-images.githubusercontent.com/4745789/143669401-7dbe6429-a802-496a-83ec-aafc58ca2989.png)\n\nGiven that it becomes tough to do something after we detect a conflict when the master-master replication is asynchronous, a possibly easier way out would be to make replication [synchronous](https://arpitbhayani.me/blogs/replication-strategies). In this setup, when one of the Master nodes accepts write, let is successfully apply to its own copy of data and synchronously propagate the write to other Master nodes before responding to the client.\n\n![Conflict Detection - Sync Replication](https://user-images.githubusercontent.com/4745789/143669672-51fcf264-97df-434e-940b-f77e3bfd3f2a.png)\n\nAlthough this approach solves the problem of detecting conflicts and getting a chance to resolve them, it makes the setup lose its main advantage of allowing multiple masters to accept writes in parallel.\n",
    "similar": [
      "architectures-in-distributed-systems",
      "mistaken-beliefs-of-distributed-systems",
      "indexing-on-partitioned-data",
      "monotonic-reads"
    ]
  },
  {
    "id": 9,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "multi-master-replication",
    "title": "Multi-Master Replication",
    "description": "In this essay, we look at what Multi-Master Replication is, the core challenge it addresses, use-cases we can find this replication in action, and the possible concerns of using it.",
    "gif": "https://media.giphy.com/media/3orieW7ikndkDjQuT6/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/140054768-d9bd1eaf-4d26-49bc-9035-024e4a7e010a.png",
    "released_at": "2021-11-03",
    "total_views": 497,
    "body": "\nA not-so-common yet super-useful replication strategy is Multi-Master replication - in which multiple nodes in the cluster accept writes, contrary to what is observed in a typical [Master-Replica replication](https://arpitbhayani.me/blogs/master-replica-replication). In this essay, we look at what Multi-Master Replication is, the core challenge it addresses, use-cases we can find this replication in action, and the possible concerns of using it.\n\n# Multi-Master Replication\n\nA Multi-Master setup has multiple nodes of a database cluster accepting the write requests. It typically depends on the client to pick a Master node, or an abstracted proxy may choose one. As soon as a Master node gets the write request, it accepts it and applies the changes. Once the update is made on one Master, it is [propagated](https://arpitbhayani.me/blogs/replication-strategies) to all other Master nodes of the cluster asynchronously, making the data eventually consistent.\n\nEach Master, thus, also acts as a replica (not read-replica) for the other Masters, reading and applying the updates on its copy of data. Each Master node can optionally form a sub-cluster by adding [read-replicas](https://arpitbhayani.me/blogs/master-replica-replication) and scaling the overall incoming reads.\n\n![Multi-Master Replication](https://user-images.githubusercontent.com/4745789/139211714-fc9266bd-ca22-48c4-9095-c6bff0ae99e6.png)\n\n# Why do we need a Multi-Master setup?\n\nAn exciting exploration is to find why we would ever need a Multi-Master setup and what kind of problem it would solve for us? Here are three key reasons to have a Multi-Master setup.\n\n## Sharing Load\n\nThe most common reason to have a Multi-Master setup is to allow our database cluster to handle more write traffic than a single node. Vertical scaling has theoretical and practical limitations, and the machine can\u2019t go beyond a particular scale.\n\nTo truly scale the database cluster, we have to scale the reads by adding read-replicas and having multiple machines that handle writes. Hence, when the writes on a database cluster become a bottleneck, have multiple Master nodes instead of a single one that can take in incoming writes allowing our cluster to share the load and handle multi-fold of write requests.\n\nTypically, clients choose one of the many Master nodes to send their Write requests. These updates are then propagated [asynchronously](https://arpitbhayani.me/blogs/replication-strategies) to other Masters keeping them in sync with the changes and making the system eventually consistent.\n\n## Maintaining a second copy\n\nThe second common scenario where Multi-Master comes in handy is when we want to keep a second consistent copy of our Master database, which is also required to accept the write requests. This sounds convoluted, but in the real world, such a requirement is widespread. Let\u2019s go through a few scenarios.\n\n### No SPoF Master\n\nJust like any other node in the database cluster, the Master node can also crash. If the only Master node of the cluster takes all the write requests, crashes, it makes the entire ineffective resulting in a massive downtime. This is a classical case of our Master node becoming the [Single Point of Failure](https://en.wikipedia.org/wiki/Single_point_of_failure).\n\nGiven that the failures and crashes are inevitable, it makes sense to have multiple masters running in a cluster and all of them entertaining the write requests. This way, if one of the Master nodes crashes, the other Master can continue to handle the write requests seamlessly, and the cluster will continue to function.\n\n### Lower latencies across geographies\n\nWhen the clients of your database are spread across geographies, the write latencies shoot up since all the writes across all geographies have to go to this one region where the Master resides.\n\nTo keep the write latencies to a minimum across geographies, we set up Multi-Master such that one Master node resides in one region closer to the user. When a client makes the write request, the request can be served from the closest Master giving a great user experience.\n\n### Upgrading the database to a newer version\n\nEvery database needs at least a yearly upgrade, and it is never easy to do it on the fly. Before the version is upgraded, every dependent service typically tests its business logic on a newer version. We need to have two databases running during this exercise - one with the older version handling production and the other with the newer version. Both of these databases require to be kept in sync, and both should accept writes. The writes on the newer database will not be as dense as on the production, considering that the service teams will test their workflows on it.\n\nA typical way to facilitate this parallel setup is to have a Multi-Master replication set up between the two databases - one with an older version serving production traffic, the other with a newer version given to application teams to test their workflows. Apart from testing their workflows, the parallel setup also helps incrementally move traffic from old to new versions keeping the blast radius at a bare minimum in case of failure.\n\nThe other two similar scenarios where Multi-Master replication comes in handy and are very similar to database upgrade are\n\n-   Encrypting the database without taking in a significant downtime\n-   Downscaling an over-provisioned database without a massive downtime\n    \n\n## Need of a split-brain\n\nThe third but particular reason for a Multi-Master setup is where having a split-brain is the necessity and the core of the system. In a general sense, split-brain is considered an erroneous situation that causes mayhem, but it is not a bug but a feature in these scenarios.\n\nA great example of such a system is Collaborative Editing tools like Google Docs, where multiple users on the same document are editing it simultaneously. Each user has its copy of data and edits as if it owns the document wholly. Another example of a split-brain use case has multiple clients using an offline database to work on the same set of values offline and then sync them with a central value store once the internet is back.\n\n# Concerns with Multi-Master\n\nAlthough Multi-Master replication is excellent and solves a wide range of problems in the real world, it comes with its own set of concerns. Before deciding if you want to have a Multi-Master setup, do consider the following concerns.\n\n## Eventual Consistency\n\nWith the replication between Multi-Master being asynchronous, the updates made on one Master will take some time to reflect on the other Masters, making the system eventually consistent. Because of this eventual consistency, a relational database running in Multi-Master mode will lose its ACID guarantees.\n\n## Sluggish Performance\n\nEvery update happening on one Master needs to be sent to every other Master node in the cluster. This data movement adds a considerable load on the network bandwidth and could lead to a sluggish network performance at scale.\n\n## Conflict Resolution\n\nThe main concern while running a database in Multi-Master mode is Conflict. Since all Master nodes accept writes, there may arise situations where the same entity is updated on multiple Master simultaneously, leading to conflicts while syncing. The way these conflicts are handled depends on the application at hand. Some use cases would suggest discarding the entire sequence of writes, while others would mean the last write wins. It becomes the responsibility of the business logic and the use case to define steps to be taken upon a conflict.\n",
    "similar": [
      "handling-outages-master-replica",
      "data-partitioning",
      "replication-formats",
      "data-partitioning-strategies"
    ]
  },
  {
    "id": 10,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "monotonic-reads",
    "title": "Monotonic Reads",
    "description": "Asynchronous replication leads to a fascinating situation where it feels like we are going through a wormhole traveling back and forth in time. In this essay, we understand why this happens and the consequences and devise a quick solution to address it.",
    "gif": "https://media.giphy.com/media/3o6Mb84ODTP8FgQQuI/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/135490742-ecf2f507-2a61-465d-a2de-515a913f5588.png",
    "released_at": "2021-10-03",
    "total_views": 348,
    "body": "Asynchronous replication leads to a fascinating situation where it feels like we are going through a wormhole traveling back and forth in time. In this essay, we understand why this happens and the consequences and devise a quick solution to address it.\n\n# Through the wormhole\n\nAs per Wikipedia, a [wormhole](https://en.wikipedia.org/wiki/Wormhole) can be visualized as a tunnel with two ends at different points in spacetime (i.e., different locations, different points in time, or both), allowing us to traverse back and forth in time again and again. So, where exactly is a wormhole in the context of a distributed datastore?\n\nSay, we have a distributed KV store having one Master and 2 Replica nodes, and we make three updates on a key `X`, the first update `U1` sets `X` as `1`, the second update `U2` sets it to `2`, while the third update `U3` sets `X` to `3`. Like in a typical Master Replica setup, the writes go to the Master, and they are propagated to Replicas through an Asynchronous replication. The reads are typically sent to any one of the Replicas at random.\n\nThe writes are propagated to the Replicas [asynchronously](https://arpitbhayani.me/blogs/replication-strategies), which means both the Replicas will have slight replication lags and say this lag on Replica 1 is of `2 seconds`, and on Replica 2 is `1 second`. As of current time instant, all the three updates `U1`, `U2`, and `U3` have happened on the Master, while only update `U1` has reached Replica 1, and it is lagging behind Replica 2 that saw updates `U1` and `U2`.\n\n![time traveling database - monotonic reads](https://user-images.githubusercontent.com/4745789/135746302-4ff940ba-9ca4-4925-9362-d5fc03f166f6.png)\n\nSay, after making the update `U3` at instant `t`, the User initiates a read that hits Replica 2. Since the update `U3` is yet to reach the Replica 2, it returned `2`, an old value of `X`. This breaks [Read your write consistency](https://arpitbhayani.me/blogs/read-your-write-consistency) and make the user feel that the recent write is lost. Say the user makes another read after this one, which now reaches Replica 1, and since the Replica 1 has just seen the update `U1`, it returns the value `1`, which is even older than the last returned value.\n\nHere we see that after the latest write `U3`, the two successive reads yielded historical values depending on which Replica it hit, giving a feel of traveling back in time. The situation becomes even more interesting when the Replica starts to catch up. Depending on which Replica the read request went to, the User would be oscilating between the old and new values of `X`, giving it a feel of going through the wormhole.\n\n# Monotonic Reads\n\nMonotonic read guarantees users to see value always moving forward in time, no matter how many or how quickly it tries to read the data. It is a weaker guarantee than strong consistency but a stronger one than eventual consistency.\n\n## Achieving Monotonic Reads\n\nThe root cause of this seemingly random fetch lies in allowing the read request to hit Replicas with different Replication Lags. For a particular Replica, the writes are always applied in order, moving forward in time. So, a niche solution for this problem is to make the read request of a user sticky to a replica.\n\n![monotonic reads](https://user-images.githubusercontent.com/4745789/135746307-2c3fc584-7154-4d13-96d9-b1a2b29c7d49.png)\n\nOnce it is ensured that a particular user's request only goes to a specific replica, that User will see updates always moving forward in time as the Replica continues to catch up with the Master.\n\nTo implement stickiness, the server can pick the Replica using the [hash](https://en.wikipedia.org/wiki/Hash_function) of the User ID instead of picking it randomly. This way, the stickiness between a user and a Replica helping us achieve Monotonic Reads.\n",
    "similar": [
      "conflict-detection",
      "architectures-in-distributed-systems",
      "conflict-resolution",
      "indexing-on-partitioned-data"
    ]
  },
  {
    "id": 11,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "read-your-write-consistency",
    "title": "Read-your-write Consistency",
    "description": "Read-Your-Writes consistency states that the system guarantees that, once an item has been updated, any attempt to read the record by the same client will return the updated value.",
    "gif": "https://media.giphy.com/media/3o6MbsTCszAbi6f1aE/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/134203710-5284102a-4d0f-43b1-bcd9-6668040ac550.png",
    "released_at": "2021-09-22",
    "total_views": 1120,
    "body": "The most common way to scale the reads hitting a distributed data store is by adding [Read Replicas](https://arpitbhayani.me/blogs/master-replica-replication). These replicas handle all the reads of the systems freeing up the Master to deal with the writes. Although Replicas do help us scale, it brings a new set of problems; and in this essay, we discuss one such issue, called \"Read-your-write\" consistency, and look at possible solutions.\n\n# The Problem\n\nIn a [Master-Replica](https://arpitbhayani.me/blogs/master-replica-replication) setup, the Writes happening on the Master take some time to reach the Replica. This delay in propagation is called Replication Lag. If a client has made a Write and is immediately trying to read the written item, this read may go to the Read Replica that is yet to sync with the Master.\n\nWhen the client issues the Read on a Replica that has yet to receive the write, it leads to an undesirable behavior wherein the client will see the old value (or null) and think that the write it made was lost.\n\n**Read-Your-Writes consistency** states that the system guarantees that, once an item has been updated, any attempt to read the record by the same client will return the updated value. This consistency makes no promises about other clients getting the updated value immediately after the Write and is meant to reassure the user that their Write is successful.\n\n## Problem in action\n\nTo ensure that the issue we are trying to address is not something made up, let's see what happens across industries when we do not ensure Read-your-write consistency.\n\nImagine you made a post on a Social Platform, and when you refreshed the page, it threw a 404 error saying Post does not exist, or when you fixed a spelling mistake on the post and refreshed the page, you still see the old text with the same spelling mistake. This inconsistency leads to a terrible user experience.\n\n![Read after Write Fails - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198510-78129b65-5c4c-4d88-a10a-39523d1886d7.png)\n\nIn some cases, caching is the root cause; but it is also possible that the Read request for the post was routed to the Read Replica, which was yet to apply the write that happened on the Master, typically due to Replication Lag.\n\nA few more examples of why we need Read-Your-Write consistency:\n - Imagine getting a match on Tinder and disappearing upon refresh\n - Imagine buying an AAPL Stock and seeing no trace of it on the orders page\n - Imagine adding items in your Amazon cart and realizing it empty when placing the order\n\n# Implementing Read-your-write consistency\n\nThe primary root cause of not having Read-your-write consistency is Replication Lag. The longer it takes for the write to propagate to the Replica, the longer our end user will see an inconsistent behavior depending on which Read Replica serves the read. So, every single solution revolves all-around reading from a place where Replication Lag is zero. We start dissecting and devising approaches to address this problem.\n\n## Synchronous Replication\n\nReplication Lag exists because the writes are propagated to Replica [asynchronously](https://arpitbhayani.me/blogs/replication-strategies). If the replication is done synchronously, every Write operation on Master is not termed completed unless it is done replicating it on all the Replicas. This way, the Master and the Replica will always remain in sync with ZERO Replication Lag, and no matter to which Replica the read is forwarded, it will always have the latest copy of the data.\n\n![Synchronous Replication - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/128765459-67347320-5b77-4722-884b-015fc1b0c5fb.png)\n\nSynchronous Replication sounds tempting and foolproof approach, but it comes at a massive cost. Synchronous Replication severely affects the write throughput of the database. More than that, write failing on any one of the Replica will choke the entire system. Gaining such Strong Consistency at the expense of write throughput and availability is not a great choice.\n\n## Pinning User to Master\n\nInstead of serving Read requests from the Replica, what if we also serve them from the Master. Forwarding all the read requests to Master defeats the purpose of creating Read Replica - scaling reads. But since we know that the Master will always have the latest copy of the data, can we devise something around it?\n\nInstead of routing all the reads from all the users to the Master, what if we routed reads of the User who recently performed the Write to the Master? This sounds promising and addresses our concern, and this exactly is Pinning the User to the Master.\n\nWhen a user performs a Write operation, for a specific time window, we pin the User to the Master node, which means every single Read and Write coming from the user will go to the Master, which means the Reads will happen from the data node that always has the latest copy of the data and hence we would achieve Read-your-write consistency.\n\nThe time window for pinning should be big enough to ensure that the Writes happened on the Master would have propagated to all the Replicas; this ensures that once the pinning window is over and reads of the user start hitting the Replica, it would continue access the latest copy of the data.\n\n![User PInning to Master - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198508-4c8bd1e4-2336-4063-8ceb-06e675c24554.png)\n\nAlthough this solves the problem well, it is not optimal when the system is very write-heavy. If in a system most users Write, this would mean the requests most users will go to the Master, most of the time, defeating the purpose of Replica and becoming the bottleneck. \n\n## Fragmented Pinning\n\nPinning a user to the Master would mean queries, both Read and Write, made by the user will hit the Master for a configured time window. But instead of pinning everything, what if we pick only a few critical reads to hit the Master; this is Fragmented Pinning.\n\nFor example, in social media, once the user made or updated a post, pin the user to the Master for 10 minutes such that the request for getting the post goes to the Master; all other reads would continue to hit the Replica.\n\n![Fragmented Pinning - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198504-5b886713-9a16-45ba-9a63-332d19b5894c.png)\n\nBy doing fragmented pinning, we ensure that most critical and most likely Read operations, during the pinned window, go to the Master, ensuring that our Master is not overwhelmed even when the system is write-heavy.\n\n## Master Fallback\n\nThere is one more way of ensuring Read-your-write consistency, but it works well for a system with a lower Replication Lag, and most queries made on the data store are for keys that exist, i.e., fewer 404s, and the approach is using Master as a fallback.\n\n![Master Fallback - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198497-099bce25-bef1-468e-84b2-69b31e1ae3e0.png)\n\nThere is no User Pinning in this approach, and all the Read operations go to the Replica while the Master node only handles Write. The Master and Replica are kept in sync using [asynchronous replication](https://arpitbhayani.me/blogs/replication-strategies). If the Read request that went to the Replica resulted in the 404, i.e., Key Not Found, the application forwards the same query on the Master node and then returns the response.\n\nSince the reads go to the Replica and the Master every time the data is not present in Replica, for this system to be efficient, we need fewer cases where this particular path would be taken, and also the Replication Lag to not inflate much.\n",
    "similar": [
      "new-replica",
      "handling-outages-master-replica",
      "multi-master-replication",
      "replication-strategies"
    ]
  },
  {
    "id": 12,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "handling-outages-master-replica",
    "title": "Handling outages in a Master-Replica setup",
    "description": "This essay talks about the worse - nodes going down - impact,  recovery, and real-world practices.",
    "gif": "https://media.giphy.com/media/xT5LMskx2CiLPpAkk8/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/132286179-14358b33-95cf-4d48-9419-5eaa727ffd56.png",
    "released_at": "2021-09-07",
    "total_views": 375,
    "body": "Master-Replica architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines. In the previous essay, we saw how a [new replica](https://arpitbhayani.me/blogs/new-replica) is set up in a distributed data store and the challenges that come with that. This essay talks about the worse - nodes going down - impact, recovery, and real-world practices.\n\n# Nodes go down, and it is okay\n> Anything that can go wrong will go wrong. - [Murphy's Law](https://en.wikipedia.org/wiki/Murphy%27s_law)\n\nOutages are inevitable; we cannot completely eradicate them, but we can be prepared enough to minimize the impact. One way to make any distributed data store robust is by assuming that the node goes down after executing every operation. Building systems around these aggressive failure scenarios is the easiest way to make any distributed system robust and fault-tolerant.\n\nA few reasons why nodes crash are: system overload, hardware issues, overheating, physical damage, or worse, a natural disaster. Nodes going down is a very common phenomenon, and it happens all the time in massive infrastructures. So, instead of panicking, deal with it by minimizing cascaded failures and speeding up the recovery.\n\nIn a Master-Replica setup, there are two kinds of nodes - Master and Replica. Let's see what happens when the nodes start crashing and recover the system after the crash.\n\n# Replica outage\n\nWhen Replica is down, the reads going to it fail. If the nodes are within an abstracted cluster, the front-facing proxy can redirect the request to another Replica upon discovering the outage.\n\nTalking about writes during the Replica outage, we know that the Replica does not handle any writes directly from the client. Still, it does pull the updates through the [Replication log](https://arpitbhayani.me/blogs/replication-formats) and re-applies the changes on its own copy of data. So, are these writes affected during the outage?\n\nTo keep track of the updates that Replica already applied to its data, it keeps track of the [Sequence Number](https://arpitbhayani.me/blogs/new-replica) of the operation. This **sequence number** is stored on disk and is atomically updated after pulling and applying update operation from the Master.\n\nWhen the Replica crashes and recovers, it reconnects to the Master, and the hindered replication resumes from the last processed sequence number from what was persisted on the disk. Once the replication resumes, the Replica will fetch all the updates that happened on the Master, apply them on its own copy of data, and eventually catch up. \n\n# Master outage\n\nMaster is the most critical component in any distributed data store. When the Master crashes and becomes unavailable, it does not accept any request (read or write) coming to the system. This directly impacts the business, resulting in the loss of new writes coming to the system. \n\nThe 3 typical steps in fixing the Master outage are:\n\n - Discover the crash\n - Set up the new Master\n - Announce the new Master\n\n## Discovering the crashed Master\n\nThe first step of fixing the crashed master is to identify that the Master crashed. This discovery step is not just limited to Master but is also applicable to Replica and other relevant nodes in the datastore. So, although we might use the term \"Master\" while discovering the crash, the process and the flow would be the same for other nodes.\n\nA typical process of detecting a crash is as simple as checking the **heartbeat** of the Master. This means either Master ping the orchestrator that it is alive or the orchestrator checking with the Master if it is healthy. In both cases, the orchestrator would expect that the Master responds within a certain threshold of time, say 30 seconds; and if the Master does not respond, the orchestrator can infer the crash.\n\n![discovering an outage](https://user-images.githubusercontent.com/4745789/132089584-a5177e7a-3104-4f86-8d9d-cbb106b7ce35.png)\n\nIn a typical architecture, the orchestrator is a separate node that keeps an eye on all the nodes and repeatedly checks how they are doing. There are [Failure Detection](https://en.wikipedia.org/wiki/Failure_detector) systems that specialize in detecting failures, and one efficient and interesting algorithm for detecting failure is called [Phi \u03c6 Accrual Failure Detection](https://arpitbhayani.me/blogs/phi-accrual) that instead of expecting a heartbeat at regular intervals estimates the probability and sets a dynamic threshold.\n\n## Setting up the new Master\n\nWhen the Master crashes, there are two common ways of setting up the new Master - manual and automated. Picking one over the other depends on the sophistication and maturity of the organization and the infrastructure. Still, it is typically observed that smaller organizations tend to do it manually, while the larges ones have automation in place.\n\n### The manual way\n\nOnce the orchestrator detects the Master crash, it raises the alarm to the infrastructure team. Depending on the severity of the incident and the nature of the issue, the infrastructure engineer will either.\n\n - reboot the Master node, or\n - restart the Master database process, or\n - promote one of the Replica as the new Master\n\nSetting up the Master after the crash manually is common when the team is lean, and the organization is not operating at a massive infrastructure scale. Master crashing is also once in a blue moon event, and setting up complex automation just for one rare occurrence might not be the best use of the engineer's time.\n\nThe entire process is automated once the infrastructure grows massive, and even an outage of a few minutes becomes unacceptable.\n\n### The automated way\n\nIn a Master-Replica setup, the automation is mainly around promoting an existing replica as the new Master. So, when the orchestrator detects that the Master crashed, it triggers the [Leader Election](https://en.wikipedia.org/wiki/Leader_election) among the Replicas to elect the new Master. The elected Replica is then promoted, the other Replicas are re-configured to follow this new Master.\n\n![electing the new master](https://user-images.githubusercontent.com/4745789/132089586-d52e558f-10e2-4f10-8807-5920f9117ea8.png)\n\nThe new Master is thus ready to accept new incoming writes and reads to the system. This automated way of setting up the new Master is definitely faster, but it requires a lot of sophistication from the infrastructure, the algorithms, and practices before implementation.\n\n## Announcing the new Master\n\nOnce the new Master is set up, either manually or elected among the Replicas, this information must be conveyed to the end clients connecting to the Master. So, as the final step of the process, the Clients are re-configured such that they now start sending writes to this new Master.\n\n![announcing the new master](https://user-images.githubusercontent.com/4745789/132089587-dc7e25c6-f43d-4be5-a2c1-e09253ec2205.png)\n\n# The Big Challenge \n\nWhat if the Master crashed before propagating the changes to the Replica? In such scenarios, if the Replica is promoted as the new Master, this would result in data loss, or conflicts, or split-brain problems if the old Master continues to act as the Master.\n\n## The Passive Master\n\nData loss or inconsistencies is unacceptable in the real world, so as the solution to this problem, the Master always has a passive standby node. Every time the write happens on the Master, it is [synchronously replicated](https://arpitbhayani.me/blogs/replication-strategies) to this stand by passive node, and asynchronously to configured Replicas.\n\nThe write made on the Master is marked as complete only after the updates are synchronously made on this passive Master. This way, the passive Master node is always in sync with the Master. So when the main Master node crashes, we can safely promote this passive Master instead of an existing Replica.\n\n![The Passive Master](https://user-images.githubusercontent.com/4745789/132282367-50feb0be-f952-4bf3-ab62-85ab4f6c86d6.png)\n\nThis approach is far better and accurate than running an election across asynchronously replicated Replicas. Still, it incurs a little extra cost of running a parallel Master that will never serve production traffic. But given that it helps in avoiding data loss, this approach is taken by all managed database services.\n",
    "similar": [
      "multi-master-replication",
      "replication-formats",
      "replication-strategies",
      "data-partitioning"
    ]
  },
  {
    "id": 13,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "new-replica",
    "title": "The New Replica",
    "description": "In this one, we take a look into how these Replicas are set up and understand some quirky nuances about Replication.",
    "gif": "https://media.giphy.com/media/l0HlyVvfbfr1RKR0I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/130425162-e02fcffe-3c61-4e90-98b6-6c7a25cb884e.png",
    "released_at": "2021-08-23",
    "total_views": 293,
    "body": "A distributed data store adds Replica to scale their reads and improve availability. This is the most common and logical way to scale the throughput of a system without massively changing the architecture. In the previous essays, we talked about [Master-Replica Replication](https://arpitbhayani.me/blogs/master-replica-replication), different [Replication Strategies](https://arpitbhayani.me/blogs/replication-strategies), and [Replication Formats](https://arpitbhayani.me/blogs/replication-formats). In this one, we take a look into how these Replicas are set up and understand some quirky nuances.\n\n# Setting up a Replica\n\nIn the [Master-Replica setup](https://arpitbhayani.me/blogs/master-replica-replication), Replica is a node that follows the Master. The updates happening on the Master are communicated to the Replica through the process called Replication. The Master publishes the updates on the Replication Log, which are then pulled by the Replica and applied on its own copy of data.\n\nThe Replica nodes are read-only in the Master-Replica setup, making this architecture pattern suitable for scale reads and improving availability. The most typical steps taken when a new Replica is set up are\n\n1.  Take a point-in-time **snapshot** of the Master data.\n2.  **Spin up** a Replica node with this snapshot.\n3.  **Start** the process on the Replica and configure it to follow the Master.\n4.  The process of **Replication** begins, and the Replica eventually **catches up**.\n\n![The new Replica](https://user-images.githubusercontent.com/4745789/130204028-db759df1-2ea9-4aa5-98f4-6cb3e2b16813.png)\n\nNow that we have talked about the general process of setting up a new Replica, let's dissect the steps and answer really quirky questions about it.\n\n## Replica keeping track of Replication\n\nOnce the Replication is set up between the Replica and the Master, one of the key things to understand is how a Replica keeps track of operations and updates that it has pulled and applied on its own copy of data.\n\nThe idea to achieve this is simple. Every update on the Master is associated with a monotonically increasing **sequence number**. Both the Master and the Replica keep track of this sequence number, and it denotes the sequence number of the last operation executed on their respective copy of the data.\n\nSince the Master generates the sequence number, it holds the latest one. The Replica could be a couple of sequence numbers behind, as it needs to pull the updates from the Master, apply the updates, and then update the sequence number. Thus, by tracking the sequence number, the Replica keeps track of the Replication, order of the updates, and understands the Replication lag.\n\n![Sequence Number: Replica](https://user-images.githubusercontent.com/4745789/130345784-8892f5f4-7ed1-4588-bbac-08ce39b7c752.png)\n\nSince the Replica persists the sequence number on disk, even if the server reboots, it can continue to resume the Replication since the reboot.\n\n## Why do we need a point-in-time snapshot?\n\nNow that we know how a Replica keeps track of the replication, we answer an interesting question; do we really need a point-in-time snapshot of Master to create a Replica?\n\nThe answer to this situation is simple; it is not mandatory to take a point-in-time snapshot of Master and create Replica out of it. We can also do it on a blank data node with no hiccups at all. The only caveat here is that when we set up Replication on a blank data node, it will have to pull in all the update operations on the Master node and apply them to its own copy of the data.\n\nWhen a Replica needs to pull in literally every single update operation and apply, it will take a far longer time to catch up with the Master. The Replica will start with an extremely high Replica lag, but eventually, this lag will reduce. Nonetheless, it will take a lot of time to catch the Master, rendering this approach unsuitable.\n\nWhen the point-in-time snapshot is taken, the sequence number of the Master, at that instant, is also captured. This way, when the Replication is set up on this copy of data, it will have far fewer operations to replicate before it catches up with the Master. Hence, instead of creating Replica from scratch, setting it up from a recent point-in-time snapshot of Master makes the Replica quickly catch up with the Master.\n\n## How does a Replica catch up with the Master?\n\nReplica pulls the replication log from the Master node and applies the changes on its own copy of data. If Replica is already serving live read requests, how it actually catches up with the Master?\n\nThe entire Replication process is run by a separate Replication thread that pulls the data from the Replication Log of the Master and applies the updates on its own copy of the data. For Replication to happen, the thread needs to be scheduled on the CPU. The more CPU this Replication thread gets, the faster the replication would happen. This is how the Replica continues to Replicate the updates from the Master while serving the live traffic.\n\n## Is it possible for a Replica never to catch the Master?\n\nIf the progress of Replication depends on the CPU cycles that the Replication thread gets, does this mean it is possible for a Replica never to catch the Master?\n\nYes. It is very much possible for a replica to never catch up with the Master. Since the Replica typically also serves the live read traffic, if some queries are CPU intensive or take massive locks on the tables, there are chances that the Replication thread might get a minimal CPU to continue to replication.\n\nAnother popular reason a Replica might never catch up with the Master is when the Master is overwhelmed with many write operations. The influx of write is more than what Replica can process, leading to an ever-increasing Replica lag.\n\nHence whenever a Replica sees a big enough Replica lag, the remediation is\n\n-   to kill read queries that are waiting for a long time, or\n-   to not let it serve any live traffic for some time, or\n-   to kill CPU intensive read queries, or\n-   to kill queries that have taken locks on critical data\n\nWe ensure that the Replication thread gets CPU that it deserves to continue the replication by taking some or all of the above actions. Our intention while fighting high replica lag is to reduce somehow the load on the CPU, whatever it takes.\n",
    "similar": [
      "replication-strategies",
      "read-your-write-consistency",
      "handling-outages-master-replica",
      "multi-master-replication"
    ]
  },
  {
    "id": 14,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "replication-formats",
    "title": "Replication Formats",
    "description": "When we are employing a Master-Replica pattern to improve availability, throughput, and fault-tolerance, the big question that pops up is how the writes happening on the Master propagates to the  Replica. In this essay, we will talk about exactly this and find out about Replication Formats.",
    "gif": "https://media.giphy.com/media/l0HlyVvfbfr1RKR0I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/129456630-73db5669-5fb9-47a6-bd13-72fd0c5a2e54.png",
    "released_at": "2021-08-15",
    "total_views": 334,
    "body": "[Master-Replica](https://arpitbhayani.me/blogs/master-replica-replication) architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines.\n\nSo, when we are employing a Master-Replica pattern to improve availability, throughput, and fault-tolerance, the big question that pops up is how the writes happening on the Master propagates to the Replica. In this essay, we will talk about exactly this and find out about Replication Formats.\n\n# Write Propagation\nOnce the write operation is successful on the Master node, the changes must be propagated to all the Replicas. This is done via a log known as Replication Log, Commit Log, or Binary Log. Once the write on the Master is successful, an event is recorded in this Log file; and this file is then **pulled** by the Replicas.\n\nOnce the Replica gets this log file, it goes through the events and starts applying the necessary changes on its copy of the data. This way, it continuously follows the changes happening on the Master node. The time elapsed between the write operation on the Master and the operation taking effect to the Replica is called Replication Lag.\n\nNow that we have a solid understanding of Write Propagation, we focus on the primary agenda of this essay, Replication Format, and talk about how these formats share the entire Replication process.\n\n# Replication Formats\nAny write operation happening on the Master is logged in the Replication log file as an event. The format in which these events are logged in the Log file is called Replication Format. The two Replication formats that are widely used across distributed data stores are Statement-based and Row-based formats.\n\n## Statement-based Format\nIn Statement-based format, the Master node records the operation as an event in its log, and when the Replica reads this log, it executes the same operation on its copy of data. This way, the operation on the Master node is executed on the Replica, which keeps it in sync with the Master.\n\nSay the Client fires an operation on the Master to bulk update all the `5` tasks of a user to be marked as `done`. The operation fired by the Client on the Master node would look something like this.\n\n```sql\n  UPDATE tasks SET is_done = true WHERE user_id = 53;\n```\n\nWhen the write operation is completed on the Master, this exact operation is recorded as an event in the Replication Log file. When the Replica reads this log file, the node executes this operation and updates the same `5` tasks on its own copy of the data.\n\n![statement based replication](https://user-images.githubusercontent.com/4745789/129456634-be745df6-541b-4e75-a1e3-4f4f625cc45e.png)\n\n### Advantages\nThe events recorded in the Replication Log are the actual operations that happen on the Master. Hence, the log files take up the bare minimum storage space required. It will not matter if the operation affects one row or thousand; it will be recorded as one event in the Log file.\n\nAnother great benefit of this format is that it can be used to audit the operations on the database because we are recording the operations verbatim in the Log file.\n\n### Disadvantages\nThe biggest and the most significant disadvantage of the Statement-based format show up when the non-deterministic operations are fired on the Master. The operations such as `UUID()`, `RAND()`, `NOW()`, etc, generate value depending on factors that are not under our control. When these operations fire on the Replica, they might generate values different from the value they yielded on the Master, leading to data corruption.\n\nSince the Replica node, apart from replicating from the Master, is also actively handling requests, some locks might be taken on some of its entities by the executing queries. When a conflicting query is fired from the replication thread, it could result in unpredictable deadlock or stalls.\n\n## Row-based Format\nIn Row-based format, the Master node logs the updates on the individual data item instead of the operation. So the entry made in the Log file would indicate how the data has changed on the Master. Hence, when the Replica reads this log, it updates its copy of the data by applying the changes on its data items. This way, the operation on the Master node happens on the Replica, and the Replica nodes remain in sync with the Master.\n\nSay the Client fires an operation on the Master to bulk update all the `5` tasks of a user to be marked as `done`. The operation fired by the Client on the Master node would look something like this.\n\n```sql\n  UPDATE tasks SET is_done = true WHERE user_id = 53;\n```\n\nIn the row-based format, instead of recording the operation, the Master node records the updates made on the data items. Since the operation in question updated `5` rows, the events recorded in the Replication Log file would contain `5` entries, one for each data item changed and would look something similar to\n\n```sql\ntasks:121 is_done=true\ntasks:142 is_done=true\ntasks:643 is_done=true\ntasks:713 is_done=true\ntasks:862 is_done=true\n```\n\nHence, one operation on the Master is fanned out as series of updates on the data items and is consumed by the Replica. The Replica then reads these events and applies the changes on its copy of the data.\n\n![row based replication](https://user-images.githubusercontent.com/4745789/129456632-ad7b67ae-7ff0-4d35-97b0-0ea6d6a3bd87.png)\n\n### Advantages\nThe major advantage of the Row-based format is that all the changes can be safely and predictably applied on the Replica. This approach is safe even with the non-deterministic operations because what gets written is the computed value.\n\n### Disadvantages\nWhen the Master node completes the operation, it takes the lock on the Replication Log file and then records the events. Since the number of events recorded in the log file will be the number of data items changed, the lock taken by the Master node will be longer, choking the throughput.\n\nAnother obvious disadvantage in this approach is fan-out. If an operation changes 5000 data items, it will result in 5000 events in the Log file, and if such operations are frequent, this will make Logfile take a lot of storage space.\n",
    "similar": [
      "handling-outages-master-replica",
      "multi-master-replication",
      "replication-strategies",
      "rum"
    ]
  },
  {
    "id": 15,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "replication-strategies",
    "title": "Replication Strategies",
    "description": "In this essay, we take a quick yet verbose look into Synchronous, Asynchronous, and Semisynchronous replication strategies and understand their implications.",
    "gif": "https://media.giphy.com/media/l0HlyVvfbfr1RKR0I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/128717208-a0f495c1-ccf6-4039-b936-f1cc29d722f3.png",
    "released_at": "2021-08-10",
    "total_views": 821,
    "body": "In a distributed system, when replication is set up between data nodes, there are typically three replication strategies - Synchronous, Asynchronous, and Semi-synchronous. Depending on the criticality of data, its consistency, and the use-case at hand, the system chooses to apply one over another. In this essay, we take a quick yet verbose look into these strategies and understand their implications.\n\nBefore we jump into the replication strategies, let's first understand the need for them. When the data is replicated, multiple copies of the same data are created and placed on multiple machines (nodes). A system replicates the data to\n\n-   improve availability, or\n-   prepare for disaster recovery, or\n-   improve performance by leveraging parallel reads across replicated data, or\n-   keep the data geographically close to the user, e.g., CDN\n\nThe Replication comes into action when the Client initiates the write on the Master node. Once the Master updates its copy of the data, the replication strategy dictates how the data would be replicated across the Replicas.\n\n## Synchronous Replication\nIn Synchronous Replication, once the Master node updates its own copy of the data, it initiates the write operation on its replicas. Once the Replicas receive the update, they apply the change on their copy of data and then send the confirmation to the Master. Once the Master receives the confirmation from all the Replicas, it responds to the client and completes the operation.\n\n![synchronous replication](https://user-images.githubusercontent.com/4745789/128765459-67347320-5b77-4722-884b-015fc1b0c5fb.png)\n\nIf there is more than one Replica in the setup, the Master node can propagate the write sequentially or parallelly. Still, in either case, it will continue to wait until it gets a confirmation, which will continue to keep the client blocked. Thus having a large number of Replicas means a longer block for the Client, affecting its throughput.\n\nSynchronous Replication ensures that the Replicas are always in sync and consistent with the Master; hence, this setup is fault-tolerant by default. Even if the Master crashes, the entire data is still available on the Replicas, so the system can easily promote any one of the Replicas as the new Master and continue to function as usual.\n\nA major disadvantage of this strategy is that the Client and the Master can remain blocked if a Replica becomes non-responsive due to a crash or network partition. Due to the strong consistency check, the Master will continue to block all the writes until the affected Replica becomes available again, thus bringing the entire system to a halt.\n\n## Asynchronous Replication\nIn Asynchronous Replication, once the Master node updates its own copy of the data, it immediately completes the operation by responding to the Client. It does not wait for the changes to be propagated to the Replicas, thus minimizing the block for the Client and maximizing the throughput.\n\nThe Master, after responding to the client, asynchronously propagates the changes to the Replicas, allowing them to catch up eventually. This replication strategy is most common and is the default configuration of most distributed data stores out there.\n\n![asynchronous replication](https://user-images.githubusercontent.com/4745789/128765466-944bf36e-6817-4cf3-9ea4-0ffa724f0d58.png)\n\nThe key advantage of using a fully Asynchronous Replication is that the client will be blocked only for the duration that the write happens on the Master, post which the Client can continue to function as before, thus elevating the system's throughput.\n\nOne major disadvantage of having a fully Asynchronous Replication is the possibility of data loss. What if the write happened on the Master node, and it crashed before the changes could propagate to any of the Replicas. The changes in data that are not propagated are lost permanently, defeating durability. Although Durability is the most important property of any data store,\n\nAsynchronous Replication is the default strategy for most data stores because it maximizes the throughput. The third type of replication strategy addresses durability without severely affecting throughput, and it is called Semi-synchronous Replication.\n\n## Semi-synchronous Replication\nIn Semi-synchronous Replication, which sits right between the Synchronous and Asynchronous Replication strategies, once the Master node updates its own copy of the data, it synchronously replicates the data to a subset of Replicas and asynchronously to others.\n\n![semi-synchronous replication](https://user-images.githubusercontent.com/4745789/128833772-d0bbae7d-5e00-4771-90e5-996326affb60.png)\n\nThe Semi-synchronous Replication thus addresses the durability of data, in case of Master crash, at the cost of degrading the Client's throughput by a marginal factor.\n\nMost of the distributed data stores available have configurable replication strategies. Depending on the problem at hand and the criticality of the data, we can choose one over the other.\n",
    "similar": [
      "new-replica",
      "handling-outages-master-replica",
      "replication-formats",
      "multi-master-replication"
    ]
  },
  {
    "id": 16,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "master-replica-replication",
    "title": "Master-Replica Replication",
    "description": "In this essay, we talk about everything we should know about Master-Replica replication pattern.",
    "gif": "https://media.giphy.com/media/l1KtVkekpcV4gKjyE/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/128563709-a95f9f55-f87b-458d-b6c4-562ef894805d.png",
    "released_at": "2021-08-07",
    "total_views": 350,
    "body": "Master-Replica architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines. In this essay, we talk about everything we should know about the Master-Replica replication pattern.\n\nA system that adheres to the Master-Replica replication architecture contains multiple nodes and each node, called Replica, holds an identical copy of the entire data. Thus if there are N nodes in the system, there will be N copies of the data.\n\n![Master-Replica Replication](https://user-images.githubusercontent.com/4745789/128564165-92d3413a-a329-4456-b055-177ed83e989a.png)\n\n# Scaling Reads\n\nWith  N nodes capable of serving the data, we easily scale up the reads by a  factor of N. Hence, this pattern is commonly put in place to amplify and scale the reads that the system can handle.\n\n# Handling Writes\n\nWith  N nodes that hold and own the data, the writes become tricky to handle.  In the Master-Replica setup, the writes go to one of the pre-decided nodes that act as the Master. This Master node is responsible for taking up all the writes that happen in the system.\n\nMaster is not any special node; rather, it is just one of the Replicas with this added responsibility. Thus in the system of N nodes, 1 node is the Master that takes in all the writes, while the other N - 1 node caters to the read requests coming from the clients.\n\n# Write Propagation\n\nOnce the write operation is successful on the Master node, the changes are propagated to all the Replicas through Replication Log (Commit Log, Bin  Log, etc.), letting the system eventually catch up.\n\nThe time elapsed between the write operation on the Master and the operation propagating to the Replica is called Replication Lag. This is one of the core metrics that is observed 100% of the time.\n\n# What happens when the Master goes down?\n\nSince the Master node takes in all the write operations, it going down is a  massive event. The write operation that happens when the Master is facing an outage results in an error.\n\nWhen the system detects such an outage, it tries to auto-recover by promoting one active Replica as the new Master by running a Leader Election algorithm. All the healthy Replicas participate in this election and, through a consensus,  decide the new Master.\n\nOnce the new Master is elected, the system starts accepting and processing the writes again.\n\n# Master-Replica in action\n\nThis is a widespread pattern that we can find across almost all the databases and distributed systems. Some of the most common examples are:\n\n - Relational databases like MySQL, PostgreSQL, Oracle, etc.\n - Non-relational databases like MongoDB, Redis, etc.\n - Distributed brokers like Kafka, etc.  \n",
    "similar": [
      "rum",
      "leaderless-replication",
      "replication-formats",
      "multi-master-replication"
    ]
  },
  {
    "id": 17,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "durability",
    "title": "\"D\" in ACID - Durability",
    "description": "Durability seems to be a taken-for-granted requirement, but to be honest, it is the most important one. Let's deep dive and find why it is so important? How do databases achieve durability in the midst of thousands of concurrent transactions? And how to achieve durability in a distributed setting?",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/126130275-10c1c4cf-483a-42d7-bd78-2f0ff4da4947.png",
    "released_at": "2021-07-19",
    "total_views": 335,
    "body": "After discussing the \"[A](https://arpitbhayani.me/blogs/atomicity)\", the \"[C](https://arpitbhayani.me/blogs/consistency)\", and the \"[I](https://arpitbhayani.me/blogs/isolation)\", it is time to take a look at the \"D\" of ACID - [Durability](https://en.wikipedia.org/wiki/Durability_(database_systems)).\n\nDurability seems to be a taken-for-granted requirement, but to be honest, it is the most important one. Let's deep dive and find why it is so important? How do databases achieve durability in the midst of thousands of concurrent transactions? And how to achieve durability in a distributed setting?\n\n# What is Durability?\n\nIn the context of Database, Durability ensures that once the transactions commit, the changes survive any outages, crashes, and failures, which means any writes that have gone through as part of the successful transaction should never abruptly vanish.\n\nThis is exactly why Durability is one of the essential qualities of any database, as it ensures zero data loss of any transactional data under any circumstance.\n\nA typical example of this is your purchase order placed on Amazon, which should continue to exist and remain unaffected even after their database faced an outage. So, to ensure something outlives a crash, it has to be stored in non-volatile storage like a Disk; and this forms the core idea of durability.\n\n# How do databases achieve durability?\n\nThe most fundamental way to achieve durability is by using a fast transactional log. The changes to be made on the actual data are first flushed on a separate transactional log, and then the actual update is made.\n\nThis flushed transactional log enables us to reprocess and replay the transaction during database reboot and reconstruct the system's state to the one that it was in right before the failure occurred - typically the last consistent state of the database. The write to a transaction log is made fast by keeping the file append-only and thus minimizing the disk seeks.\n\n![Durability in ACID](https://user-images.githubusercontent.com/4745789/126114187-0febc1ad-e35f-4d49-991c-8a5d8a0d9221.png)\n\n# Durability in a distributed setting\n\nIf the database is distributed, it supports Distributed Transactions, ensuring durability becomes even more important and trickier to handle. In such a setting, the participating database servers coordinate before the commit using a Two-Phase Commit Protocol.\n\nThe distributed computation is converged into a step-by-step process where the coordinator communicates the commit to all the participants, waits for all acknowledgments, and then further communicates the commit or rollback. This entire process is split into two phases - Prepare and Commit.\n\n# References\n - [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n - [Durability - Wikipedia](https://en.wikipedia.org/wiki/Durability_(database_systems))\n - [Two-phase commit protocol](https://en.wikipedia.org/wiki/Two-phase_commit_protocol)\n - [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n",
    "similar": [
      "isolation",
      "atomicity",
      "bitcask",
      "consistency"
    ]
  },
  {
    "id": 18,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "isolation",
    "title": "\"I\" in ACID - Isolation",
    "description": "Isolation is the ability of the database to concurrently process multiple transactions in a way that changes made in one do not another.",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/124763085-36d1db80-df51-11eb-985e-1d98e9b78cba.png",
    "released_at": "2021-07-05",
    "total_views": 301,
    "body": "After talking about the \"A\" and the \"C\" in ACID, let's talk about the \"I\" in ACID - Isolation. In this one, we do a micro-dive into Isolation in the context of database. We will take a detailed look into Isolation, understand its importance, functioning, and how the database implements it.\n\n# What is Isolation?\n\nIsolation is the ability of the database to concurrently process multiple transactions in a way that changes made in one does not affect the other. A simple analogy is how we have to make our data structures and variables thread-safe in a multi-threaded (concurrent) environment.\n\nAnd similar to how we use Mutex and Semaphores to protect variables, the database uses locks (shared and exclusive) to protect transactions from one another.\n\n![https://user-images.githubusercontent.com/4745789/124764636-caf07280-df52-11eb-8d6b-d9d316d31102.png](https://user-images.githubusercontent.com/4745789/124764636-caf07280-df52-11eb-8d6b-d9d316d31102.png)\n\n# Why is Isolation important?\n\nIsolation is one of the most important properties of any database engine, the absence of which directly impacts the integrity of the data.\n\n## Example 1: Cowin Portal\n\nWhen 500 slots open for a hospital, the system has to ensure that a max of 500 people can book their slots.\n\n## Example 2: Flash Sale\n\nWhen Xiaomi conducts a flash sale with 100k units, the system has to ensure that orders of a max of 100k units are placed.\n\n## Example 3: Flight Booking\n\nIf a flight has a seating capacity of 130, the airlines cannot have a system that allows ticket booking of more than that.\n\n## Example 4: Money transfers\n\nWhen two or more transfers happen on the same account simultaneously, the system has to ensure that the end state is consistent with no mismatch of the amount. Sum of total money across all the parties to remain constant.\n\nThe isolation property of a database engine allows the system to put these checks on the database, which ensures that the data never goes into an inconsistent state even when hundreds of transactions are executing concurrently.\n\n# How is isolation implemented?\n\nA transaction before altering any row takes a lock (shared or exclusive) on that row, disallowing any other transaction to act on it. The other transactions might have to wait until the first one either commits or rollbacks.\n\nThe granularity and the scope of locking depend on the isolation level configured. Every database engine supports multiple Isolation levels, which determines how stringent the locking is. The 4 isolation levels are\n\n- Serializable\n- Repeatable reads\n- Read committed\n- Read uncommitted\n\nWe will discuss Isolation Levels in detail in some other essay.\n\n# References\n\n- [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n- [Isolation - Wikipedia](https://en.wikipedia.org/wiki/Isolation_(database_systems))\n- [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n- [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)\n- [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)\n",
    "similar": [
      "atomicity",
      "durability",
      "consistency",
      "image-steganography"
    ]
  },
  {
    "id": 19,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "consistency",
    "title": "\"C\" in ACID - Consistency",
    "description": "In the context of databases, Consistency is Correctness, which means that under no circumstance will the data lose its correctness.",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/124225698-86657100-db25-11eb-8284-9771f5a0b545.png",
    "released_at": "2021-07-02",
    "total_views": 387,
    "body": "In this short essay, we dive deep and understand the \"C\" in ACID - Consistency.\n\nIn this quick read, we will take a detailed look into [Consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems)), understand its importance, functioning, and how the database implements it.\n\n# What is Consistency?\n\nIn the context of databases, Consistency is Correctness, which means that under no circumstance will the data lose its correctness.\n\nDatabase systems allow us to define rules that the data residing in our database are mandated to adhere to. Few handy rules could be\n\n - balance of an account should never be negative\n - no orphan mapping: there should not be any mapping of a person whose entry from the database is deleted.\n - no orphan comment: there should not be any comment in the database that does not belong to an existing blog.\n\nThese rules can be defined on a database using Constraints, [Cascades](https://en.wikipedia.org/wiki/Foreign_key#CASCADE), and Triggers; for example, [Foreign Key constraints](https://en.wikipedia.org/wiki/Foreign_key), [Check constraints](https://en.wikipedia.org/wiki/Check_constraint), On Delete Cascades, On Update Cascades, etc.\n\n![Consistency ACID Database](https://user-images.githubusercontent.com/4745789/124226533-e7417900-db26-11eb-8e88-1c50a9391c44.png)\n\n### Role of the database engine in ensuring Consistency\nAn ACID-compliant database engine has to ensure that the data residing in the database continues to adhere to all the configured rules. Thus, even while executing thousands of concurrent transactions, the database always moves from one consistent state to another.\n\n### What happens when the database discovers a violation?\nDatabase Engine rollbacks the changes, which ensures that the database is reverted to a previous consistent state.\n\n### What happens when the database does not find any violation?\nDatabase Engine will continue to apply the changes, and once the transaction is marked successful, this state of the database becomes the newer consistent state.\n\n# Why is consistency important?\nThe answer is very relatable. Would you ever want your account to have a negative balance? No. This is thus defined as a rule that the database engine would have to enforce while applying any change to the data.\n\n# How does the database ensure Consistency?\nIntegrity constraints are checked when the changes are being applied to the data.\n\nCascade operations are performed synchronously along with the transaction. This means that the transaction is not complete until the primary set of queries, along with all the eligible cascades, are applied. Most database engines also provide a way to make them asynchronous, allowing us to keep our transactions leaner.\n\n\u2728 Next up is \"I\" in ACID - Isolation. Stay tuned.\n\n# References\n - [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n - [Consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems))\n - [Foreign Key Constraints](https://en.wikipedia.org/wiki/Foreign_key)\n - [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n - [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)\n - [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)\n",
    "similar": [
      "image-steganography",
      "isolation",
      "durability",
      "atomicity"
    ]
  },
  {
    "id": 20,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "atomicity",
    "title": "\"A\" in ACID - Atomicity",
    "description": "A single database transaction often contains multiple statements to be executed on the database. In Relational Databases, these are usually multiple SQL statements, while in the case of non-Relational Databases, these could be multiple database commands.",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/124224260-ef97b500-db22-11eb-8d0f-cc808093eb00.png",
    "released_at": "2021-06-28",
    "total_views": 748,
    "body": "In this short essay, we dive deep and understand the \"A\" of ACID - Atomicity.\n\nIn this quick read, we will take a detailed look into [Atomicity](https://en.wikipedia.org/wiki/Atomicity_(database_systems)), understand its importance, and learn about implementing it at various levels.\n\n# What is atomicity?\n\nA single database transaction often contains multiple statements to be executed on the database. In Relational Databases, these are usually multiple SQL statements, while in the case of non-Relational Databases, these could be multiple database commands.\n\nAtomicity in ACID mandates that each transaction should be treated as a single unit of execution, which means either all the statements/commands of that transaction are executed, or none of them are.\n\nAt the end of the successful transaction or after a failure while applying the transaction, the database should never be in a state where only a subset of statements/commands is applied.\n\nAn atomic system thus guarantees atomicity in every situation, including successful completion of transactions or after power failures, errors, and crashes.\n\n![Atomicity ACID](https://user-images.githubusercontent.com/4745789/124223798-0e497c00-db22-11eb-868d-8faefc44361c.png)\n\nA great example of seeing why it is critical to have atomicity is Money Transfers.\n\nImagine transferring money from bank account A to B. The transaction involves subtracting balance from A and adding balance to B. If any of these changes are partially applied to the database, it will lead to money either not debited or credited, depending on when it failed.\n\n# How is atomicity implemented?\n\n## Atomicity in Databases\nMost databases implement Atomicty using logging; the engine logs all the changes and notes when the transaction started and finished. Depending on the final state of the transactions, the changes are either applied or dropped.\n\nAtomicity can also be implemented by keeping a copy of the data before starting the transaction and using it during rollbacks.\n\n## Atomicity in File Systems\nAt the file system level, atomicity is attained by atomically opening and locking the file using system calls: open and flock. We can choose to lock the file in either Shared or Exclusive mode.\n\n## Atomicity at Hardware Level\nAt the hardware level, atomicity is implemented through instructions such as Test-and-set, Fetch-and-add, Compare-and-swap.\n\n## Atomicity in Business Logic\nThe construct of atomicity can be implemented at a high-level language or business logic by burrowing the concept of atomic instructions; for example, you can use compare and swap to update the value of a variable shared across threads concurrently.\n\nAtomicity is not just restricted to Databases; it is a notion that can be applied to any system out there.\n\n\u2728 Next up is \"C\" in ACID - Consistency. Stay tuned.\n\n# References\n - [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n - [Atomicity - Wikipedia](https://en.wikipedia.org/wiki/Atomicity_(database_systems))\n - [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n - [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)\n - [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)\n",
    "similar": [
      "isolation",
      "durability",
      "consistency",
      "image-steganography"
    ]
  },
  {
    "id": 21,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "architectures-in-distributed-systems",
    "title": "Architectures in Distributed Systems",
    "description": "While designing a Distributed System, it is essential to pick the right kind of architecture. Usually, architectures are evolving, but picking the right one at the inception could make your system thrive.",
    "gif": "https://media.giphy.com/media/3orieKSJONEWV51x3q/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/122890572-6d103800-d361-11eb-92e0-c0f226c6821c.png",
    "released_at": "2021-06-22",
    "total_views": 563,
    "body": "While designing a Distributed System, it is essential to pick the right kind of architecture. Usually, architectures are evolving, but picking the right one at the inception could make your system thrive.\n\nSo, here are 4 common architectures in Distributed Systems\n\n## Client-server\n\nTo render some information to the end-user, the client contacts the server (holding the data) with a request; the server sends back the data requested. The client is smart enough to understand how to request the data, post-process it, format it, and then serve it to the end-user.\n\nThis kind of architecture is not common in our day-to-day web applications. Still, it is prevalent when multiple services share a common database (server) and request data directly from the database.\n\n## 3-tier\n\nThis is one of the most widely used topologies out there. Unlike client-server architecture, the clients in a 3-tier architecture are not smart and are stateless. This architecture introduces a middle layer holding the business logic. The client talks to the business layer, and this business layer talk to the server (holding the data).\n\nMost of the web applications are 3-tier applications where your client (browser) talks to the business layer (webserver), which in turn queries the server (database) for the data. The business layer processes and formats the data (optional) and sends it back to the client. The client does not know how the data is being fetched from the server, making it stateless.\n\n## n-tier\n\nAs an extension to the 3-tier architecture, the n-tier application is where your middle layer (business layer) talks to another service to get information. This is typically seen when there is multiple independent business logic in the system.\n\nA classic example of an n-tier architecture is Microservices based architecture. Each service is responsible for its information, and the service communicates with other services to get the required data. Thus, a 3-tier application typically evolves into an n-tier application.\n\n## Peer-to-peer\n\nPeer-to-peer architecture is typically a decentralized system wherein no special machines hold all the responsibilities; instead, the responsibility is split across all the machines equally. The peers act as both clients and servers, and they communicate with each other to serve the request.\n\nA couple of popular examples of P2P architecture are BitTorrent and Bitcoin networks. The n-tier architecture can optionally evolve into a P2P, but this evolution is not that popular. Usually, going P2P is a choice that is made during the inception of the service.\n\n# References\n - [Distributed Systems - Wikipedia](https://en.wikipedia.org/wiki/Distributed_computing)\n",
    "similar": [
      "conflict-detection",
      "mistaken-beliefs-of-distributed-systems",
      "conflict-resolution",
      "monotonic-reads"
    ]
  },
  {
    "id": 22,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "mistaken-beliefs-of-distributed-systems",
    "title": "Mistaken Beliefs of Distributed Systems",
    "description": "In this essay, we learn about a set of false assumptions that programmers new to distributed applications invariably make. Understand these to build robust distributed systems.",
    "gif": "https://media.giphy.com/media/SqmkZ5IdwzTP2/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/122346879-11137100-cf67-11eb-95a1-5e717dd53810.png",
    "released_at": "2021-06-17",
    "total_views": 358,
    "body": "The only way to infinitely scale your system is by making it distributed, which means adding more servers to serve your requests, more nodes to perform computations in parallel, and more nodes to store your partitioned data. But while building such a complex system, we tend to assume a few things to be true, which, in reality, are definitely not true.\n\nThese mistaken beliefs were documented by [L Peter Deutsch](https://en.wikipedia.org/wiki/L._Peter_Deutsch) and others at Sun Microsystems, and it describes a set of false assumptions that programmers new to distributed applications invariably make.\n\n## Myth 1: The network is reliable;\n\nNo. The network is not reliable. There are packet drops, connection interruptions, and data corruptions when they are transferred over the wire. In addition, there are network outages, router restarts, and switch failures to make the matter worse. Such an unreliable network has to be considered while designing a robust Distributed System.\n\n## Myth 2: Latency is zero;\n\nNetwork latency is real, and we should not assume that everything happens instantaneously. For every 10 meters of fiber optic wire, we add 3 nanoseconds to the network latency. Now imagine your data moving across the transatlantic communications cable. This is why we keep components closer wherever possible and have to handle out-of-order messages.\n\n## Myth 3: Bandwidth is infinite;\n\nThe bandwidth is not infinite; neither of your machine, or the server, or the wire over which the communication is happening. Hence we should always measure the number of packets (bytes) of data transferred in and out of your systems. When unregulated, this results in a massive bottleneck, and if untracked, it becomes near impossible to spot them.\n\n## Myth 4: The network is secure;\n\nWe put our system in a terrible shape when we assume that the data flowing across the network is secure. Many malicious users are constantly trying to sniff every packet over the wire and de-code what is being communicated. So, ensure that your data is encrypted when at rest and also in transit.\n\n## Myth 5: Topology doesn't change;\n\nNetwork topology changes due to software or hardware failures. When the topology changes, you might see a sudden deviation in latency and packet transfer times. So, these metrics need to be monitored for any anomalous behavior, and our systems would be ready to embrace this change.\n\n## Myth 6: There is one administrator;\n\nThere is one internet, and everyone is competing for the same resources (optic cables and other communication channels). So, when building a super-critical Distributed system, you need to know which path your packets are following to avoid high-traffic competing and congested areas.\n\n## Myth 7: Transport cost is zero;\n\nThere is a hidden cost of hardware, software, and maintenance that we all bear when using a distributed system. For example, if we use a public cloud-like AWS, then the data transfer cost is real. This cost looks near zero from a bird's eye view, but it becomes significant when operating at scale.\n\n## Myth 8: The network is homogeneous.\n\nThe network is not homogeneous, and your packets travel to all sorts of communication channels like optic cables, 4G bands, 3G bands, and even 2G bands before reaching the user's device. This is also true when the packets move within your VPC through different types of connecting wires and network cards. When there is a lot of heterogeneity in the network, it becomes harder to find the bottleneck; hence having a setup that gives us enough transparency is the key to a good Distributed System design.\n\n# References\n\n- [Fallacies of distributed computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)\n- [The Eight Fallacies of Distributed Computing](https://www.youtube.com/watch?v=JG2ESDGwHHY)\n",
    "similar": [
      "conflict-detection",
      "architectures-in-distributed-systems",
      "conflict-resolution",
      "data-partitioning-strategies"
    ]
  },
  {
    "id": 47,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "rum",
    "title": "The RUM Conjecture",
    "description": "While designing any storage system the three main aspects we optimize for are Reads, Updates, and auxiliary Memory. RUM Conjecture states that these three form a competing triangle and we could only optimize two at the expense of the third.",
    "gif": "https://media.giphy.com/media/1n7B7bJ917pqZGHG9m/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/83344735-64009300-a328-11ea-858a-587d440136f1.png",
    "released_at": "2020-05-31",
    "total_views": 670,
    "body": "The RUM Conjecture states that we cannot design an access method for a storage system that is optimal in all the following three aspects - Reads, Updates, and, Memory. The conjecture puts forth that we always have to trade one to make the other two optimal and this makes the three constitutes a competing triangle, very similar to the famous [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem).\n\n![https://user-images.githubusercontent.com/4745789/83323578-6eb21e00-a27d-11ea-941b-43e875169c97.png](https://user-images.githubusercontent.com/4745789/83323578-6eb21e00-a27d-11ea-941b-43e875169c97.png)\n\n# Access Method\n\nData access refers to an ability to access and retrieve data stored within a storage system driven by an optional storage engine. Usually, a storage system is designed to be optimal for serving a niche use case and achieve that by carefully and judiciously deciding the memory and disk storage requirements, defining well-structured access and retrieval pattern, designing data structures for primary and auxiliary data and picking additional techniques like compression, encryption, etc. These decisions define, and to some extent restricts, the possible ways the storage engine can read and update the data in the system.\n\n# RUM Overheads\n\nAn ideal storage system would be the one that has an access method that provides lowest Read Overhead, minimal Update Cost, and does not require any extra Memory or Storage space, over the main data. In the real-world, achieving this is near impossible and that is something that is dictated by this conjecture.\n\n### Read Overhead\n\nRead Overhead occur when the storage engine performs reads on auxiliary data to fetch the required intended main data. This usually happens when we use an auxiliary data structure like a Secondary Index to speed up reads. The reads happening on this auxiliary structure constitutes read overheads.\n\nRead Overhead is measured through Read Amplification and it is defined as the ratio between the total amount of data read (main + auxiliary) and the amount of main data intended to be read.\n\n### Update Overhead\n\nUpdate Overhead occur when the storage engine performs writes on auxiliary data or on some unmodified main data along with intended updates on the main data. A typical example of Update Overheads is the writes that happen on an auxiliary structure like Secondary Index alongside the write happening on intended main data. \n\nUpdate Overhead is measured through Write Amplification and it is defined as the ratio between the total amount of data written (main + auxiliary) and the amount of main data intended to be updated.\n\n### Memory Overhead\n\nMemory overhead occurs when the storage system uses an auxiliary data structure to speed up reads, writes, or to serve common access patterns. This storage is in addition to the storage needs of the main data.\n\nMemory Overhead is measured through Space Amplification and it is defined as the ratio between the space utilized for auxiliary and main data and space utilized by the main data. \n\n# The Conjecture\n\nThe RUM Conjecture, in a formal way, states that\n\n> An access method that can set an upper bound for two out of the read, update, and memory overheads, also sets a lower bound for the third overhead.\n\nThis is not a hard rule that is followed and hence it is not a theorem but a conjecture - widely observed but not proven. But we can safely keep this in mind while designing the next big storage system serving a use case.\n\n# Categorizing Storage Systems\n\nNow that we have seen RUM overheads and the RUM Conjecture we take a look at examples of Storage Systems that classify into one of the three types.\n\n## Read Optimised\n\nRead Optimised storage systems offer very low read overhead but require some extra auxiliary space to gain necessary performance that again comes at a cost of updates required to keep auxiliary data in sync with main data which adds to update overheads. When the updates, on main data, become frequent the performance of a read optimized storage system takes a dip.\n\nA fine example of a read optimized storage system is the one that supports Point Indexes, also called Hash-based indexes, offering constant time access. The systems that provide logarithmic time access, like [B-Trees](https://en.wikipedia.org/wiki/B-tree) and [Skiplists](https://en.wikipedia.org/wiki/Skip_list), also fall into this category.\n\n## Update Optimised\n\nUpdate Optimised storage systems offer very low Update Overhead by usually using an auxiliary space holding differential data (delta) and flushing them over main data in a bulk operation. The need of having an auxiliary data to keep track of delta to perform a bulk update adds to Memory Overhead.\n\nA few examples of Update Optimised systems are [LSM Trees](https://en.wikipedia.org/wiki/Log-structured_merge-tree), [Partitioned B Trees](http://cs.emis.de/LNI/Proceedings/Proceedings26/GI-Proceedings.26-47.pdf), and [FD Tree](http://pages.cs.wisc.edu/~yinan/fdtree.html). These structures offer very good performance for an update-heavy system but suffer from an increased read and space overheads. While reading data from LSM Tree, the engine needs to perform read on all the tiers and then perform a conflict resolution, and maintaining tiers of data itself is a huge Space Overhead.\n\n## Memory Optimised\n\nMemory Optimised storage systems are designed to minimize auxiliary memory required for access and updates on the main data. To be memory-optimized the systems usually use compress the main data and auxiliary storages, or allow some error rate, like false positives.\n\nA few examples of Memory Optimises systems are lossy index structures like [Bloom Filters](https://en.wikipedia.org/wiki/Bloom_filter), [Count-min sketches](https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch), Lossy encodings, and Sparse Indexes. Keeping either main or auxiliary data compressed, to be memory efficient, the system takes a toll on writes and reads as they now have additionally performed compression and decompressions adding to the Update and Read overheads.\n\n![https://user-images.githubusercontent.com/4745789/83323560-55a96d00-a27d-11ea-9d33-4001c672b920.png](https://user-images.githubusercontent.com/4745789/83323560-55a96d00-a27d-11ea-9d33-4001c672b920.png)\n\nStorage System examples for RUM Conjecture\n\n# Block-based Clustered Indexing\n\nBlock-based Clustered Indexing, sits comfortably between these three optimized systems types. It is not read Read efficient but also efficient on Updates and Memory. It builds a very short tree for its auxiliary data, by storing a few pointers to pages and since the data is clustered i.e. the main data itself is stored in the index,  the system does not go to fetch the main data from the main storage and hence provides a minimal Read overhead.\n\n# Being RUM Adaptive\n\nStorage systems have always been rigid with respect to the kind of use cases it aims to solve. the application, the workload, and the hardware should dictate how we access our data, and not the constraints of our systems. Storage systems could be designed to be RUM Adaptive and they should possess an ability to be tuned to reduce the RUM overheads depending on the data access pattern and computation knowledge. RUM Adaptive storage systems are part of the discussion for some other day.\n\n# Conclusion\n\nThere will always be trade-offs, between Read, Update, and Memory, while either choosing one storage system over others; the RUM conjecture facilitates and to some extent formalizes the entire process. Although this is just a conjecture, it still helps us disambiguate and make an informed, better and viable decision that will go a long way.\n\nThis essay was heavily based on the original research paper introducing The RUM Conjecture.\n\n# References\n\n- [Designing Access Methods: The RUM Conjecture](https://stratos.seas.harvard.edu/files/stratos/files/rum.pdf)\n",
    "similar": [
      "master-replica-replication",
      "leaderless-replication",
      "replication-formats",
      "multi-master-replication"
    ]
  },
  {
    "id": 55,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "uid": "sliding-window-ratelimiter",
    "title": "Sliding Window based Rate Limiter",
    "description": "A rate limiter is used to control the rate of traffic sent or received on the network and in this article we dive deep and design a sliding window based rate limiter.",
    "gif": "https://media.giphy.com/media/5YuhLwDgrgtRVwI7OY/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/78276848-3c13cf80-7531-11ea-8186-99cb1da58e50.png",
    "released_at": "2020-04-05",
    "total_views": 1056,
    "body": "A rate limiter restricts the intended or unintended excessive usage of a system by regulating the number of requests made to/from it by discarding the surplus ones. In this article, we dive deep into an intuitive and heuristic approach for rate-limiting that uses a sliding window. The other algorithms and approaches include [Leaky Bucket](https://en.wikipedia.org/wiki/Leaky_bucket), [Token Bucket](https://en.wikipedia.org/wiki/Token_bucket) and Fixed Window.\n\nRate limiting is usually applied per access token or per user or per region/IP. For a generic rate-limiting system that we intend to design here, this is abstracted by a configuration key `key` on which the capacity (limit) will be configured; the key could hold any of the aforementioned value or its combinations. The limit is defined as the number of requests `number_of_requests` allowed within a time window `time_window_sec` (defined in seconds).\n\n# The algorithm\nThe algorithm is pretty intuitive and could be summarized as follow\n\n> If the number of requests served on configuration key `key` in the last `time_window_sec` seconds is more than `number_of_requests` configured for it then discard, else the request goes through while we update the counter.\n\nAlthough the above description of the algorithm looks very close to the core definition of any rate limiter, it becomes important to visualize what is happening here and implement it in an extremely efficient and resourceful manner.\n\n## Visualizing sliding window\nEvery time we get a request, we make a decision to either serve it or not; hence we check the `number_of_requests` made in last `time_window_sec` seconds. So this process of checking for a fixed window of `time_window_sec` seconds on every request, makes this approach a sliding window where the fixed window of size `time_window_sec` seconds is moving forward with each request. The entire approach could be visualized as follows\n\n![Sliding window visualization](https://user-images.githubusercontent.com/4745789/78364339-eac01a80-75da-11ea-8f65-633fd779afac.png)\n\n## The pseudocode\nThe core of the algorithm could be summarized in the following Python pseudocode. It is not recommended to put this or similar code in production as it has a lot of limitations (discussed later), but the idea here is to design the rate limiter ground up including low-level data models, schema, data structures, and a rough algorithm.\n\n```py\ndef is_allowed(key:str) -> Bool:\n\"\"\"The function decides is the current request should be served or not.\nIt accepts the configuration key `key` and checks the number of requests made against it\nas per the configuration.\n\nThe function returns True if the request goes through and False otherwise.\n\"\"\"\n    current_time = int(time.time())\n\n    # Fetch the configuration for the given key\n    # the configuration holds the number of requests allowed in a time window.\n    config = get_ratelimit_config(key)\n\n    # Fetch the current window for the key\n    # The window returned, holds the number of requests served since the start_time\n    # provided as the argument.\n    start_time = current_time - config.time_window_sec\n    window = get_current_window(key, start_time)\n\n    if window.number_of_requests > config.capacity:\n        return False\n    \n    # Since the request goes through, register it.\n    register_request(key, current_time)\n    return True\n```\n\nA naive implementation of the above pseudocode is trivial but the true challenge lies in making the implementation horizontally scalable, with low memory footprint, low CPU utilization, and low time complexity.\n\n# Design\nDesigning a rate limiter has to be super-efficient because the rate limiter decision engine will be invoked on every single request and if the engine takes a long time to decide this, it will add some overhead in the overall response time of the request. A better design will not only help us keep the response time to a bare minimum, but it also ensures that the system is extensible with respect to future requirement changes.\n\n## Components of the Rate limiter\nThe Rate limiter has the following components\n\n - Configuration Store - to keep all the rate limit configurations\n - Request Store - to keep all the requests made against one configuration key\n - Decision Engine - it uses data from the Configuration Store and Request Store and makes the decision\n\n## Deciding the datastores\nPicking the right data store for the use case is extremely important. The kind of datastore we choose determines the core performance of a system like this.\n\n### Configuration Store\nThe primary role of the Configuration Store would be to\n\n - efficiently store configuration for a key\n - efficiently retrieve the configuration for a key\n\nIn case of machine failure, we would not want to lose the configurations created, hence we choose a disk-backed data store that has an efficient `get` and `put` operation for a key. Since there would be billions of entries in this Configuration Store, using a SQL DB to hold these entries will lead to a performance bottleneck and hence we go with a simple key-value NoSQL database like [MongoDB](https://mongodb.com) or [DynamoDB](https://aws.amazon.com/dynamodb/) for this use case.\n\n### Request Store\nRequest Store will hold the count of requests served against each key per unit time. The most frequent operations on this store will be\n\n - registering (storing and updating) requests count served against each key - _write heavy_\n - summing all the requests served in a given time window - _read and compute heavy_\n - cleaning up the obsolete requests count - _write heavy_\n\nSince the operations are both read and write-heavy and will be made very frequently (on every request call), we chose an in-memory store for persisting it. A good choice for such operation will be a datastore like [Redis](https://redis.io) but since we would be diving deep with the core implementation, we would store everything using the common data structures available.\n\n## Data models and data structures\nNow we take a look at data models and data structures we would use to build this generic rate limiter.\n\n### Configuration Store\nAs decided before we would be using a NoSQL key-value store to hold the configuration data. In this store, the key would be the configuration key (discussed above) which would identify the user/IP/token or any combination of it; while the value will be a tuple/JSON document that holds `time_window_sec` and `capacity` (limit).\n\n```json\n{\n    \"user:241531\": {\n        \"time_window_sec\": 1,\n        \"capacity\": 5\n    }\n}\n```\n\nThe above configuration defines that the user with id `241531` would be allowed to make `5` requests in `1` second.\n\n## Request Store\nRequest Store is a nested dictionary where the outer dictionary maps the configuration key `key` to an inner dictionary, and the inner dictionary maps the epoch second to the request counter. The inner dictionary is actually holding the number of requests served during the corresponding epoch second. This  way we keep on aggregating the requests per second and then sum them all  during aggregation to compute the number of requests served in the required time window.\n\n![Request Store for sliding window rate limiter](https://user-images.githubusercontent.com/4745789/78384914-b0657600-75f8-11ea-8158-981ac3ecd46d.png)\n\n\n## Implementation\nNow that we have defined and designed the data stores and structures, it is time that we implement all the helper functions we saw in the pseudocode.\n\n### Getting the rate limit configuration\nGetting the rate limit configuration is a simple get on the Configuration Store by `key`. Since the information does not change often and making a disk read every time is expensive, we cache the results in memory for faster access.\n\n```py\ndef get_ratelimit_config(key):\n    value = cache.get(key)\n\n    if not value:\n        value = config_store.get(key)\n        cache.put(key, value)\n\n    return value\n```\n\n### Getting requests in the current window\nNow that we have the configuration for the given key, we first compute the `start_time` from which we want to count the requests that have been served by the system for the `key`. For this, we iterate through the data from the inner dictionary second by second and keep on summing the requests count for the epoch seconds greater than the `start_time`. This way we get the total requests served from start_time till now.\n\nIn order to reduce the memory footprint, we could delete the items from the inner dictionary against the time older than the `start_time` because we are sure that the requests for a timestamp older than `start_time` would never come in the future.\n\n```python\ndef get_current_window(key, start_time):\n    ts_data = requests_store.get(key)\n    if not key:\n        return 0\n    \n    total_requests = 0\n    for ts, count in ts_data.items():\n        if ts > start_time:\n            total_requests += count\n        else:\n            del ts_data[ts]\n\n    return total_requests\n```\n\n### Registering the request\nOnce we have validated that the request is good to go through, it is time to register it in the store and the defined function `register_request` does exactly that.\n\n```python\ndef register_request(key, ts):\n    store[key][ts] += 1\n```\n\n## Potential issues and performance bottlenecks\nAlthough the above code elaborates on the overall low-level implementation details of the algorithm, it is not something that we would want to put in production as there are lots of improvements to be made.\n\n### Atomic updates\nWhile we register a request in the Request Store we increment the request counter by 1. When the code runs in a multi-threaded environment, all the threads executing the function for the same key `key`, all will try to increment the same counter. Thus there will be a classical problem where multiple writers read the same old value and updates. To fix this we need to ensure that the increment is done atomically and to do this we could use one of the following approaches\n\n - optimistic locking (compare and swap)\n - pessimistic locks (always taking lock before incrementing)\n - utilize atomic hardware instructions (fetch-and-add instruction)\n\n### Accurately computing total requests\nSince we are deleting the keys from the inner dictionary that refers to older timestamps (older than the `start_time`), it is possible that a request with older `start_time` is executing while a request with newer `start_time` deleted the entry and lead to incorrect `total_request` calculation. To remedy this we could either\n\n - delete entries from the inner dictionary with a buffer (say older than 10 seconds before the start_time),\n - take locks while reading and block the deletions\n\n### Non-static sliding window\nThere would be cases where the `time_window_sec` is large - an hour or even a day, suppose it is an hour, so if in the Request Store we hold the requests count against the epoch seconds there will be 3600 entries for that key and on every request, we will be iterating over at least 3600 keys and computing the sum. A faster way to do this is, instead of keeping granularity at seconds we could do it at the minute-level and thus we sub-aggregate the requests count at per minute and now we only need to iterate over about 60 entries to get the total number of  requests and our window slides not per second but per minute.\n\nThe granularity configuration could be persisted in the configuration as a new attribute which would help us take this call.\n\n### Other improvements\nThe solution described above is not the most optimal solution but it aims to prove a rough idea on how we could implement a sliding window rate limiting algorithm. Apart from the improvements mentioned above there some approaches that would further improve the performance\n\n - use a data structure that is optimized for range sum, like segment tree\n - use a running aggregation algorithm that would prevent from recomputing redundant sums\n\n## Scaling the solution\n\n### Scaling the Decision engine\nThe decision engine is the one making the call to each store to fetch the data and taking the call to either accept or discard the request. Since decision engine is a typical service engine we would put it behind a load balancer that takes care of distributing requests to decision engine instances in a round-robin fashion ensuring it scales horizontally.\n\nThe scaling policy of the decision engine will be kept on following metrics\n\n - number of requests received per second\n - time taken to make a decision (response time)\n - memory consumption\n - CPU utilization\n\n### Scaling the Request Store\nSince the Request Store is doing all the heavy lifting and storing a lot of data in memory, this would not scale if kept on a single instance. We would need to horizontally scale this system and for that, we shard the store using configuration key key and use consistent hashing to find the machine that holds the data for the key.\n\nTo facilitate sharding and making things seamless for the decision engine we will have a Request Store proxy which will act as the entry point to access Request Store data. It will abstract out all the complexities of distributed data, replication, and failures.\n\n### Scaling the Configuration Store\nThe number of configurations would be high but it would be relatively simple to scale since we are using a NoSQL solution, sharding on configuration key `key` would help us achieve horizontal scalability.\n\nSimilar to Request Store proxy we will have a proxy for Configuration Store that would be an abstraction over the distributed Configuration Stores.\n\n## High-level design\nThe overall high-level design of the entire system looks something like this\n\n![Rate limiter high-level design diagram](https://user-images.githubusercontent.com/4745789/78460031-1cb8a600-76db-11ea-94f4-b821244993b3.png)\n\n## Deploying in production\nWhile deploying it to production we could use a memory store like Redis whose features, like Key expiration, transaction, locks, sorted, would come in handy. The language we chose for explaining and pseudocode was Python but in production to make things super-fast and concurrent we would prefer a language like Java or Golang. Picking this stack will keep our server cost down and would also help us make optimum utilization of the resources.\n\n# References\n - [Rate Limiting - Wikipedia](https://en.wikipedia.org/wiki/Rate_limiting)\n - [Rate-limiting strategies and techniques](https://cloud.google.com/solutions/rate-limiting-strategies-techniques)\n - [An alternative approach to rate limiting](https://www.figma.com/blog/an-alternative-approach-to-rate-limiting/)\n - [Building a sliding window rate limiter with Redis](https://engagor.github.io/blog/2017/05/02/sliding-window-rate-limiter-redis/)\n - [Everything You Need To Know About API Rate Limiting](https://nordicapis.com/everything-you-need-to-know-about-api-rate-limiting/)\n",
    "similar": [
      "consistency",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "image-steganography",
      "multiple-mysql-on-same-server-using-docker"
    ]
  }
]