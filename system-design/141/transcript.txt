so deployments are stressful what if something goes wrong what if you forgot to handle an edge case that was also was during unit test integration test or an internal queue iteration putting such code into production can take down your entire infrastructure and could cause a massive outage in order to handle such situations gracefully and provide us with an early warning about some things wrong we have something called as a canary deployment in this video we take an in-depth look into canary deployments learn why canary deployments are called canary deployments it's a very interesting story understand how they are actually implemented talk about the pros and the cons of this deployment pattern and conclude with one really solid use case where you absolutely need canary deployment but before we move forward i'd want to talk to you about a code based course or system design that i have been running since march 2021 right if you're looking to learn system design from the first principles this course is for you yeah because this is a cohort based course it will not just be me rambling a semi-optimized solution thinking it's the most amazing solution out there instead it will be a collaborative environment where every single person who is part of the cohort will can pitch in his or her ideas and we will evolve our system around that right every single problem statement comes with a brainstorming session where we all together brainstorm and evolve our system that's why everyone understands the kind of trade-offs we made while making that decision instead of just saying hey we'll use a particular queue we'll have the justification why we use only that queue why we use that particular database why sequel why not no sql right how are we leveraging throughput how are we ensuring that our system scales that's the highlight of this course this course is taken by more than 500 engineers to date spanning nine countries and seven cohorts right people from all top companies have taken this course and the outline is very intriguing it's very exciting so we start with week one around we start with the core foundation of the course where we design online offline indicator then we try to design our own medium then we go into database where we go in depth of database logging and take and see few very amazing examples of data log or database logging in in action and how do we ensure that our system scales through that then the third week is all about going distributed where we design our load balancer i'll have oculus of the actual code of a toy load balancer and understand how tcp connections are managed and how simple it is to build load balancer then week four is about all about social networks week five is all about building your own storage engines like we'll build that intuition on if you were to ever design your storage agent how would you do that right then week six is about building high throughput system seven is about building uh ir systems basically information retrieval systems and adult designs where we design our own message brokers like sqs where we design distributed tasks scheduler and we conclude the course with week eight where we talk about the super clever algorithms that has powered or that has made those systems possible right i have also attached a video verbatim as is from my first code where we designed and scaled instagram notifications i will highly encourage you to check this video out right and now back to the video so what is canary deployment canada deployment is a pattern that allows us to roll out our code features changes whatever you want to ship to an initial subset of users before we roll it out to 100 of them so the idea here is that when we are rolling out our things or when we are rolling out our changes to 100 of the users we have to be 100 sure that this would not cause an outage right so this is our way of ensuring that whenever that whatever we are rolling out it's proper so the idea here is that we roll out to an initial subset of users how we do that for example if we have seven servers in production out of that seven servers on one server you will deploy this new code or the new change that you have right and then you will monitor this server that is the cpu consumption correct is is the overall memory utilization okay are there any abrupt increase or abrupt rise in exception on this particular server are there any bugs in the latest code that that you have just shipped right so you'll do all of those things on this one small server on which not sponsor but one production server on is you deployed your code right if that is well you are you are pretty confident that hey this is exactly how my code used to should work it is working fine there is no spike in cpu no spike in memory uh everything seems to be working fine then you incrementally roll out it to let's say two servers then four servers and then eventually to all seven servers so thus your does your canary deployment pattern acts as your early warning indicator which which warns you pretty early if something's wrong in your code right this is how we avoid a massive outage because if we already detected on that one server that hey something's wrong you would not go and proceed to 100 of deployment right so then you would be immediately rolling back your changes from that one server so the number of users getting affected of the number of requests getting affected will be very few so overall flow when you have a canary deployment would look something like this you would deploy the new version of your code or the new changes that you have to one server right and then you would monitor the vitals of that server cpu ram error rates pick your favorite vitals or pick the vitals which is specific to your use case and monitor them do a side by side comparison on old infra and new infra see how that is behaving once and also you may need to optionally test your business logic changes if any on this one server if all's good then you roll it out to remaining uh you you roll it out to remaining servers that you have right and then if something's wrong that you are not okay with the changes that have been just shipped onto the server maybe uh there was a gigantic memory leak which caused the server to crash exceptions and whatnot you would be rolling back from that from that particular server right so rollbacks become pretty simple deployments become incremental you get confidence whenever you are rolling out to 100 you know that nothing would go wrong an interesting story why the name canary deployment why like because basically canary is a very cute little yellow bird why a deployment strategy is named after canary the story is pretty interesting in 1920s uh during coal mining the miners like what happens in a coal mine there are a lot of basically toxic gases which release so what miners used to do back then was they used to take caged panery they used to take caged basically canaries into mines they used to put this cute little bird into cage and they used to put it into coal mine if the level of toxicity in the uh or the overall gases that is that is coming out in the coal mine if there is high level of toxicity in that the canary would die now when the canary dies and the miners see that what would what the minus would do because miners would know that uh the the overall toxicity in this air is very high they would immediately evacuate so this the death of canary indicated that the toxicity is high and it alerted the miners to evacuate immediately so this is how one obviously it's not good for the canary but humans are cruel and they do that so we use that system early in 1920s in some parts it's still being used as our early warning indicator so in our canary deployment that one server that we picked up is acting as that canary which would tell us if everything's good or everything's not something's bad so that we can we can take those preventive measures and not roll it out and not go into a catastrophic situation so it is our early warning indicator and this is the story or this is the reason why canary deployments are called canary deployments okay so how now the next part how can any deployments are implemented obviously theory is okay but how they are actually implemented so there are like the true deployment of canary or sorry the true implementation of canary deployment depends on your use case but in a generic sense if you are doing a server based rollout what it would look like is you need to add a few bits and pieces in your infrastructure to get that thing done so what we do is we create a small parallel infrastructure just like how we have for our existing for our existing service let's say you are building an auth service and for that you want to ship out the new version of your code so you will create a parallel set of api servers with the similar conflict curve with the similar configuration as your old one but there would be only one or two servers that you would put there right and then you will create a parallel infrastructure you'll put a load balancer in front of it configure the rules such that five percent of the traffic goes to this new setup 95 of the traffic goes to this old setup so whenever the request comes in request would come to your load balancer so now here it's not necessarily that you have to use a load balancer you can use any proxy any router so these are generic terms that like you can the idea is to route five percent of traffic to new setup 95 of traffic to old setup however you want to do it you can do it you can use a load balancer you can use an api gateway you can use a db uh you can use a normal router as well right pick your favorite infrastructure component but you need to put that in front once that that proxy layer that is sitting in front it gets the request it will transfer 95 of the request to the old setup five percent of the request to the new setup and now that you are already monitoring the vitals on the new setup you would know that if it's going good or not going good or what's wrong and what's not and then you can take a call on what to do next right okay so now let's talk about the advantages or the pros of using of doing a canary deployment there are several pros of doing it first it allows us to test the changes in production although the word testing and production in one sentence does not good is not good but still it allows you to test the changes in production with real use with real users with real traffic and that is pretty solid because not all cases might be covered during a unit test integration test or qa if that's an idle situation but then it is always better that you know exactly how your code is performing when it hits production so it allows you to do that right so you create two separate fleets again they said same exact infrastructure you create two separate fleets you monitor that thing but it gives you that power to test your changes on the actual users on the actual traffic that you get second is a very big advantage that you get rollbacks are much faster so now what you have is you have your users making your request coming to your proxy layer proxier transferring five percent to new infra sorry basic five percent to new infra 95 percent to old infra and now let's say you identified hey something's wrong i want to immediately roll back roll backs are so simple that on your proxy layer just configure instead of 595 000 this way zero percent traffic would go to a new infra 100 traffic would go to old infra this way as soon as you detected something's wrong you immediately rolled back and it was just a simple configuration change right and this is how you get click although some users were some users were affected when that bad code was pushed on canary but it did not impact 95 percent of the users pretty pretty big win right so some users were affected but most of the users were not but your rollbacks were pretty simple because you just had to do one simple configuration chain and you were sorted okay next advantage minimal blast radius this is what we talked about that if a new version of our code has bugs or it crashes has memory leaks anything when it is deployed on canary and when you roll it back it is only affecting the request that goes to that canary machine it is not affecting the other servers at all so you have a minimal blast radius blast radius is something when you have a blast who all are affected here with this your blast radius is just five percent or one percent depending on how many uh request gets affected it's very minuscule because you are not rolling out two hundred percent definitely it will be definitely in single digit right so very minimal blast radius so you are basically reducing the overall impact of your bad code push next zero downtime deployment this is amazing like everyone loves zero downtime deployments right so with this what you get is because it's a canary setup you have what do you have you have a new fleet you have an old fleet five percent of traffic goes to old uh five percent of traffic goes to the new fleet ninety five percent traffic goes to the old fleet now as you gain more confidence you can increase this percentage that hey now i want ten percent traffic to go to new then 25 then 50 and then 100 percent so then basically what you are doing is you are slowly cutting off the traffic from your old version of your code and slowly moving it to the newer version of it this way from 0 to 100 you are slowly doing this roll out of your new codes new features new changes so your deployments become zero downtime one thing to note that when the traffic would increase always make sure that your canary uh your your your oh your basically your canary fleet has enough machines to handle that load don't forget that it needs to be auto skid right but the overall idea is you get zero downtime deployment okay next we could deploy even when we are unsure about the new release there are so so many times i have seen that we are unsure about the changes that we made there is not enough time for qa and we have to ship the code because it is a business priority so what you do you do a canary deployment right you just take each other let's just push the code into the production we'll see what happens but instead of rolling it out to hundred percent you roll it out to just one machine to see if it is doing well if it is doing well you roll it out to 100 otherwise you roll back so even when you are unsure about your release because you are not done you didn't have time to do it you can still put the things in production with minimal impact you can very easily sense if it is working fine or not okay next advantage we can use canary setup to do a b testing a b testing is extremely important in order to not make abrupt changes like for example i'll give you an example let's say you are part of search team and what you folks did is you had an old version of search let's say search v1 and then you change some you applied some machine learning change some algorithm and new now you have a brand new version of search you are so excited about it and you think it would improve i said you think it would improve the overall ctrs and key search metrics but is that so is it really improving the metric how do you know that for that in order to be very sure that moving from v1 to v2 would definitely improve the relevance of your search result you need to do a b testing so you need to run both of these versions in parallel so you do exactly what you did you use canary deployment to do this the new version of your code with all the machine learning and all the fancy stuff your search v2 is having a separate fleet search even is having a separate fleet five percent of traffic goes to search v2 95 percent of traffic goes to search v1 and when you are very definite that you definitely see an improvement with search v2 in overall key metrics of your search like ctr and whatnot then you basically roll it out and then you are pretty sure that hey search v2 is definitely better than v1 then you slowly increase the traffic and you move out of v1 and move entirely to v2 right so av testing is pretty solid it's pretty simple it's pretty straightforward with canada setup one thing to note over here here when we talked about uh moving five percent of traffic 95 percent of traffic to the users what we definitely talked about it's like we just use numbers to do so but these strategies are very specific to your use case for example you might want to start with let's say only users in india gets this feature no one else for example whatsapp upi payments so this feature of upi payments on whatsapp should only be available in india because upi is not it does not exist anywhere else right so when they rolled out this feature this new apk having upi payment feature should only be good going to people in india no one else that is a canary deployment where the segmentation is done with respect to geography other use cases would be user cohorts for example all users older than 25 years of age but younger than 30 years of age working in an i.t company uh is your cohort so roll out the feature to those specific users otherwise the third one pretty simple random selection pick 25 at random people or not just random people any like it's literally you push the code into one of the server you don't care you push random request random five percent or ten percent of request to that server you don't care which user is that that's a random selection that you do then beta users for example you have you have uh uh you have a product launch and you have or basically a better example google play uh ask you uh now and then to let that do you want to sign up for a beta program where you get early releases of an app right so when a new version of an app is released you have an option to release it to the beta users who signed up for that that is also a kind of a kind of deployment right or basically a flavor of canary deployment right it's more about a b testing kind of but the idea of canary deployment still remains the same which are rolling out the changes to a particular set of users right next one is sticky plus random selection in which you select few people at random and you would always send a new version to those people right and that is a random selection of users or then other use cases internal employees so i think facebook has two versions of that app the blue version and the yellow version yellow version is an internal version blue version is an external version that we all get so whenever a new version of a facebook app is released it is first released internally where their internal employees use it their internal qa internal employees are using it once they are happy they relate to the general public right here the selection criteria totally depends on your use case there is no one strict rule to do to do that okay okay now cons of doing a canary deployment first of all engineers will be habituated to test in production and although it was an advantage that we could test attendance in production if we do it often people would be habituated hey it's okay if we don't test uh on our local machines or on our dev environment we have canary deployment to test those things in production so although it's good for exceptional use cases but doing it often will lower your overall engineering practices that you would want to have in your org right so all that's good but excessiveness of that is pretty catastrophic for our culture next architecting a canary deployment is complex as you saw you need extra infra components like load balancer api gateway proxy router what not or at least one of them to put to sit in front of it then you need separate fleet of machines their own scaling policies monitoring becomes a pain so you need a parallel setup to do that so it's not very straightforward but with right set of abstraction you can build a solid automation around that but it's still complex to architect on the first like on day one the next is parallel monitoring setup because we need to know how our traffic is performing or how our servers are performing on or harvesting how is code performing on the old setup and on the new setup you need to do that side by side comparison so what you definitely need to have is you need to have a parallel monitoring setup so that you know that the cpu coming from canary machines is this and cpu coming from normal machine is this so that you can do side by side comparison for example in this small diagram that i have drawn i have two charts of cpu utilization one is old one is new old is floating around five percent new is floating around forty percent you can only definitely tell when you have this exact side-by-side comparison of the same metric coming in from your normal fleet or from the old version and from the canary version of it so you need to have a parallel monitoring system it's not as simple as it sounds ensuring that all the key metrics or all the key vitals are monitored and measured it's critical okay that was the cons about it now the final thing when do you absolutely need canary deployment i'll give you a very solid example let's say you rewrote your entire service let's say you are part of the auth team who drives the entire authentication for yourself for your organization and now the service was written in java was getting very it was having memory leaks was getting slower you heard about that hey golang is the new the the the next big thing the most performant framework or the most performant language out there let me port my changes to golang now when you are doing an entire service rewrite entire business logic needs to be changed from java to golang there might be chances you would be missing out on something some edge cases not not not handled something weird happening you have never managed golang in production you're not sure how that would work out so that's what you do it's you would never go from zero to one from java to golang uh in one shot you would always have a parallel setup one for java servers one for golang servers your you would then move traffic five percent to go sorry five percent to golang and ninety five percent to java you will see how it is performing if it is genuinely performing well and doing good uh no bugs nothing and if sorry if there are any bugs you would be iterating on the code base of your golem to ensure that those are fixed you do that side by side comparison of cpu memory error rates exceptions what not just to ensure it's correct and once you have confidence then you move 10 traffic then 25 then 35 then 50 then 75 and then 100 so this way when you are totally moving out of java and into golan this cannot be a flip of a switch it needs to be incremented this is where canary would come in extremely handy because this you cannot do without canary you need to have a parallel setup where you are incrementally rolling out changes from java to golang and just to check if everything's working fine or not once your confidence hundred percent confident about it then you make the hundred percent change to go like right and this is where you would absolutely need canary setup right so yeah nice that's all about canary setup i hope it was interesting and amusing if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sub i post three in-depth engineering videos every week and i'll see you in the next one thanks again