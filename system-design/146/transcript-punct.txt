so one of the simplest deployment strategies that make deployment a breeze for us is rolling deployment. it is one of the most widely adopted deployment strategies, purely because of its simplicity and cost effectiveness. most of the deployment tools out there has this as the default deployment strategy. in this video, we take an in-depth look into what rolling deployment is, how it is actually implemented, how to tune it to get the max out of it, some key challenges that we face during adoption, and conclude with understanding the pros and cons of adopting it. but before we move forward, i'd like to talk to you about a course on system design that i have been running for over a year now. the course is a cohort based course, which means i won't be rambling a solution and it will not be a monologue. instead, a small, focused group of 50- 60 engineers every cohort will be brainstorming systems and designing it together. this way, we build a solid system and learn from each other's experiences. the course to date is enrolled by 600 plus engineers spanning nine cohorts and 10 countries. engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say. the coolest part about the course is the depth we go into and the breadth we cover. we cover topics ranging from real-time text communication for slack to designing our own toy load balancer, to greek business live text commentary, to doing impressions counting at scale for any advertisement business. in all we would cover roughly 28 questions and the detailed curriculum, uh split week by week, can be found on the course page, which is linked in the description down below. so if you're looking to learn system design from the first principles, you will love this course. i have two offerings for you. the first one is the live cohort discourse which you see on the left side, and the second one is the recorded course, which you can see on the right side. the live code base course happens every two months and it will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to binge learn system design, i would highly recommend you going for the recorded one. otherwise, the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page at with binary dot me slash master class and i would highly recommend you to check that out. i put the link of uh the course in the description down below. so if you are interested to learn system design, go for it. check out the link in the description down below and i hope to see you in my next cohort. thanks, so what is rolling deployment? rolling deployment is a deployment strategy that slowly replaces the previous version of your application with the newer version of your application, and it is most commonly implemented by replacing the underlying infrastructure, which means, out of nine servers that we have, which initially runs the version one of all of which basically runs the version one of our code, on all of them we slowly start taking one server upgraded to version two, then another server upgraded to version two, right. so, slowly and steadily, we would have from zero to one to two to nine. all servers will eventually serve the version two of our code. now, this is the core idea behind rolling deployment. for example, in case of our kubernetes containers, we have our services running on kubernetes. so what kubernetes does when you, when you basically trigger a deployment with this strategy, it would replace one container at a time, that it would have a docker image, or we would have a container ready with a new version of our code and let's say we have hundred containers running. it would start replacing those containers one by one so that eventually we would have all the containers running with the new version of the code and the old or in the, and every single one of the old container will be terminated. similar to that, even on normal virtual machines like ec2 servers we do that same thing. we have multiple ec2 servers and then we start upgrading them one by one, either terminating the old one or basically replacing the old, or basically replacing the code on the old with the new one. so this is the core idea behind rolling deployment strategy and this is, although slow, but it is very resilient, very simple and gradual. so one key thing over to one key thing to note over here is that rolling deployment is incremental, and this is a boon and a bane. so what happens is, because rolling deployment is incremental, you have a you. you are continuously having the incoming surge of user request which is coming to your load balancer, which is then moving forward to our api servers. now what would happen during the time of the deployment, you would have some set of servers which are serving the old uh, the which are basically serving the version one of your code, and some servers which are serving the new version of your code. so which means that, depending on which server the request goes to, you will either get response generated from the version one of your code or version two of your code. so this is a very critical point, that whenever we are adopting something like rolling deployment strategy, we have to ensure that we are okay when part of our infrastructure is serving old code and other part serving the new code. and this is where backward compatibility and forward compatibility becomes extremely crucial- that you cannot push forward your changes which are not backward compatible, which are not forward compatible. you have to ensure that because during rolling deployment, a part of your infrastructure will have old code and other part will have the new code serving simultaneously, right? so how do we implement it? that's all theoretical part, but how? how do we actually implement it? so, rolling deployment: the key idea here is to we have to do it gracefully. so when i say gracefully, which means that no existing request should uh, should be terminated abruptly and everything should be served with no downtime, right, and the code of everything should be served and then only instance should be terminated and then during the entire deployment there should not be any downtime whatsoever. so how it is actually implemented it can be split into micro and baby steps, uh, as i'll walk you through right now. so the step number one: let's say i have my infrastructure with, let's say, eight or so, with, let's say, eight servers. so what we'll do is we'll pick one server at a time. so let's say we picked the first server. we pick a server for this deployment. the first thing that we do is we stop the incoming traffic to it so that now server cannot accept, or your server would not get any new incoming request. how can you do that? for example, this server would be behind a load balancer. you take this server out of your load balancer. it does not mean that the existing request would not be, would not be served. it's just that when you take the server out of the load balancer, it is just stopping the incoming request coming to it. once we know that we have cut that server off from incoming request, then comes the next step. what we'll do is we'll wait for the existing request to complete, because obviously, when we have taken out the server out of a load button, during that time there would be some request that the server would have accepted it is processing and then it would be sending out the response. so we are not stopping the outbound from the server. the idea here is to wait. wait until the server serves, or basically, it basically completes the processing of all the requests that it accepted and, once they are all, send the response to like when everything is served. when we know that the server has no request, which is it is processing right now and it is not getting any incoming request, that is when what we do is we change the infra. now, here there are two ways to do it. so, for example, first strategies: we do. we do not want to replace the infrastructure, which means we will keep the server as this. what we'll do is we'll ssh into this server, pull the latest code or the artifact, for example, java, jar, golang, binary or basically python source code. we pull the latest code, we or we basically pull the artifact, whatever we need, and we restart the process. so this is something that we do when we don't want to replace the infrastructure. this means that now on this server. we are not getting any incoming request, we don't have to serve any because we have complete serving of or we have completed sending response to all of our existing requests. now, what we are doing? we are pulling the code, we are restarting the process. so now this server has the new version of our code. right, this is when we don't want to replace the infra. then if suppose we have to replace the infra, right? so in that case, what we do? we basically terminate the server and we create the new server with the new set of configuration, new configurations in new code, new artifact and all. how do we do it? for example, if you are on aws, you can use launch configurations to do that, right? so with launch configurations, you can cre, you can pre-bake an ami that does this for you, which has the latest version of your code. but the idea here is that at the end of this step, we need to have one server that is running our application and it is running the new version of our application, right? so with no infrared replacement, we would just pull the code and restart the process. with infrared replacement, we would terminate this server and we would spin up a new server with the new code in it. after this step, we would have a server that is not yet serving any request. so the next step here would be to attach this server with the new code that we just have to, uh, basically behind our load balancer. as soon as we add server behind a load balancer, it would start getting the incoming request from the user, and this way we have upgraded this one server from version one of our code to version two of our code, and this we repeat this process for every single server that we have on our infrastructure, and this is how rolling deployment is implemented. so just to summarize it, what we do is we pick a server for deployment, we stop the incoming traffic to it by removing or by basically, by basically detaching it from the load balancer, so now it won't get any new incoming traffic. we wait for the existing request to complete. this is extremely important because if we don't wait, then what would happen is user would suddenly see, basically, connection breakdown, or or your request suddenly stop responding, or some some error or some abruptness in the system would be observed, depending on what your interfacing clients are. then, once you have waited for all the existing requests to be gracefully completed, then, depending on if you, if you don't want to replace your infrastructure, then you pull the latest code on that server and you restart the process and if you want infrared placement, you terminate the server and you create a new server with the new version of your code. and now that the server that you have need your infra replaced or not replaced, this new server will have- or the server will have, uh- the new version of your code, which now you will attach back behind the load balancer. now it will start getting the incoming traffic and would start serving the newer responses that you have configured. and we repeat this entire process for all the servers that are out there. this is exactly how rolling deployment is implemented in any tool of your choice. or, if you want to ever implement you on your own, you can do this exact same thing, right. so how do we tune rolling deployment? obviously, rolling uh or doing a rolling deployment, one server at a time, seems too time consuming. even while explaining it seemed too long. right, and that is exactly what happens behind the scenes. so, because it is too slow, what sort of things that you can tune on is, instead of picking one server at a time, pick n servers concurrently. so instead of doing once or at a time, let's do three servers at a time or two servers at a time, depending on the number that you are comfortable with. for example, if your infrastructure has 1000 servers, doing one server at a time would take ages for you to complete the deployment. so that's where you might want to pick five servers at a time. but if you only have 10 servers in your infrastructure, two server at a time seems like a good choice to server, as in 20 of your infra, which is still big, but still that's okay, right? so increase the number of concurrent servers on which you would want to do the deployment parallely, and this is something that depends on the number of servers that you have. the number n should not be too small, otherwise it would take too long to complete. it should not be too large, otherwise any hiccups that you have during the deployment will affect the majority portion of your infra. so totally depends on how you would want to tune it. second way to tune it is something called as a double half deployment. this is a very interesting deployment strategy on or not any deployment strategy, but a way to implement, uh, basically rolling deployment. so what we do is, let's say, i have four servers and on four servers i have version one of my code running. and what i did is i have updated my base image with the new version of the code so that any new server that spins up, it spins up with the new version of the code and it is attached, and it is attached behind the load balancer. so now what we do? we have four servers, we know that we have four servers. now we double the infrastructure, which means earlier we had four servers, now we create four more servers. now, when the new four servers are created, they are created from the new version of your code, right? something implemented using launch configuration, auto scaling group, what not? right? so the idea here is: from four servers we go to eight servers, where the for the old four servers would have the old version of your code and the news. four servers which are created are having the new version of your code. and then what you do? you have the infrastructure and while and while terminating the half of the instances, you provide that they terminate the instances- it will create the half the number of instances that work in the ascending order of their creation. so, for example, if i double the infrastructure. or if i developed a server, what i would have is i would have four servers that were created, let's say, a week back and forth hours, which are created right now, and then when i say terminate the servers, uh, in order of their creation time, it would terminate the older ones first, so the four old servers would be created and eventually i'll be, i'll be left with four servers with new version of your code. so from four old, or from four servers with old version of your code, we move to four servers with new version of your code with no downtime. so this is one very interesting way to implement a rolling deployment with double half strategy and, again, obviously this looks very fancy and, to be honest, very fun to implement. but one thing that we have to be very sure of when we adopt this, this way to implement, is that when you are doubling your infrastructure, you have to be ready for your database to support those. many concurrent connections may not be request, but concurrent connections definitely, because what would happen is every database has a limit on the maximum number of connections that it can accept or it can handle concurrently. when you are doubling on infrastructure, you might have a connection pool that would double the connections that are being made onto the database, which might take down your database or or which might give you an error that, uh, basically cannot create more connections, something like that. you will get an error right. so ensure that your db cache any stateful components that you have that are able to handle large number of connections may not be request, but connections definitely, but this is a very interesting strategy to implement. and the third one is something that we already discussed, where we are terminating one server at a time and spinning it with the new code. just, i'm not going to reiterate that again, but doing n at a time, doing one at a time, is something that's up to you. or even double half right, okay. so now let's conclude with understanding the pros and the cons of adopting it. so we'll start with the pros of adopting rolling deployment. first of all, it is much, much, much faster than blue-green deployment, so faster resin. blue-green deployment requires you to set up an entire parallel infrastructure. it takes quite a bit of time to do it, so it is much faster than blue green deployment. you can tune n so that you can do five servers at a time or ten servers at a time, it would complete very quickly and you'll start getting changes in one uh. sorry, you can start getting uh. you'll start seeing changes incrementally rather than doing it in one shot. so it's not a flip of a switch. instead, we are doing an incremental or, uh, more likely, a a rollout, in which we are affecting few servers at a time, right. second, the deployment incurs zero downtime. we saw during any uh the, the few implementations that we saw about rolling deployment. there was not an instant where we did not have any servers serving any kind of code, be it old version, bit new version. we always had capacity to serve or to handle the incoming request from the user. this is excellent, because what this gives us is- this gives us this confidence that we would never have a downtime when we are adopting rolling deployment. third, rollouts are gradual. so when we are rolling out our changes, we are rolling out a few servers at a time and incrementally increasing it. so this, when we are doing this uh deployment, we can monitor on how on, is there an elevated error rates, is there something which is going weird? so this would give us that confidence that, hey, now, because of my, changes are rolling, which means it would be slowly rolled out. i have this confidence that if something is going bad i can quickly roll back, so that's a very big advantage that we get. next, any defects affect only a fraction of users. again the same thing. where, in case your deployment fails, there is a bug in the code, your server is not starting, it would affect, let's say, you rolled it out to five servers at a time, so for the first high five hours you'll only get to know key array. your process is not starting up or something weird has happened. so what you can do is you can very quickly roll back. again, a very super advantage that you get out of rolling deployment. and this strategy is very cost effective because you saw you always had like up, except from the double half implementation. you are taking one server out, upgrading it and then putting it back, so you are not incurring a lot of cost. your cost remains fairly constant depending on the number of servers you have, so no abrupt changes in the cost. now let's discuss the cons of adopting rolling deployment. there are few concerns, not major one, but few concerns that we have to always think of when we are adopting rolling deployment. first of all, there is no environment isolation, which means that you would have your infrastructure which is partially serving the old version of your code and the new version of your code, so there is no isolation between them. a request can go to any one of them server depending on your load balancers configuration. so you have to be aware of this when you are adopting a rolling deployment strategy. second, the changes- second and third will combine changes. we roll out has to be forward as well as backward compatible, now that on our infrastructure we'll have few set of servers that serve old code, few servers or some new code, which means that old devices, old users, old application should be supported both on the old version of your responses and the new version of your responses. so any changes you push- your database changes, epa server changes, anything has to be forward compatible and backward compatible. this is extremely important when you are adopting a rolling deployment. in most cases, if you don't have this sort of compatibility then you will not be able to choose rolling development and blue green deployment is a better choice. next, one deployment takes a long time to complete. we saw if we had hundreds- uh, if we had, if we had 1000 servers and do we do one server at a time deployment. it would take a very long time to complete, but obviously by tuning it well, you can. you can complete the deployment much faster, which we saw as a pro in some- uh, basically some- time back. so the idea here is to lick. obviously you will go for one server at a time. it would take ages for you to complete. so that's where just increase the number of servers that you would want to deploy concurrently, addressing your concern there. and the final thing: your stateful applications will be affected during deployment. so let's say, if on your api server where you're doing the deployment, there was some data that you have stored, like some cache, some, some session, some sort of statefulness, and now that server is replaced, it will take time for the new server to rebuild the entire state locally. so your stateful applications, where you are storing some state on your api server- but it is not purely stateless- in some state which is stored on your repair servers, it will be affected during the deployment. so again, if you have a very heavy stateful application, you might not want to choose rolling deployment to solve that problem, but if you are okay having that minor aberration, then rolling deployment works just like a charm, right? so, yeah, that's it. that's it about rolling deployment. uh, i hope. i hope you found it interesting. few micro details that i i tried to paint out of it because it's not just hunky dory every time. it's not a grass all green. there are few concerns, few interesting insights from how i've seen it implemented across industry. that's it for this one. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel what's up. i post three in-depth engineering videos every week and i'll see you in the next one. thanks,