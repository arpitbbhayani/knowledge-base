in this video, we take an in-depth look into what rolling deployment is, how it is actually implemented, how to tune it to get the max out of it, some key challenges that we face during adoption, and conclude with understanding the pros and cons of adopting it.
rolling deployment is a deployment strategy that slowly replaces the previous version of your application with the newer version of your application, and it is most commonly implemented by replacing the underlying infrastructure, which means, out of nine servers that we have, which initially runs the version one of all of which basically runs the version one of our code, on all of them we slowly start taking one server upgraded to version two, then another server upgraded to version two, right.
so what kubernetes does when you, when you basically trigger a deployment with this strategy, it would replace one container at a time, that it would have a docker image, or we would have a container ready with a new version of our code and let's say we have hundred containers running.
it would start replacing those containers one by one so that eventually we would have all the containers running with the new version of the code and the old or in the, and every single one of the old container will be terminated.
we have multiple ec2 servers and then we start upgrading them one by one, either terminating the old one or basically replacing the old, or basically replacing the code on the old with the new one.
so this is the core idea behind rolling deployment strategy and this is, although slow, but it is very resilient, very simple and gradual.
you are continuously having the incoming surge of user request which is coming to your load balancer, which is then moving forward to our api servers.
now what would happen during the time of the deployment, you would have some set of servers which are serving the old uh, the which are basically serving the version one of your code, and some servers which are serving the new version of your code.
so which means that, depending on which server the request goes to, you will either get response generated from the version one of your code or version two of your code.
so this is a very critical point, that whenever we are adopting something like rolling deployment strategy, we have to ensure that we are okay when part of our infrastructure is serving old code and other part serving the new code.
you have to ensure that because during rolling deployment, a part of your infrastructure will have old code and other part will have the new code serving simultaneously, right?
so when i say gracefully, which means that no existing request should uh, should be terminated abruptly and everything should be served with no downtime, right, and the code of everything should be served and then only instance should be terminated and then during the entire deployment there should not be any downtime whatsoever.
the first thing that we do is we stop the incoming traffic to it so that now server cannot accept, or your server would not get any new incoming request.
it's just that when you take the server out of the load balancer, it is just stopping the incoming request coming to it.
what we'll do is we'll wait for the existing request to complete, because obviously, when we have taken out the server out of a load button, during that time there would be some request that the server would have accepted it is processing and then it would be sending out the response.
wait until the server serves, or basically, it basically completes the processing of all the requests that it accepted and, once they are all, send the response to like when everything is served.
when we know that the server has no request, which is it is processing right now and it is not getting any incoming request, that is when what we do is we change the infra.
we do not want to replace the infrastructure, which means we will keep the server as this.
so now this server has the new version of our code.
but the idea here is that at the end of this step, we need to have one server that is running our application and it is running the new version of our application, right?
so the next step here would be to attach this server with the new code that we just have to, uh, basically behind our load balancer.
as soon as we add server behind a load balancer, it would start getting the incoming request from the user, and this way we have upgraded this one server from version one of our code to version two of our code, and this we repeat this process for every single server that we have on our infrastructure, and this is how rolling deployment is implemented.
so just to summarize it, what we do is we pick a server for deployment, we stop the incoming traffic to it by removing or by basically, by basically detaching it from the load balancer, so now it won't get any new incoming traffic.
then, once you have waited for all the existing requests to be gracefully completed, then, depending on if you, if you don't want to replace your infrastructure, then you pull the latest code on that server and you restart the process and if you want infrared placement, you terminate the server and you create a new server with the new version of your code.
and now that the server that you have need your infra replaced or not replaced, this new server will have- or the server will have, uh- the new version of your code, which now you will attach back behind the load balancer.
this is exactly how rolling deployment is implemented in any tool of your choice.
obviously, rolling uh or doing a rolling deployment, one server at a time, seems too time consuming.
so, because it is too slow, what sort of things that you can tune on is, instead of picking one server at a time, pick n servers concurrently.
for example, if your infrastructure has 1000 servers, doing one server at a time would take ages for you to complete the deployment.
so that's where you might want to pick five servers at a time.
but if you only have 10 servers in your infrastructure, two server at a time seems like a good choice to server, as in 20 of your infra, which is still big, but still that's okay, right?
so increase the number of concurrent servers on which you would want to do the deployment parallely, and this is something that depends on the number of servers that you have.
this is a very interesting deployment strategy on or not any deployment strategy, but a way to implement, uh, basically rolling deployment.
so what we do is, let's say, i have four servers and on four servers i have version one of my code running.
now, when the new four servers are created, they are created from the new version of your code, right?
so the idea here is: from four servers we go to eight servers, where the for the old four servers would have the old version of your code and the news.
four servers which are created are having the new version of your code.
or if i developed a server, what i would have is i would have four servers that were created, let's say, a week back and forth hours, which are created right now, and then when i say terminate the servers, uh, in order of their creation time, it would terminate the older ones first, so the four old servers would be created and eventually i'll be, i'll be left with four servers with new version of your code.
so this is one very interesting way to implement a rolling deployment with double half strategy and, again, obviously this looks very fancy and, to be honest, very fun to implement.
but one thing that we have to be very sure of when we adopt this, this way to implement, is that when you are doubling your infrastructure, you have to be ready for your database to support those.
so ensure that your db cache any stateful components that you have that are able to handle large number of connections may not be request, but connections definitely, but this is a very interesting strategy to implement.
and the third one is something that we already discussed, where we are terminating one server at a time and spinning it with the new code.
so we'll start with the pros of adopting rolling deployment.
you can tune n so that you can do five servers at a time or ten servers at a time, it would complete very quickly and you'll start getting changes in one uh.
instead, we are doing an incremental or, uh, more likely, a a rollout, in which we are affecting few servers at a time, right.
we saw during any uh the, the few implementations that we saw about rolling deployment.
there was not an instant where we did not have any servers serving any kind of code, be it old version, bit new version.
this is excellent, because what this gives us is- this gives us this confidence that we would never have a downtime when we are adopting rolling deployment.
so when we are rolling out our changes, we are rolling out a few servers at a time and incrementally increasing it.
where, in case your deployment fails, there is a bug in the code, your server is not starting, it would affect, let's say, you rolled it out to five servers at a time, so for the first high five hours you'll only get to know key array.
and this strategy is very cost effective because you saw you always had like up, except from the double half implementation.
now let's discuss the cons of adopting rolling deployment.
first of all, there is no environment isolation, which means that you would have your infrastructure which is partially serving the old version of your code and the new version of your code, so there is no isolation between them.
a request can go to any one of them server depending on your load balancers configuration.
so you have to be aware of this when you are adopting a rolling deployment strategy.
we roll out has to be forward as well as backward compatible, now that on our infrastructure we'll have few set of servers that serve old code, few servers or some new code, which means that old devices, old users, old application should be supported both on the old version of your responses and the new version of your responses.
this is extremely important when you are adopting a rolling deployment.
in most cases, if you don't have this sort of compatibility then you will not be able to choose rolling development and blue green deployment is a better choice.
next, one deployment takes a long time to complete.
we saw if we had hundreds- uh, if we had, if we had 1000 servers and do we do one server at a time deployment.
you can complete the deployment much faster, which we saw as a pro in some- uh, basically some- time back.
so let's say, if on your api server where you're doing the deployment, there was some data that you have stored, like some cache, some, some session, some sort of statefulness, and now that server is replaced, it will take time for the new server to rebuild the entire state locally.
that's it about rolling deployment.