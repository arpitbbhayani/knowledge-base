rolling deployment is a deployment strategy that slowly replaces the previous version of your application with the newer version of your application, and it is most commonly implemented by replacing the underlying infrastructure, which means, out of nine servers that we have, which initially runs the version one of all of which basically runs the version one of our code, on all of them we slowly start taking one server upgraded to version two, then another server upgraded to version two, right.
so what kubernetes does when you, when you basically trigger a deployment with this strategy, it would replace one container at a time, that it would have a docker image, or we would have a container ready with a new version of our code and let's say we have hundred containers running.
it would start replacing those containers one by one so that eventually we would have all the containers running with the new version of the code and the old or in the, and every single one of the old container will be terminated.
you are continuously having the incoming surge of user request which is coming to your load balancer, which is then moving forward to our api servers.
now what would happen during the time of the deployment, you would have some set of servers which are serving the old uh, the which are basically serving the version one of your code, and some servers which are serving the new version of your code.
so this is a very critical point, that whenever we are adopting something like rolling deployment strategy, we have to ensure that we are okay when part of our infrastructure is serving old code and other part serving the new code.
you have to ensure that because during rolling deployment, a part of your infrastructure will have old code and other part will have the new code serving simultaneously, right?
so when i say gracefully, which means that no existing request should uh, should be terminated abruptly and everything should be served with no downtime, right, and the code of everything should be served and then only instance should be terminated and then during the entire deployment there should not be any downtime whatsoever.
what we'll do is we'll wait for the existing request to complete, because obviously, when we have taken out the server out of a load button, during that time there would be some request that the server would have accepted it is processing and then it would be sending out the response.
wait until the server serves, or basically, it basically completes the processing of all the requests that it accepted and, once they are all, send the response to like when everything is served.
when we know that the server has no request, which is it is processing right now and it is not getting any incoming request, that is when what we do is we change the infra.
but the idea here is that at the end of this step, we need to have one server that is running our application and it is running the new version of our application, right?
so the next step here would be to attach this server with the new code that we just have to, uh, basically behind our load balancer.
as soon as we add server behind a load balancer, it would start getting the incoming request from the user, and this way we have upgraded this one server from version one of our code to version two of our code, and this we repeat this process for every single server that we have on our infrastructure, and this is how rolling deployment is implemented.
then, once you have waited for all the existing requests to be gracefully completed, then, depending on if you, if you don't want to replace your infrastructure, then you pull the latest code on that server and you restart the process and if you want infrared placement, you terminate the server and you create a new server with the new version of your code.
for example, if your infrastructure has 1000 servers, doing one server at a time would take ages for you to complete the deployment.
but if you only have 10 servers in your infrastructure, two server at a time seems like a good choice to server, as in 20 of your infra, which is still big, but still that's okay, right?
or if i developed a server, what i would have is i would have four servers that were created, let's say, a week back and forth hours, which are created right now, and then when i say terminate the servers, uh, in order of their creation time, it would terminate the older ones first, so the four old servers would be created and eventually i'll be, i'll be left with four servers with new version of your code.
so this is one very interesting way to implement a rolling deployment with double half strategy and, again, obviously this looks very fancy and, to be honest, very fun to implement.
and the third one is something that we already discussed, where we are terminating one server at a time and spinning it with the new code.
instead, we are doing an incremental or, uh, more likely, a a rollout, in which we are affecting few servers at a time, right.
where, in case your deployment fails, there is a bug in the code, your server is not starting, it would affect, let's say, you rolled it out to five servers at a time, so for the first high five hours you'll only get to know key array.
a request can go to any one of them server depending on your load balancers configuration.
we roll out has to be forward as well as backward compatible, now that on our infrastructure we'll have few set of servers that serve old code, few servers or some new code, which means that old devices, old users, old application should be supported both on the old version of your responses and the new version of your responses.