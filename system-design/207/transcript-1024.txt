so a company typically needs a data platform for their business decisions and strategies, to power the data science and train ml models and generate the product insights that makes their product better by the day.
you need all of them at one place in a in a singular format, so that you can you can fire the queries that you want and generate insights out of it, right.
people typically use basically Amazon redshift or Google bigquery for this purpose, but you can build your own on top of hdfs if you want to.
so these data warehouses are typically optimized for Gigantic analytics query, not for transactional purposes.
so what that core need is that they have their data distributed across bunch of databases.
now, from this data warehouse people will get like different teams, let's say insights, your analytics team, your product management team, your machine learning team, your data science team and your business teams- and who not, can directly query this one data warehouse which has data from all the databases in this one structured format.
you have profile data in another database, but you want to join them so that you can do some.
so typically, data warehouses are structured data because you have to file analytics query needs to be structured, so, but your transactional databases might be key Value Store might be non-relational databases, something around that.
right, then each team might need a different View for the same set of tables that you are querying.
right, so each team will have its own set of requirement, its own set of queries, that how these two table wants to be joined and kept together to empower their use case.
so now, this makes it very interesting because if you have hundreds and thousands of tables, you can merge them, you can join them in any way that you would want and then generate different views for different teams so that they can consume them right now.
this makes building data platform fun, interesting and very challenging right now.
your inside steam, your analytics team, needs this data at one place so that they can query it and it, and here you can very clearly see that auth and profile table for the transactional needs needs to be separate, because odd table will do your login, logout stuff, while the profile information is there to store your profile information.
you have to join them, you have to merge them, you have to like, for example, authen profile, make sense that you keep auth and profile together joined, because it- because it's a one-to-one mapping, you typically would want all the information together for almost all of your use cases.
so your data is entirely merged, joined and replicated into user detail core table.
right now, each team is free to create derived tables the way they want.
the idea is pretty simple: a table that you create from a select query from above.
and the output of this query is the derived table that you are creating, called user details.
right, so you can create the way you want the derived tables out of it.
now this yaml based config is like if you would want to create a derived table, you would already know, okay, these are the code tables that I have.
now, for example, you want to create a new derived table right now.
so in the yaml file you specify that, hey, I want to create this new derived table having this n columns, where you specify the type of the column and constraints and whatnot.
which means that, when I'm creating this, when I'll be firing a SQL query to merge the data or to join the data from two core tables, and, if I'd want to, if I and if I'm basically, uh, putting the output of this select into my derived table, should I be merging with the existing data or should I be appending to the existing data or should I be replacing the entire derived table?
you may want to specify how you want to partition a derived table, because it's very much possible that one node is not enough to support the huge amount of data that you have.
so you have to specify the partition key on that so that the derived table itself is partitioned across your data nodes right in your bigquery setup that you have.
so the idea, this SQL query that you specify over here, will be fired repetitively by the schedule operating on this window, the output of which would be either merged, append or replaced in the table that you specify, right?
so one yaml file for each derived table that you would have, right?
you get the data and then you're putting it into your bigquery and for each ml file you are creating one table, either merging, appending or replacing.
right now, from this bigquery, from this bigquery or data warehouse, what you get your teams, because this team, like insights product ml, they are the one who specify the yaml file.
you typically expect a millisecond latency with your MySQL or your mongodb database that you have, but you cannot expect it with a data warehouse.
so what you first need is specify your yaml file, take that data from different database and have it, and you create a derived table out of it.
you can choose whatever destination you want to, but you take the derived table- some derived table that you need for online processing, to power end user use cases- from this bigquery into Skylar DB and then your prediction service can use this Kyla DB and there would be a job that is running in your airflow somewhere which is repetitively pulling this and putting it over here.
right, just like how we specified configuration, the yaml, there might be some more configuration that you can add that would take the data out from bigquery- some tables- not all one or two tables- and put it into Skyler DB for your online processing.
you can replicate the data, create derived tables out of it and power your use cases.
Airways typically, what most data engineering teams use to build workflows like this right.