so when a company scales, they adopt micro services and each service typically gets its own independent database. with data being distributed across so many databases that emerges a need to unify them, to extract deep product insights, make strategic business decisions, train ml models and so much more. in this video, we take a super detailed look into how Discord built its unified data platform that processes trillions of data points and petabytes of data every single day. but before we move forward, I'd like to talk to you about a course on system design that I've been running for over a year and a half now. the course is a cohort based course, which means I won't be rambling a solution and it will not be a monologue at all. instead, a small, focused group of 50 to 16 genus will be brainstorming the systems and designing it together. this way, we build a very solid system and learn from each other's experiences. the course is enrolled by 800 plus Engineers, spanning 12 codes and 12 countries. Engineers from companies like Google, Microsoft, GitHub, slack, Facebook, Tesla, Yelp, Flipkart, dream11 and many, many, many more have taken this course and have some wonderful things to say. the course is focused on Building Systems, the way they are built in the real world. we will be focusing heavily on building the right intuition so that you are ready to build any and every system out there. we will be discussing the trade-offs of every single decision we make, just like how you do in your team. we cover topics ranging from Real Time text communication for slack to designing our own toilet balance, live text commentary to doing impressions counting at scale. in all, we would be covering roughly 28 systems, and the detailed curriculum, split week by week, can be found in the course page linked in the description down below. so if you are looking to learn system design from the first principles, you will love this course. I have two offerings for you. the first one is the live cohort based course and the second one is the recorded offering. the Live code base course happens once every two months and will go on for 8 weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to learn and want to binge learn system design, I would recommend going you for the recorded one. otherwise, the Live code is where you can participate and discuss the systems and its design. live with me and the entire code. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page. arpitbani dot me slash masterclass. I repeat at with many, dot me slash masterclass and I would highly recommend you to check that out. I've also put the link of this course page in the description down below and I'm looking forward to see you in my next cohort. so a company typically needs a data platform for their business decisions and strategies, to power the data science and train ml models and generate the product insights that makes their product better by the day. so can they not directly work with the database? why do they even need a data platform in the first phase? so first reason: each service needs are most probably would have its own separate database right now when they have it. each service is independent in choosing its own database. for example, the profile service might say, hey, I'll use mongodb, while the payment services I'll use MySQL. now, one is mongodb, one is MySQL. there is a unified way to query them right. third is: you cannot make cross database joins and make sense out of data that is distributed. you need all of them at one place in a in a singular format, so that you can you can fire the queries that you want and generate insights out of it, right. and if you want to make a distributed join- that's extremely expensive and extremely computational, heavy, people typically don't do that- right. then your analytics query that you are firing might show the transactional DB. so let's say you have your database, which is actually serving a production traffic and you are firing queries on that. so your gigantic analytics query- maybe you are, you are basically firing it in a distributed board- it still can choke down your database and affect your production workload. right, there are. they are basically workarounds on that, but it makes it extremely expensive to do so. so there has to be a better way, which is where what typically we do when we build a data platform is we get all the data from all the database, duplicate them and put it in a data warehouse. slash, data Lake, right? so the idea here is you take all of your transactional data across all of your micro services that you need and you put them together in one data warehouse. people typically use basically Amazon redshift or Google bigquery for this purpose, but you can build your own on top of hdfs if you want to. right. so data warehouse, data Lake: you put all of the data in there to one place and this one place becomes at one point where every one in the company can query and make sense out of the data. obviously, they can continue, or they will continue, to use their existing database for the transactional needs. for example, your payment service is being powered by one one simple MySQL instance that you have. you'll continue to use it for your transactional leads. but for your analytical needs, for your, for your offline processing, you can fire queries, analytics queries, on this data warehouse. right, it makes it simple. so these data warehouses are typically optimized for Gigantic analytics query, not for transactional purposes. right, okay, now let's dive deep into how Discord does it. right. so they call, they have named their data platform, derived very interesting name, but again, they have named it: derive. so what that core need is that they have their data distributed across bunch of databases. they want to replicate it and put it into a data warehouse. now, from this data warehouse people will get like different teams, let's say insights, your analytics team, your product management team, your machine learning team, your data science team and your business teams- and who not, can directly query this one data warehouse which has data from all the databases in this one structured format. this means that, let's say, if you have your, if you have your payment service in one, you have your payments data in one database. you have profile data in another database, but you want to join them so that you can do some. basically, let's say your, uh, let's say you want to do analytics with respect to which region gets how much of payment, or what kind of user gives you what kind of payment, or rather, what kind of user gives you how much of payment. right, if you want to do, if you want to do this sort of analysis, you need this, you need this sort of information: that data from payment GB, data from profile DB, needs to be pressed present in one database, which is where data warehouses, data Lakes, come in very handy, right? so this is where the internal teams, like insights, product machine learning team, they get all of that at one place, right? few challenges, few challenges before we jump in. first challenge is non-relational data sources. so typically, data warehouses are structured data because you have to file analytics query needs to be structured, so, but your transactional databases might be key Value Store might be non-relational databases, something around that. so you, you're the jobs that you write, that extracts data from that and then converts it into a structure. so you take- you basically take- a semi-structured data, you convert it into structured information, having same unified C mind. you put it into Data Warehouse. right, then each team might need a different View for the same set of tables that you are querying. for example, let's say you have your, you have your orders detail, you have your payments information, you have your authentication profile information. right now, let's say tables- table one, table two, Table Three. you want these three table to be combined in a particular join order on a particular attribute and you want to use it for recommendation, then table 1 and table file in some join or in some way. you want to merge it and use it for your retention analysis, then table 2 and table 4 to get deeper payment insights. right, so each team will have its own set of requirement, its own set of queries, that how these two table wants to be joined and kept together to empower their use case. right. so now, this makes it very interesting because if you have hundreds and thousands of tables, you can merge them, you can join them in any way that you would want and then generate different views for different teams so that they can consume them right now. this makes building data platform fun, interesting and very challenging right now. let's go deep into just understanding this one use case. well, so now, for example, let me give you a very practical use case for this. so let's say you have your authentication table, you have your profile table. so authentication Table stores your email name, password and basic information. profile tables stores your profile information, for example, your time zone, location, preferences and whatnot. and then you have orders table in which you have placed orders. now these three tables, like your auth and profile, can be part of one database. orders is part of your orders database. right, but now what do you need? your inside steam, your analytics team, needs this data at one place so that they can query it and it, and here you can very clearly see that auth and profile table for the transactional needs needs to be separate, because odd table will do your login, logout stuff, while the profile information is there to store your profile information. but for most of of your derived use cases, these are pretty much the same. so what happens is what Discord did here is they created derived tables, but from core table. so what they Define is they Define first they have your transactional tables, which is typically what a micro service directly interacts with. right. then they create core tables. now this core tables are the ones which are very commonly. you have to join them, you have to merge them, you have to like, for example, authen profile, make sense that you keep auth and profile together joined, because it- because it's a one-to-one mapping, you typically would want all the information together for almost all of your use cases. so you typically join them and create a new view called user detail. that's a new table, it's a replicated. so your data is entirely merged, joined and replicated into user detail core table. so first you have transactional tables, then you create a bunch of code tables. now this code tables could be joined or straight away copy paste from your transaction table. now, from this Code table, this becomes the input to derived tables. now this derived tables, you can take any number of tables or queries that you are firing to join multiple tables from your code table and you create derived tables out of it. right now, each team is free to create derived tables the way they want. so what is a derived table? or even, basically, what is the code table? the idea is pretty simple: a table that you create from a select query from above. so, for example, select star from auth, inner join profile, where authid equal to profile dot user ID, simple inner join between the two table. and the output of this query is the derived table that you are creating, called user details. right? similarly, let's say you want to merge, let's say, for insights to get detailed payment information, User detail and payment stable needs to be merged and joined so that you have one table that holds all the payment DB, payment information, along with details, so that inside scene can directly fire and do the analysis that they want to. right, so you can create the way you want the derived tables out of it. right, and that's how you can very beautifully see how your hierarchy of tables is created. first is transactional tables. from there you create code tables, which are very commonly merged and joined. from this code tables you create derived tables. while defining a derived table, you have to specify what query should I fire on core tables repeatedly to populate my derived table? that's the idea behind it. okay, so now how do we create a derived table? now, it's what Discord did. is they provided a yaml based configuration? now this yaml based config is like if you would want to create a derived table, you would already know, okay, these are the code tables that I have. each table is this kind of schema. now, for example, you want to create a new derived table right now. this derived table would be what it would be? be combination of some of the core tables. now, when that is a, when that is a combination of some of the core tables. so in the yaml file you specify that, hey, I want to create this new derived table having this n columns, where you specify the type of the column and constraints and whatnot. you specify the strategy. now, this strategy is essential to understand. so this strategy could be three foot: merge, append or replace. which means that, when I'm creating this, when I'll be firing a SQL query to merge the data or to join the data from two core tables, and, if I'd want to, if I and if I'm basically, uh, putting the output of this select into my derived table, should I be merging with the existing data or should I be appending to the existing data or should I be replacing the entire derived table? right, because these are the three possible cases that you would want to handle, right? so you would Define the strategy, then you define the schedule schedule is: how frequently do you want to refresh this derived table? this does not mean so. this typically means that you are not getting very real time data, like as soon as your transition data is updated. you are populating over here. you would have jobs running periodically, let's say, once every five minutes, ten minutes, one hour, one day, depending on the configuration that you would want to run. you can define a schedule and a window. then you Define partition key. so how do you want to? because when you are merging huge data- now this is- this is not just one minuscule table that you're merging- this table itself could be distributed. this table itself would have large amount of data. when you're not joining two huge tables, you are eventually creating one gigantic table. you may want to specify how you want to partition a derived table, because it's very much possible that one node is not enough to support the huge amount of data that you have. so you have to specify the partition key on that so that the derived table itself is partitioned across your data nodes right in your bigquery setup that you have. and then, on that particle, how do you want to Cluster it? which means, how do you want to sort it. so what's your partition key, what's your sort key, so that your queries become efficient? and then you specify from which data set you would want to get and the SQL statement that you would want to fire repetitively, right? so the idea, this SQL query that you specify over here, will be fired repetitively by the schedule operating on this window, the output of which would be either merged, append or replaced in the table that you specify, right? and while it the data is getting populated, the data will be partitioned by this partition key and clustered by this sort key. right? this is the whole yaml configuration that you provide. right now. you'll get an idea like what we are actually trying to do. it's such a beautiful piece of concept where you are literally taking any data, any databases, and you are putting them together into the way you'd want, the way you would want to consume. you're putting it into your data warehouse, right? this way your use cases become so efficient. so your insights, data science and ml team can operate independently without any intervention from the data engineering team, right? okay, now let's take a detailed look into the architecture of this. now, the architecture is very simple, yet very interesting. so here, what do that we know that we have a bunch of yaml files in which the, the configuration is specified. so one yaml file for each derived table that you would have, right? so one yaml file for each derived table. each ml file has its own kubernetes pod on which the the table updation works. right? so each ml file gets its own kubernetes part that is continuously running, that is, keeping the derived table updated. right now, the kubernetes port can connect to any of the data sources that are there in the infrastructure, right? maybe? all surveys, profile service, payment service, order service, notification service and whatnot. right, because in yaml file, in the SQL of yaml, you can provide any table. so these are core tables or transactional tables in which are getting all the data and putting it over there. right now, when the table updation, you fire the SQL query into this corresponding tables. you get the data and then you're putting it into your bigquery and for each ml file you are creating one table, either merging, appending or replacing. right now, from this bigquery, from this bigquery or data warehouse, what you get your teams, because this team, like insights product ml, they are the one who specify the yaml file. so they would know, okay, okay, this is the table name in which I'll find my data, which is periodically updated, right, so, depending on the schedule, it will update and they can seamlessly get unified view of data and they can do their processing time. really very simple, but very interesting, right, okay, now? what else now here? what you need more importantly is how you actually like from where, like in the last one, let's say, you consumed 100 000 rows. now you want, you need to know that this time I want to start with hundred thousand and first row you need to store this meta information somewhere that last time my job ran, it populated these many rows. it read: till this may, uh, iterated this ID. so in my next iteration I want to start from this position, because you would not be generating all the data every time again. so that is where you have a metadata log where you are storing all of this information, along with information like: how long did it take for my last run, what was the data lineage and other meta information that you would want to store. it's over here, right, so it uses the metadata log, the yaml file, the sources to act, to execute the query and load the data in bigquery, right, okay, but is this the end right once the data is in bigquery. now, bigquery is a data warehouse. because it is a data warehouse, you cannot expect a sub second or a sub millisecond latency over there. it takes time for you to fire a query and get the response because the amount of data is huge. you typically expect a millisecond latency with your MySQL or your mongodb database that you have, but you cannot expect it with a data warehouse. but there are use cases which are online use cases, which means, let's say, for example, you have a prediction service, somehow you have a prediction service and you want this prediction service to serve the predictions to your end users in real time. but the data that this prediction service needs is a derived data, which means it's a combination of a bunch of data distributed across bunch of databases. so what you first need is specify your yaml file, take that data from different database and have it, and you create a derived table out of it. once you have created a derived table out of it, but this denive table is where it's in bigquery, but bigquery cannot give you millisecond latency. so what you need is to pull the data out from bigquery and put it into a real-time database or a database that supports, or a transactional database, a database. it supports millisecond latency. so what Discord uses is: they have this. they have this connector that exports the data from the bigquery into Skylar DB. you can choose whatever destination you want to, but you take the derived table- some derived table that you need for online processing, to power end user use cases- from this bigquery into Skylar DB and then your prediction service can use this Kyla DB and there would be a job that is running in your airflow somewhere which is repetitively pulling this and putting it over here. right, just like how we specified configuration, the yaml, there might be some more configuration that you can add that would take the data out from bigquery- some tables- not all one or two tables- and put it into Skyler DB for your online processing. this way- I across this chord- you can literally pick and choose the tables the way you want. you can replicate the data, create derived tables out of it and power your use cases. this means that there is no need of explicit data engineering intervention to get your things done and every team is empowered with a bunch of data so that they can make the best decision possible, because data doesn't lie, and this, this is the beauty of building a very generic self-served data platform. typically, when a company hits the scale with bunch of microservices, this sort of use cases emerge. right now, few technologies that you can explore to get a deeper understanding of it: Yaman to know how to write it and process it. second, basic knowledge of SQL is fine. third, airflow: if you have not heard of airflow, just Google about it and read about it. Airways typically, what most data engineering teams use to build workflows like this right. fourth is about this metadata log on. how do you store, how do you do incremental processing of data? then bigquery, or data of lake, or data warehouse, in case you are unaware, right, so touch upon those points. it's almost entire data engine domain. just Google about it. it's very fun field to be part of. right, and that is it. I hope you found it interesting. like this is the heart and soul of most data Lakes, data platforms that you would ever see across the industry. the idea Still Remains the Same: creating a self-served platform out of it. this entire architecture that we just discussed is taken from the discord's engineering block, which I've Linked In the description down below. highly, highly, highly recommend you to check that out to get deeper understanding of what they actually do, right? so yeah, that is it. that is it for this one. I'll see in the next one. thanks, Saturn [Music]. thank you.