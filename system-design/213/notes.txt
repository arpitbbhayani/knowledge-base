How Shopify balances the shard without downtime

rebalances shards How shopity without downtime any GET / products/shop2 People host their shops can on shopity their database. -routing modules and they use Mysal as NGINY Current Architechere - Y Y 1. Shops are distributed across pods' Shop I Shop 3 All share database Shop 4 2. shops in a pod a Shop 2 Request to proxy, and Pod I 3. come NGINX Pod a it routes it to the corresponding pod. Every row in table has a column 'shop-id' that tells which shop it belongs to pod to Moving shop from one another Do this without challenge: iterate through all the tables t downtime: " any pick rows with specific shop-id more those rows to adb of another pod do need to move? why we Resource intensive shops on the same shard may 1. risk failure due to over-utilization 2. inconsistent database utilization shards across

How to decide which shop lives in which shard? Distribution based number good idea on of shops is not a because we may end up having two 'heavy shops on one shard. The decide depens 'heuristics' we want to apply way we on S 1. historical database utilization &) ata Science Team 2. historical traffic on the shop decides the optimal distribution 3. Forecasting (private request) based on these factors / products/shop2 Moving the shops GET Critical constraints -routing module > NGINX 1. Shop must be entirely available - No data loss Y 2. or corruption Shop 1: Pod 1 3. Shop 2: Pod] Shop I No strain unnecessary on infra Shop 2 Three high-level phases routing Table Pod I 2. Batch Copy and Tail Binlog 2. Cutover 3. Update routing table

Phase 2: Batch tail copy and Binlog shopify internal tool (also opensourced) named uses an ghostferry. 1. So through tables and pick races *B1 DB2 with the 'shop-id' and write a them to another database. I, in a transaction ~ Batch Copy While batch copy is happening, 2. & Insent, orders, (...), keep track of newer changes happening Update, orders, (...), on DB by consuming Binlog (Write ahead log) Insent, orders, (...), 3 We need to filter out enteries for shops * that don't interest us. or just note the binlog f coordinates *B1 capture, filter, apply >CDC > Queue To speed up-Read multiple tables in parallel while batch copy and tailing happens, One DB continues to serve request.

Phase 2: Prepare for cutover Once batch copy is complete, consume all the newer unites through Binlog capture, filter, apply APPLY CDC > I Queue BB2 Wait until the 'lag' is down to seconds (near-realtime almost done the queue and i.e. We are consuming newer events are almost immediately consumed. The writes to source Ds is stopped. " I two seconds very short duration one Capplication logic has retries) The source PB's binlog coordinate are recorded done. and as soon as target DB reaches that we say replication writes to At this stage:1. No new source DB 2. source db = target db

Phase 3: Update routing table Once we have confidence GET / products/shop2 of no dataloss, we update routing table -routing module > NGINX and traffic is switched on. shops"Poda Y New request thus to flows Shop 2: Pod] the new pod. Shop 2 routing Table *Cutover is completed in Pod I short window to minimize downtime avery Shop I Next steps Pod I 1. Validate and verify the correctness 2. prune the data from old database