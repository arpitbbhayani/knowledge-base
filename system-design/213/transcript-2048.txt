so people can host their shops on Shopify, and Shopify uses MySQL lizard primary database and this is what their current architecture looks like.
now this pod- please don't confuse it with the kubernetes, but Paul- is just a logical grouping of shops which share a common database.
right now, here these two databases, they do not have any data in common.
nginx, through its routing module, decides where to forward the request to requests, goes to the corresponding part and serves from that corresponding database.
right now, within this database, each table would have a column called shop ID which help us identify that this row belongs to this shop.
like if you have an orders table in that order stable, there will be in in that order stepper, there will be a column called shop ID which tells that, hey, this order belongs to this particular table.
now, when A Shard becomes hot, which means, let's say, there's a huge amount of load coming on shop one, which makes this pod one hot.
right, because a lot of reads and writes are going to this database.
so how do you move data from one database to another without having a downtime, which is what makes it extremely challenging but fun at the same time.
so a normal approach to think about it is: hey, let me just iterate through all the tables of my database, pick the rows with a specific shop ID and I move this rows from one database to another database.
the problem is downtime, because a shop can have huge number of rows.
while it is having huge number of rows and let's say, the migration is going to take one hour, you can't just say, hey, my site would be down for one hour.
so which is where, without incurring any downtime, you have to move data from one database to another, which is what makes it really interesting problem to solve.
first reason that what happens when a Shard becomes hot, like this is like if, if, nothing could happen, if rather, nothing would happen when there is an excessive load, you can just say, hey, why do we even need to get?
but two critical reasons why you would want to move data or you would want to move shop from one pot to another.
here we clearly see that if there is a huge number of requests coming on one shop, it would make this database piping hard, which means that there is a chance, there is an increase in probability of this fragment of infrastructure failing, which you should not let happen, because one shop spacing this excessive load takes down other shops with it- is unacceptable, right?
so, which is why we think of: hey, let's do rebalancing of shards such that the load across all of my pods is fairly equal, right, okay?
so the first problem comes: how do we decide which shop lives in which Shard?
so this is typically done by the data science team, because their job is to Crunch data, to apply those models which tell you that, hey, this shop, this should move to this chart, this shop should move to another shot, right?
so the factors that you might consider that, the first normal factor that you might come to a conclusion that, hey, let's just move it with respect to the number of shops in a database.
every database handles 100 shops, but that's not good, because when you do that, it's very much possible that there might be a pod which is handling three, or like many, hot shops, which means shops which are very resource intensive.
you don't want that to happen, right so, which is why you apply heuristics to see, hey, where which shot can move to which shot.
few things that you can consider: you can consider historical database utilization, which would tell you, hey, this is where this shop should lie, this is where this shop should lie, this is the basic, this is the pattern of utilization of this particular shop.
otherwise, you can look at historical traffic that you see and, more importantly, you can use forecasting, because, let's say, there is a shop which calls you and tells you: hey, uh, we are having a big, big flash sale, so we would want- uh, so we are expecting this much of traffic.
so you would just move their shop to a isolated place so that that cell can go really well, right?
so, second, without having any downtime, right, okay?
first, that the shop must be entirely available, which means that there should not be any, any perceivable downtime in the shops, right?
so no matter what happens, no matter where you are moving data, no matter how good you are making that infrastructure or their performance, downtime is not expected, because it would impact not only your Revenue but your customers revenue, and that's more important.
second, there should not be any data loss or corruption, because you cannot just say, hey, because I move the data from One DB to another and then, like, I just missed few rows, that cannot happen.
so, then, this entire problem statement of moving shop from one to another- can be broken into three phases.
so phase one: batch copy and tailing bin log.
and now, yeah, by the way, all of this thing, whatever I'm discussing, is taken from shopify's engineering blog, which I've Linked In the description down below.
right, okay, first step, the most or the most basic one, but really important one.
you do batch copy of data from One DB to another DB.
we know that every Row in the table has a shop ID as one of its column.
you iterate through all the tables, pick the rows that has a specific shop ID and you write them into another database in a transaction.
this way you ensure that there is a bulk copy happening from 1 DB to another.
you might create like you might just have a process or a machine that is just basically editing through the tables and copying and pasting the rows over here, right while this batch copy is happening.
you like this batch copy?
obviously, while this batch copy is happening, there are new updates which are flowing in the main DB.
so, which means that when you start copying, you also have to keep track of whatever new changes are coming in into this DB.
so all the updates that happen on the DB, they go to this bin log and it's really easy to write a parser for that, or there are tools available as well, which you can go through, which contains events that happened on the database.
so from there, this would ensure that the first step is ensuring that your you are batch copying the data from One DB to another while you are keeping track of changes that are happening live on the database.
second way to do it is just make a note of Bin log coordinates.
here, obviously, performance optimization: you just don't need to have just one thread which is copying the data from the batch copy.
okay, now, second step: once you say you have made note of that, either that bin log coordinate, that hey, up until this 10, or, in case you are using Q, in either case, you would know that you have completed your batch copy till that point.
right, you know that because you will keep track, hey, from this point, I am anyway capturing all the live events that were happening.
so you cop it till that, state you, you apply your for Loop until that particular bin log coordinate is hit.
when you do that, which means that now your batch copy is done, which means that by enter time, when the batch copy started from your first DB, all of the data is present in your second DB.
right now, what Remains, the things that remain is that you have to copy the new things, like from the time that you started a batch copy.
whatever new rights have happened onto the DB, you have to take and apply it over there.
but here a critical thing- that you don't just have to apply all the updates, but only the updates that are for your shop, right?
right, so we start consuming the newer rides that are happening in the DB.
so here what happens is that, because the batch copy would be very fast, you are bulk copying the data of the shop that you are interested in another DB.
then you are, you have kept track of from this point, I did it right- and then you're starting from that new point and just filtering the events that you are interested in and applying the changes into another DB.
so this way there will be some lag, some lag between the new rights happening in the main DB and being- and that right being- updated in another DB.
you may call it replication lag, right, but in general just a lag that is happening with you then.
this way, your users would not see any perceivable downtime, but you would still have this small cutover phase where your DB is not accepting the rights because you are switching the database, right, so you just have to be aware of this fact that you might need to have retrace there, right?
right, once the difference was few seconds, one or two seconds, you stop the incoming traffic onto your database or onto your, onto your API site and you waited for everything to be going down, right.
so, once all the changes that you know- because you are not accepting any new new rights- once you have consumed everything, you would know that, hey, everything is caught up.
at this stage, there are no new rights coming onto the source DB and your Source DB is equal to Target DB because the data on both the databases is exactly the same.
there are no new rights coming in.
this is the time when you make that switch, where you enter the third phase and you update the routing table.
this is that that nginx layer in which you have defined, or you have defined, a custom routing module which says that, hey, for this shop, you go to pod one, but now it changes to go to pod 2..
so when this happens what you would have- and here we are just talking about database side, because API server speed up is pretty trivial, because it's entirely stateless.
hey, shop one that used to go to pod one now goes to pod 2..
once that change is done in the routing table, you start accepting the request and all of the requests for shop one that used to go to pod one will now go to part two without having any downtime, because the database as is basically caught up, right.
you apply all the updates coming in from the bin log.
once all of this is done, you start accepting the rights because you can't wait.
you can't wait, I'll validate and then I'll start accepting the rights, because the problem is that you cannot have a longer downtime, because it should not be perceivable by your- by your- customer.
right, but this is how, overall, the flow happens.
or, and this is how you actually solve the hot node problem, or hot partition problem, when you would want to move data from one node to another without having any downtime, right?
just few, just few cosmetic changes here and there, right, and yeah, that is our and that is exactly how Shopify does it.
thanks, [Music].