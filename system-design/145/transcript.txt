charting is super important to scale specially when one database server is unable to handle all the incoming load there are two types of sharing horizontal sharing and vertical shading horizontal sharding is when we split the table by rows and keep them on separate servers in vertical sharding we distribute the table across multiple database servers for example all the payment related tables goes in one database server all authenticated related table goes in another database server one key thing to note here we are not splitting the table we are distributing the table across multiple servers so vertical sharding comes in super handy when we are moving from monolith to micro services where for example you are you are splitting out your you are splitting out your payment code base from your model it and that's where you would want to give it you would want to give the payments microservice its own database right so this is where uh this type of vertical sharding comes in very handy all this sounds simple yet awesome theoretically but how would we actually implement this in this video we take an in-depth look not on the theoretical side of vertical sharding but on the implementation side of it we'll see exactly how vertical sharding is implemented that too with minimal doubting and what are the exact steps to do it but before we move forward i'd like to talk to you about a course on system design that i have been running for over a year now the course is a cohort based course which means i won't be rambling a solution and it will not be a monologue instead a small focused group of 50 60 engineers every cohort will be brainstorming systems and designing it together this way we build a solid system and learn from each other's experiences the course to date is enrolled by 600 plus engineers spanning nine cohorts and ten countries engineers from companies like google microsoft github slack facebook tesla yelp flipkart dream 11 and many many many more have taken this course and have some wonderful things to say the coolest part about the course is the depth we go into and the breath we cover we cover topics ranging from real-time text communication for slack to designing our own toy load balancer to greek buses live text commentary to doing impressions counting at scale for any advertisement business in all we would cover roughly 28 questions and the detailed curriculum uh split week by week can be found on the course page which is linked in the description down below so if you're looking to learn system design from the first principles you will love this course i have two offerings for you the first one is the live cohort discourse which you see on the left side and the second one is the recorded course which you can see on the right side the live cover based course happens every two months and it will go on for eight weeks while the recorded course contains the recordings from one of the past cohorts as is if you are in a hurry and want to binge learn system design i would highly recommend you going for the recorded one otherwise the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings the decision is totally up to you the course details prerequisites testimonials can be found on the course page at binary dot me slash master class and i would highly recommend you to check that out i put the link of uh the course in the description down below so if you're interested to learn system design go for it check out the link in the description down below and i hope to see you in my next cohort thanks so what is sharding sharding is when you split your database across multiple database servers right so where you have one database and now what you are doing is you are splitting your database across three shards these charts are actual physical database servers so you have taken your entire data you have split it in some way across three separate machines to very well handle the incoming load so in vertical sharing what we are doing is we are distributing the table across multiple charts which means let's say you had one database which had portables t1 t2 t3 t4 now you are splitting this database into three smaller shards first shard is responsible of handling table t2 second chart is responsible for handling table t1 and t4 third chart is responsible for handling table t3 so this way with the focused set of tables you are keeping them together in one database classically speaking this is this comes it very handy when you're transitioning from monolith to microservices sorry and another reason for us to do vertical sharding is or to do this stable movement is to do to basically better manage the load for example if you kept a table t2 in this or rather uh if you have one database which is let's say chart2 which handles table t1 and t4 and there is a lot of incoming load coming in for table t1 so what and which made your database server 2 or your shar2 very hot which means it is not able to perform at its full capacity because it is constantly under pressure so to better manage the load what you would want is you would want to physically take one of the table let's say you took the table t1 and put it into another chart which is not so under pressure let's say you took the table t1 move from chart2 to sha 3 to reduce the load or on chart two right so this physical movement is made very simple when you have scripts handy right so obviously this is not just theoretical we actually implement this so what we'll do is in this video we'll talk about how to you know how to actually implement uh vertical sharing and it's not difficult it's really not difficult but how do we actually implement it so the core problem statement here is you have one database which has multiple tables what you want to do is you would want to spin up another database and move one of the table from this database to another database you can extend this to multiple tables but the idea still remains the same so the problem statement we operate on is move one table from one database server to another database server how do we do that right so before we directly jump into the moving part of it what we definitely want to do is we would want to store meta information like for example what you'll have is given that now after you do vertical sharding you'll have multiple databases handling multiple tables so now when your api server when you get at the end you'll get some user request coming on to your api server how would your api server know to which database server should it connect to for example if your request is coming for authentication you need to talk to authentication database servers which has those authentication related tables where where would we store this information and this information should be very it should be stored in a very secure way not just from a security perspective but more from the consistency perspective where every single one of your api servers should get a consistent view of it right so that is requirement number one so where do we store this meta configuration that hey these tables are stored in these databases so that my api server will have this information from this source or this configuration that hey if i want to query on authentication table let's say authentication users table which database should i correct it will get from this meta configuration and then it will connect to that particular database server to read the particular information and send it back to the user so we need to store this information somewhere and this has to be a consistent view across all api servers so it cannot be still second now that we would be moving the table from one machine or from one database server to another database server this configuration will have to be changed so in the mapping that we stored that this table is present in this database after we move the table this configuration needs to be changed that hey now this table is present in another database for example table t2 which was present on database 1 is now moving to database 2. when this configuration needs or this configuration will be changed and once it is changed what we would want is we would want this information to be reactively sent to all api servers which means that your your otherwise what would happen is you have changed the configuration which means your table has moved from one database to another database but what if your api server doesn't know about it if api server doesn't know about this movement it would continue to connect to the old database server which is wrong so that is where we would want this information to be reactively sent to api servers so that's why we would want a tool that solves these two problems for us really well first of all storing a meta information and being reactive in sending out updates on what altered so that is where the one tool that fits really beautifully well over here is zookeeper zookeeper stores this exact same or zookeeper is meant to solve this exact same problem so what zookeeper would do is it would hold the meta information meta information like table t1 is present on database 1 table t2 is present on database 2 right something around it would hold this information and if anyone changes the configuration on zookeeper zookeeper will proactively send updates to all the three api servers so zookeeper has something called as watch so you can watch a particular configuration and if that configuration changes the watch function on your api servers will be triggered and in that they can fetch and query and get the latest configuration and immediately that gets updated so this way instead of your api servers proactively polling hey is there any configuration change or not we are making these things reactive so the idea here would be whenever an api request comes in it comes to our api server api server will have the copy of configuration from the zookeeper it would be kept in sync api server will know which database server to talk to it will talk to that particular database server compute the result and send it back to the user any configuration change happened in zookeeper it would be proactively sent to all the three api servers so zookeeper is not just acting as a configuration meta store but it is also acting as someone who can proactively broadcast or proactively send updates to every api server which is corrected so this way whenever we alter any table from one to another we update the configuration the information will be proactively sent to api servers which can now then connect now can start connecting to a more relevant database right so this is where zookeeper comes in now let's talk about how to actually move tables from one mysql server to here i am taking example of mysql server but the steps remains the same no matter which database you pick so now what we have to do let's say we have tables t1 and t2 both of them are present on database 1 so db1 and what we want to do is move table t2 from db1 to db2 so what are the exact steps of us doing that step number one we dump the table first of all whenever we are moving table from one to another what we would want to do is we would want to dump this table locally and then upload this table into some another database so that is exact step number one so if you are using mysql you can leverage a utility called mysql dump it's a it's a official utility that is provided by mysql with which you can literally dump the table into various format like sql csv various format most of the people prefer sql format it dumps the table into a simple sql file locally and what you can do is this file would also contain the position of the bin log so what is bin log bin log is your basically commit log where every single operation that happened onto your database gets logged into this bin log file and it also keeps checking hey in one transaction uh when when you are invoking my sequel dump in one shot it would take the entire dump of the table and mark that hey in this bin log i've read till this position so it would take all of this information and dump it into a sql file now why this or why this is important so what happens is bin log is the one where you have all the uh all the operations that ever happened on your database logged sequentially and when you have this information there you need to know that hey in my dump i have information till this point so that when you create a new database it can start reading after that position so bin lock position is extremely important to be logged and which is where mysql dump as a utility comes in very handy so you so you basically took the dump of your database into a sql file and which also contains the bin lock position it contains both of this information now step number two you took your dump dot sql file and load it into your db2 so your db2 was the one where you had to move your table so you dump that table so in the first step we dump the entire table along with the bin lock position on your local machine supposedly on your local machine and after taking this dump from and and now you're taking this dump and applying it on to db2 so when you restore this dump onto another database this is not entire database getting copied it's just that one table which is getting copied or one table which is getting created and the row getting inserted on that right so now after this step you have db2 with that one table with that entire data up till the time when you took the snapshot when you dumped it till that point it has everything but obviously what you would also have is your main database is still accepting right it is not down your main database is still accepting the rights so the new entries would keep on getting added in bin log new updates would be made onto the database so now what we do once we restore the table onto another database let's say db2 what we have to do is we have to start a replication so that's where you can write a simple replication job there are a lot of utilities out there that does this where what you'll do is you'll read the updates from db1 for that particular table and apply them onto db2 this way your db2 had the old data till a particular time from the snapshot that you took and then all the recent dinners that have happened on the db1 while you were doing this will also get replicated to db2 this is an explicit job that you will write you in most cases you will not be able to use the mysql's default replication because there are some extra steps or extra checks that you might want to apply filter out some things and then apply so you might not be able to use a default one but that's where a small replication there are tons of utilities out there in every language that you can think of so you can use that which would basically read the bin log from the position and that is why the position is important let's say we snapshotted a bin log at 156th position which means up till up till bite offset 156 i have consumed everything or it or up till bite offset 156 everything is part of my dump dot sql file so now when i start the replication i'll start reading from line 156 right so post 156 offset i'll start consuming all the updates that happened this way all the incoming rights that are happening on db1 will then start replicating onto dp2 this way eventually my the two the two databases db1 and db2 for that one table will come in sync right will be very small difference between them the the replication would be constantly happening right so this way we have ensured that on our db2 we took the snapshot of table 1 loaded it and now replication has begun so that now db2 the the table t2 and db2 the table t2 on db1 they are both in sync so once we know that they are almost in sync this is where you have to do a failover or not really failure but a cut over where you have to stop the traffic on db1 and move the traffic to dp2 so how do we do that so what we do here is we know that if the replication lag between the two databases for that table is very low let's say it's literal literally 0.00001 something like that you'll see that number in the statistics uh if this is very low what you'll do is you'll do a cut over when you do a cutover what would you do you would want to stop the rights going for the table on db1 and you would have to start taking rights on db2 for the table bank reads and writes both right so now what you will do is on table on db1 you will rename the table as soon as you rename the table any incoming queries that is coming in for that table it would give you table not found error for example you had table t2 on db1 and you changed it to t2 underscore bak well basically to underscore backup if you change it to that the incoming queries that are coming for the table for t2 the table does not exist and rename operation is atomic it's very fast so that's where for that duration any incoming request for table t2 on db1 will fail because it would give a table not found error then basically that is power that is exactly what we want we want to stop the incoming rights onto this database onto this database for this table and then what we do the next step is you update the entry zookeeper remember zookeeper for all the configuration so in zookeeper what we'll say is we'll specify that hey now table t2 is present on db2 earlier the entry was table t2 arrow dv1 now the entry would be table t2 arrow db2 so when these two steps are to be done manually right so someone some developer something a script is executed manually or something because you have to monitor what's the replication like and is this the correct time or not this cannot be done easily automatically so that is where someone has to do it manually so when that changes you you update the entry in zookeeper for that duration you will get table not found errors but as soon as you update the entry that now table t2 is present on db2 zookeeper as i said reactively sends information to all api servers using watch as it sends information to api server api server then any new incoming request coming in for table two for table t2 because you updated in zookeeper zookeeper proactively send that information to api server api server will now start connecting to db2 right so for that small duration for the small time window when you were renaming the table updating the zookeeper and the updates reactively going to the api server for that duration you will get table not found errors otherwise everything would work just fine just fine so the overall flow or the not really an architecture but overall flow to do vertical partitioning would look something like this you will have your users who is constantly sending you reads and write requests there is a read and write request going through load balancer hitting one of the request servers api server let's say an authentication request so api server would know where authentication table lies in how would it know using zookeeper so zookeeper what it would do is zookeeper will have all the meta information with it so table 1 table 2 present in database 1 for example all the tables are present in database 1 and then api server would know which database to connect to it will connect to that database fire the query get the response and read back to the user that's the happy flow now you would want to move that table from one database to another database let's say table t2 from db1 to db2 so that is where the partitioning service coming this is normal script that you wrote like just naming it funky but it's just a normal script that you have written that does all of this part and an engineer of yours is doing it right nothing fancy like you yourself are doing it right so just naming it something just to give itself an identity nothing more so the partitioning service would kick in and the what it would do is the exact four steps that we talked about so this is not not at all theoretical actually practical this is exactly how it is implemented in industry right so engineer then fires this thing uh moves the table uh like doing that mysql dump restoring it in another place starting the replication where the two where the two databases that one table across two databases getting replicated when the replication lag is zero you do a cut over as soon as you do a cut over you fire the update on to zookeeper saying that now table t2 is present on db2 as soon as the uh the configuration is updated on zookeeper the update is sent to all the api servers the newer requests that come in will go to that corresponding database so newer request for t2 will go to db2 instead of db1 and this is how vertical partitioning is actually implemented so here if you would have seen because we have a small time window where you will get table not found errors right because when you are updating the zookeeper or when you are renaming the table in db1 updating the zookeeper and this update is getting sent to episode for that small time we do hardly 5 millisecond 10 5 millisecond 10 millisecond at max 15 millisecond not more than that for that duration you will get table not found error so your rights will be affected so what does this mean this means that we are preferring consistency over availability we have given up on availability for some time but we have made our systems consistent which means that we any we'd not be like that some right that are there during the downtime that are gone but all the rides that happened will eventually propagate with replication like will almost zero it will immediately catch up and you'll have a very consistent view of data no matter what the approach that we discussed works for small to medium size even to some extent large size of table but huge tables will be problematic with this approach so that's why with huge tables you typically don't migrate huge tables you take smaller tables and put it into different shards keeping huge tables intact in one database the reason why that would be a challenge is because it will take because if a table is huge it means it is taking large amount of rights if it is taking large amount of rights your db1 and bb2 for that huge table might never come in sick or the replication lag would always be there because it is taking that many or that much of rights so whenever you are doing whenever you are moving one table from one database to another database you would typically want to move it or we would want to typically move small or medium size table or instead of moving a huge table from one database to another right so this is just one minor caveat that we have to remember whenever we are implementing vertical starting and again everything that we discuss is not at all theoretical it's exactly how we implement it you can very easily mimic this entire thing locally and i would highly encourage you to do it just run a small instance just run a small process of zookeeper have two mysql server running one on three three zero six one one three three zero seven write it script take the dump note the bin lock position write a replicator it's very simple but you'll get a very solid understanding on how it is actually it is very fascinating on the kind of tactics companies has to apply or products has to apply in order to ensure that we handle the skill that we want and business survives right these are few few of those gems and i love this approach where you see the micro decisions we took the exact granular steps that we went into so as to implement vertical shading so nice that's it that's it for this one i hope you learned like you understood that vertical partitioning is not just theoretical it actually happens in your world i showed you in practical example the exact way it is actually implemented in real world so i would highly again i cannot stress this more go and implement this locally it is not at all difficult you will learn much more than just watching this video right so nice that's it that's it that's it folks for this one i hope you found this interesting amusing and you learned something new if you did give this video a massive like so again if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sub i post three in depth engineering videos every week like this one and i'll see you in the next one thanks