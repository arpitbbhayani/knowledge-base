so. elasticsearch is a great search engine, but Yelp was not happy with its performance and hence they built their own HTTP player on top of Lucin. in this video, we take an in-depth look into the architecture, performance improvements and key design decisions that Yelp took while reconceptualizing their own search engine. but before we move forward, I'd like to talk to you about a course on system design that I've been running for over a year and a half now. the course is a cohort based course, which means I won't be rambling a solution and it will not be a monologue at all. instead, a small, focused group of 50 to 60 Engineers will be brainstorming the systems and designing it together. this way, we build a very solid system and learn from each other's experiences. the course is enrolled by 800 plus engineer, spanning 12 codes and 12 countries. ingenious from companies like Google, Microsoft, GitHub, slack, Facebook, Tesla, Yelp, Flipkart, dream11 and many, many, many more have taken this course and have some wonderful things to say. the course is focused on Building Systems the way they are built in their real world. they will be focusing heavily on building the right intuition so that you are ready to build any and every system out there. we will be discussing the trade-offs of every single decision we make, just like how you do in your team. we cover topics ranging from Real Time text communication for slack to designing our own toilet balance, live text commentary to doing impressions counting at scale. in all, we would be covering roughly 28 systems, and the detailed curriculum, split week by week, can be found in the course page linked in the description down below. so if you are looking to learn system design from the first principles, you will love this course. I have two offerings for you. the first one is the live cohort based course and the second one is the recorded offering. the Live code base course happens once every two months and will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to learn and want to binge learn system design, I would recommend going you for the recorded one. otherwise, the Live code is where you can participate and discuss the systems and its design life with me and the entire cohort. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page: arpitbanime masterclass. I repeat, arpitbani dot me slash masterclass, and I would highly recommend you to check that out. I've also put the link of this course page in the description down below and I'm looking forward to see you in my next cohort. help is a website where businesses list themselves, people engage with them for their services and post reviews about them. now, to make Discovery better, you help users. Yelp used to use elasticsearch, but they soon built their own. why, why was the time like? why did it happen that Yelp had to replace elasticsearch? now, the first reason that they provided is that elasticsearch does document based replication. now what does this mean? this means that let's say you have your search service, which is powered by elasticsearch, and on elasticsearch cluster you have three nodes: right and inelastic search. when you configure an index, you have to specify the number of replicas of it. so now what happens? let's say you index a document, the right goes to the primary replica or the primary node. right from there, the replication kicks off and that same operation is applied on the replica node right. so now let's say, if you index a document in one, let's say this is the primary node. when you index a document there, now the same operation would be done on replica, which means that when you are writing something, when you are indexing a document, the right operation not only happens on the master but also on the replica. now, this means that because the same index operation is going to happen there on the replica, which means that if I would want to scale up, I would also need to scale the CPU power of it, because obviously, indexing a document requires you to process the text, split it, put it into inverted index and whatnot. right, there is a lot of work that needs to be done. so with what does this mean? this means that when you are scaling out, it just does not mean that add more nodes with cheap CPU or with with low rank CPU here. also, just because you want to support right high, right load, your replica also needs to have enough CPU capacity. now, this means that scaling out is not cheap. it comes with its own cost, right? that is reason number one. reason number two is that there is an uneven load distribution. now, here we see, there are multiple data nodes on which the data resides. so these are physical servers on which the data decide. but what could happen is, let's say, when you're creating an index in elasticsearch, you specify the number of shards of that Index. this is independent of the number of nodes that you have in the elastic circuit. so I only have an index having five shards, but split across only three nodes, which means that there is a chance of having uneven distribution. let's say there is a logical shard in elastic search which is very heavily queried, which means- and let's say a couple of them, they all are put into this one elastic search node that you have. so which means that there are chances of uneven load distribution and some Shard, some physical data node would become more hot. it would become hotter than others because of this uneven load distribution. so when such things happen, this is typically what uh, uh, your uh, which is typically what Engineers do when there is an uneven distribution, they use the elasticsearch head plugin and then they do re-allotment of shards across physical data nodes and the data gets copied and the load gets distributed there, right. so this is typically manual. rebalancing is required in that case. third reason is auto scaling is challenging. you would have heard that if with elasticsearch- most probably when people is elastic search- they always always, always provision for Peak. I've been in situation multiple times. so we always over provision our elasticsearch cluster and we are always provisioned for the peak, even though in most cases we don't see that much offload right. so that is where the problem is why. because Auto scaling is not easy. with elastic search that becomes a problem because spinning up elasticsearch node where it gets data copied and not what not- it takes a lot of, lot of lot of time to do that, plus rebalancing, remigration of data. a lot of things happen behind the scenes, which is why it makes Auto scaling really complicated when it comes to elastic search. so these were the three critical reasons why Yelp thought of: hey, we can't use elasticsearch, it's not performant, let's build our own search engine. so obviously, what they did is they wrote their own layer instead of elastic. so now first we need to understand what elasticsearch actually is. search is actually a search engine built on top of leucine, so Lucin is the one that powers the actual inverted index and ranking and whatnot. so Lucin is the actual search engine. elasticsearch gave a very nice user-friendly HTTP Json based endpoints on top of that. so that is what elastic surgery so, which means every node of your elasticsearch cluster internally is actually running leucine. to do this solution is not a separate server server, but it's a library that does this: file management, inverted index management, ranking and whatnot. so which means that you can keep losing as is and rewrite the HTTP layer which elasticsearch provides. so what elasticsearch does is it is makes using leucine simple by exposing HTTP endpoints and distributed, because cluster Management on Lucin does not support that using is just single node search engine, right, elasticsearch makes it distributed. plus, it had other features like replication, sharding, uh learning to rank, custom Fields, analytics and whatnot. right, but most of the speech, most of these features, Yelp did not really require. obviously some of them DS, but not all of them. so what Yelp is doing is help, uh, re-architecture this part. they kept losing as is and they replace elastics or, like the HTTP layer that elasticsearch provided, they use Lucin and they build their own HTT player on top of Lucy, which is what they are calling nrt search. so what they wanted to leverage? they wanted to leverage two key features of leucine. first is the near real-time segment: replication. so let's go a deeper into Lucina, what it actually does. so leucine has something called as segments. each segment. now segment is where the actual data resides. so, for example, if you have indexed 10 documents, the 10 documents are stored in a segment and these segments are immutable, which means that once a segment is created, it is created, it would not alter, right, so that segment is immutable. so what happens is, let's say, you have a Lucy node and you have a primary and a replica in Lucin, if you want to do that. so because segments are immutable, whenever a write happens, whenever you update a segment or, sorry, whenever you create a new segment, a replica node can literally just pull this segment out because it is immutable. nothing would change there. it can literally just pull this segment out in one shot and- sorry, pulling it would copy the segment from the primary node into the replica node and then replicant can start serving its request for that particular segment. so this makes scaling very fast and efficient. so, which means that once the segment is created in the primary, you, like the replica node, can just make an API call or just access it, because it's a physical segment on the disk, and just copy it and basically copy it to its disk, making it much, much faster to do this. so there is no need of explicit re-indexing of the data like an elasticsearch. we had to re-index on every replica here. there is no need of doing that. there is no need of reindexing, because the data is already indexed and stored in segment in the form that is, that is directly consumable. so that with this, what implies that if I want my replica to start serving the request, it does not have to do re-indexing of the document. instead it's just have to copy the segment from its primary node into the replica node. much, much, much efficient. this is feature that Lucin, provided elasticsearch is not using right. so, which is why Yelp thought key lets like we need this feature, we'll write our own layer. second reason is concurrent search. obviously elasticsearch is also concurrent, but because the search happens on segment, because segment is the entity where your inverted index are stored in leucine, if you have three segments, I can make three parallel queries on three individual segments and then get my queried answer, or either get my search queries answer right. but this is not possible with elasticsear, because in elasticsearch you have a replica in which all the data is there, because it has abstracted those things out for Simplicity. so here to, in order to leverage multiple cores, you can actually fire parallel queries over segments in the same node, making it much, much, much, much more efficient. so these are the two features that uh, Yelp wanted to leverage of leucine, which elasticsearch abstracted right. obviously they did it for the betterment, for the ease of use, but Yelp wanted that near real-time performance out of it. so now let's see how Yelp implemented it. this is where, just like how elasticsearch is an HTTP layer on top of leucine. what they used is: Yelp used a project by some guy named Mike. it's called lucene server project. now this Lucian server project there is, like I'll link the I link the blog in the description below. you can check out this source code. it's not that difficult to understand. you can skim through it quickly. so Lucian server project is the HTTP layer on top of leucine. now this leucine server project, just like how elasticsearch provided HD pillar on top of losing enabled elasticsearch solution server project is the HTTP server running on top of leucine. so you would have a leucine index there and it just provides an HTTP interface, like basically making it simpler for you to talk to it, right? so the key implementation details: first, using grpc. so what? Lucin server does not support grpc, it's normal rest based thing. but what Yelp did is they thought, hey, we should not be using rest and Json because it's not that efficient. so they rewrote their interface layer and they used grpc and protobuf. grpc protobuf makes it really simple, really fast, really efficient, because it saves a lot of time to do serialization and deserialization of the data. so, which is where what they did is they replace the HTTP Json layer and they replaced it with grpc and so that when your primary and replica they have to sync the segments or they have to like replicas to pull the segment from the primary. this all communication happens over grpc, right, but still, obviously there would be some cases within Yelp where they would have to support rest and Json, because all the clients cannot just change all of a sudden to grpc, right. so, which is where they use grpc Gateway, which is a very famous library, to create rest endpoints. it is Json based endpoints on top of grpc, so it creates that code that you can directly run as a server and it basically masks all the grpc protobuf details out of it. right, so you can still have your backend running grpc, but while if you'd want to consume over rest, slash Json, you can do that with grpc Gateway and this generates a code which runs a server. grpc Gateway is not an entity, it's a library that does that, and whoever can directly talk grpc can directly talk to this lucene server because they have the grpc layer there. they can directly talk to it and get their- basically- queries answered right, so that is great. so this is what their key design decision was. next is about failovers, because obviously things that anything that could go down would go down and you have to be prepared for that. so now, one key challenge that comes with uh, architecting or just playing with raw Lucine, is time. it takes, like whenever node fails, for you to spin up a new node. it takes a lot of time and typically the time is taken because when you learn, when you start a new node and you would want to set up the leucine on that or download those segments and keep it handy, what you would typically need to do is, let's say, whenever a write comes to primary, you write it to the disk. once a segment is created, you periodically back it up to S3 or your blob storage and then, when a new node boots up, that would download it from the S3 right and then, once the download is complete, it would start serving the request. this is the simplest approach that people do, which is what actually takes a long time for you to do. failovers, because when a new node is spin up, it has to do a lot of network. i o to get those entire segments downloaded and then start serving. so there has to be a better way, which is where what they did is something really simple, really simple, that we all use day in and day out. so Yelp is built on top of AWS. so what they use is they use EBS volume. so if you have used AWS- in case you haven't just checked the documentation, it's brilliant one on EBS. so EBS is elastic block storage which literally plugs into any C2 instance that you want. so what they did? when a node, uh, is running, let's say I started my primary node, when I'm writing, I'm not writing to the local disk of that server. instead, I'm writing to EBS volume. now EBS volume is detachable, and attachable to any other ec2 instance if you would want to. so, which means think of it like your hard disk. you just plug out your hard disk and put it into some other machine. it will just start working fine. so which is what they did? so their nrt search. so they named their search engine as nrt search, which is near real time search, and while writing it there, uh, they don't write to the local disk of that server, they write it to the EBS volume attached to it. so, which means if my primary node goes down, they just spin up another ec2 machine, which takes like 10 seconds, and then they just attach the same EBS volume there. so everything would just run immediately, like 12, 30 seconds, and your server is up and running to serve it. it would not require you minutes to download the file and serve those things. so this is a very interesting approach that we all use day in, day in and day out, but they leverage it so beautifully to make their failovers very, very, very, very quick. right, so this made a recovery faster. the it basically made them, uh, their Boot and normal server boot time really really, really fast so that they can really start serving requests as soon as almost the server starts. in just couple of seconds they are done with that, right, okay, so now the next challenge that comes in is around primary replica sync. so now here, what happens is when you are writing something to the primary, when the index is created, now this needs to be notified to replica so that it can pull the data right. similar to how it happens with elastic and elasticsearch, there is a duplication of operation on primary and replica with leucine. that's their key feature, right? so when you do the right, the right happens on and a new segment is created. once the segment is created, segments are immutable, so replica can just copy the segment onto the server and start serving it, right? so here what happens is, whenever the write happens, whenever a new segment is created on the primary, this new segment, the primary, notifies all the replica about it. that, hey, I am the primary, this is the new segment that is created. replica would know about it, and then replica can pull the data from the primary. that entire segment will be copied onto a replica. this way, the the replication becomes, or the lag becomes, really very less. once the segment is created, it's literally copied to replica. replica can start serving the request instead of doing re-indexing on two different machines, right? so this is really brilliant of the strategy, where they are actually using the core feature of losing that they always wanted to use in order to reduce the overhead of CPU or reindexing multiple times the same document, right? apart from this, for that search, which is nrt search, they also implemented a bunch of features that elasticsearch provides so that they come because they use elastic search. there might be teams that would be using core elastic search features, which is where they added a bunch of features on top of their own on in their own implementation, which is nrt search, so that they come very power to elasticsearch, right, but they were not really very fancy and they are more focused on search versus analytics. so, because they were rewriting it, they could prioritize what to implement and what not to implement. okay, what next? performance Improvement? so the first one is called virtual charting. so we know that in leucine there are segments and search happens over segments, right, so the data is stored in segments, the inverted index are part of segments and search happens over segment. now, the worst thing that you can do is allocate one such thread for one segment. if you do this now, let's say there are hundreds of segments and to do concurrent search across all of them, if you look at one search thread for each segment, that would have you incur or create a lot of threads. now, obviously there is. so there has to be a cap to the number of threads that you do for concurrent searching, right? if you create a large number of threads, it would make our operating system run out of memory into a process crash, right, which is where what do you do? is they introduce a concept called virtual sharding. now what virtual sharding does? so, instead of having one search thread for every segment, what they do is they Club multiple of them and allocate them to one search thread. now, this way, a and the way the allocation happens is a pure greedy approach, which means that they sort the number of segments by the number of document it contains and allocates them such that there is a consistent work across all the search threads. this way, it allows you to leverage the power, the computation power that you have, the memory that you have, in the most efficient way. so this is the concept of virtual sharding, which means they are virtually sharding the segments into slices, and which is what they called. uh, the group of segment is basically slices and search thread is allocated to a slice, right, this is a way to ensure consistent work across right. second optimization is basically parallel fetching of document Fields. this is a classic one where you might have documents stored at one place. now, when you would want to fetch the document with the, the actual detail of the document, you may do it in parallel, which is what the second, optimization, is all about, and the third one is about segment level search timeout to maintain consistent SLA. so now, when you are searching within a segment, right, so when you are searching on a segment through which is which lies within a slice- now slice is just a virtual concept, right, it's just a virtual chart. so when you're searching on a segment, there has to be an SLA that you maintain. let's say, if it is taking you 5 Seconds- 10 seconds, worst case- to search within it, you can't just let your client wait for a very long time, which is where what you do over here is you have timeouts that you have applied on segment level searches. this way, you say that, hey, if you don't return, like if this thread does not respond in this n milliseconds, you would just go with an empty response from that segment. obviously you are. you are gathering data from multiple segments, but from that one segment, if it's not meeting an SLA, you would be going past it. right? this is typically done to ensure that you have consistent SLA on the client Side by compromising on the correctness less of your information. like you may not be searching across all the document because one of the segment took time. you are not just letting your client to wait for a long duration, you are introducing timeout over there, right still now let's talk about migration. so obviously, building is fine, but now they had to do seamless switch from their existing elastic search load to this nrt search that they built. so now what they did is they used to have, or they built their own proxy layer called Apollo proxy. this is the elasticsearch proxy that they built and they used. so, by default, all the requests came to Apollo, which is their elasticsearch proxy, and it went to elasticsearch Cluster that they had. but now what they were doing is they built nrt search. right, this nrd search is elasticsearch like thing. so what they would want to do is they configure this proxy such that they would migrate or they would move some traffic to an RT search, While most of the traffic to elastic sir, to just test the correctness of the system. so what they first did? they first did something called as a dark launch. now, what is dark lot? this is an important concept that you should know as Engineers. so dark launch is something where you, let's say, you have built a system which is equivalent of some system. so now what you would want to do is you would want to see if your system is giving out the same result as the old system. so you just don't launch it and make it available to the public, because they might see wrong results. you are not sure that it is the exact same result. that user would see no difference between them. so in that case what people do is they send request to both elasticsearch as well and nrd search as well. but five percent request goes to New. so 95 request, so all request. uh, sorry, 95 percent of request goes. sorry, my bad. 100 of requests goes to elasticsearch, but five percent of them goes to nrt search and they compare the output of both of them. so if they are same, which means that the so so that they can test the correctness of the result on the production environment, right, so they compare the result for correctness. if they are good, then they increase the proportion. then they increase from five percent to ten percent, ten percent to fifteen percent, twenty percent and so on and so forth. right, this is how they test the correctness of the system. so it does not mean user is seeing responses from nrt search, user is saying responses from elasticsearch. only once they were sure about the correct list. they started to do this again, this bifurcation, and now they started to move five percent traffic to nrt search and 95 traffic to elastic search. and now the traffic that was moved to nrt search, users are shown response from nrt search. and then slowly they increase the percentage of energy search and reduce the percentage of elastic search. and slowly they migrated 100 to NRC, 200 to nrt search. so it's like a feature knob that they turned. so first they did a dark launch to test the correctness once they were sure. then they did a rollout, a face rollout: five percent, ten percent, fifteen percent and then slowly 200 percent. right, and this is how they're beautifully architected and reconceptualized: search on top of Lucy, right, just like what elasticsearch does. they built kind of their own elastic surge, but specific for their needs, and this is what is brilliant about their design, right? all of this thing that I explained is taken from the engineering block which I have Linked In the description down below. highly, highly, highly recommend you to check that out and go through the details of it. it's fascinating. on what they did. how did it? brilliant, brilliant, brilliant, right. again, highly much rate for all of you, great. so yeah, that is it for this one. I hope you found it interesting. I hope you found it amazing. I'll see you in the next one. thanks, [Music].