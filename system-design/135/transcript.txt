serverless computing is one of the hottest topic of discussions today but the term serverless is slightly misleading and it definitely does not mean that there is no server executing your code it is not something that seamlessly fits in every use case out there and we have to be extremely cautious while making this decision of adopting it in this video we talk about what serverless computing is see why it was built in the first place learn about five real world use cases that become super efficient with serverless architecture understand the advantages and more importantly the disadvantages of adopting serverless and conclude with acknowledging when to use and when not to use this computation pattern but before we move forward i'd want to talk to you about a code based course or system design that i have been running since march 2021 right if you're looking to learn system design from the first principles this course is for you yeah because this is a cohort based course it will not just be me rambling a semi-optimized solution thinking it's the most amazing solution out there instead it will be a collaborative environment where every single person who is part of the cohort will can pitch in his or her ideas and we will evolve our system around that right every single problem statement comes with a brainstorming session where we all together brainstorm and evolve our system that's why everyone understands the kind of trade-offs we made while making that decision instead of just saying hey we'll use a particular queue we'll have the justification why we use only that queue why we use that particular database y sql why not nosql right how are we leveraging throughput how are we ensuring that our system skills that's the highlight of this course this course is taken by more than 500 engineers to date spanning nine countries and seven cohorts right people from all top companies have taken this course and the outline is very intriguing it's very exciting so we start with week one around we start with the core foundation of the course where we design online offline indicator then we try to design our own medium then we go into database where we go in depth of database logging and take and see few very amazing examples of data log or database logging in in action and how do we ensure that our system scales through that then the third week is all about going distributed where we design our load balancer i'll have oculus the actual code of a toy load balancer and understand how pcb connections are managed and how simple it is to build load balancer then week 4 is about all about social networks week 5 is all about building your own storage indents like we'll build that intuition on if you were to ever design your storage agent how would you do that right then week 6 is about building high throughput system 7 is about building uh ir systems basically information retrieval systems and adopt designs where we design our own message brokers like sqs where we design distributed task scheduler and we conclude the course with week 8 where we talk about the super clever algorithms that has powered or that has made those systems possible right i have also attached a video verbatim as is from my first code where we designed and scaled instagram notifications i will highly encourage you to check this video out right and now back to the video so in a traditional computation environment what you have is you typically have a server which has specific ram specific cpu specific hard disk that runs your code and when your user end user makes an api call the api hits your server server does that computation and then it returns the response right so whenever we whenever we provision a server let's say an ec2 machine or a docker container or basically something similar what we typically do is we allocate some ram some cpu to that machine hypothetically let's say we allocated a 4 gb ram and a and a dual core cpu to that machine and this configuration is enough for us to handle 1000 requests per second so what does this mean so when the server is running so when you spin up a server that is continuously running and when that server is continuously running which means that we have provisioned a server to handle a continuous load of 1000 requests per second right so which means that when like you are expecting a steady pattern like this but what if your pattern looks something like this it's very spiky it's it's very bursty so then what do you what does this mean is let's say this is that peak of 1000 requests per second but you have provisioned your server to be handling for uh to be handling 1000 requests per second continuously by giving it 4 gb ram and 2 core cpu but that peak is only happening for just one percent of your time what about other 99 percent of times which means that your server your your you have over provisioned your server by a massive margin just to handle that peak load of 1000 requests per second while for other use cases like for other times when you are not even seeing 10 requests or 5 requests or two requests per second why are you wasting 4 gb of ram on that machine this this particular optimization where where gave rise to serverless architecture so the idea of serverless architecture is what if what if we have an infrastructure as a service that scales up and down as per traffic is built per execution and is self-maintained and fault tolerant the idea here is pretty although these are a lot of buzzwords but let me just give you a very nice view of this so what happens so what if you deploy your code onto not your server on the serverless architecture onto some infrastructure and what that infrastructure does is as you get let's say you start with five requests per second then you get 100 requests per second then you get 200 so the infrastructure should automatically scale up be it horizontal scaling weight vertically we don't know infrastructure scales up or scales out depending on what their configuration is but it is on demand right so which means that when you have five requests 10 requests per second the infrastructure is pretty small when you get 100 requests per second it becomes big then it drops down so in particular again scales down so infrastructure is pretty elastic in that sense so like depending on the incoming traffic your infrastructure would automatically scale up and scale down and you don't have to worry about it and when you suddenly hit that pick your infrastructure would automatically scale up to handle the peak traffic of 1000 requests per second and then it would drop down accordingly so depending on your request pattern or a traffic pattern your infrastructure would automatically scale up and scale down and here the beauty is you'll say hey this is what our auto scaling group does but what the the difference that serverless execution makes is that it is built per execution so you don't have to worry about when to scale up when to scale down it is all abstracted by the vendor that you are using let's say using aws aws will abstract this for you you have just deployed your core business logic onto aws lambda which is their serverless offering and it would automatically scale up and scale down depending on the request it comes in you don't even know the rules that they have applied and what you get build is you get built per execution which means every time a request comes in your lambda function runs you are build for the time that which the lambda function runs which means that when you are seeing five requests per second hypothetically let's say it it cost you one dollar for every request that comes in here when uh you provisioned a machine to handle 1000 requests per second you are continuously paying for that much of amount although no matter if you get request or not but with serverless what happens is when you are getting five requests per second you are paying five dollars per second and getting 10 requests per second you're paying 10 dollars per second when getting 100 requests per second so the amount of time for which your your lambda function was executing you are billed for that which means when there is no traffic you are not build even a penny that is the best part so serverless architecture are typically focused at a better cost optimization which means that you are not you are not over spending on your infrastructure by creating a traditional big server instead when your traffic is bursty you can save a lot of money by adopting serverless pattern right this is the core id and the core notion behind serverless architecture so the key highlight here is your serverless function does not so one typical myth is that serverless implies that there is no server like like it basically servers does not mean that there is no server on which your code is running like obviously your code needs a server to run but it implies the word serverless implies that you are not managing or scaling the server so it's off your shoulders and you can slowly focus on writing your core business logic and basically nothing extra so depending what your traffic pattern is the serverless function or the co the server on which your serverless code is running that would automatically scale up scale down uh horizontally scale you don't really have to worry about that so that is elastic that is abstracted for you right so this is the basic idea of serverless on cost so it started with cost optimization but it had a few um other amazing advantages one of the biggest advantages that you get out of that is that now when you adopt serverless all you focus on is writing your core business logic you don't care about managing infrastructure os patching security patches and whatnot you don't really have to you or your sre team or anyone in your organization you don't really need to worry about it you have just dip you just written your simple flask server which host 10 apis that you wanted you put it into serverless environment and you forgot about it because it is running like if you get traffic you will get built if you don't get traffic you won't get built if you get large traffic the serverless would automatically scale up if you don't get any traffic it would be sitting idle or you will not be even charged up any right so because of which it is like when you are building or when your team is very lean and small serverless seems like a way to go so who who gives you this offering so all major cloud providers give you serverless offering in some or the other flavor but the core idea behind serverless still remains the same on-demand pricing right and elastic infrastructure so aws gives you aws laminar functions gcp gives you google cloud functions cloudflare gives you cloudflare workers azure gives you azure uh azure serverless functions right so depending on which cloud you are using you can pick any of this up or like you can pick the corresponding uh serverless offering of this and see how it uh and and actually adopt it for your use case right now let's talk about where where to use or where to use serverless like where to use service architecture is there any genuinely real world applications for serverless architecture there are some amazing benefits we'll talk about five of them let's start with the first one let's say you are building a chatbot i'm i'm just taking a very common very simple use case just to make you understand on uh the advantages of using serverless over here so let's say we're building a chat bot and that chatbot integrates with slack on slack whenever someone types in slash holidays so you implemented a slash command on slack and whenever types in someone whenever someone types in slash holidays what you get is you have to fetch the holidays from your database and you want to render it onto slack so that your employee sees hey these are the holidays so he or she can plan uh their holidays or their vacations or their trips according to that so now what would happen is like do you really expect that this kind of use case would get constant traffic no so does this mean that do you really need to keep a server let's say 1gb big server continuously running for this use case i don't think so that would be a lot of wastage of money because you're constantly running a server for something that gets traction once every week for example once every week every employee that would be the max that it would get right so why would you want to provision a fully fledged ec2 machine to run this or to support this so best use case for very infrequent uh traffic like uh infrequent traffic pattern like this is serverless where what you do is you configure a serverless you you deploy your business logic on serverless on slack you can configure a hook which says that whenever this command fires in trigger this api and that api would be hosted on serverless and when that would happen your serverless function would kick in as soon as you get this and as soon as a serverless function kicks in it would run that execution it would then talk to the database fetch the holidays and render it over here so you don't really need to have a server constantly running for this obviously this is an over simplification of the architecture but you get digest where for an infrequent access or infrequent traffic pattern like this you don't need a constantly running machine serverless would do the job if you get if this particular holidays command is fired once a week once a week every employee only that much of time you will be billed you don't really have to manage the server scale it and what not it is all done automatically by the serverless environment the second real world use case is most of you would love this use case is say you're building an online judge for your uh for your competitive programming like your dsa practice like for example lead codes pods and whatnot that perfectly fits in the serverless use case so whenever you would submit or whenever a user would submit a code for automatic evaluation what you can actually do is you can trigger a serverless function with the code submission and the test cases that you have and the serverless would execute the code because it is stateless there is no server running every time the serverless function executes it's a brand new execution as soon as it's a brand new execution when you execute your code on that serverless machine you don't really worry about anything because then once the execution is complete the serverless function is destroyed or the or the the underlying infrastructure is destroyed or it's basically stateless won't go into internals of it but it's basically stateless so upon every submission you can trigger a serverless function which then can update the status of each test case in the database and you might have a traditional server api service on which the the your user is conditionally refreshing or seeing the status of how many test case pass and fit you you may have that normal status sec uh normal status check apis hosted on raw ec2 instances but for the core execution core execution or core evaluation of the user's submission can be very well modeled on serverless architecture so now with this in place you don't really care how many how many submissions have happened in a minute because your server list will automatically scale out and handle the traffic that it is getting and when there is no submission let's say midnight 3 a.m only one person is using your platform to do that you'll only be built for that one person you are not constantly provisioning a big server to handle that load that is the beauty of the serverless architecture online judge is one of the best use cases where or is one of the most suitable use cases for serverless architecture again the pattern if you see over here the traffic is bursty scale is unpredictable y2 and plus here the execution of your function would be time bound right which means that within one minute all the test case should run otherwise you will get a time limit exceeded very efficient on a server less rather than keeping your massive servers running the third use case is vending machines let's say you are let's say you are coca-cola and you have and you have bunch of vending machines deployed across many many vending machines deployed across the globe and what you want to do is whenever a purchase is made through a vending machine you want to mark it on your database that hey from this vending machine this purchase was made so that you can manage your inventory so now what you can do over here is on the back end side you can have a serverless function that anytime the sale hap or any time vending machine pay any purchase happens the vending machine can make an api call to your exposed serverless function and that service function would update the database with the inventory that from this vending machine this item was uh was purchased which means that you would know when to stock this vending machine right so this is very well modeled in serverless because it becomes extremely cost efficient here if you see what is the typical traffic pattern are you expecting a continuous traffic from a vending machine no twice a day thrice a day 10 times a day that's it right so for that why do you want to keep a massive server running and even if you multiply it across the globe you're not getting enough traffic for you to justify a big server running again a very perfect use case for your serverless architecture the fourth one is cron jobs so what typically happens is let's say you are on aws and you are using the managed rds or manage database offering which is rds and what you want to do is you want to trigger a database backup let's say every day at 4pm and when you do that obviously rds exposes some endpoints through which you can trigger a backup but you still need someone to trigger that api that triggers the backup how do you schedule it obviously rds does give you that scheduling feature but hypothetically let's say it does not and you want to build this crown for that where would you deploy this cron how would you like you will you have a special server running on which only cron tabs are running very inefficient very costly this is very well modeled on serverless because on every serverless offering out there you can schedule execution so you can create a serverless function you can specify that execute the service function every day at 4pm within that write your code that triggers rds api to trigger a backup so now you don't need a separate server to host your cron tabs it is automatically hosted centrally on cloud you don't have to manage this see if this server goes down spin up another one and whatnot you are remove like you are devoid of all the complications that come with managing servers because now you like your cloud vendor would guarantee that the server that your serverless which is scheduled to be executed at a particular time will execute at that particular time so it saves you bunch of your servers and obviously like always because this serverless function is executing just once every day you'll be billed only for that small execution which is let's say 100 millisecond of your api call hypothetically 100 milliseconds you will only build 400 millisecond every day rather than a server constantly running for that solid use case for serverless fifth and final use case that i want to talk about here is batch and stream processing so typically what would happen is you might have an asynchronous flow where upon some api request you put a message into your message broker let's say sqs or rabbitmq or something and rather let's pick sqs for now you put a message into sqs you have written a bunch of consumers that have to do some action what you typically do is you typically have a bunch of servers who are constantly pulling your sqsq to fetch the message and do that processing right but here what would happen is what if there are no messages over here you would still need to have some servers running because that's what your scaling policies you are you're optimizing for peak although you might not go for like 1000 servers but at least two servers during non-p cars so even for that why do you want to have that server running and make that polling on sqs to fetch the messages and check the existence of it and then do the processing here what serverless can do is serverless can make it reactive which means on every cloud provider you can plug serverless to their messaging or to to their message queues for example aws lambda functions with aws sqs and what you can configure is whenever there is a message on sqs trigger this lambda function so you don't have to have servers which are constantly polling sqs and running your business logic instead you are making all of this reactive as soon as message comes up your aws itself will trigger lambda function and in the lambda function you'll get the squeeze message as an input once you've got that message you do your usual processing so batch and stream processing are like infrequent batch and stream processing are perfect use cases for serverless uh for for serverless architecture because you don't have to manage the servers and it is and it all seamlessly works like it just works right because and supposedly if you get a spike if you get a spike in the number of messages your serverless would automatically scale up you don't really have to worry about it that's the best part so it takes up a lot of infrastructure management part of your shoulders right okay enough about your use case let's talk about general advantages that you get out of serverless computing first of all as you would have seen the five example you don't need to manage or maintain your servers though as soon as you have a virtual server or ec2 machine or docker container what comes with this is you have to manage them by managing what does this mean you have to ensure that if this server goes down another server should take its place for example that that particular current time example that we talked about you cannot even have two servers running that crontab because then the same function would be executed twice right so that's another complication that you need to handle but you still need to guarantee that the code is definitely or the cron is definitely executed plus you need to have like access controls over your machines you need to have security patches like if once a while you will discover that your servers are having a massive security loophole which you would need to patch by not having an ec2 machine by not having a server by using serverless that entire headache is offloaded to your cloud provider let's say aws gcp why why would you even have to worry about this thing when you can just focus on writing your business logic right big advantage of adopting sorry and obviously the i will talk about disadvantages where you should not be using it but with respect to advantage this is a gigantic advantage that you get second is pay as you go pricing so most serverless functions they are built on the amount of time there execute although i said that per execution it is built it is more about the the amount of time or the time for which the function was executing it will be built exactly for that and that clock takes let's say every millisecond or every 100 millisecond so which means that if your function executed for 100 millisecond and let's say it cost you one cent so if your function executed for 200 milliseconds you'll be charged two cent for that a 5 millisecond uh 500 millisecond and 5 cent for that right so it typically depends on the amount so whenever a server uh whenever a serverless executes you can configure key this much of ram and this much of cpu is required for this execution and depending on what you have configured you will be built accordingly that according to that right so this space you go pricing is extremely cost efficient because if you having a very bursty traffic you don't need to constantly provision a server that is handled or basically that is provision for the peak very cost efficient third is no capacity planning with serverless because it automatically scales up scales out like horizontal scaling and vertical scaling is abstracted you don't even know what's happening behind the scene so you don't have to do capacity planning hey if i get 1000 requests per second and i need to have 10 servers if i get 3000 requests per second i need to have let's say 300 servers something like that you don't have to worry about it because your serverless is abstracting scaling for you so you and the server is the way it is designed it is designed to be extremely precise like you might you as human might not be able to configure your scaling policies well but serverless would be very quickly like obviously machine learning ai and whatnot you will put it all the buzzwords and it would automatically do that efficient scaling for you and here that's the best part because now you as an engineer you are offloaded from the ticket array i need to do this all right my upper selling policy is not good all right i have not auto updated auto scaling policy after my latest spike so you might be leaking money and whatnot so you you are all uh this burden is all taken care of right and most traditional architectures most traditional are getting like uh the server full way of computing they are they are mostly overwhelmed when you see a sudden surge of request because that machine is not able to handle and it takes time for you to scale up but serverless does this thing seamlessly because it is meant to do that now these are the three key advantages now let's talk about disadvantages these are more important advantages like when to not use serverless or rather overall disadvantages and then we'll talk about when not to use serverless inventory servers but overall four major disadvantages of serverless is first of all you need to understand that serverless obviously we talked about all the good stuff of serverless but this is not that silver bullet that you will use at every use case hey this is cost efficient okay this is good or it is great no that won't work serverless computing has major disadvantages that you need to be aware of the biggest one is cold start problem with cold start problem what happens is as i said when there is no traffic you are not built for that which means the underlying infrastructure for your serverless function might not even be up the container on which it is running it might be sleeping it might be turned off and when the request comes in the machine needs to boot up the under although the boot up happens quickly but it needs to boot up so what happens is the first request that you get after some time takes a long time for it to execute because this contains your booting time and then it executes and then the further request they run very well right and then when the request drops the machine sleeps again right the function slip and then when you get another request the function woke up again so again the cold start problem so because for the first request after a certain amount of time would occasionally have very high latency so you are expecting a latency of let's say 200 millisecond but for the first request that comes in it might be let's say one second it might be 5x or 10x of what you are expecting and that is pretty possible like that is the code like every serverless uh function or every service architecture suffers from this cold start problem there are ways to solve it but it is but you have to be very of the fact that there might be some requests that are spiky or that has higher latency than what you would have expected biggest other one so if you are not if you do not if you have a very strict sla to meet where you cannot have these spikes ever then you have to go for a server then you have to go for a traditional ec2 based architecture the second problem is that or the second disadvantage is that this serverless architecture is not built for long running process so every serverless function you configure the maximum there is a maximum amount of time for which a serverless function would execute and aws it is 15 minutes which means that you cannot deploy a logic that would take more than 15 minutes to execute for example you cannot run a massive web reduce job that may take 30 minutes for its execution because it is not meant for serverless disks because serverless the underlying hardware would sleep or would be terminated or a request would be terminated as soon as it hits 15 minutes right and this execution time is what you can configure but there would be a global max time which is roughly around 15 minutes for almost all all major serverless functions so if you are having a workload that requires a long running process serverless is not for you right a traditional server is always a better choice in that the third is testing and debugging is tough like replicating a serverless environment locally is tricky debugging is extremely tricky uh most in most cases uh the logs that you like the to principle right on the serverless function goes into the logging which is offered by your cloud let's say aws lambda function you are using the logs go to aws cloud watch debugging that information is very is is a big pain but if you have a very small use case you don't have a gigantic complex workflow that is very easily debugged and test and put into serverless so that's why small small utilities are typically deployed on serverless rather than gigantic complex applications they are not right small typical simple small suite utility they are deployed and sold because testing and debugging is a little tricky the fourth is vendor locking when i say vendor locking what does this mean that hey if you are on aws and you're using aws lambda function now let's say you want to move out of aws a or or if someone is giving you better serverless flow let's say cloudflare is giving you a better pricing or if it is giving you better execution or better resiliency you would want to move that but moving from one vendor to another vendor is a big pain because the way the code is written is specific to that vendor the way it is executed the input argument so that function is specific to the vendor so a lot of things would need to change plus the way logging happens on every cloud provider is different so you typically are logged in by that vendor when you are adopting serverless so if i am running my workload on aws laminar functions then it will be very hard for me to move only the only the serverless workload onto some other although it's better it's more resilient it's very cost efficient as compared to aws but i cannot move that and because it because i have to change a lot of part it's not a seamless transition right so now to conclude like final two points let's acknowledge like when should look obviously we discussed the real world use cases at what is in disadvantages let's talk about when should you not use serverless now this is just a summary of what we just discussed but when by making this decision you can typically think of when not to use serverless first of all when the load is almost constant and predictable that's when you don't use serverless so if you know that the workload is remaining fairly constant in let's say 1000 requests per second adopting serverless for that will be costlier as compared to a traditional server architecture because you can predict the load which means that you are provisioning an infrastructure for that specific constant and predictable load that you are getting so serverless is a very cost inefficient choice in that matter second long-running process execution we just talked about key serverless are not meant for long-running process let's say you're building uh uh you're building a data pipeline in which you have to run a map reduce job that runs for let's say 30 minutes or 40 minutes that can you cannot model it on serverless because serverless function would stop its execution after 15 minutes right when you need multi-tenancy multi-tenancy is where each of the customer gets its own separate isolated infrastructure serverless is not isolated you can typically isolate your api servers per customers but serverless all execute on the same underlying infrastructure given to you by your cloud provider so all of them will be using the same interest same underlying infrastructure which means that you cannot get complete multi-tenancy out there it might be a hybrid sort of stuff but you will not get complete isolation for each of your uh for each of your customers fourth one you want to you like if you you should not be using serverless because like when you are adopting it just because it is cool and this is the biggest mistake that i've seen people make where they blindly adopt serverless just because it's cool and it's cost efficient and all without understanding their use case without understanding what kind of traffic they would be getting they just adopted serverless because it's cool and that is the worst reason of adopting solid like which is exactly why you should not be adopting serverless right so never enough serverless just because it is cool now let's talk about when should you use serverless just as a conclusion when you want to quickly build and prototype and test and deploy your application like i am running a lot of my workloads on serverless and to be really honest to run my course it is literally taking me three rupees of monthly infrastructure cost because my entire workload is on serverless and i'm totally on aws so that's my total infrastructure cost just three rupees every month because i was if if i would have deployed that code onto my ec2 machine i would have easily paid one thousand two thousand rupees a month but now i'm paying only three rupees which includes gst as well right so that's that's how amazing serverless can be because i have a very bursty traffic very small traffic why would you why would i want to run a massive server for that even 1gb server is massive for that use case like i'm not getting enough traffic on that right second is your use case is small and lightweight this is what i talked about my use case is very small and lightweight and there is not much complexity or debugging or testing involved there basic apis i wanted to expose over internet which i did third is when my traffic is bursty and when your traffic is bursty which means that you want to quickly scale up and quickly scale down depending on the traffic you get otherwise you'll be built for a longer time so servers like uh traditional servers will you will be provisioning it to handle the peak but your peak is only there for one one percent of your time why would you want to do that right so traffic is bursty and your peak to zero alternates right and it's very bursty that's when you would want to go for a serverless architecture but again yeah that's all about serverless in this one we talked about real-world use cases five genuine real-world use cases that fit serverless algorithm very well advantageous offered disadvantages of it and as an as a conclusion when to use and when not to use serverless architecture so yeah that's it for this one i think this video is a little longer but how we went into in-depth into understanding the serverless part of it i'm glad we did so yeah that's it for this one if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sub i post three in-depth engineering videos every week and i'll see you in the next one thanks again