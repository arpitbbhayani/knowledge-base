in this video, we talk about what serverless computing is, see why it was built in the first place, learn about five real world use cases that become super efficient with serverless architecture, understand the advantages and, more importantly, the disadvantages of adopting serverless, and conclude with acknowledging when to use and when not to use this computation pattern.
which means that your server, your, your, you have over provisioned your server by a massive margin just to handle that peak load of 1000 requests per second, while for other use cases, like for other times, when you are not even seeing 10 requests or 5 requests or two requests per second, why are you wasting 4 gb of ram on that machine?
you have just deployed your core business logic onto aws lambda, which is their serverless offering, and it would automatically scale up and scale down depending on the request it comes in.
you don't even know the rules that they have applied, and what you get build is you get built per execution, which means every time a request comes in, your lambda function runs.
you are build for the time that which the lambda function runs, which means that when you are seeing five requests per second- hypothetically, let's say it- it cost you one dollar for every request that comes in here.
so the amount of time for which your your lambda function was executing, you are billed for that, which means when there is no traffic, you are not build even a penny.
so serverless architecture are typically focused at a better cost optimization, which means that you are not- you are not- over spending on your infrastructure by creating a traditional big server.
obviously your code needs a server to run, but it implies- the word serverless implies- that you are not managing or scaling the server.
so, depending what your traffic pattern is- the serverless function or the co, the server on which your serverless code is running- that would automatically scale up, scale down- uh, horizontally scale.
you put it into serverless environment and you forgot about it because it is running like: if you get traffic, you will get built.
so, depending on which cloud you are using, you can pick any of this up or, like, you can pick the corresponding uh serverless offering of this and see how it uh and and actually adopt it for your use case.
do you really need to keep a server, let's say, 1gb big server- continuously running for this use case?
obviously this is an over simplification of the architecture, but you get digest where for an infrequent access or infrequent traffic pattern like this, you don't need a constantly running machine.
so, upon every submission, you can trigger a serverless function which then can update the status of each test case in the database and you might have a traditional server api service on which the, the- your user, is conditionally refreshing or seeing the status of how many test case pass and fit you.
here the execution of your function would be time bound right, which means that within one minute all the test case should run.
so it saves you bunch of your servers and obviously, like always, because this serverless function is executing just once every day, you'll be billed only for that small execution, which is, let's say, 100 millisecond of your api call.
you will only build 400 millisecond every day, rather than a server constantly running for that solid use case for serverless.
batch and stream processing are perfect use cases for serverless- uh, for for serverless architecture, because you don't have to manage the servers and it is and it all seamlessly works like it just works, right, because- and supposedly, if you get a spike, if you get a spike in the number of messages, your serverless would automatically scale up.
you cannot even have two servers running that crontab, because then the same function would be executed twice, right?
you need to have security patches, like if, once a while, you will discover that your servers are having a massive security loophole which you would need to patch by not having an ec2 machine, by not having a server, by using serverless, that entire headache is offloaded to your cloud provider, let's say aws, gcp.
so most serverless functions, they are built on the amount of time there execute.
with cold start problem, what happens is, as i said, when there is no traffic, you are not built for that- which means the underlying infrastructure for your serverless function might not even be up- the container on which it is running, it might be sleeping, it might be turned off and when the request comes in, the machine needs to boot up the under, although the boot up happens quickly, but it needs to boot up.
like every serverless uh function or every service architecture suffers from this cold start problem.
the second problem is that, or the second disadvantage, is that this serverless architecture is not built for long running process.
there is a maximum amount of time for which a serverless function would execute and aws it is 15 minutes, which means that you cannot deploy a logic that would take more than 15 minutes to execute.
so if you are having a workload that requires a long running process, serverless is not for you, right, a traditional server is always a better choice in that.
uh, the logs that you like, the to principle right on the serverless function goes into the logging which is offered by your cloud, let's say aws lambda function.
so if you know that the workload is remaining fairly constant in, let's say, 1000 requests per second, adopting serverless for that will be costlier as compared to a traditional server architecture because you can predict the load, which means that you are provisioning an infrastructure for that specific, constant and predictable load that you are getting.