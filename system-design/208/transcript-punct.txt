so. designing a highly available and performant service is really difficult, but bookingcom does it really well. the rating and review service is one of the most critical services for booking and in this video we dive deep into how they designed and scaled it to ensure that they seamlessly handle a peak traffic of more than 10 000 requests per second. but before we move forward, I'd like to talk to you about a course on system design that I've been running for over a year and a half now. the course is a code based course, which means I won't be rambling a solution and it will not be a monologue at all. instead, a small, focused group of 50 to 60 Engineers will be brainstorming the systems and designing it together. this way, we build a very solid system and learn from each other's experiences. the course is enrolled by 800 plus Engineers, spanning 12 codes and 12 countries. ingenious from companies like Google, Microsoft, GitHub, slack, Facebook, Tesla, Yelp, Flipkart, dream11 and many, many, many more have taken this course and have some wonderful things to say. the course is focused on Building Systems the way they are built in the real world. we will be focusing heavily on building the right intuition so that you are ready to build any and every system out there. we will be discussing the trade-offs of every single decision we make, just like how you do in your team. we cover topics ranging from Real Time text communication for slack to designing our own toilet balance side, to cricbuzz live text commentary to doing impressions counting at scale. in all, we would be covering roughly 28 systems, and the detailed curriculum, split week by week, can be found in the course page linked in the description down below. so if you are looking to learn system design from the first principles, you will love this course. I have two offerings for you. the first one is the live cohort based course and the second one is the recorded offering. the Live code base course happens once every two months and will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts. as it's. if you are in a hurry and want to learn and want to binge learn system design, I would recommend going you for the recorded one. otherwise, the Live code is where you can participate and discuss the systems and its design life with me and the entire cohort. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page. arpitbani dot me slash masterclass. I repeat at with many dot me slash masterclass and I would highly recommend you to check that out. I've also put the link of this course page in the description down below and I'm looking forward to see you in my next cohort. so, on bookingcom, you can book flights, hotels, stays, tours and much more, but the core reason why bookingcom thrives is because of its review system. because the thing is that, using reviews, people make informed decisions that, hey, do I need to book this hotel, or is this hotel, or is this stay, or is this, too, really good enough? and the more important part over here is that reviews are authentic, which means that a person cannot post a review unless that person has booked something on bookingcom, which means that all the reviews that you see on booking, they're genuine. now, this is really important for a system, why? because when you see, when people know that the information that they are being thrown at, or like the information that they see I, is authentic, they make a lot of decisions according to that. so now people are coming to booking to read the reviews and then make an inform District: hey, I want to book this particular Hotel. so now what does this mean? this means that reviews become their top of the funnel. so people come on bookingcom to read the reviews about a place or about a stay or about a tour, and they book after that. so this becomes the top of the funnel, which means that it drives their Core Business, which means that reviews service cannot go down. classic, classic, classic problem statement. so here this service needs to be supporting a very high availability and a very low latency, because this service is something that everyone is having their eye on, that this service has to be up and running, because this is what is driving their core feature. no one really thinks about rating and review system that much, but here, because this is the top of the funnel, it becomes critically important for an organization to have it up and running 24: 7.. right, okay, now that we understand why ratings and review service needs to be highly available, we would take a look at how they do it, but this gives us a hint on how they would be thinking about testing. Okay, so we need High availability, we need low latency. so obviously, with respect to when you think about ratings and review service, it would be a very simple, rest based API in which you can get a review, create a review, get a bunch of reviews, list a review by location, by uh tools, by accommodation and whatnot. right, so it will be assumed that that's a very simple rest based service written in Python, Java or basically whichever framework you would want. that's a normal based endpoint which is exposed right now. given that, how do you make this thing highly available and low latency? now let's start with that. now, the kind of traffic that they expect on this particular service is a peak traffic of 10 000 requests per second, with the P99, which is 99 percentile of the response time, to be 50 millisecond. now this indicates that the reviews that you see on booking, they have to be mostly served from cash and pre-materialized Views. now, what are materialized views you create? you can, on your relational databases, you can create materialized views so that you don't spend time doing redundant joins again. so this materialized views make your reads much faster, right, and plus a centralized cache like radius, memcache, couch based something, something around that you can use to build it right. but this gives a critical hint that, because we need low latency, we would go for a something like a cache based solution. so cache and pre-metalized Views are two classic ways to get the max performance out of the system. okay, that being said, now let's think about the amount of data that they have, because this gives us Glimpse around the the overall database architecture that they might have. okay, so amount of data is what they say is. they have 250 million reviews and Hyper and let's say, in each reviews, each review contains answers to some objective questions, like basic questions, like on the scale of one to five: how much did you write this? what did this place have a b or c you? you check check on that, right? then rating on various parameters and a textual feedback, right? so, given this, the overall size let's assume is 2 KB. so if we assume 2 kilobyte per review and let's say there are 250 million reviews, so 250 million multiplied by 2 KB is is basically 500 GB. so it's decent enough storage. it's not very small, it's not very huge, but it's decent in a 500 GB just to store ratings and reviews to render on the interface. it's pretty decent, right? obviously, not all reviews are getting serviced. the most recent one or the most popular ones are getting server, but still this is big enough storage. now, this means that for us to have a response time under 50 millisecond that we just observed, like P99 is less than 50 millisecond, which means that we need to have our database started. it's not because of high amount of data, but more about high amount of load that we are seeing on the system, which means that we cannot just rely on a single data node. because it's a very high traffic- 10 000 requests per second- OneNote definitely cannot handle it with ease, so we would add a bunch of replicas, so we would typically have a master which is taking all the rights, replica which is serving a lot of reads and, and very importantly, because we want the service to be highly available. for example, what if you have hosted this service in a database and that region, that AWS region or the gcp is another, or that Azure region is down, it's cut off due to any reason, maybe the database data center is down. so what do you need to do? you? because you cannot just say, hey, AWS is down, I can't do anything, because your business comes first, even if AWS is down or gcp is down, a Roger is down, right. so what we do is we have to make this service highly available. so even if our database is down or that region is down, our system still should be working so instead. so, apart from having a bunch of read replicas to serve the read request, you would have a replica that is replicated, that keeps the replicated data in another availability Zone, which means that, let's say, if your Singapore data center is down, it will be served from, let's say, Mumbai data center. right, that's the idea behind it. so you would have one replica in the same availability Zone and another replica in another level, in other availability zone. right, this would make your service highly available, right? okay, given that we have 500 GB of data and a lot of ridiculous coming, and obviously one node should not be able to- or OneNote is not able to- support that much of request, you Shard the data. now, on which parameter would you shut? typically, an accommodation ID, right? accommodation ID is, let's say, hotel ID, or two, right, something around that. let's call it octal ID. so, like, this is the accommodation idea on which I am sharding. but now, uh, very interesting thing crops up: that how do you route the request? so let's say a request came in for a particular occupation, that, hey, give me all the reviews. or let's say, let's create a review for this particular accommodation. that how would your review service know which Shard to connect to? which means there has to be some routing logic over here. and most classic, most typical way to do it is a hash based routing where you take the accommodation ID, you mod it with the number of shards that you have, you get an index and then that particular node, you go and fire the query and create the rating or review right. this is a very classic way to do it. but again, with classic Way Comes classic challenge. now here what happens is, let's say you have three nodes and you have sharded the data according to that to handle the load and what not. the data is split very well between this or among these three nodes. but now the challenge comes in. what if you would want to add a fourth node? or let's say you saw enough traffic but now you have to reduce it. now obviously you, if you are provision three shards but you are not seeing enough traffic, that like you are seeing a traffic that could be very well handled by just one node, why would you want to have three nodes, like two extra nodes? that would just leak money. so that's where you would want to downscale it. so you would want to make this scalable. so add more nodes and remove more nodes on the fly. now the challenge: with a normal hash based routing, it's very evident that, hey, if you add or remove more nodes, the mod number of shards this would change, which would require you to do re-indexing and repartitioning of the entire data, which means it is not easy for anyone- for anyone, given that there is 500 GB worth of data- to re-partition the data among the new set of nodes. right, and that would be extremely costly. so you typically don't do that, which means you would be leaking money. so adding more nodes to handle more Peaks? it's not possible, so you have to be always over provisioned for the peak that you are about to handle. so that is where to make this thing really efficient- cost efficient per se. what you want is: you want an ease of yeah, like. you want it to be easy for you to add and remove data nodes from your cluster, which means that you should not be having to do the re-indexing and repartitioning of the entire data set. it needs to be minimal. this is a classic hint towards a classic algorithm called consistent hashing, because consistent hashing is that one tool, or rather, is that one algorithm that solves this problem beautifully well. so if you folks don't know about consistent hashing, I would highly recommend you to read my blog post on consistent hashing, which also contains its implementation. you can find it at with many dot me slash, blog slash consistent hashing. I'll also link it in the description down below. I've talked about not just a theory but the implementation part of it as well, and it just takes a sorted array and binary search to implement consistent hashing. it's nothing fancy in it, right? but let me spend some time talking about consistent dashing now. the core idea of consistent hashing is very simple. you structure it in ring and whatnot, like you will read it there, but what it answers it? it answers data ownership. it does nothing fancy, it just says: given a particular review, where would it lie? simple, all it does is data ownership, nothing more, nothing less. like. given this, given this, where would we put this consistent hashing logic in the review service? right, because review service was the one that holds the routing information that earlier, if you are using hash based routing, the request, com, the request came to review service. review service would be Computing the hash and a mod number of shards and find the index and put it there, right? so, similarly, the consistent hashing that you implement will be part of your review service, right? so when the request comes in, using consistent touching, it would find a where to forward the request to. it would go and hit the request on that particular shot, right? this solves that problem. so, consistent hashing, beautiful- algor them to do minimal data transfer, and that's what the key highlight of consistent hashing is. so what do you get? you get data ownership in very quick time and, second, you do minimal data movement. so, in case, if one of the node is removed from uh, so if basically one of the node is removed from the ring, the request would be automatically going to the next node. right now, that is what what a consistent hashing does. so, in case you add node to that, you add more node. you just have to copy data from one of the peripheral nodes. that's it, and that's the beauty of consistent hashing, right? so I've linked my blog in the description down below. go read that to understand consistent hashing in depth, along with implementation. right? but now, given that we would have consistent hashing implementation, the routing logic implementation, on the review service that we have, what is the practicality of it? now? how do you resize because, ah, transistorizing, if you use that, does not mean that everything is automatically sorted. most people think like that. but it's not right. you have to do a bunch of stuff. now let's say: let's say I want to scale up my database cluster, which means instead of having three nodes, I want to, let's say, add a fourth node. when I want to do that, when I add the fourth node, I would first find the location of it in my consistent hashing ring that I have. I'll place it there, and then I find, okay, it's placed over here. so now the request that was going over here, like wherever the request was going, you would copy the data from that dependent node onto this. so what you need is you add a node in the ring. you first copy the data from the other node, like the minimal data transfer that we are talking about, from one of the previous nodes that you have. you copy the data, you wait until the data that needs to be moved over here it moves there. once it has moved, then you notify the review service that, hey, now you can use the other ring. so just to give you a a nice crisp gist of it is that your review service has a consistent hashing ring available. using that, it basically routes the request to the corresponding chart. now, when you add one new node into this consistent hashing ring, you first wait until some data, until the data that needs to be moved to this particular node moves there, right. and once that data is moved, once that node is ready to accept the request, then you notify the review service about the existence of this new node in the new ring. so you create an instance of consistent hashing ring. that's, it's just a logical array. that's it, right? you then tell the review service: hey, now you use this ring instead of the old ring. then the next request that comes in starts hitting the new node, right. so now you would have four nodes in your cluster, right, and everyone serving the request. now, this is what you do, practically right. so add and remove the node that you would want to copy the data that needs to be moved by by balancing- and obviously not a lot of data would happen, only data from the peripheral nodes would move, nothing more, right, and that's the beautiful. it's about minimal data transfer. and you notify the review service to start consuming the new ring, right. so this way, the new request goes to the new nodes and you balance a lot across four nodes. now, right, this technique is can be used across any service that you are designing. so wherever you see a problem of you wanting to build a service that needs to scale up and down at will and there is a lot of data that is moving because of your routing logic, you employ consistent hashing at that location. once you employ consistent hashing at that location, your problem is typically solved, because it ensures that you have to do a minimal data movement when you add or remove nodes from the consistent hashing ring that you have. and again, do remember that all consistent hashing answers is data ownership. it does not. it's not a magical function that would automatically scale any system. don't think of it like that. all it does, all it does, is answer data ownership. that, hey, given this thing, where who owns this? that's all it does, nothing more, nothing less, and that's really important for you to know. don't think it's such a magical solution, right? okay, now that's a practicality of resizing. now, what does high level architecture of this entire design look like? it's a pretty simple one, right? but what do you have? you have a bunch of users firing lot of requests onto a review service. review service has a consistent hashing implementation who's using which it basically routes the request to the corresponding sharded, to the corresponding Shard of your sharded database. now, here you typically use MySQL because they mentioned that they use pre-materalized views, which means it has to be relational database. so they use a relation database to store ratings and reviews and whatnot, and they and this ownership info and, uh, the database cluster is mentioned in the consistent hashing ring, depending on which your review service would router request to the corresponding node. review service uses a cache because we saw that P99 response time has to be less than 50 milliseconds, which means that they would be, or they are, using cash to serve most of the latest reviews, but still a bunch of when users score. let's say, you render first 10 reviews through cash, but beyond that, the request comes to the DB, which is where prematuralized views comes in. right then, for each of the master, for each of the sharded database that you have, you would have a replica in the same availability Zone and a replica in another availability zone. now these, both of these replications, are asynchronous replication and not synchronous replication, because you don't need synchronous, synchronous replication across different availability zone is you should not be doing that. it would increase your latency by a huge amount. so that's why you do this asynchronously and our synchronous replication within the same availability Zone as well. now, key things that they what they did. they did Metalized views on your database to improve latency. then your high cash hit ratio to ensure P99- less than 50 milliseconds. and third, because availability is critical, they have a copy of data, of a read replica present in different availability zones so, in case if the node goes down or the region cuts off, you would be serving the request from the database present in another availability Zone. and this is how you think about any system at hand. what are the key criterias that you definitely have to have? for example, availability, low latency, low latency. you add cash availability, you ensure that everything is available, everything in your architecture goes down, and then you build on top of that, and that's how you should be thinking about any problem at hand. there is no magic question answer thing in system design. everything is a trade-off. understanding and reading between the lines is really important to do so, and just to reiterate, consistent hashing again, is not a magical solution, not at all a magical solution, you have to know. it just does two things: answers data ownership and requires you to do minimal data movement when you add or remove a data node from it. but you have to do the data movement. consideration does not do anything, right, it just tells you data ownership, right? okay? so yeah, that's how bookingcom does it in this entire thing. this entire piece is taken from bookingcom engineering blog around their ratings and review system, a link. I've linked that blog in the description down below. I would highly, highly, highly recommend you to check that out. I have also linked my blog on consistent hashing: the theory and, more importantly, implementation in Python. it's a very simple array and binary search based implementation. even a first year kid would understand it. it's just binary search and array, nothing more, nothing less. it's that simple, right again, I would highly recommend you to implement it on your own right. so if you find it interesting, if you find it amusing, consider supporting this channel by hitting the thanks button down below. a ton of effort goes behind the scenes to create such deep engineering content. so, yeah, that is it for this one. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel a sub and I'll see in the next one. thanks again. [Music].