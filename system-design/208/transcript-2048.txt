designing a highly available and performant service is really difficult, but bookingcom does it really well.
the rating and review service is one of the most critical services for booking and in this video we dive deep into how they designed and scaled it to ensure that they seamlessly handle a peak traffic of more than 10 000 requests per second.
because the thing is that, using reviews, people make informed decisions that, hey, do I need to book this hotel, or is this hotel, or is this stay, or is this, too, really good enough?
so now people are coming to booking to read the reviews and then make an inform District: hey, I want to book this particular Hotel.
so people come on bookingcom to read the reviews about a place or about a stay or about a tour, and they book after that.
so this becomes the top of the funnel, which means that it drives their Core Business, which means that reviews service cannot go down.
so here this service needs to be supporting a very high availability and a very low latency, because this service is something that everyone is having their eye on, that this service has to be up and running, because this is what is driving their core feature.
no one really thinks about rating and review system that much, but here, because this is the top of the funnel, it becomes critically important for an organization to have it up and running 24: 7..
right, okay, now that we understand why ratings and review service needs to be highly available, we would take a look at how they do it, but this gives us a hint on how they would be thinking about testing.
so obviously, with respect to when you think about ratings and review service, it would be a very simple, rest based API in which you can get a review, create a review, get a bunch of reviews, list a review by location, by uh tools, by accommodation and whatnot.
right, so it will be assumed that that's a very simple rest based service written in Python, Java or basically whichever framework you would want.
given that, how do you make this thing highly available and low latency?
now, the kind of traffic that they expect on this particular service is a peak traffic of 10 000 requests per second, with the P99, which is 99 percentile of the response time, to be 50 millisecond.
now this indicates that the reviews that you see on booking, they have to be mostly served from cash and pre-materialized Views.
so this materialized views make your reads much faster, right, and plus a centralized cache like radius, memcache, couch based something, something around that you can use to build it right.
but this gives a critical hint that, because we need low latency, we would go for a something like a cache based solution.
okay, that being said, now let's think about the amount of data that they have, because this gives us Glimpse around the the overall database architecture that they might have.
obviously, not all reviews are getting serviced.
now, this means that for us to have a response time under 50 millisecond that we just observed, like P99 is less than 50 millisecond, which means that we need to have our database started.
because it's a very high traffic- 10 000 requests per second- OneNote definitely cannot handle it with ease, so we would add a bunch of replicas, so we would typically have a master which is taking all the rights, replica which is serving a lot of reads and, and very importantly, because we want the service to be highly available.
so, apart from having a bunch of read replicas to serve the read request, you would have a replica that is replicated, that keeps the replicated data in another availability Zone, which means that, let's say, if your Singapore data center is down, it will be served from, let's say, Mumbai data center.
right, this would make your service highly available, right?
okay, given that we have 500 GB of data and a lot of ridiculous coming, and obviously one node should not be able to- or OneNote is not able to- support that much of request, you Shard the data.
accommodation ID is, let's say, hotel ID, or two, right, something around that.
so let's say a request came in for a particular occupation, that, hey, give me all the reviews.
that how would your review service know which Shard to connect to?
and most classic, most typical way to do it is a hash based routing where you take the accommodation ID, you mod it with the number of shards that you have, you get an index and then that particular node, you go and fire the query and create the rating or review right.
now here what happens is, let's say you have three nodes and you have sharded the data according to that to handle the load and what not.
now the challenge: with a normal hash based routing, it's very evident that, hey, if you add or remove more nodes, the mod number of shards this would change, which would require you to do re-indexing and repartitioning of the entire data, which means it is not easy for anyone- for anyone, given that there is 500 GB worth of data- to re-partition the data among the new set of nodes.
you want it to be easy for you to add and remove data nodes from your cluster, which means that you should not be having to do the re-indexing and repartitioning of the entire data set.
so if you folks don't know about consistent hashing, I would highly recommend you to read my blog post on consistent hashing, which also contains its implementation.
I've talked about not just a theory but the implementation part of it as well, and it just takes a sorted array and binary search to implement consistent hashing.
the core idea of consistent hashing is very simple.
given this, given this, where would we put this consistent hashing logic in the review service?
right, because review service was the one that holds the routing information that earlier, if you are using hash based routing, the request, com, the request came to review service.
review service would be Computing the hash and a mod number of shards and find the index and put it there, right?
so, similarly, the consistent hashing that you implement will be part of your review service, right?
it would go and hit the request on that particular shot, right?
right now, that is what what a consistent hashing does.
that's it, and that's the beauty of consistent hashing, right?
go read that to understand consistent hashing in depth, along with implementation.
but now, given that we would have consistent hashing implementation, the routing logic implementation, on the review service that we have, what is the practicality of it?
now let's say: let's say I want to scale up my database cluster, which means instead of having three nodes, I want to, let's say, add a fourth node.
when I want to do that, when I add the fourth node, I would first find the location of it in my consistent hashing ring that I have.
so now the request that was going over here, like wherever the request was going, you would copy the data from that dependent node onto this.
so what you need is you add a node in the ring.
you first copy the data from the other node, like the minimal data transfer that we are talking about, from one of the previous nodes that you have.
once it has moved, then you notify the review service that, hey, now you can use the other ring.
so just to give you a a nice crisp gist of it is that your review service has a consistent hashing ring available.
now, when you add one new node into this consistent hashing ring, you first wait until some data, until the data that needs to be moved to this particular node moves there, right.
and once that data is moved, once that node is ready to accept the request, then you notify the review service about the existence of this new node in the new ring.
so you create an instance of consistent hashing ring.
you then tell the review service: hey, now you use this ring instead of the old ring.
then the next request that comes in starts hitting the new node, right.
so now you would have four nodes in your cluster, right, and everyone serving the request.
so add and remove the node that you would want to copy the data that needs to be moved by by balancing- and obviously not a lot of data would happen, only data from the peripheral nodes would move, nothing more, right, and that's the beautiful.
and you notify the review service to start consuming the new ring, right.
now, right, this technique is can be used across any service that you are designing.
so wherever you see a problem of you wanting to build a service that needs to scale up and down at will and there is a lot of data that is moving because of your routing logic, you employ consistent hashing at that location.
once you employ consistent hashing at that location, your problem is typically solved, because it ensures that you have to do a minimal data movement when you add or remove nodes from the consistent hashing ring that you have.
and again, do remember that all consistent hashing answers is data ownership.
you have a bunch of users firing lot of requests onto a review service.
review service has a consistent hashing implementation who's using which it basically routes the request to the corresponding sharded, to the corresponding Shard of your sharded database.
so they use a relation database to store ratings and reviews and whatnot, and they and this ownership info and, uh, the database cluster is mentioned in the consistent hashing ring, depending on which your review service would router request to the corresponding node.
review service uses a cache because we saw that P99 response time has to be less than 50 milliseconds, which means that they would be, or they are, using cash to serve most of the latest reviews, but still a bunch of when users score.
let's say, you render first 10 reviews through cash, but beyond that, the request comes to the DB, which is where prematuralized views comes in.
right then, for each of the master, for each of the sharded database that you have, you would have a replica in the same availability Zone and a replica in another availability zone.
and third, because availability is critical, they have a copy of data, of a read replica present in different availability zones so, in case if the node goes down or the region cuts off, you would be serving the request from the database present in another availability Zone.
you add cash availability, you ensure that everything is available, everything in your architecture goes down, and then you build on top of that, and that's how you should be thinking about any problem at hand.
understanding and reading between the lines is really important to do so, and just to reiterate, consistent hashing again, is not a magical solution, not at all a magical solution, you have to know.
it just does two things: answers data ownership and requires you to do minimal data movement when you add or remove a data node from it.
this entire piece is taken from bookingcom engineering blog around their ratings and review system, a link.
I have also linked my blog on consistent hashing: the theory and, more importantly, implementation in Python.