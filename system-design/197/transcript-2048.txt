so, when a product is catering to a large audience, providing contextual information becomes important, and at the scale of Airbnb, it becomes non-optional.
they collated all the data and structured it into a knowledge graph that today Powers their search, Discovery and trip planner services.
in this video, we take a detailed look into how Airbnb designed its Knowledge Graph, some key components of it and some key design decisions it took while architecting it.
now, to have all of this information- obviously all of this information is present there on each of their own respective databases, but to have them collated, helping users make the best decision, you have to have this information at one place, making them easily accessible.
Facebook also has their own Knowledge Graph and whatnot.
so here we would take a look at how Airbnb does that right.
they have most of their data, like most of the transactional data present in the relational databases, but why they cannot use that information to power this functionality?
because, like with relational databases, you typically know that it's a most common way to store stuff you have.
for example, you might have a table for hotels, you might have table for events, a table for experiences and what not, and then what you would have is you would have foreign key references connecting to all of them.
so relational databases are meant to work very well with the transactional information, for example, payments, the way we do it, affordable users, post even for this, for the transactional thing where I'm getting something, putting something, that sort of stuff on an individual entity level, it works really, really well.
for example, what if we would want to take the user on this journey of finding a city that hosts a type of experience in July and August?
for example, the user is trying to book a, uh, to basically book a trip in the month of July and August, right, but he or she is looking for a very specific type of experience and you and he or she is open for multiple cities, so you'd want to list down top five cities where this type of experience is there in the month of July and August.
now this is not something that you can write, a SQL verified, because there could be a lot of types of information and the way you have you might have a micro service.
so answering such queries with traditional relational databases is a big pain.
so that is where, like here, if you look at this, we would want users to have enriched exploration, uh, experience.
it's like basically surfing through the data.
this is where knowledge graphs come in.
this is where knowledge graphs come in.
so now, how is Knowledge Graph structured at Airbnb?
so the key component, the key infrastructure components, are specific highlighted first is the graph storage, where they would be actually storing the knowledge graph.
second is graph query apis through which your search service, planner service and whatnot will be using or will be querying this graph storage tools Rel to surface and element information and then a storage mutator so that this information can be updated by some, by some peripheral services.
right, we'll start with the first one: graph storage.
you might think, hey, graph needs to be stored.
let me just use a graph database.
no, graph database is very like.
you might just work very well with the normal relational DB as well, but no one's denying that you cannot use a graphdb.
Airbnb did not use it because they didn't have expertise in using them right.
so what they do is Airbnb stores their entire Knowledge Graph in a relational database.
the idea is where in the, in the, in a normal graph also, you have nodes and edges, in the relational database also, within split across multiple table, they store this very information, nodes and edges.
but the way, the way any graph, you, if you are designing anything, anything related to graph, typical three things that you might have to think of: it's subject, verb and object.
for example, New York is in USA, so subject, verb and object, basically two nodes connected, VI and Edge, right.
so this is this triplet is what most graph databases use to store this information, which we can very well mimic in a normal relational database within a table with three columns.
so each node type in the graph has a different schema.
so there is a way in this, in their own Knowledge Graph, to specify that, hey, this is my node type.
for this node type, this is the schema, so that when they're making an entry of this in the database they can have that particular type of schema, like they would be able to ingest that particular type of value.
then, similar to nodes, they have edges types.
for example, landmark in city is an edge type.
that, hey, Taj Mahal is a landmark in the city of Agra, right?
so this is how your relation is, your relationship is defined and each Edge type would have that.
you need to have that strict constraints that landmark in a city can connect only Landmark from, like a landmark to a city, right, and they didn't go for graph DB.
again, just to emphasize on that: that keeping things simple is the heart and solid scale is to make it operations, because operations overhead is there and they do not have enough expertise to like.
so they use relational DB to build it right.
and and the knowledge graph that they have needs to be periodically dumped for offline consumption.
for example, I don't have to always query the graph, my knowledge graph, to get some information, for example, let's say, if I want to do some search engine ranking or if I'd want to run some recommendation.
so, instead of every for everything, firing query to Knowledge Graph will be too expensive because it would put unnecessary load onto your knowledge graph.
so that's why, periodically, the data from the knowledge graph, this particular database, is dumped into a format that other service, other services can consume from, and they typically do it for offline ranking, processing, recommendations and whatnot.
right now we covered graph storage.
now let's talk about graph query API.
so how a normal service- let's say search- wants to use this knowledge graph, how would they query it?
so a graph typically is meant to be traversed, because you don't know what kind of query you'll get.
but you would have to Traverse the graph and find the most relevant information out of it.
so, for example, you might want to find all the places, like all Place nodes, connected via with a particular City node.
this way, if I want to do this, I would want to Traverse a graph, so there has to be a way through which I should be able to provide a query.
now this could be a simple Json format in which they are expressing this query and this query layer understands it, understands that particular Json and then goes to the knowledge graph, converts this type of query into an into your relational database, understandable SQL query, fires on the relational database, gets the output and sends the response back to the user.
the idea is this graph query layer.
it takes an input, a Json, which is a very sophisticated nested query, converts it into SQL, fires it onto the database, gets the response and sends it back to the client.
this is the role of graph query API layer and this totally depends on how they would want to build it.
they typically did it with simple Json.
you can have your own custom format, no one stopping you from doing that, but the idea is keep it simple Json.
everyone understands user that right.
now here, and the key thing here is: so, say, we have some information in our knowledge graph, but obviously, let's say I have listings.
now that listing changed, right, so the description of the listing chain.
for example, let me use a simpler word, let's say you have a City information, so City will have its own micro service, or have its, or is its, in its own database.
now you would want something change in that.
and now this chain needs to be propagated to this knowledge graph so that whenever someone fires to this knowledge graph it gets the latest data.
you might say, hey, when something changes over here, let me invoke a Knowledge Graph API to update the information.
but just imagine, at a scale of Airbnb, so many updates across so many entities would be happening in so like so frequently now all of them making synchronous updates to this knowledge graph is catastrophic.
so synchronous updates would make everything slow, expensive and time consuming.
this is where what storage mutator does is it exposes like.
now anyone who wants to update something in a specific format can put a message into Kafka.
this mutation, this storage mutation, accepts those message and updates the knowledge graph.
this way, the mutation layer exposes like, or rather, it allows us to do bulk updates in our knowledge graph.
and this is such a beautiful design because in most cases, people think, hey, Knowledge Graph is a service, let me update it through an API.
but if you do that, you are putting unnecessary pressure onto this knowledge graph, while you are making things slower to do bulk updates and you might also want to expose a bulk and pot and whatnot.
let it happen over Kafka, because in Kafka message would remain persisted, they would ingest the data slowly, it would be consumed and the knowledge graph would be updated.
so what you have is your simple graph has a storage.
your relational DB is the storage right now, your graph query layer, your end user, which is maybe your search service, your client search service, planner service, product listing page, all of them- they use graph query layer to fire a synchronous query to this graph.
so graph query, instead of directly talking to the relational database, they are talking to a storage abstraction.
if you don't want to do that, don't implement it.
let directly your API Handler invoke uh the like, basically convert it into SQL query and fire it.
but this storage abstractions job would be to primarily take the Json input, convert it into the SQL query and fire it onto the relational database, get the response, send it back to the user done.
second is: this is what your normal flow would look like then: periodically, your relational database is dumped into a data warehouse- maybe a redshift may be S3 somewhere.
relational database periodically dumped, uh onto your data warehouse so that it can be consumed by search or recommendation service for offline processing.
right now, you would have a normal ingestion flow where, through the mutation, a like a batch or a streaming ingestion job would be run, whose job would be to take in regular updates that are happening from different sources through mutation, pass it and update it in your relational database.
right, this is what your regular flow would look like.
and then for bulk mutation that comes in, some external mutation- something that you would want to do, except those events through Kafka and those updates from other sources- can happen both ways: asynchronously through Kafka or synchronously through graph API.
both ways are fine, and this is a very simple, extremely simple, high level architecture of airbnb's uh, of airbnb's Knowledge Graph- extremely simple.
thanks [Music].
thank you.