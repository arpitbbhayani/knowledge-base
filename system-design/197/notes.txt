Architecture of Airbnb s Knowledge Graph

Airbnb 's knowledge graph Airbnb holds experiences , places , hotels , events , restaurants , markets , homes , and much more . While planning a trip . it is important to show all critical information to the user for making an optimal decision . This information is stored in a knowledge Graph can we not store it in Relational DB ? - most common storage one row for each entity - - linkage through Foreign key references Relational Databases work well with transactional data where we want to access certain rows , but - find city that hosts a type of experience in July and August in LA where there Huts Islands available find neighbourhood - are or ^ Answering such queries is a big pain with relational databases within the data ' we want to surf . . . .

Infrastructure Knowledge Graph infrastructure has 3 key components Mutation - graph storage - graph Query API - Storage Graph Storage The entire knowledge Graph is stored Relate in a relational database , but it is Database structured as Nodes and Edges as tuples < subject , verb . Object > Each node type has a diff schema eg : NewYork IS IN USA Name , GPS cord , , eg : location _ Marriot -17 IS IN USA Date , - , Event Name , . Venue 7 reference id Each edge type stores nodes that connect eg : landmark in city landmark 2 place - - why didn't they use Graph DB ? Operation Overhead . Expertise knowledge graph is periodically dumped for offline consumption eg : recommendation / ranking

Graph Query API Traverse the graph by specifying the path . Path is just a sequence of edges data + filters eg : [ landmark - in - city , city - with - mountain / location filter : : USA LA ' ' ' ' Find all Eg : place nodes connected with city node ' ' with edge type contains _ location ' ' scenic such that - # listing > 5000 - category = storage Mutation say , some information in the graph needs to be changed J So ! , we can expose an API endpoint to do so Simple . . Graph St . Not really ! mutations is Doing large number of synchronous Slow time 1 . 2 . expensive 3. consuming Henie it better to hunt . is for an asyni alternative

Along with sync API updates , the mutation can be sent over Kafka making the flow efficient search updates from SYNC > graph query < > product list sources a ASYNC Vvv er 0 0 Abstract" Mutation < > storage Kafka a ^ Ingest or " MP usual data ingestion Relational Batch / streaming Database IT > search Data warehouse > Recommend "