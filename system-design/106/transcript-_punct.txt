so, while designing any system, we need to ensure that a system is not obvious and we do not abuse any peripheral systems. we depend on a technique to do this is called throttling and rate limiting, and the component that typically handles this is called as a rate limit. it is a very common thing to believe that the rate limiter is always external and it prevents our system from going down, but this is not totally true. in this video, we will understand what throttling and rate limiting is, why do we need it in the first place and see five use cases where throttling- both external and internal- is super useful. but before we move forward, i'd want to talk to you about a code based course, or system design, that i have been running since march 2021, and if you're looking to learn system design from the first principles, this course is for you. yeah, because this is a cohort based course. it will not just be me rambling a semi-optimized solution, thinking it's the most amazing solution out there. instead, it will be a collaborative environment where every single person who is part of the cohort will can pitch in his or her ideas and we will evolve our system around that right. every single problem statement comes with a brainstorming session where we all together brainstorm and evolve our system. that's why everyone understands the kind of trade-offs we made while making that decision. instead of just saying, hey, we'll use a particular queue, we'll have the justification why we use only that queue, why we use that particular database, why sequel? why not no sql right? how are we leveraging throughput? how are we ensuring that our system scales? that's the highlight of this course. this course is taken by more than 500 engineers to date, spanning nine countries and seven cohorts right. people from all top companies have taken this course and the outline is very intriguing. it's very exciting. so we start with week one around. we start with the core foundation of the course where we design online, offline indicator. then we try to design our own medium. then we go into database, where we go in depth of database locking and take and see few very amazing examples of data log or database locking in action and how do we ensure that our system scales through that. then the third week is all about going distributed, where we design our load balancer. i'll walk you through the actual code of a toy load balancer and understand how tcp connections are managed and how simple it is to build load balancer. then week four is about all about social networks. week five is all about building your own storage. indians, like we'll build that intuition on, if you were to ever design your storage agent, how would you do that? right? then week six is about building high throughput system. seven is about building uh ir system- basically information retrieval systems, and adobe designs, where we design our own message brokers like sqs, where we design distributed task scheduler. and we conclude the course with week 8, where we talk about the super clever algorithms that has powered or that has made those systems possible. right, i have also attached a video, verbatim as is from my first quote, where we designed and scaled instagram notifications. i will highly encourage you to check this video out. right? and now back to the video. so what is throttling? throttling is a technique that ensures that the flow of data request or anything that is sent to a target system, is sent at a rate which is acceptable to the target system. right, this would ensure that the target system does not go down if there are many users or many subsystems trying to send data to one such target machine. we need to ensure that this target machine is not overwhelmed by the number of requests that it gets right, which is where a component called rate limiter kicks in, who ensures that the data is forwarded to the target system at an acceptable rate right. throttling is generally perceived as a defensive measure, where you are being defensive of your system not going down, and there are three typical strategies to like when you are, when you know that there are far too numb, like far too many requests coming at your rate limiter, what you like, basically what your throttling mechanism would do, what your rate limiter would do. there are typically three things. first, slowing down. in case of slowing down, what you will do is when the when there are lot of request coming in on to your rate limiter, it would slow down and slowly drip the request to the target machines, which means that your rate limiter is acting as a buffer right. a classic example of this is a normal queuing, like a normal message broker, like sqs, revit or something, where a lot of requests that is coming in are buffered there and the consumers are reading it at their own pace, where the pace at which they're comfortable with, so the systems are not overwhelmed and there is a buffer to keep all of those stuff handy. second is when a throttler rejects a request. so it might be possible, where you are getting a lot of request and if you get requests which are larger than, or or or which is more than a particular number of request, you would want to reject it, where you know that your system is not able to handle, will not be able to handle the load, so you will be rejecting those requests. a classic example of this is when you get a lot of request coming on to your api servers. this is a synchronous communication. there is no buffering happening there. so then, if you are, if you know that your eq servers are designed to handle 100 requests per second, but if you are getting 1 million requests per second, what will you do with those others? you will be rejecting them. so, which is where, when you get all of this request, your rate limiter can choose to reject the surplus request, will only allow 100 requests per second to drip in, but would reject the surplus request. so there is no buffering here. and the third one is ignore. so this ignore is similar to reject, but when rejecting, you typically tell the client that your request is rejected, while in case of ignoring, you say: i'll not even tell the client what, i'll just ignore, right? so this is a very popular technique to fool an attacker, where your attacker sees 200 okay, but behind the scenes your load balancer has actually rejected the request. instead of sending 429 as an error code, you just send 200 okay to the attacker and he and that person thinks that you have accepted the request and he or she is actually abusing your system. but in fact that doesn't. uh, but in fact that is not happening. instead, you are just ignoring the request from the load balancer while returning a successful response to the attacker. so, multiple ways. and obviously it's always the combination of these three that you will apply on the kind of end points that you have. it's not always like one over other, right? so why do we even need throttling in the first place? so throttling, we know that it is all about streamlining, whatever comes to a target machine, right, but why? why do we need to do that? first reason: to prevent system abuse. right, there might be, like you, you have built your system and you love it, but then you see, like, but there are a lot of users who are using your system, and then one user goes rogue and says, okay, i'll just thought, i'll just make as much of api calls as i can to bring your system down. so, in order to prevent your business, in order to prevent your product, in order to prevent a service from going down, what would you do? you would want to add a rate limiter to ensure that your system is not going down, your system is not overwhelmed. second, to only allow traffic that could be handled. now, here, this is different from attack. like these are all genuine traffic. no user has gone wrong, no user has gone rogue here. this particular use case talks about when you, you have a website and some very famous person on twitter tweeted about it and your website went viral. all the users coming onto your website, they are all legitimate users, none of them is an attacker. but if you know that, if you accept requests from all of those users, those http requests coming in or those packets coming in, you know that your database will go down. right, you would know that you will not be able to handle those many requests. so now, what would you do? you would want to allow the traffic that could be handled so that your website can still continue to function for the existing set of people. right, this is where you need to vary. you need to be very of the fact on which request to serve and which request to not. third is control consumption cost. so typically, when you consume a request, you would be using some compute power, your ec2 machines, virtual servers- uh, some cash, some clusters, what not- to serve those requests. what you would want to enjoy is if you accept a large number of requests from the user and you are spinning up a lot of infrastructure to process it. that would add to a lot of cost right. a classic example of this is a very expensive machine learning model which you might be running, so you might have exposed ml as a service and there are a lot of users hitting your system and what would happen is every request that comes in requested to spin up a huge gpu machine to do some sort of processing right. but if you accept a large chunk of request, your infrastructure cost will blow up, your aws cost or your aws bill will shoot sky high, and that might be a problem. so which is where, in order to ensure that you are operating well within the limits you would want to ensure that you would? you typically want to ensure that you are accepting the request that could be handled within your acceptable limits. now this- another use case for this uh control consumption cost- is where if you are accepting a lot of request, then and you are, and you have to process it, and that becomes and it might not be worthy enough for you to execute those process or to run those process on your infra. so you might want to, just like you know, and it's okay for me, to drop those requests. i don't just don't want to bloat up my infra right there. you would be there this particular technique comes in handy. and fourth, and the most important one, most important one is to prevent cascading failures. to prevent cascading failures like when it's not just that when a lot of request comes in, only one system is going down. when that particular system goes down, it will have ripple effect, it will have cascading effect over every other system which is connected to it. so if one system goes down, it might take down other systems with it and slowly and steadily, your entire website will go down. so, which is where having a rate limiter at the, at the source point itself, would ensure that the first system is not going down. so it will prevent the gas setting failures from happening right. so now let's look at use cases of throttling and now in this one we will look at five use cases of throttling: two external rate limiters, three internal rate limiters. use case first use case: very common: prevent catastrophic ddos attack. ddos attack are are there, like when your website grows. when your business is popular, website is popular, there will be some attacker trying to abuse your system and you have to be very defensive about it that, no matter how big the attack is, your website is not going down. so which is where what you do is any request that comes in always hits a, always hits a rate limiter first. the rate limiter will ensure that it is only forwarding the request which can be handled by your infra, which is a legitimate request, and only those requests go to a system. request from an attacker will not go. now you say how, how will i identify someone is an attacker? obviously you'll see a surge of request from a particular ip or from a particular user or from a particular access token. you will be preventing, you will be configuring rules in your rate limiter to safely do not accept any more requests from this user, do not accept any more request from this authentication token. and what happens to the surplus request in case of a ddos attack, they are all dropped. they are all dropped the rate. it's the job of the rate limiter to not buffer it, just drop those requests preventing your system from going down or from getting abused. this is an example of an external rate limiter which this rate limiter interfaces right with your users or right with your external public facing inputs. second is to gracefully handle the surge of users. this is similar to an abuse, but dissimilar at the same time. similar with respect to the traffic pattern, dissimilar with respect to these all being legitimate users. so, again, as i said in the use cases, as i said in the previous part, let's say your website went viral. there are a lot of legitimate users coming in. none of them is an attacker, but you have not scaled up your infra. if you get a lot of request, your db would go down and it will take everything with it, which is where you need to be very of the fact that if there is a legitimate search in the users, i need to gracefully handle my system handling those requests. it is okay for me to send to the user key: hey, my website is slowing down. i am expecting too much of traffic, so please come back after some time, rather than saying the website is done like, like, basically, uh, rather than giving them a generic error called 503, service temporarily unavailable. so with this rate limiter, what you would still do is there will be requests of some users that would go through. request of other users might just drop right. this way you are continuing to function for a partial set of users while not continuing to function for other set of users. so at least your business is not completely affected, but partially affected. third use case: now this use case starts for internal rate limiters. now i'll given a very solid example. let's say you are a ci cd company, like, let's say, circle ci, who offers multi-tiered pricing. let's say there is three: tier one, tier two, tier three: because you are a cicd company, people will use your tool to build, uh, their binaries, build their images, run their tests right, build their entire ci cd. so now you have three tiers where you have tier one, which is a free tier, in which you give 200 minutes of build time for free. then you have tier two, in which you get thousand minutes of build time for, let's say, five dollars a month. and the tier three is where you charge customer fifty dollars a month and you give infinite bill time. now here, what you need to do is: this is not something which is external, right? this is something which is internal to your ecosystem. so what? any time any request comes in for the user to build something, it comes to our api server. api server, before triggering the build, what it would do is it would check with the rate laboratory: hey, has this user crossed the limit? is? or, basically in simple term, is this request allowed? has this user used his or her quota? if not, i will go and trigger the build workers for that particular job. otherwise i would not right, and here the build workers will keep on pushing the build state. uh, the build statistics in the database. your rate limiter will use the database to see how much each customer has consumed and depending on that, it would say true or false, or yes or no to a pay server. an api server can then either reject the request or accept the request and continue with the build. so this is a case of an internal rate limiter where you are using this to ration the, to ration the compute capacity to customer, depending on the amount of money that that customer has paid you. so, depending on a multi-tiered pricing that you had, you wanted to ration your compute to that corresponding consumer. fourth use case is you are not over using a third-party system. for example, let's say you are consuming an extremely expensive third-party api and they have on-demand pricing. for example, you are using a service from a third-party provider which is giving you very deep, very amazing, deep learning capabilities, but every single api call is costing you five dollars. if that is the case, what you need to ensure for your cost to not bloat up, for your cost, your infrastructure cost or your bill for that external vendor is not bloating up. you would want to have an internal rate limiter. you like. these are your worker instances. they, before making call to this expensive vendor, they call, they invoke or they check with the rate limiter: hey, should i make a call to this vendor or not? right, because every single call going to this vendor is five dollars every api call. so which is where, because it is very expensive, you need to ensure that, hey, you are operating within the limits, otherwise your infra cost or your bill with the vendor will go up right, which is why- this is another example- your rate limiter is not just external but also internal. and fifth and the final risk is that i want to talk about is to protect an unprotected system. so a classic example of this is hard deleting. we know that hard deleting is hard on the db because it requires a lot of rebalancing. and you know what? not a lot of days, kyo is rebalancing and what not. so now, because your db does not give you rate limiting by default. and if, let's say, you want to delete a million rows- a million rows hard delete onto your database, if you do it in one query, your db will definitely choke up. there will be massive table locks being taken, your db will become unresponsive, your website will go down, which is where to protect your unprotected system, in this case, db. what you would do is you would want to streamline the delete request that are coming in right. so let's say you want to delete a million rows in one shot. instead of deleting it in one shot, you would do like every one or one thousand rows, every two or one thousand or something like that. you would russian or you would want to distribute the load uniformly across a time span. let's say you want to delete a million rows every single day. split it into 24 hours time. do it some number of deletes every minute or some number of deletes every hour. this way, you are not making or you are not bombarding your unprotected system with an expensive call and ensuring that your unpredicted system is not going down again. an example of an internal load: internal rate limiter: right. so rate limiters- again busting them with. rate limiters are not only external, they help you more on the internal cases as well, where you would want to streamline, uh, when you would want to protect your unprotected system, keep your infrastructure cost in check and whatnot. right, nice, i hope you learned something new today. uh, that, basically, that's it for this video. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel a sup. i post three in-depth engineering videos every week and i'll see you in the next one. thanks,