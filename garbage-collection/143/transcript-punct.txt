so we have been told that caching improves performance, but is that totally true? what would happen if we try to apply caching to something like market soup garbage collector? will it improve its performance? today, we understand the key patterns that are essential for our caching to work. if those patterns does not exist, our caching would be ineffective, no matter how large of a cache you apply. the concept we discussed today is something that holds true universally and is not just restricted to the worlds of garbage collector. today, we take an in-depth look into caching, how it works, why it works, and understand why there would not be a significant performance improvement if we apply caching to our markets of garbage collector. but before we move forward, i'd want to talk to you about a code based course, or system design, that i have been running since march 2021, and if you're looking to learn system design from the first principles, this course is for you. yeah, because this is a cohort based course. it will not just be me rambling a semi-optimized solution, thinking it's the most amazing solution out there. instead, it will be a collaborative environment where every single person who is part of the cohort will can pitch in his or her ideas and we will evolve our system around that right. every single problem statement comes with a brainstorming session where we all together brainstorm and evolve our system. that's why everyone understands the kind of trade-offs we made while making that decision. instead of just saying, hey, we'll use a particular queue, we'll have the justification why we use only that queue, why we use that particular database, why sequel? why not no sql? right? how are we leveraging throughput? how are we ensuring that our system scales? that's the highlight of this course. this course is taken by more than 500 engineers to date, spanning nine countries and seven cohorts. right, people from all top companies have taken this course and the outline is very intriguing. it's very exciting. so we start with week one around. we start with the core foundation of the course, where we design online offline indicator. then we try to design our own medium. then we go into database, where we go in depth of database logging and take and see few very amazing examples of data log or database logging in in action and how do we ensure that our system scales through that. then the third week is all about going distributed, where we design our load balancer. i'll walk you through the actual code of a toy load balancer and understand how tcp connections are managed and how simple it is to build load balancer. then week four is about all about social networks. week five is all about building your own storage engines, like we'll build that intuition on. if you were to ever design your storage agent, how would you do that right? then week six is about building high throughput system. seven is about building uh ir systems, basically information retrieval systems and add-on designs where we design our own message brokers, like sqs, where we design distributed task scheduler. and we conclude the course with week 8, where we talk about the super clever algorithms that has powered or that has made those systems possible. right, i have also attached a video, verbatim as is from my first code, where we designed and scaled instagram notifications. i will highly encourage you to check this video out. right? and now back to the video. so as a quick refresher, our market sweep garbage collection algorithm has two phases: the mark phase and the sweep phase. the idea of the mark phase is to start from the root nodes, which contains your global, variable, global thread instances and what not. it starts from there, starts marking the objects that are reachable from them, and once it starts marking the objects reachable from them, these objects are live objects because they are reachable from the root node. the objects that are not reachable from the root nodes are typically something which is garbage which requires a cleanup. so this way the unreachable objects after the mark phase are swept down and hence the phase is called sweep phase, right. so garbage collection is a very important activity. if we do not do garbage collection, our memory usage would or or we would have, or we would have- gigantic memory leaks in our program because they are not getting garbage collected and the memory consumption will, over time, will continuously shoot up. so that's why we need automatic garbage collection and this garbage collection needs to be extremely fast because we do not want the cpu to use its cycle to do garbage collection. we want our garbage collection to take as minimal of a time as possible so that we get enough cpu cycles for our actual business logic. say, if you are building the next facebook, you want cpu to do uh, to basically spend time creating a post, lacking a post, versus doing garbage collection. so we know that uh, mark and sweep is as simple as a dfs problem. so can we use caching to improve the performance, like if? if i would say like: hey, let's improve the performance of my garbage collector. the first thing that would come to your mind would be: hey, can i catch something? can i use caching to improve the performance of it or not? right, so does that work over here? so, in order to understand if caching would improve the performance- and this again it's very- uh, this is a very basically use case agnostic concept. you can apply it at any places. it's here you're trying to establish the key patterns that caches exploit, like, if you are applying caching to a particular use case, it needs to satisfy few patterns. then you would get performance benefit. otherwise, caching is just that extra component that you are adding. so in order to do that, in order to understand that, let's see what exactly is a cache. so a cache is literally anything, anything that stores data so that our future requests are served faster. now, what? what is a cache? basically, it's like ram for disk, it's like l1 cache for your ram. so the cache is something which is extremely fast and it sits very close to cpu, so that you don't have to waste time doing that recomputation again. for example, the two classic things that we know we cache are: first, which is result from previous computation. so if you have already spent time computing something. let's say you spend time computing the blogs of a user. you cache it so that you don't have to fire that expensive sql query again. second is where we copy the data from a slower storage to a faster storage. for example, you know that a user has just logged in. then he or she is very likely to access the recent post that he or she made. so what if? if you go, let's say, this information is present in your database, what if you go to your database and fetch the data? it would take, let's say, 100 milliseconds for you to do it. but if you prefetch it and you keep it in your ram, you can get it, you can serve it in two milliseconds. so caching also means like you can like by caching can also mean that you are, you are keeping or you are pulling the data from a slower storage into a faster storage. right, and here the data that you would be pulling in into your faster storage should be like: issue is likely to be accessed. if it is not likely to be accessed, you are then just wasting, uh, your storage to pull that thing out from a store. slow, so that putting it in the faster storage. but this is what the idea of caching is all about. like, just to give you a perspective on uh from memory onwards the cache. so if, in order for you to extract a value or a range of value from your main memory, if it takes 50 nanoseconds, like reading one value takes 50 nanoseconds, then an l3 cache would do it in 10 nanoseconds and l2 cache can do it in five nanoseconds and l1 cache, which is literally present in your cpu code, can do it in one nanosecond. right, so it's a 50x improvement from your main memory. we think main memory is fast. l3 cache, l2 cache, l1 cache are even faster and obviously they are much, much, much more expensive. and but that's the thing. if you know that something is really needed, instead of putting it in like, instead of you having the data in your main memory, which is 50x lower, you would want to put it in an l3 cache or an l2 cache or an eleven cache right, which should significantly boost your performance. so now comes the crux of it: like, what are we looking at? what should we cache right? when would adding a cache improve the performance of our application? so if our application exhibits this two property, then caching would be effective. the two properties are temporal locality and spatial locality. what temporal locality says is that a recently accessed memory location or data is more likely to be accessed again. which means, let's say, if i have accessed a particular memory location, then i'm more likely to access that location again. then i can put it in a cache. so that is temporal locality. basically, temporal means time related, so it's time related locality. so the idea here is if i'm accessing something, i'm more likely to access it again. for example, if i take example of a traditional web 2 application, it would be a blocking application if i am on my profile and i am more likely to refresh the page, which means that the profile information of my as a user can be cached. so first call can be can take some time, but if i'm more likely to access my own profile, i would be cashing that information in a faster storage so that i can retrieve it faster and send it out. right, that is first. second is spatial locality: spatial around space. so what spatial locality says is that if a location is recently accessed, the locations adjacent to it will be accessed soon, right? so, for example, one thing that works really well with spatial locality is that databases. so on our databases, everything is read as a block, right? so spatial locality. if you know that, if the query that you have fired is a select star query across all the rows, your database engine would know that, hey, now this block is gone, then this block will be read, then this block will be red, then this block will be red. if it knows that a particular block is red, you are very likely to access the adjacent log. your database engine can actually prefetch that block and put it in the cache, so that- and, um, sorry, so that n, so that we don't have to make another disk i o, it can prefetch it. it would give you, it will give your application a huge boost. that is spatial locality. now for caching to work, right, for caching to work, or for caching to not rather work, but for caching to give us a significant performance improvement, it needs to have at least one of these two: temporal locality, spatial locality. otherwise caching would not give you any performance benefit, right? so the way modern hardwares work is something called as hardware free patching or cache prefetching. so the idea here is exactly the same that we talked about, that if we know that a particular location is accessed- the adjacent location is bound to be accessed soon- then i'll just prefetch it and put it into my cache so that the next time- or not next time, so when that particular block is accessed, it is present in your l1 cache or your caching channel, right? so this is how you would get a very significant performance boost in your use case. so how? how does this work? so your cash transparently sits between your main memory, or rather your slower storage, and your access request. so now what would happen is if, let's say, i took a purple block, which is so, let's say, i have six disk blocks, or i have six memory locations, right, a purple- sorry, a blue, a pink and then white. so blue is the one that is recently accessed. so the blue block will grow and sit into my cache, so it is part of my cache. your cache will be much smaller than your main memory, right? and now, when i'm doing that, when the one- the blue block- is getting pulled or is getting accessed, my hardware would know that, hey, the pink block is adjacent to that, very likely to be accessed, so let me prefetch it. so it would prefetch the pink block and put it into cache. so next time when the request comes in for the pink block. that's the first time that the request came in for the pink block. it would go, or it would not have to go, to the main memory to get the data or to the slower storage to get the data. it is already there in the cache, right? so your response time will be much faster. here we are exploiting spatial locality, right, and how the data prefetching works. it works in two ways. the first way is your hardware is intelligent enough though the modern course of uh, intel, amd arm. pick your favorite processor, they they access the memory. sorry, they know the access pattern on how you are accessing the data and they take the smart decision: hey, this data is more likely to be accessed, so let me just prefetch it. right. so hardwares are intelligent enough nowadays to take a call on prefetching. second is most hardware exposes a prefetch instruction which you tell to the hardware: hey, this is something that i know will be required, prefetch it. it would take it and put it in the cache, right, so it like. here we are going into details of hardware: main memory to l1 cache. but this same concept can be applied through your database redis as a cache in normal web to applications as well. so the core idea of caching remains the same, no matter what the application is, your application needs to exhibit- or your use case needs to exhibit- spatial locality or temporal locality for it to leverage caching. otherwise your, if you add cash, no matter how big you uh add cash into, not really you you'd get, you'd not gain a performance boost at all. right, so now can we leverage caching to speed up our garbage collection process? the answer is no. why- and that's the point part- like we always think, hey, caching, yeah, we would get performance benefit now, but with garbage collection that does not happen. so why? that was why that does not happen. because our markets with garbage collection does not exhibit a temporal locality, not a special locality. now let me talk about it. why so to leverage, uh the use case, we should at least have temporal special. we know that. now let's see how temporal locality is not meant. so with temporal locality, what we know is that where the temp uh with mark consume garbage collector, what we do is we have an object in memory, right? so the variable that we created of the class object or the class instance that we created, it would have a mark bit right. so during the mark phase we are starting from the root node, going to all the objects which are reachable from them and setting the marked bit as fun, correct. and that's when this iteration is continuously happening. so here, the object that you set the mark bit as one for for the object that you did, you are never going to access that object again, like immediately. again you would be going through all the objects that are reachable and then you would be invoking the sweep face right. so which means? and even in the sweep base, those objects are never accessed which were marked, because that's where the sweep piece is all about. so with marked bit set s1, the object which are marked as live, they are never accessed again, they never exist again. even if you put it in your cash, they are not getting any significant benefit, not at all right. so that's where adding a cash to your market, to garbage collector, would not solve, because markets of garbage collector is not exhibiting temporal locality. then let's see if it exhibits special locality, spatial locality. you know that with respect to spatial locality, you would know that if you accessed a particular location you are very likely to access adjacent location over here. but if we talk about the way mark and ship garbage collector works, is that what you do is you start with a particular object. you mark that object as life. you find all the objects that are referenced from that object. for example, if i have an object for student, then it might have a field called school, which is a reference to a school object. right? so what i'll do? i'll mark the student object as live. then i'll go to the school object, mark that object as life, correct? so the way this works is that it is doing a random look up on your memory, because school object can be present somewhere. your student object will be present somewhere in the memory. they need not to be adjacent. if they need not be adjacent, then if we try to cache the next object which is sitting next to a student object- that is few times so then you, what you would have to typically do is you would have to typically, uh, prefetch the object which is referenced from that other object, and that is an explicit pre-patching operation, because your hardware cannot smartly do it. it would not know what you are trying to do or what your algorithm is trying to do, right? there are ways to navigate that, but you basically get the idea right. it is not as straightforward. so if your hardware exposes a prefetch, then you might want to preface that other object and then the next object, and then the next one. in any case, that object is red, it's kept in the memory, but you're not getting significant performance out of it. out of the box, you might have to do, you might have to pull some strings on your own, do an explicit prefetch, but it's still not solving the problem out of the box for you, right? so if you're going for a blunt spatial locality where you are caching the next adjacent object, that is definitely not going to work. in case of a marking sub garbage collector, you have to do smart prefetching if you would ever want to optimize that. and that too is not as straightforward as- not as straightforward as we would want it to be. right, because the data is not physically adjacently placed, right, and the lookups are going to be randomly in memory, right, there are ways to optimize it. but you basically get the idea on why special locality would not work. but so now we have established that, hey, caching would not improve the performance by a massive margin. then is there a small, small scope, small scope where, basically, caching would solve the problem or would improve performance, not by a huge margin, but a small margin. obviously there are always, like when you add cash: if you are smart with respect to the access pattern you have, you can still gain some performance out of it. let me walk you through one of such use case. in any programming language that you have, you would have a very skewed distribution. the skewed distribution would be that some objects or some type of objects are low cardinality objects, which means they are fewer in numbers but they are reference large number of times. a simple example of that is type objects like, for example. i am doing this as a very crude example, right, and i will tell you how it actually happens in programming languages. a very good example is that my student type object, like, let's say, i have an object of type student, so it would have attribute called type. it would have id, name, age, school, whatever you put in there, the type would be the type student. that would also be an explicit object that this is type student. so student itself is an object like the type. student itself will be an object, right, which will have the functions that it holds and other information, and this object is like this object of type student will be associated to every student object. let's say i have 100 student, all of them would have a type attribute. all of them will point to the student type object that i have right. so this is a low cardinality, highly referenced object because student type will be there across all the students that i have. so what if we cache the type objects, low cardinality, highly reference thing, we cache that right. this is a place like if you cache the, if you catch this particular thing, if you catch the type objects, you will still get a significant performance book because it would not have to go time and read that particular location, it can just access it immediately. but this is a very crude example in real programming languages, let's say c python source code, or if you explore basically chromeva java engine, a chrome v8 javascript engine, what you will find is that how does your python runtime know that a is an integer, b is a string? like you created a equal to 10, b equal to hello, how would it know that a is an integer, b is a string? it has a very similar structure to this, very similar, okay. so that's where the data types of your languages, the type objects for your data type in your languages can be cached. through this you will get not significant but a minor performance boost, right, and this is how you would try to gain as much of performance for your garbage crusher for your use case as possible. and it totally depends on if your application actually exhibits temporal locality or spatial locality. it needs at least one of them, at least one of them- for your caching to be effective, otherwise you are just adding caching unnecessarily, right? okay, nice, that's it. that's it for this one. i hope, uh, you understand caching very well now, like around temporal locality, special locality. now, whenever you are designing anything, not just garbage collector, be any application out there, uh, web to application, web 3 application, like all the fancy things like social media, live streaming, what not. and if you plan to add cash over that, just see, do you have at least temporal locality or special locality? if you have it, then you cache it. otherwise it's very futile. if you add something to the cache, eventually it is neither never going to be accessed or, if it is going to be accessed, after a very long time and it is already evicted. so you are just wasting your cpu cycles into doing something which is not fruitful, right? so, yeah, just keep those things in mind whenever you are designing your next big system. so yeah, that's it. that's it for this one. uh, uh. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel a sub. i post three in-depth engineering videos every week, and i'll see you in the next one. thanks.