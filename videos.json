[
  {
    "id": 167,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "6_yFb7icd_c",
    "title": "Conflict Resolution in Hash Tables with Open Addressing",
    "description": "Although chaining is a popular way of handling Hash Table Collisions, there is a very interesting way of achieving the same and it is called Open Addressing. The key highlight of Open Addressing is that it does not require any additional data structure to hold the collided keys, making them space efficient.\n\nIn this video, we look at Open Addressing, the core idea behind it, lay the foundation for probing functions and understand how the implementation of our core Hash Table functions changes with this scheme in place.\n\nOutline:\n\n00:00 Agenda\n02:35 Introduction\n03:06 Open Addressing\n04:15 Probing Functions\n08:45 Hash Table Operations",
    "img": "https://i.ytimg.com/vi/6_yFb7icd_c/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Q2gnin2fe9c2uLIA5L/giphy.gif",
    "duration": "18:6",
    "view_count": 204,
    "like_count": 12,
    "comment_count": 5,
    "released_at": "2022-07-15",
    "gist": "",
    "notes_gd": "https://drive.google.com/file/d/1bvdtGMKVou-bfuOHzX3izdx2FHfQTpYS/view?usp=sharing",
    "slug": "conflict-resolution-in-hash-tables-with-open-addressing"
  },
  {
    "id": 166,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "9rb8oILi4lU",
    "title": "Conflict Resolution in Hash Tables with Chaining",
    "description": "Collisions happen in Hash Tables as we are trying to map a huge space of application keys in a small array. But there are ways to solve it and one of the most common ways is called Chaining.\n\nIn this video, we go in-depth about collisions in Hash Tables, resolve them through chaining, explore some really granular details that would help us squeeze the best performance out of our implementation, and most importantly look at a different data structure to implement it.\n\nOutline:\n\n00:00 Agenda\n02:29 Introduction\n03:42 Chaining\n04:58 Chaining with Linked List\n08:03 Adding a key in the Hash Table\n14:04 Deleting a key from the Hash Table\n15:47 Lookup a key in the Hash Table\n17:15 Chaining with Binary Search Trees",
    "img": "https://i.ytimg.com/vi/9rb8oILi4lU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/vciXulg7fZGvo9p01m/giphy.gif",
    "duration": "20:55",
    "view_count": 358,
    "like_count": 26,
    "comment_count": 4,
    "released_at": "2022-07-13",
    "gist": "Collisions are inevitable in Hash Tables, and a common way of handling them is through Chaining using Linked List. But can we use some other data structure?\n\nCollisions are inevitable in Hash Tables as we are mapping a large range of application keys on a smaller range of array slots. So, there are chances that the hash function spits out the same value for two different application keys.\n\nBecause Hash Tables cannot be lossy, we need a way to handle these collisions and allow storing of multiple keys that are hashed to the same slot. A way to achieve this is Chaining and it is very commonly implemented through a Linked List.\n\n## Data Structures\n\nTo accommodate this, the Hash Tables are now implemented as an array of Linked List where all the keys that collide are placed.\n\n### Adding a Key\n\nWhile adding a key to the hash table, we first pass it through the hash function and get the slot index. We then create a node holding the key and add it to the linked list.\n\nWe can add the new node at one of the 3 places\n\n- always at the head of the list - O(1)\n- always at the tail of the list - O(1)\n- in the middle of the list while maintaining a sort order - O(p)\n\n### Deleting a Key\n\nTo delete a key, we first pass it through the hash function and get the slot index. We then iterate through the linked list present at the slot, element by element, and locate our key of interest.\n\nWhile iterating, we keep track of the pointer to the previous node so that once we reach the node to be deleted, we adjust the pointers and free up the node.\n\n### Key Lookup\n\nKey lookups are similar to delete operations. We first pass the key through the hash function to get the slot index. We then iterate through the list present at the slot and locate our key. The operation requires us to iterate the list iteratively.\n\n## Other Data structures for chaining\n\nLinked List is not the only data structure that we have to use to chain the collided keys. Depending on the use case, access pattern, and constraint we can pick a data structure that suits us.\n\nFor example, if our array is small and we cannot resize it then we may end up having a large number of collisions. If we are trying to read, then iterating over this list will reduce the throughput as it is a linear scan.\n\nTo optimally perform a key lookup, when the collisions are high, we can use a self-balancing search tree, like BST or Red-Black. This way, we get an optimal lookup performance on keys hashed to the same slot.",
    "notes_gd": "https://drive.google.com/file/d/1so447uSDzZZuJXAyMB_jGqHCM0RX3hbL/view?usp=sharing",
    "slug": "conflict-resolution-in-hash-tables-with-chaining"
  },
  {
    "id": 165,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "jjW8w8ED3Ns",
    "title": "Internal Structure of a Hash Table",
    "description": "One of the most common data structures that we all use every single day is Hash Table. Every language has its own implementation and nomenclature for it. Python calls it dictionary, while Java calls it Hash Map. But the core idea remains the same: it holds pairs of keys and values and supports insertions, updation, and lookups in constant time.\n\nBut how are they implemented? What is its internal structure? In this video, we talk about what are hash tables, how are they structured internally, and lay a foundation to understand their constant-time implementation.\n\nOutline:\n\n00:00 Agenda\n02:38 Introduction and Applications of Hash Tables\n05:12 Core ideas to construct Hash Tables\n07:07 Step 1: Application keys to Integer Hash Keys\n09:38 Naive Implementation of Hash Table using Array\n13:38 Step 2: Integer Hash Keys to a smaller range\n17:43 Adding more keys on the fly\n19:07 Do we really need the Keys to Hash Key step?",
    "img": "https://i.ytimg.com/vi/jjW8w8ED3Ns/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/8OYw2Tm8gnrGw/giphy.gif",
    "duration": "23:12",
    "view_count": 2712,
    "like_count": 97,
    "comment_count": 10,
    "released_at": "2022-07-11",
    "gist": "Hash Tables are implemented through simple arrays, but how?\n\nHash Tables are so powerful, that OOP based languages internally uses it to power Classes and sites members. Symbol tables that holds the variables mapped to memory location are also powered though hash tables.\n\nThey are designed to provide constant-time key-based insertion, updation, and lookups while being space efficient at all times.\n\n## Core Ideas to construct Hash Tables\n\n- convert application keys to wide-ranged (`INT32`) hash keys\n- onovert hash keys to a smaller range\n\n## Application Keys to Hash Keys\n\nHash Tables should support storing any object as key and to power that the keys are first hashed to a big integer range (provided by user) typically `INT32`. This hash key is then further used to decide how and where the KV pair would be stored in the data structure.\n\n## Naive Implementation\n\nA naive implementation of Hash Table would be to create an array of length INT32. To store the KV in it, we pass the key through the hash function spitting out an integer. We use this key and store the KV pair at this index in the array.\n\nAlthough this would give us constant time insertion, updation, and lookups, but it is highly space in-efficient, as we would need to allocate at least `4` * `INT32` = `16GB` of ram to just hold this array, with most of the slots left empty.\n\n### Hash Keys to Smaller Range\n\nThis step is designed and introduced to make our Hash Table space efficient. Instead of having a huge array of length `INT32`, we keep it proportional the the number of keys inserted. For example, if we inserted 4 keys then our holding array could be around 8 slots big.\n\nTo achieve this, we map the hash key into a small range (same as the length of the array) and place our key at that very index. This allows us to remain space-efficient while sporting fast and efficient insertions, updations, and lookups.\n\n## Adding more keys\n\nThe small limited sized array will not be able to hold large number of keys and hence after a certain stage we would need a larger array to hold the data. This is done by resizing the holding array and is typically made 2x every time it is full enough.\n\nThus, this two step implementation allows to power near constant time insertions, updations, and lookups while remaining space efficient.",
    "notes_gd": "https://drive.google.com/file/d/1IKYDzO-mZEHDQYEsyoksFkF3DVyTCy6l/view?usp=sharing",
    "slug": "internal-structure-of-a-hash-table"
  },
  {
    "id": 164,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "LeT_s-UFw-U",
    "title": "Dissecting GitHub Outage and things to do after that",
    "description": "Outages happen and in such a tense situation, the main priority is to get the system back up, but is that it? Is everything done when the service is up and running?\n\nSo today let's spend some time talking about the aftermath of an outage. There are many things to take care of once the outage is mitigated and in this video, we dissect a GitHub outage, understand what happened, and look at a set of common practices that we follow to ensure closure.\n\nOutline:\n\n00:00 Agenda\n02:31 The outage and mitigation\n07:23 Solve data inconsistency\n09:51 Invalidate the cache\n11:03 Setup alerting\n13:53 Take preventive measures\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/LeT_s-UFw-U/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6MbhbYBsqTrbP2qQ/giphy.gif",
    "duration": "16:24",
    "view_count": 401,
    "like_count": 22,
    "comment_count": 5,
    "released_at": "2022-07-08",
    "gist": "Getting the service up is P0 during the outage, but that is not all. There are a few other things that we need to take care of once the issue is mitigated.\n\n## Resolve Inconcistencies\n\nDuring the outage, the business logic or database must have crashed while handling in-flight requests; hence there is a very high chance of the data becoming inconsistent.\n\nFor example: if the process crashed while transferring money from one account to another; it is possible that money got deducted from one account but did not credit to another.\n\nHence once the outage is mitigated, the first thing to be done is to ensure that the data does not remain in an inconsistent state. Achieving this depends on the usecase at hand and the tech stack involved.\n\n## Invalidate the Cache\n\nAnother place that needs attention is cache invalidation. If the outage sustains for a long time, there is a chance that some of the entries in the cache are not valid anymore.\n\nHence, depending on the usecase, it is advised to go through the cache entries and delete the invalid once ensuring the end-user sees a globally consistent view of the data.\n\n## Audit alerting and monitoring\n\nOnce the outage is mitigated and the inconsistencies are resolved, it is better if we take a look into existing alerting and monitoring practices so that we ensure that we get alerted at the right time.\n\nIn most cases, you would find that a few things could always be improved when it comes to monitoring the right things, and hence an outage is an opportunity to get alerting and monitoring improvements prioritized.\n\n## Take preventive measures\n\nEvery outage has a root cause, and it is super-important to fix the root cause to ensure that the over never happens again for the same reason. This requires us to go in-depth and take preventive measures to ensure complete closure.\n\nFor example, re-auditing all the INT32 columns and moving them to INT64 to ensure that we never repeat the outage due to integer overflow.",
    "notes_gd": "https://drive.google.com/file/d/1pDHYlKlQsTB_oDvqG9mpsvfKkHWDnkRG/view?usp=sharing",
    "slug": "dissecting-github-outage-and-things-to-do-after-that"
  },
  {
    "id": 163,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "Of3FS2qDM28",
    "title": "Dissecting GitHub Outage - Why should we localize failures?",
    "description": "Outages are inevitable; but we should design our architecture such that if a component is down, it should not lead to a complete outage. It is easy to say this, but hard to implement.\n\nIn this video, we dissect yet another GitHub outage, gather a few interesting insights about their Microservices architecture, and spend some time discussing and understanding the importance of having a smaller blast radius and a fool-proof way of achieving that.\n\nOutline:\n\n00:00 Agenda\n02:36 What happened\n04:30 Root Cause\n05:14 Insights about their architecture\n08:50 Master failover did not happen\n13:40 Long-term fixes and Localizing failures\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/Of3FS2qDM28/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oEduUNILoKY1sciqI/giphy.gif",
    "duration": "20:33",
    "view_count": 381,
    "like_count": 29,
    "comment_count": 4,
    "released_at": "2022-07-06",
    "gist": "Outages are inevitable; but we should design our architecture and ensure that if a component is down, it should not lead to a complete outage.\n\n## What happened with GitHub?\n\nGitHub saw a lot of failures with their Actions service and this led to delays in queued jobs from being processed. The root cause was some infrastructure error in the SQL layer.\n\n## Insights about their architecture\n\nA couple of insights about their architecture\n\n### Synchronous dependency\n\nAlthough GitHub Actions look like a single feature, internally it consists of multiple microservices. Some of these, have a synchronous dependency on the database. Because of this, when the DB had a hiccup, the entire Actions feature was hindered.\n\n### Zero trust communication\n\nThe service that was most affected in this outage handled communication; why would services need authentication? they are all internal the infrastructure?\n\nMicroservices talk. The communication needs to be protected with auth so that any engineer/service gone rogue cannot abuse the system in any capacity. Only authenticated and authorized services are allowed to take action.\n\n## What about automatic failover?\n\nGiven that the outage happened on the database layer, why did the database did not auto-recover? it is a standard procedure and configuration that would have just promoted a replica to be the new master.\n\nAlthough it is a common config, during this outage the metrics did not show any issue with the database, and hence the auto-failover was never triggered. It took a long time to even understand the root cause and then start mitigation.\n\n## Long-Term Fixes\n\n### Update the automation scripts\n\nThe automation that reads the telemetry and decides to do a failover needs to be updated so that such failures are detected and action is taken.\n\n### Localizing failures\n\nAn important long-term change that needs to be driven is to localize the failure. In this outage, we learned how a hiccup in one database/service causes downtime of all dependent Microservices. This shouldn't have happened as the Microservices are supposed to solve this very problem.\n\nA good way to ensure that the blast radius of the outage is minima; is by ensuring the failures are localized, implying, that when a service is down, only the service is affected while everything else is functioning perfectly fine.\n\nA common approach to getting this loose coupling is by powering inter-service communication through the asynchronous medium instead of synchronous API calls. Thus, if something breaks, we could fix it and continue to process the messages.",
    "notes_gd": "https://drive.google.com/file/d/1V_nuSi4KDsuTD_p0beIWoBDmRN9T5d5V/view?usp=sharing",
    "slug": "dissecting-github-outage-why-should-we-localize-failures"
  },
  {
    "id": 162,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "GCx0aouuEkU",
    "title": "Backend for Frontend Pattern in Microservices",
    "description": "As your application evolves, supporting multiple types of clients like Desktop, Mobile apps, etc becomes tricky. The backend becomes complicated, and you start applying a  lot of hacks to serve them properly.\n\nIn this video, we take a look at an interesting high-level architectural pattern in microservices, that solves this exact problem, called Backend For Frontend, fondly called BFF. We would see in detail what this pattern is all about, where it is used, how to implement it, look at its advantages and disadvantages, and conclude by building an understanding of when to use it.\n\nOutline:\n\n00:00 Agenda\n02:47 Supporting multiple clients\n10:05 Backend for Frontend Pattern\n16:27 Advantages of BFF\n21:25 Disadvantages and challenges with BFF\n24:35 When to use BFF",
    "img": "https://i.ytimg.com/vi/GCx0aouuEkU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/d9WRhx9qErqCY/giphy.gif",
    "duration": "29:2",
    "view_count": 1877,
    "like_count": 96,
    "comment_count": 11,
    "released_at": "2022-07-04",
    "gist": "Say, we are launching an e-commerce website that people can use from their desktop and place orders. We want to render the product details that include - name, description, sellers, variants, reviews, and faqs. The site did well and now we decide to launch a mobile app.\n\nGiven that the mobile has limited real estate, it is very hard to render the same information that we do on the Web. Hence, we choose not to render Reviews and FAQs. Thus even if we receive the reviews and faqs in the response, while rendering we choose not to render them.\n\nThis is a waste of user bandwidth, data, and processing power. Ideally, we should not be sending unnecessary fields from the backend. So, how do we implement this?\n\n## Backend for Frontend\n\nBFF is a layer that sits between the clients and the backend. Every type of client has a dedicated BFF; for exampDesktop Web has its own BFF while Mobile has its own.\n\nDepending on the request, the BFF then talks to the backend, grabs the data, filters out the unnecessary fields, and responds. It can also optionally transform the data in a client-specific format.\n\nThis way, we keep the backend simple and apply all presentation-level hacks and tweaks on BFF.\n\n## BFF and Microservices\n\nBFF acts as a perfect abstraction for underlying microservices. For an API, it can connect to necessary microservices, gather the responses, and respond. This ensures that we are not fetching data that we don't need.\n\nFor example, Given compatibility and constraints, a Desktop BFF can talk to Orders, Sellers, and Reviews Services, while a Mobile BFF can talk to Orders, Sellers, and AR services to respond to the same API endpoint.\n\n## Advantages\n\n- we can add client-specific tweaks and hacks on BFF\n- we can hide sensitive information from specific clients\n- we can patch client-specific vulnerabilities on respective BFF\n- we can pick the best communication stack for the client and BFFs\n- we can have a general-purpose backend supporting all kinds of clients\n\n## Disadvantages\n\n- a large chunk of code would be duplicated\n- needs to support high fan-out, hence pick a stack that suits\n- there is a slight increase in latency with the new network hop\n- by adding BFFs we are adding more moving parts that need to be managed, maintained, monitored, and deployed\n\n## Adopting BFF\n\nWe should adopt BFF when\n\n- the interface between different clients varies significantly\n- the communication format/protocol is different from what your backend supports (eg: legacy integration might need XML while your backend serves JSON)",
    "notes_gd": "https://drive.google.com/file/d/19qxDMRlcABJ466roqny6x5ktFY7Gq5JU/view?usp=sharing",
    "slug": "backend-for-frontend-pattern-in-microservices"
  },
  {
    "id": 161,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "bycFzB6yrK0",
    "title": "Dissecting GitHub Outage - Multiple Leaders in Zookeeper Cluster",
    "description": "Distributed Systems are prone to problems that seem very obscure. GitHub had an outage because a set of nodes in the Zookeeper cluster ran an election and elected a second leader. How obscure is that?\n\nIn this video, we dissect a GitHub outage that would tell us how weird is the world of Distributed Systems. We also talk about Zookeeper and its importance for managing a Kafka cluster. We see what happened during the outage, and how GitHub was able to mitigate it with ZERO data loss and a brilliant fallback strategy.\n\nOutline:\n\n00:00 Agenda\n02:37 Zookeeper and Kafka\n05:27 What happened\n06:25 Bootstrapping Zookeeper Node and Leader Election\n09:06 Broker connecting to the new cluster\n14:05 Recovery\n15:55 Dead Letter Queue and Fallback Strategy\n18:07 Key Learnings\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/bycFzB6yrK0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l4pTi9xLGNINzfyXm/giphy.gif",
    "duration": "21:9",
    "view_count": 926,
    "like_count": 50,
    "comment_count": 13,
    "released_at": "2022-07-01",
    "gist": "The split-brain problem in Distributed Systems is not theoretical. GitHub had an outage because their Zookeeper cluster ended up having two leaders, leading to writes to Kafka failing.\n\nZookeeper is an essential component for Kafka as the clients connect to get information about the brokers and cluster internally using it to manage its state.\n\n## What happened?\n\nDuring scheduled maintenance the Zookeeper nodes were getting upgraded/patched and during this time, many new nodes were added \"too quickly\" to the zookeeper cluster.\n\nWith a lot of new nodes added \"too quickly\", they were unable to self-discover or understand the topology and the way the bootstrap code of Zookeeper is written, they thought the cluster was leaderless. This made them trigger a Leader Election.\n\nGiven that a lot of new nodes were added, they formed the majority and elected a new leader. These nodes formed a logical second cluster operating independently. Thus the cluster is now having a split-leadership problem.\n\n## Broker Connected\n\nOne of the Kafka broker (node) connected to the second cluster and found that no other nodes are present (because second zookeeper cluster had no entries) and hence elected itself as the controller.\n\nWhen the Kafka clients (producers) were trying to connect to the Kafka cluster they got conflicting information which led to the 10% of writes failing.\n\nIf the number of brokers connecting to the new cluster were more, then it could have led to even data consistency issues. But because only one node connected, the impact was minimal.\n\n## Recovery\n\nThe zookeeper cluster would have auto-healed but it would have taken a long time to converge, and hence a quick way to fix this is to manually update the zookeeper entries and configure it to have a single leader.\n\nTo keep things clean, the nodes that were part of second zookeeper cluster could have been deleted as well.\n\n## Ensuring zero data loss\n\nEven though 10% of write requests failed, why did it not lead to a data loss? the secret is Dead Letter Queue.\n\nIt is a very standard architectural pattern that ensures zero data loss even when the message broker (queue) crashes. The idea is to have a secondary queuing system that you can push messages to if the write on the primary fails.\n\nAll the messages that client tried to write to Kafka failed, but they were persisted in DLQ which they processed later.\n\n## Key Learnings\n\n- Make consumers idempotent\n- Automate cluster provisioning\n- Have a DLQ for all critical queueing systems\n- Have retries with exponential back-offs on clients",
    "notes_gd": "https://drive.google.com/file/d/1ut8trVZ5IF4hB6amfKpZSzPJl_8eRiCk/view?usp=sharing",
    "slug": "dissecting-github-outage-multiple-leaders-in-zookeeper-cluster"
  },
  {
    "id": 160,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "4mVJQJbw6Vw",
    "title": "GitHub Outage  - How databases are managed in production",
    "description": "So, how are databases managed in production? When the master goes down, how a replica is chosen and promoted to be the new master? Is this a manual step or something that can be automated?\n\nIn this video, we dissect yet another GitHub outage and apart from understanding what happened, we would spend a significant amount of time learning how databases are managed in production, and the kind of tools companies use to ensure optimal DB performance and High Availability.\n\nOutline:\n\n00:00 Agenda\n02:39 What happened?\n03:20 ProxySQL and why it is used in production?\n09:14 Orchestrator and what it does?\n14:12 Anti-flapping policy of orchestrator\n18:10 Root cause and mitigation\n19:46 Key takeaways\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/4mVJQJbw6Vw/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/1vZcbntQ1jVcSA78gw/giphy.gif",
    "duration": "23:41",
    "view_count": 979,
    "like_count": 73,
    "comment_count": 13,
    "released_at": "2022-06-29",
    "gist": "Managing databases in production is not easy and it does require a lot of tooling to keep it up and running. GitHub had an outage that gives us a glimpse of the toolings they use in production. So, here's what happened\n\n## Incident Summary\n\nThe GitHub team updated the ProxySQL and a week later a master node in a critical database cluster crashed. A replica was promoted as the new master, and it also crashed. The team tried to manually recover, but the manually promoted replica also crashed.\n\nFinally, the GitHub team did a full revert - reverted all the code changes, and also downgraded the ProxySQL version. Post this, things become normal and the master did not crash.\n\n## ProxySQL\n\nProxySQL is a database proxy that sits between the servers and the database cluster. The server seamlessly connects to the ProxySQL and fires the usual SQL queries and the proxy forwards it to the data node as per the configuration.\n\n### Why do we need ProxySQL\n\n- better connection management\n- as a cache for SQL query responses\n- a gatekeeper to handle security and routing\n\n## Orchestrator\n\nOrchestrator is a MySQL topology management tool that helps us achieve High Availability on a MySQL cluster. It keeps an eye on all the nodes and takes corrective actions whenever something goes wrong.\n\nWe configure Orchestrator to keep an eye on the master node and as the node crashers, it promotes a replica to become the new master. Given that all of this happens automatically, it just takes a few seconds for the cluster to recover from the master crash.\n\n### Anti-flapping policy\n\nA very common cascading failure happens when the master fails and a replica is promoted to be the new master. Due to the high load, say the new master also crashed. The cycle thus continues until all nodes crash leading to a complete outage.\n\nThe anti-flapping policy of Orchestrator prevents this complete outage by not promoting replica to master until the cool-off period ends. Once the replica is promoted to be the new master, Orchestrator does not promote another replica until the cool-off period ends.\n\nAlthough the master is down, this anti-flapping policy ensures that we are at least partially functional and can continue serving some reads. Along with this, we see only a small subset of nodes are thrown in the fire and hence have fewer data nodes to recover.\n\n## Mitigation\n\nTo mitigate the issue, the GitHub team\n\n- reverted the ProxySQL version\n- reverted the code that required an upgraded version of ProxySQL\n\nWith this full revert, the master node stopped crashing and things become normal again.",
    "notes_gd": "https://drive.google.com/file/d/1Fqad5kpmlvrNMFXSojlOvBwOAOu7c2nd/view?usp=sharing",
    "slug": "github-outage-how-databases-are-managed-in-production"
  },
  {
    "id": 159,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "Wby0d9Li5Hw",
    "title": "Best practices that make microservices integration easy",
    "description": "Running microservices in isolation does not make any sense. To get something done, multiple microservices need to talk to each other and put a task to completion. This requires the services to interface with each other.\n\nHow would the services interface and integrate? is there a common way to do it?\n\nIn this video, we talk about 4 best practices we should follow while designing microservices to encourage inter-service integration. These practices would help us keep interfacing simple, easy, and intuitive.\n\nOutline:\n\n00:00 Agenda\n02:36 Introduction\n03:00 Backward and Forward Compatability\n06:29 Make API interface tech agnostic\n09:01 Dead simple consumption\n11:00 Hide internal implementation details",
    "img": "https://i.ytimg.com/vi/Wby0d9Li5Hw/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/BZhvKu7MT0n2voRhtf/giphy.gif",
    "duration": "14:8",
    "view_count": 713,
    "like_count": 43,
    "comment_count": 7,
    "released_at": "2022-06-27",
    "gist": "Running Microservices in isolation does not make any sense; it is natural for them to work together and solve a bigger problem. This would require each microservice to expose a well-defined API interface simplifying others to talk to it.\n\nThe following are the best practices that we should follow that would make interfacing and integration easy.\n\n## Forward and Backward Compatability\n\nWhile rolling out any changes in a microservice, we need to ensure they are both forward and backward compatible. If not, it would break the consumers or interfacing microservices.\n\nThree key places where we need to be extra careful are\n\n- while changing the database schema\n- while changing the API response body\n- while changing the message format in async communication\n\nWe can ensure forward and backward compatibility if we\n\n- never abruptly delete any column/attribute\n- never abruptly change the type of the column/attribute\n\nWe should always roll out breaking changes in phases ensuring the dependent services remain unaffected.\n\n## Make APIs tech-agnostic\n\nTech evolves quickly in the world of software engineering, and hence we would always feel like using the new shiny thing available. While having that urge, we should always ensure we are not picking the technology that would induce tight coupling.\n\nFor example, we should not pick a framework that would require the interfacing services to be written in a particular language or require them to use a specific tech stack. This would take away autonomity from the interfacing services as we are dictating which stack to use.\n\n## Dead simple consumption\n\nMicroservices are built to interact with other services and get things done. So, the core focus should be to make things super simple for anyone to integrate.\n\nIt does not matter how good your LLD is if the API interface is hard to integrate. Be consumer-centric while desinging the interface of a microservice and ensure you have\n\n- simple API\n- simple data format\n- use common protocols\n\n## Hide internal implementation details\n\nNever let other microservice learn about the internal implementation detail of your service. If they interact using internal details this would create a tight coupling between the two services.\n\nInternal details could be\n\n- broker for internal communication\n- building dependency on transitive dependencies\n- allowing directly connecting to the private database\n\nIt is always safe to hide the internal implementation details and expose a strict interface to interact with the service. The interface could be REST, gRPC, or anything that your org uses.",
    "notes_gd": "https://drive.google.com/file/d/1vEur6ubY3ro_7C0JKEjs93ClboPJIMh5/view?usp=sharing",
    "slug": "best-practices-that-make-microservices-integration-easy"
  },
  {
    "id": 158,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "VPZo8cO1HbI",
    "title": "Dissecting GitHub Outage - Downtime due to Rate Limiter",
    "description": "Rate limiters are supposed to avoid downtimes, but have you ever heard that a rate limiter caused a downtime? This happened with GitHub, where a big chunk of their users saw elevated error rates.\n\nIn this quick incident dissection, let's take a look at a high-level overview of how GitHub does their A/B experiments, how a low-level decision led to this incident for a large chunk of users, and conclude with some key things we all should learn from this outage.\n\nOutline:\n\n00:00 Agenda\n02:32 What happened?\n02:47 What is A/B Experimentation?\n05:33 A/B Experimentation at GitHub and what failed\n11:12 Mitigation and Long-term Fix\n12:04 Key Takeaways\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/VPZo8cO1HbI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/5XIH3sxd50onu/giphy.gif",
    "duration": "16:11",
    "view_count": 912,
    "like_count": 44,
    "comment_count": 9,
    "released_at": "2022-06-24",
    "gist": "Rate Limiters are supposed to avoid downtimes, but what if they turn out to be the root cause of a major outage?\n\nA large chunk of GitHub users saw elevated error rates and this happened after deploying their A/B Experimentation service. So, what went wrong? but before that let's understand what is A/B experimentation\n\n## What is A/B Experimentation?\n\nIt is hard to decide which UI is better and hence before rolling out any critical UI change to all the users, a company tests it through an A/B experiment.\n\nA set of users are chosen at random and a fraction of them are shown the new variation while others are shown the old one. Key vitals and metrics of the features are measured and compared to decide if the variation is indeed an improvement.\n\nIf the metrics are positive and significantly better then the new variation is rolled out to 100% of the users. This way companies ensure that the features that are rolled out are genuine improvements in the product.\n\n## A/B Testing at GitHub\n\nEvery server that needs to participate in any A/B experiment fetches a configuration file that is dynamically generated using, say, Config Generator service.\n\nThe configuration allows granular controls for the A/B experiment and holds critical information that shapes experimentation. When any server requests for a config file, the request hits the config service and it, in turn, generates the file and sends it back to the user.\n\n## What failed?\n\nBecause a lot of requests were made to the Configuration Service, the rate limiting module of the service started throttling and it prevented the configuration file to be generated and sent to the servers.\n\nThis affected the users who were part of this experiment and they saw elevated error rates as the frontend did not have the necessary information it required to power the experiment.\n\n## Mitigation and Long-term Fix\n\nAs quick mitigation, the GitHub team disabled the dependency on the dynamically generated file and it restored the services to normal.\n\nTo ensure the outage would not happen due to the same reason, the Config Generator service would generate and cache the configuration files so that when a request comes, the file could be served directly from the cache instead of generating on the fly which was time consuming.\n\n## Key Takeaways\n\n- avoid sync dependencies between services and prefer async\n- classify the services by severity tiers and run periodic audits of tier-1 services to ensure they are architected well and there are no blindspots",
    "notes_gd": "https://drive.google.com/file/d/17xmYQ9tXQfhMUc85uYfkA880VH1ANEKH/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-rate-limiter"
  },
  {
    "id": 157,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "ZirBDq1JwpY",
    "title": "Dissecting GitHub Outage - Master failover failed",
    "description": "Companies announce their planned maintenance, what happens during that? Could something go wrong while running maintenance?\n\nGitHub team was switching their Master databases from one node to another; while doing this something went wrong and the new database crashed. This led to data divergence and a production incident that lasted over 5 hours.\n\nIn this video, we dissect this incident and understand what happens during planned maintenance, what went wrong with GitHub, how GitHub mitigated it, and understand some really cool things about switching databases and solving data divergence.\n\nOutline:\n\n00:00 Agenda\n02:42 What happened?\n03:29 Scaling reads with Read Replicas\n04:40 Planned Database Maintenance\n10:08 Database crashed and quick mitigation\n11:44 Data Divergence between two masters\n13:54 Remediating Data Divergence\n18:23 Read Replica taking time to spin up\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/ZirBDq1JwpY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/ZsVhGBa2QB7U31BnPo/giphy.gif",
    "duration": "22:30",
    "view_count": 782,
    "like_count": 30,
    "comment_count": 16,
    "released_at": "2022-06-22",
    "gist": "Master failover failed for GitHub leading to a 5-hour long incident, let's see what happened.\n\n## Incident Summary\n\nFor five hours, GitHub users observed delays in data being visible on the interface and API after it was written on the database. This happened during the maintenance when they were switching the Master DB.\n\n## Planned Maintenance\n\nPlanned maintenance is a popular way for companies to take a small downtime and execute all the maintenance activities. Some activities for which we do plan database maintenance are\n\n- applying security patches\n- apply version upgrades\n- parameter tuning\n- hardware replacement\n- periodic reboots\n\nA popular activity during database maintenance is to switch the Master node i.e. shift the traffic coming from the master node to a new node so that we could patch the old instance.\n\nFor a very short duration, when the config switch is happening, the database would be unavailable leading to a small outage; and this is expected behavior.\n\n## Database Crash\n\nDuring the failover, when the traffic was moved to the new database, the `mysqld` process crashed. This led to incoming writes failing. To quickly mitigate the issue, the team moved the traffic to the old database. This solved the issue and the site was up and running.\n\n## Something interesting happened\n\nThe new database before crashing served the write traffic for 6 seconds. So, after the crash when the traffic was redirected to the old database, it did not have the data that was written in that 6 seconds window.\n\nThis is a huge concern, as it would lead to bad UX, and in the worst case consistency failures. So, how to remediate this issue?\n\n## Remediating master failovers\n\nIn order to remediate this, we take the help of the Write Ahead Log or Commit log of the database. Whenever we do a failover, we always keep track of the `BINLOG` coordinates.\n\nOnce we moved the traffic to the old database, all we have to do is iterate through the BINLOG and apply all the changes that happened on the new database post the noted coordinate on the old database.\n\nThis would re-create or modify the exact data that was written to the new database on the old database, leading to zero data loss or consistency breach.\n\n## Cleaning up the mess\n\nTypically when we have such a failover, it is better that we restore the read replicas and hence GitHub team rotated all the replicas. Creating a read replica takes time, given the scale of GitHub.\n\nIt took them 4 hours to set up replicas and 1 hour to re-configure the cluster hence for over 5 hours the incident was affecting the users.",
    "notes_gd": "https://drive.google.com/file/d/1uzicMXBufPqkPTzrxyQOS6zn1x6WURwi/view?usp=sharing",
    "slug": "dissecting-github-outage-master-failover-failed"
  },
  {
    "id": 156,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "xbtE9IF4yRc",
    "title": "Things to remember while building Microservices",
    "description": "An engineer working on Microservices should not only just focus on engineering; there are so many other aspects to look at that holds the potential to create massive impact. To be honest, Building microservices is much more than an engineering problem to solve. There are many pitfalls to avoid and decisions to make and build a solid functional high-performing team in the process.\n\nIn this video, we look at some engineering and non-engineering things to remember while building Microservices, like it is a massive Growth Opportunity, everyone should be okay with architecture evolution, the importance of periodically addressing Technical Debt, and striking the right balance to keep engineering and business aligned to the same strategic goals.\n\nOutline:\n\n00:00 Agenda\n02:53 Massive scope for growth\n05:11 Conflicts are inevitable\n11:15 Architecture Evolves\n13:36 Technical Debt\n16:39 Backlash while enforcing standardization\n20:47 Business more critical than engineering",
    "img": "https://i.ytimg.com/vi/xbtE9IF4yRc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/56ikf9jD4ZK6s/giphy.gif",
    "duration": "24:7",
    "view_count": 758,
    "like_count": 32,
    "comment_count": 4,
    "released_at": "2022-06-20",
    "gist": "Microservices are much more than an engineering problem to solve and let's look at some key engineering and non-engineering things to remember while building them.\n\n## Growth Opportunity\n\nMicroservices are a great growth opportunity given there are so many engineering and non-engineering challenges to solve. Every engineer or early leader should grab this opportunity to showcase his/her prowess and earn leadership brownie points.\n\n## Conflicts are inevitable\n\nEngineers are passionate, and intra-team and inter-team conflicts are inevitable. But the conflicts should be gracefully by\n\n- taking data-driven informed decisions\n- consulting senior engineers\n- sometimes moving on without creating a fuss\n\n## Architecture Evolves\n\nVision evolves and so would our architecture. The decisions we took while building it might not be the best today and hence we should be okay with the code getting scrapped and seeing some dramatic changes in the flow.\n\n## Technical Debt\n\nWe cannot always build a Microservice in the best way possible. Engineering teams are always running against time and aim at delivering things quicker.\n\nThis requires us to cut some corners and make some inefficient decisions and this is called Technical Debt. Over time such debt piles up and reduces the development velocity.\n\nIt is important to clean up technical debt periodically, by reserving ~10% of the bandwidth of every engineer every sprint.\n\n## Enforcing Standardizations\n\nStandardizations are essential for microservices as it provides a clear set of guidelines to use for building them.\n\nTo ensure that every team is not building and setting up their conventions from scratch, we create a Template that everyone can use and build on top of it. This aligns with the DRY principle ensuring we do not waste time doing repeated things.\n\nSome engineers and teams might see standardization as strangling, and hence we might see a potential backlash. In order to ensure this is adopted smoothly\n\n- we should have a forum that decides the standards instead of a central team\n- the forum should have proper reasoning behind every decision taken for the template\n\nThis would help us keep such templates and best practices inclusive while ensuring a positive sentiment all around.\n\n## Business over Engineering\n\nThis is offensive, but it is true. Engineering exists because the business does and hence while building microservices we should always remain aligned with the strategic goals.\n\nFor example, if the business priority is profitability we should not have over-provisioned microservices that leak money.",
    "notes_gd": "https://drive.google.com/file/d/1R92VdVZn7zSQAglUg_vJ7EeR1YFFtoBL/view?usp=sharing",
    "slug": "things-to-remember-while-building-microservices"
  },
  {
    "id": 155,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "0q61wIUmVDY",
    "title": "Why should we have a standard way of building Microservices?",
    "description": "We all love creating microservices, but what if every team creates its own microservice uniquely and uses its own conventions? it would create massive chaos of practices, protocols, frameworks, and conventions.\n\nThis propels us to have some standardization on how we build a microservice and hence in this video, we talk about, why it is essential to standardize some aspects of a microservice, how enforcing a few practices helps us in the long run, and, look at the 3 most essential verticals that you should definitely standardize while adopting microservices.\n\nOutline:\n\n00:00 Agenda\n02:39 Need for standardization\n09:42 Standardizing Monitoring\n14:21 Standardizing Interfaces\n18:39 Standardizing Tolerance",
    "img": "https://i.ytimg.com/vi/0q61wIUmVDY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oKIPiIAG10HXkKPNC/giphy.gif",
    "duration": "23:50",
    "view_count": 547,
    "like_count": 22,
    "comment_count": 5,
    "released_at": "2022-06-17",
    "gist": "We all love creating microservices, but there should be a standard way of creating them?\n\nIf we give complete autonomy to engineers to build their services then we will end up with the chaos of languages, frameworks, tools, and conventions. Hence standardization becomes essential to limit the chaos.\n\nStandardization allows us to keep the entire system coherent and uniform and the 3 verticals that need standardization are:\n\n## Monitoring\n\nMonitoring one microservice is relatively a simpler problem, and with easy-to-integrate tools, it becomes a breeze. Few examples\n\n- logging tools - ELK or EFK\n- APM tools - NewRelic or DataDog\n- status code monitoring on cloud consoles\n\nMonitoring how a request originates from the user and goes through various services is important and this problem of Distributed Tracing can be solved through tools like Zipkin and AWS X-Ray.\n\nMicroservices need a standard way of reporting key metrics like CPU, Memory, Disk, and App Metrics into a central system like NewRelic or CloudWatch allowing everyone to get the necessary transparency.\n\n## Interfaces\n\nThere are 100s of ways through which the microservices could talk to each other. This demands standardization and we need to consolidate on a few ways of interfacing.\n\nConsolidating to REST for user-to-service communication and gRPC for service-to-service communication seems to be the most popular choices. But it is totally up to you to pick the ones.\n\nWe cannot allow the communication to happen over a variety of formats and hence we need to consolidate on a few like - JSON and ProtoBuf.\n\nWe also need to standardize the nomenclature and how REST endpoints are written. This would ensure that the entire engineering team feels familiar while writing any inter-service communication.\n\nOther aspects that need standardization are Versioning, Timeouts, and Retries. Having this standardization would allow us to write common libraries to abstract out the implementation complexities and save redundant efforts.\n\n## Tolerance\n\nEvery service needs to shield itself from getting bombarded. Standardizing the tolerance would save us redundant efforts and implementations. A common protection layer would not only prevent the service from getting overwhelmed but also ensure no cascading failures.\n\nSome standard configurations could be\n\n- limit the number of incoming calls from a service\n- ability to cut off incoming and outgoing requests from/to a service\n\nSuch abilities would help us reduce outage times and prevent cascading failures as engineers would be aware of exactly what to do to cut off.",
    "notes_gd": "https://drive.google.com/file/d/1JrK5rL7SxQxBCDJTzRhHMQ5r-4Hx8-1q/view?usp=sharing",
    "slug": "why-should-we-have-a-standard-way-of-building-microservices"
  },
  {
    "id": 154,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "yTSq6hJFmUg",
    "title": "10 Challenges in Adopting and Implementing Microservices",
    "description": "We always hear great things about Microservices. But, every few months every senior engineer gets a feeling, can we not go back to a simpler time and have monolithic architecture again?\n\nIn this video, let's understand the 10 challenges that come with adopting microservices. Today, we would spend time talking about both engineering or tech challenges and organizational challenges that we all should be aware of and address while we adopt microservices.\n\nOutline:\n\n00:00 Agenda\n02:34 Managing Microservices\n05:25 Monitoring and Logging\n08:26 Service Discovery\n10:44 Authentication and Authorization\n12:17 Configuration Management\n14:09 There's no going back\n16:56 Fault Tolerance\n18:43 Internal and External Testing\n20:05 Design with Failures in mind\n21:23 Dependency Management is a Nightmare",
    "img": "https://i.ytimg.com/vi/yTSq6hJFmUg/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/14ppamy2aEFhOE/giphy.gif",
    "duration": "26:46",
    "view_count": 711,
    "like_count": 38,
    "comment_count": 4,
    "released_at": "2022-06-15",
    "gist": "We always hear great things about Microservices. But today let's talk about the top 10 challenges that come with adopting Microservices.\n\n## Managing Microservices\n\nAs the number of microservices increases, managing them becomes tough. If there is no plan or accountability then we might end up with a lot of tiny microservices or with a huge macro-service.\n\n## Extensive Monitoring and Logging\n\nMonitoring what happens across the entire infra is critical. Along with this, we would also need an ability to trace end user request path spanning services - also called Distributed Tracing.\n\n## Service Discovery\n\nIt does not take much time for our services to grow beyond 100 and at that scale, discovering a service becomes a pain requiring us to put Service Discovery.\n\n3 ways to do it are\n\n- a central service registry\n- load balancer-based discovery\n- pure service mesh implementation\n\n## Authentication and Authorization\n\nInter-service communication should be secure to ensure that a service does not abuse others; hence we need to put auth in place that allows authorized services to talk to each other.\n\n## Config Management\n\nEvery microservice has a set of configs, like DB passwords, and API Keys. Committing them to the repository is an unacceptable practice, and we would not want every service to have its own config server.\n\nHence we need to have about a central config management system that is fault tolerant, robust, and scales well.\n\n## No going back\n\nIt is extremely difficult to move back to monolith after the teams have tasted microservices. A few reasons would be\n\n- services are written in various languages\n- teams used to being autonomous\n- teams have adopted new tools and processes\n\n## Fault Tolerance\n\nOutages are inevitable and as engineers, we always try to minimize them. A way to achieve this is to keep services loosely coupled that keep outages isolated ensuring no cascading failures.\n\n## Internal and External Testing\n\nEnd-to-end testing becomes complex as it is hard to spin up environments with all services running fine.\n\n## Design with Failures in mind\n\nRobust microservices require a counter-intuitive approach, and we need to assume everything would collapse after every line of code. Then we amend the code and architecture to handle it and re-iterate.\n\n## Dependency Management is a nightmare\n\nManaging dependencies across services is tough and it leads to a slowdown. The 3 kinds of dependency to be careful about\n\n- sync dependency on other services\n- services sharing a common library\n- service depending on data coming from other services",
    "notes_gd": "https://drive.google.com/file/d/1qOCi_CaT5qYO_Qbm-6UB2IN-qcGc8AYB/view?usp=sharing",
    "slug": "10-challenges-in-adopting-and-implementing-microservices"
  },
  {
    "id": 153,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "qoAox0FGzRQ",
    "title": "What are Microservices?",
    "description": "Everyone is doing Microservices, but what are they after all? From a distance, it looks like a function put over the network. Is it really just that? There are so many things to explore about microservices, so let me introduce you to this world today.\n\nIn this video, we talk about what are microservices, understand how it starts with a simple monolith and eventually evolve into microservices, look at their key characteristics, understand their advantages, and conclude with some anti-patterns that we all should keep in mind to ensure we do not do it wrong.\n\nOutline:\n\n00:00 Agenda\n02:38 Idea of a Microservice\n09:26 Monolith Architecture\n16:19 Monolith to Microservice\n18:49 Characteristics of a Microservice\n21:09 Advantages of Microservices\n24:06 Anti-patterns in Microservices",
    "img": "https://i.ytimg.com/vi/qoAox0FGzRQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Nx85vtTY70T3W/giphy.gif",
    "duration": "27:34",
    "view_count": 1671,
    "like_count": 80,
    "comment_count": 9,
    "released_at": "2022-06-13",
    "gist": "Everyone's talking about Microservices, but what exactly are they? Are they just a set of functions hosted over a network or something more?\n\nMicroservices in simple terms are like the regular functions from our programming language but just a more extensive set of responsibilities and served via a network.\n\nSo, you may have a microservice that handles everything related to Payments, a service that handles Notifications, and a service that deals with analytics. We see each service having focussed responsibility.\n\nMicroservices are no silver bullet, and they would not magically solve all the problems you have. It has its own fair share of drawbacks.\n\n## Monolith\n\nAlmost all products start monolith where every feature is put into a single codebase which is deployed as one artifact across all the servers. For example, the code handling payments, notifications, and analytics are all part of the same codebase deployed within the same binary.\n\nMonoliths are always simple to build, develop, test, and scale. Given their simplicity, they are the go-to option for anyone starting up. With a lean team working on monolith would ensure very quick feature delivery.\n\n### Disadvantages of Monolith\n\n- monolith is tightly coupled\n- the deployment artifact - binary/JAR is bulky\n- the tech stack is homogeneous\n- bug in one module affects other modules\n- scaling one module requires scaling everything\n- large monolithic codebase is intimidating and it slows down delivery\n\n### Monolith to Microservices\n\nMigrating from monolith to microservices is a slow process and to start you would club a related set of functions and fork out a service out of it. The process would be repeated for other sets of functions, eventually breaking the entire monolith.\n\n## Characteristics of Microservices\n\n- microservices are autonomous\n- microservices are focused and specialized\n- microservices are built around a business usecase/need\n\n## Advantages of Microservices\n\n- agility: small independent teams can move much faster\n- scaling: you can precisely scale one service as per the load\n- freedom: you can pick the best-suited tech stack for the service\n- given the scope is focused, a microservice is simple to understand\n- microservices can be reused across the platform\n- if a service goes down, it is easy to isolate it using a circuit breaker\n\n## Anti-patterns\n\n- do not start with microservices; start with a monolith\n- do not make services too small; they should a larger responsibility\n- don't reinvent the wheel, use existing tooling as much as possible",
    "notes_gd": "https://drive.google.com/file/d/1SaDvF80ZirInE4XyVRu8F6PnIm-1b8vl/view?usp=sharing",
    "slug": "what-are-microservices"
  },
  {
    "id": 152,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "9GHh_U-ud9U",
    "title": "Dissecting GitHub Outage - Downtime they thought was avoided",
    "description": "Has it ever happened to you that you anticipated that something would go wrong, you pro-actively fixed it, but it still went wrong? This exact thing happened to GitHub on March 1, 2021, in a situation for which they were prepared for 6 months. This is a very amusing incident as it teaches us a lot about the blind spots that exist in every system out there.\n\nIn this video, we talk about an outage that GitHub faced with their Actions service. and take an in-depth look into what happened, why it actually happened, and what GitHub did to prepare for this very situation, but it still went wrong.\n\nOutline:\n\n00:00 Agenda\n02:46 What happened?\n05:12 Table Exploded with Data and IT reached its limit\n07:03 GitHub's Anticipation and What went wrong?\n12:27 Mitigation\n14:04 Impact on the Search Service\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/9GHh_U-ud9U/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3xz2BLBOt13X9AgjEA/giphy.gif",
    "duration": "17:44",
    "view_count": 421,
    "like_count": 24,
    "comment_count": 9,
    "released_at": "2022-06-10",
    "gist": "GitHub thought they avoided an outage by fixing a possible root cause 6 months in advance, but fate had different plans.\n\n## Check Suites and Workflows\n\nWhen we push any changes to any GitHub repository there are some checks that run. We can see them on our Pull Request and it basically prevents us from merging the PR until all checks are successful. We can also add custom checks on our own to the workflow.\n\nAn entry is made in the database for every execution of the check suite. This is a high frequency that would lead to heavy ingestion in the database table. A side effect it would be that the auto-incrementing ID column, which is typically a 32-bit integer would exhaust leading to the writes getting failed.\n\n## GitHub's Anticipation\n\nGitHub anticipated this situation 6 months in advance and they altered the column from 32-bit integers to 64-bit integers ensuring that even when the ID range exhausts the 32-bit limit it would lead to downtime.\n\nBut still, the team faced an outage, how? what happened?\n\n## What exactly happened?\n\nThe service was able to create entries in the database about the check suite execution as the database supported 64-bit integers, but there was one external dependency that unmarshalled JSON strings to native objects which only supported 32-bit integers.\n\nThe service was responsible for pulling the jobs from the database and putting it in the queue to be picked up by executors. This service depended on the library and hence it was unable to execute the checks. This led to all the checks remaining in the pending state during the course of this outage.\n\n## Impact on the Search service\n\nThe search service was also impacted by this as the indexing used queue as the source. Since the newer jobs were not put in the queue, they were not indexed in the search cluster (eg: ElasticSearch), and hence when the user searched, they could not find the latest checks and workflows.\n\n## Mitigation\n\nIn order to mitigate the issue, the GitHub team released a code fix. Speculation: they would have updated the library version that would support 64-bit integers, or they might have quickly forked and patched it with the changes, or they might have written some ad-hoc job that temporarily pulled the jobs and put them in the queue.\n\nThis incident shows that no matter how big the company gets and how prepared are you for an extreme event, there would always be some blind spots in the system that would bite us back.",
    "notes_gd": "https://drive.google.com/file/d/1J9wbS2sK5BrW5Ftgdz2tmPrt86tAhQaA/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-they-thought-was-avoided"
  },
  {
    "id": 151,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "5pYLlYsy6fQ",
    "title": "API Composition Pattern in Microservices",
    "description": "Say, we have very happily created 6/7 microservices and everything is going well. Now for a new usecase that is introduced in the product, we have to talk to not one but 3 services together to compile a response. So, what is a good way of implementing and supporting this kind of request? A high-level pattern that helps us do it is called API Composition.\n\nIn this video, let's in-depth talk about this super simple pattern to query Microservices, see what it is, how to implement it, understand how it not only helps in improving end user experience, and conclude by going through the advantages and disadvantages of adopting it.\n\nOutline:\n\n00:00 Agenda\n02:54 Introducing API Composition\n04:53 Implementing API Composition using API Gateway\n06:44 Sequential vs Parallel Invocation\n09:21 Improving end-user experience using API Composition\n12:43 Branch or Multi-level API Composition\n14:03 Advantages of API Composition and Gateways\n18:49 Disadvantages of API Composition and Gateways",
    "img": "https://i.ytimg.com/vi/5pYLlYsy6fQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l2Jee4sAi8emF3Q40/giphy.gif",
    "duration": "25:50",
    "view_count": 1382,
    "like_count": 81,
    "comment_count": 15,
    "released_at": "2022-06-08",
    "gist": "Say, we have 3 microservices - Order, Payments, and Logistics - and to get the order details we need data from all of them, merge it, and then respond to the client. A common pattern to achieve this is API Composition.\n\n## API Composition\n\nIt is a high-level pattern to query microservices. It puts a composer right in the middle abstracting out the microservices.\n\nWith the composer sitting in between, the request from the client first hits the composer, and the composer then talks to the relevant services to get the response. It then merges the responses before sending them to the client.\n\n## Implementing API Composition\n\nInstead of building it from scratch, we can use tools that specialize in achieving this - ex: API Gateways like KrakenD, Kong, and AWS API Gateway.\n\n## Improving user's experience using composer\n\nAn API Composer not only helps in making the backend simpler, but it also helps in gaining a good UX.\n\nIf we do not have an API composer, the client (browser/app) would have to make multiple API calls to microservices to get the information and render the interface. The multiple calls would require multiple round trips of the data increasing the latency and will also eat up the user's data.\n\nBy having an API composer sitting in between the client would only need to make one API call and the fan-out happening at composer will be within the infra. This would reduce the latency for clients and improve the UX.\n\n## Branch Composition\n\nFor a complex usecase, it is quite possible that a downstream service may use another composer to reach out to another set of services to get things done. A dependency like this would create a multi-level API composition also called Branch composition.\n\nThis would create a hierarchical dependency between services solved through multiple API composers and it is a common pattern observed in complex e-commerce platforms.\n\n## Advantages of using API Composition\n\n- Simple to implement\n- Client has a single point to interact\n- Hides the implementation complexities\n- Security and Limiting applied only to the composer\n- Can cover the \"bad\" design decisions with a shiny new interface\n- Hides legacy system allowing us to gradually move out of it\n\n## Disadvantages of using API Composition\n\n- If the dataset we fetch from microservices is large, it would make the composer in-efficient\n- Overall availability is challenged as the number of services increase\n- Having a transactional data consistency is difficult\n- Composer needs to be managed and maintained\n- Composer may become a bottleneck at scale",
    "notes_gd": "https://drive.google.com/file/d/1e5AqRKDRQ8c_3rqWWfBTDa3cKXkuCvTt/view?usp=sharing",
    "slug": "api-composition-pattern-in-microservices"
  },
  {
    "id": 150,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "df2QgLW0QC4",
    "title": "Dissecting GitHub Outage Downtime due to creating an Index",
    "description": "GitHub wanted to optimize their SQL query performance, and they had to reverse a database index. Instead of getting a performance boost, thy incurred a downtime of more than 60 minutes. Imagine the state of the team who wanted to do good, but were stuck in this fix.\n\nThis outage gives us a super-in-depth insight into MySQL and its indexing structure. It is indeed fascinating to see the kind of optimizations engineers have to make while operating at scale.\n\nIn this video, let's dissect this outage and understand what happened, why GitHub had to flip their index, how things went wrong and affected end users like us, and conclude with 3 key takeaways from this outage.\n\nOutline:\n\n00:00 Agenda\n02:47 What happened?\n03:52 Need of Reversing an Index\n07:21 What could go wrong during and after Indexing?\n14:52 Cascading Failures\n17:16 Mitigation and Key Takeaways\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/df2QgLW0QC4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26FxBCxSr8V7sHWdG/giphy.gif",
    "duration": "22:49",
    "view_count": 856,
    "like_count": 56,
    "comment_count": 12,
    "released_at": "2022-06-06",
    "gist": "Imagine you created an index on a table and instead of boosting the performance, it lead to an outage \ud83e\udd26\u200d\u2642\ufe0f GitHub ran a migration to reverse an index and it lead to a 60 mins outage.\n\nNote: The example we have taken is pure speculation, the official incident report had minimal information about the outage. But the write-up will make you aware of possible challenges that might come during such situations.\n\n## Reverse an index\n\nReversing the order of the index is done when we have a multi-column index and the query requires different sorting orders on them; for example, `DESC` on `date` and `ASC` on `user_id`.\n\nFor a query to be optimally executed on the database, we would need an index that physically stores the index ordered by the `date` in the descending order and `user_id` in the ascending order.\n\nMySQL by default stores any index in `ASC` order and hence, GitHub had to run the migration to reverse the order of the index and gain a boost.\n\n## What could go wrong?\n\nA reverse index would require a Full Table Scan during the creation putting a load on the database. Also, upon changing the order of the index, it is possible we overlook another query that is more frequent but optimal with the old order.\n\nThe database does its best to create an optimal execution plan and it might not use the reverse index we just created. We can solve this by specifying Index Hints like `USE INDEX` and `FORCE INDEX`, ensuring that it uses our index to evaluate the query.\n\n## Cascading Effect\n\nBecause one of the queries was doing a Full Table Scan, it put a load on the database which had a cascading effect on the service eventually propagating to the end user. All the intermediate services will timeout giving a degraded experience to the user.\n\n## Key Takeaways\n\n### Never blindly trust ORM\n\nORMs are designed to make our lives simpler but they might not generate the most optimal queries, and hence it is always better to periodically audit the queries and ensure they are optimal.\n\nPoorly generated queries will put a load on the database choking the entire performance.\n\n### Check the query execution plan\n\nWhile updating a query or changing a schema always check the query execution plan. We can get the execution plan for any query using the `EXPLAIN` statement.\n\nThe diff in the plan would give tell us if any of our queries would perform a full table scan.\n\n### Audit the queries and indexes\n\nKeep an inventory of the queries we fire and the indexes it uses during execution. So, whenever we change any index, we can quickly run an audit and ensure zero regressions.",
    "notes_gd": "https://drive.google.com/file/d/1JrtyE2wt9ikic3iOTy36NYKxEPESMt4a/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-creating-an-index"
  },
  {
    "id": 149,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "48YZzGi7QMk",
    "title": "Dissecting GitHub Outage - Repository Creation Failed",
    "description": "Imagine you trying to create a new GitHub repository and it call is failing, failing for 53 minutes. This happened with GitHub in April 2021 when for 53 minutes people were unable to create any new repositories. Upon investigation, they found out that the root was scanning secrets. Two seemingly different usecases took down one of the most important APIs.\n\nThis has to be one of the most amusing outages that I have seen in recent times.  In this video, we dissect this outage, understand the root cause of it, look at the importance of secret scanning, and conclude with an understanding of their mitigation process.\n\nOutline:\n\n00:00 Agenda\n02:49 What happened?\n03:20 Secret scanning and its importance\n08:28 Repository Creation Flow\n09:15 What lead to an outage?\n17:26 Outage mitigation\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/48YZzGi7QMk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/VIo556t5920j07cCR4/giphy.gif",
    "duration": "20:59",
    "view_count": 494,
    "like_count": 25,
    "comment_count": 7,
    "released_at": "2022-06-03",
    "gist": "Just imagine you trying to create a repository on GitHub and it is not working, and this happened to GitHub in April 2021 when their users were not able to create a new repository.\n\nThe root cause for this outage was something that seems unrelated - Scanning Secrets. The root cause makes this outage super interesting to dissect.\n\n## What is Secret Scanning?\n\nOur API servers need to talk to peripheral components like Databases, Cache, SaaS services, etc. This communication involves some sort of authentication and authorization through auth tokens, passwords, or secret keys.\n\nDevelopers tend to commit the secrets in the settings/constant files and push them to GitHub. What if the repository content gets leaked? What if GitHub itself has a data breach and the attacker gets access to the private repositories?\n\nIf the secrets like AWS access keys, auth tokens, and DB passwords are leaked and the attacker can then get the dump of the data and ask for a ransom. Or they may even abuse the infrastructure to perform some illegal activities or mine cryptocurrencies.\n\nHence, GitHub periodically runs a job that checks all the repositories for any secrets that are committed and warns the user about it.\n\n## Repository Creation Flow\n\nWhen a repository is created an entry is made into the Secret Scanning table which is then used by a job that scans for potential secrets and notifies the owner.\n\n## What led to the outage?\n\nThe GitHub team ran a data migration in which they moved the Secret Scanning table from a common database to its own cluster allowing it to scale independently.\n\nGitHub team was unaware of this dependency! and hence after the migration of the table happened to a different database the creation of a new repository started failing to lead to this outage. It is interesting to see such mature products having blindspots.\n\n## How did GitHub mitigate it?\n\nThe mitigation strategy of GitHub was to roll back the migration. Although it is unclear from the incident report on what exactly they did but there are a few speculations\n\n1. they could have recopied the table quickly to the old database\n2. whitelisted the database so that applications could connect\n3. old table would have been intact and hence they would have just renamed and made it active again.\n\nAgain, it is pure speculation given we do not have any insider information nor they specified in the report. It would have been fun to have gone through their actual mitigation steps. We could have learned so much, but nonetheless, we did learn a few interesting insights from this outage.",
    "notes_gd": "https://drive.google.com/file/d/1KtNWAH5YHC9-qVxvbxLTXF3u5yGV7l-t/view?usp=sharing",
    "slug": "dissecting-github-outage-repository-creation-failed"
  },
  {
    "id": 148,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "la2q1vFA5q0",
    "title": "Database per Service Pattern in Microservices",
    "description": "Microservices should be loosely coupled and autonomous so that they can take their own decision and be as performant as they can be. A high-level architecture pattern that allows us to achieve this is the database-per-service pattern in which each microservice owns its database and take independent decisions about it.\n\nIn this video, we quickly talk about the database-per-service architecture pattern, look at how it helps in modeling massive systems, understand the advantages of adopting it, and conclude by going through some drawbacks of this architecture pattern.\n\nOutline:\n\n00:00 Agenda\n02:39 INtroducing Database Per Service Pattern\n04:02 Modeling Social Networks\n07:33 Advantages of adopting Database Per Service Pattern\n16:22 Downsides of adopting Database Per Service Pattern",
    "img": "https://i.ytimg.com/vi/la2q1vFA5q0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3TCBh8i6H3bz2/giphy.gif",
    "duration": "22:49",
    "view_count": 2493,
    "like_count": 106,
    "comment_count": 12,
    "released_at": "2022-06-01",
    "gist": "Should every microservice have its own database?\n\nMicroservices need some persistence to store the state of the application. They also need to be loosely coupled with others so as to be autonomous and having a separate database for itself really helps, and this is the Database Per Service pattern.\n\n## Modeling a social network\n\nThe importance of having a database per service can be seen when we model a social network. For every usecase, we would need a specialized database to keep our latencies to a bare minimum.\n\n- Chat: a partitioned non-relational database like Cassandra\n- Auth: a simple relational database with replicas like MySQL\n- Profile: a nonrelational schemaless database like MongoDB\n- Analytics: a massively scalable columnar storage like Redshift\n- Videos: blob storage like S3\n\n## Advantages of Database Per Service Pattern\n\n### Loosely coupled components\n\nBecause the databases are separate, the services could not connect to other databases and would have to directly talk to each other making them loosely coupled.\n\n### Specific DB for specific usecase\n\nServices can pick the best database for their usecase making them super performant and efficient. For example, picking a Graph database to model relations in social networks instead of relational.\n\n### Granular control and scaling\n\nServices can choose their scaling strategies as per the load it is getting; be it vertical, horizontal, replicas, partitioned, or decentralized.\n\n### Smaller blast radius\n\nWhen any database is experiencing an outage only the services that are directly or indirectly dependent on it get affected and everything else continues to operate normally. For example, we can continue to accept the payments even when the profile service is down.\n\n### Compliance\n\nCompliance requires us to make changes in how our data is stored and moved across. Separate databases would help us in implementing changes on a fraction of data instead of the whole.\n\n## Downsides of Database Per Service Pattern\n\n### Transactions are distributed and expensive\n\nIf we need strong consistency across, we would need distributed transactions and those are expensive and complex to implement.\n\n### Conveying updates requires brokers\n\nConveying updates from one service to another would require us to have message brokers, thus adding more things to manage and maintain.\n\n### More infra to manage\n\nHaving a database per service bloats up our infra and we would need to build expertise in maintaining and managing them.",
    "notes_gd": "https://drive.google.com/file/d/1D7bEh7tacBZsBxzxnVbH4OvFuTOgY-yh/view?usp=sharing",
    "slug": "database-per-service-pattern-in-microservices"
  },
  {
    "id": 147,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "uFGJVQvR59A",
    "title": "Everything you need to know about REST",
    "description": "REST is how browsers talk to our servers. 99.99% of all your API requests that originates from your browser and go to your API servers are REST. So, what is REST? How is it different from HTTP? Is it a protocol or what? There are so many unanswered questions like this.\n\nIn this video, we in-depth talk about REST, understand the foundations of it, see how and why it gels so well with HTTP, find out why everyone is using it, and conclude by going through the downsides of using REST over HTTP.\n\nOutline:\n\n00:00 Agenda\n02:38 Introduction to REST\n03:14 Everything is a Resource\n07:20 Representation in REST\n09:07 Underlying protocol in REST\n09:20 REST and HTTP\n17:32 Downsides of using REST over HTTP",
    "img": "https://i.ytimg.com/vi/uFGJVQvR59A/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/A6aHBCFqlE0Rq/giphy.gif",
    "duration": "26:20",
    "view_count": 2378,
    "like_count": 141,
    "comment_count": 18,
    "released_at": "2022-05-30",
    "gist": "Is REST all about just exposing an HTTP endpoint?\n\nREST is an abbreviation that stands for Representational State Transfer. A lot of complex words but stick with me.\n\nREST is a specification that suggests how a client should demand/request information from the server and how the server should respond; it does not enforce anything.\n\n## Everything is a Resource\n\nIn REST, everything is a resource. Any actionable entity in your application eg: book, student, customer, or order is a resource for REST. The client can demand action on a resource like get, create, delete, or update.\n\nREST does not put any restriction on how the data is stored in your application, but it allows clients to specify how it wants the data from the server eg: XML, JSON, CSV, etc. So long as the server supports the representation the server would respond in the demanded format.\n\n## Underlying Protocol for REST\n\nREST does not enforce any restriction on the underlying protocol to be used. All it cares about is that we have a defined way to act on a resource. So, we can implement REST over HTTP or even hardware like USB.\n\n## Why REST over HTTP?\n\nThe most common implementation of REST is over HTTP and it gels very well together. Why so?\n\nHTTP has verbs like GET, POST, DELETE, and PUT and these become actions on our resource. The resource is specified by the URL of the HTTP request. For example, to get a student's details whose `id = 1` we fire\n\n```\nGET /students/1\n```\n\nWe are representing the student with `id = 1` using the URL `/students/1` and are specifying action `GET` using the HTTP verb `GET`. Similarly to update the student with `id = 1` we could\n\n```\nPOST /students/1\n```\n\nHere we see how REST is resource-oriented and we have a way to specify the action to be taken on it.\n\n### Existing tooling\n\nOne very important reason why HTTP is so commonly used in implementation is the availability of existing tooling. We can reuse the existing set of tools to get the job done, which includes.\n\n- using existing HTTP Clients like Postman, cURL, Requests\n- use caches like Ngnix, HA Proxy, and Varnish to boost performance\n- use monitoring tools like Distributed Tracing\n- use load balancers to uniformly distribute the load\n- use SSL to get out-of-the-box security\n\n## Downsides of doing REST over HTTP\n\n- all clients would have to serialize and deserialize the HTTP body\n- all clients would have to repetitively handle failures and retries\n- some web servers might not support verbs like PUT, DELETE\n- HTTP payloads are JSON and hence huge\n- HTTP would not support switching underlying protocols",
    "notes_gd": "https://drive.google.com/file/d/18edhW0hhSUp-7Vn3-Mz8Xfge8xRBmJ6W/view?usp=sharing",
    "slug": "everything-you-need-to-know-about-rest"
  },
  {
    "id": 146,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "9kjUG_yvVqM",
    "title": "An in-depth introduction to Rolling Deployments",
    "description": "One of the simplest deployment strategies that make deployment a breeze is Rolling Deployment. It is the most widely adopted deployment strategy purely because of its simplicity and cost-effectiveness. Most of the deployment tool has this as their default deployment strategy.\n\nIn this video, we take an in-depth look into what Rolling deployment is, how they are implemented, how to tune it, some key challenges we face during adoption, and conclude with an understanding of the pros and the cons of adopting it.\n\nOutline:\n\n00:00 Agenda\n02:37 Introduction to Rolling Deployments\n05:43 How to implement Rolling Deployment\n11:32 Tuning Rolling Deployments\n15:59 Pros of Rolling Deployment\n18:37 Cons of Rolling Deployment",
    "img": "https://i.ytimg.com/vi/9kjUG_yvVqM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Kxn4rf7eceDpGBGOZY/giphy.gif",
    "duration": "21:41",
    "view_count": 870,
    "like_count": 44,
    "comment_count": 1,
    "released_at": "2022-05-27",
    "gist": "Rolling Deployment is a deployment strategy that slowly replaces the previous version of the application with the new one by replacing the underlying infrastructure.\n\nSay we have 9 servers and each one is running version 1 of the code. With rolling deployment, we roll out our changes i.e. version 2 of the code to one server at a time eventually covering all 9 servers. This is the core idea of the rolling deployment, however, the implementation could vary a bit.\n\nA key thing to note here is the fact that deployment is incremental in nature which means during the deployment there would be a few servers that are serving the old version of the code and the remaining servers serving the newer version. Hence the changes we push should be both backward and forward compatible.\n\n## Implementing Rolling Deployment\n\nRolling deployments are always gradual and graceful, and we typically happen through the below steps\n\n1. Pick a server for deployment\n2. Stop the incoming traffic by removing it from the load balancer\n3. Wait for the existing requests to be completed\n4. if we are not replacing the infra, pull the latest code and reload\n5. if we are replacing the infra, delete the server and launch a new one with the new code\n6. attach the server behind the load balancer and start serving the requests\n\n## Tuning Rolling Deployment\n\n### Concurrent Servers\n\nInstead of deploying to one server at a time, we can deploy changes to `n` servers concurrently. This would complete the entire deployment quicker.\n\nChoosing the appropriate `n` is critical as a small value would mean the deployment takes ages to complete and a large one would affect the availability during deployment.\n\n### Double-Half Deployment\n\nAn interesting way to implement rolling deployment is to double the infrastructure and then delete the older half.\n\nSay, we have 4 servers with version 1 of our code so in order to deploy the changes we add 4 new servers with the new version of the code to the infra taking our total count to 8, and then delete the 4 older servers. This way, what remains are the 4 servers with the newer version of the code.\n\n## Pros of Rolling Deployments\n\n- Cost efficient\n- Rollouts are gradual\n- Rollbacks are simple\n- Deployment incurs zero downtime\n- Much faster than Blue-Green Deployment\n- Any hiccup during deployment affects only a fraction of the users\n\n## Cons of Rolling Deployments\n\n- No environment isolation\n- Naive deployment takes a long time to complete\n- Stateful applications are affected during deployment\n- Changes we rollout should be backward and forward compatible",
    "notes_gd": "https://drive.google.com/file/d/1Mtox_ulRNSajmbVXXyLyJrhV3GZouOiv/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-rolling-deployments"
  },
  {
    "id": 145,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "9iAJjtvBwyI",
    "title": "Implementing Vertical Sharding",
    "description": "Sharding is super-important when you want to handle the traffic that cannot be handled through one server. Sharding comes in two flavors - Horizontal and Vertical. In horizontal sharding, we split the table by rows and keep them on separate servers. In vertical sharding, we distribute the tables across multiple database servers.\n\nFor example, keeping all the payments-related tables in one database server, and all the auth-related tables in another. Vertical sharding comes in super handy when we are moving from monolith to microservices. All this sounds simple yet awesome theoretically, but would we actually implement it?\n\nIn this video, we take an in-depth look, not at the theoretical side of vertical sharding, but at the implementation side of it. We will see how Vertical Sharding is implemented with minimal downtime and what are the exact steps to do it.\n\nOutline:\n\n00:00 Agenda\n03:17 Introduction to Vertical Sharding\n05:23 Implementing Vertical Sharding\n05:55 Picking a configuration store\n10:34 Moving a table from one server to another\n18:58 Summarizing the overall flow",
    "img": "https://i.ytimg.com/vi/9iAJjtvBwyI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif",
    "duration": "24:41",
    "view_count": 1047,
    "like_count": 69,
    "comment_count": 19,
    "released_at": "2022-05-25",
    "gist": "Vertical sharding is fine, but how can we actually implement it? \ud83e\udd14\n\n## Vertical Sharding\n\nVertical sharding is splitting a database by the tables. Shards will hold a subset of tables. For example, all payments-related tables go to one shard, while all auth-related tables go to another.\n\nSo, how to implement it?\n\n## Need for a configuration store\n\nFor our API servers to talk to the correct database we would need a configuration store that holds the information for all the tables mapped to the database server that holds it.\n\nFor example, the Users table is present on DB1 while Transactions on DB2\n\nWhenever the request comes, the API servers first check the config to find which DB holds the table and then fire the SQL query to that specific database for the table.\n\n### Reactive update\n\nAll API servers will cache the configuration to avoid an expensive network call to get the database ensuring we get a solid boost to the performance.\n\nWhen a table is moved from one database server to another, the configuration will be updated and hence the changes would need to be reactively propagated to all the API servers. Hence our config store needs to support reactive communication.\n\nThis is where we choose Zookeeper which is resilient and battle-tested to achieve this.\n\n## Moving tables\n\nSay, we are moving table `T2` from database server DB1 to DB2. Moving the table from one server to another is done in 4 simple steps.\n\n### Dump the table `T2`\n\nWe first dump the table `T2` from DB1 transactionally using the utility `mysqldump` that not only dumps the table data but also records the position in the `binlog`. This is like taking a point-in-time snapshot of the table.\n\n### Restore the dump\n\nWe now restore the dump to database DB2. This way we will have a database server with the table `T2` containing data till a certain point in time.\n\n### Sync table `T2` on DB1 and DB2\n\nWe now setup the replication from DB1 to DB2 specifically for sync changes happening on table `T2`. It is done through a custom job that will use the recorded `binlog` position and start syncing from it.\n\n### Cutover\n\nOnce the table `T2` is synced with almost 0 replication lag on DB1 and DB2 we cutover. We first rename the table to `T2_bak` and update the config in Zookeeper.\n\nAs we rename the table any queries going to DB1 for table `T2` will start throwing \"Table not found\" errors, but as Zookeeper will propagate the changes to all API servers they would use DB2 to fire any query on table `T2`, thus completing the table movement.\n\nThis is how you can implement vertical sharding.",
    "notes_gd": "https://drive.google.com/file/d/1AxijzqfIksP_QOKc9eKhuXba7apUugCT/view?usp=sharing",
    "slug": "implementing-vertical-sharding"
  },
  {
    "id": 144,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "iqapVyfoFqc",
    "title": "Dissecting GitHub Outage: Downtime due to an Edge Case",
    "description": "In August 2021, GitHub experienced an outage where their MySQL Master database went into a degraded state. Upon investigation, they found out it was due to an edge case. So, how can an edge case take down a database?\n\nIn this video, we understand what happened, take a fictional example of how an edge case can put extra load on the database, and conclude with an exciting way to make our SQL queries fool-proof.\n\nOutline:\n\n00:00 Agenda\n00:27 Outage Overview\n07:38 What could have happened?\n08:51 A fictional example of an edge case taking down DB\n17:18 How to avoid edge cases on SQL queries\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/iqapVyfoFqc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/243UHkbIOZJGj7FZ84/giphy.gif",
    "duration": "21:32",
    "view_count": 1153,
    "like_count": 56,
    "comment_count": 12,
    "released_at": "2022-05-23",
    "gist": "An edge case took down GitHub \ud83e\udd2f\n\nGitHub experienced an outage where their MySQL database went into a degraded state. Upon investigation, it was found out that the outage happened because of an edge case. So, how can an edge case take down a database?\n\n## What happened?\n\nThe outage happened because of an edge case which lead to the generation of an inefficient SQL query that was executed very frequently on the database. The database was thus put under a massive load which eventually made it crash leading to an outage.\n\n### Could retry have helped?\n\nAutomatic retries always help in recovering from a transient issue. During this outage, retries made things worse. Automatic retries added the load on the database that was already under stress.\n\n## Fictional Example\n\nNow, we take a look at a fictional example where an edge case could potentially take down a DB.\n\nSay, we have an API that returns the number of commits made by a user in the last `n` days. The way, this API could be implemented is to get the `start_date` as an integer through the query parameter, and the API server could then fire a SQL query like\n\n```\nSELECT count(id) FROM commits\nWHERE user_id = 123 AND start_time > start_time\n```\n\nIn order to fire the query, we convert the string `start_time` to an integer, create the query, and then fire it. In the regular case, we get the correct input and then compute the number of commits and respond.\n\nBut as an edge case, what if we do not get the query parameter or we get a non-integer value; then depending on the language at hand we may actually use the default integer value like `0` as our `start_time`.\n\nThere is a very high chance of this happening when we are using Golang which uses `0` as the default integer value. In such a case, the query that gets executed would be\n\n```\nSELECT count(id) FROM commits\nWHERE user_id = 123 AND start_time > 0\n```\n\n  \nThe above query when executed iterates through all the rows of the table for a particular user, instead of the rows for the last 7 days; making it super inefficient and expensive. The above query would put a huge load on the database and a frequent invocation can actually take down the entire database.\n\n### Ways to avoid such situations\n\n1. Always sanitize the input before executing the query\n2. Put guard rails that prevent you from iterating the entire table. For example: putting `LIMIT 1000` would have made you iterate over 1000 rows in the worst case.",
    "notes_gd": "https://drive.google.com/file/d/1PI302Cvutu8LJKmZu7fiIVOAqRhiSMek/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-an-edge-case"
  },
  {
    "id": 143,
    "topic": {
      "id": 0,
      "uid": "garbage-collection",
      "name": "Garbage Collection",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3MvruOP09U1bWmcjxLIMwR",
      "bgcolor": "#F0E6FF",
      "themecolor": "#A66AFF"
    },
    "yt_video_id": "LkympMbgmCo",
    "title": "Why caching would not speed up Mark-and-Sweep GC?",
    "description": "So, caching doesn't always work!\n\nWhat would happen if we apply caching to speed up our Mark and Sweep Gargabge collector? Will it improve its performance?\n\nToday we understand the key patterns that are essential for our caching to work. If those patterns do not exist, caching would be ineffective. The concept we discuss today is something that holds true universally and is not restricted just to the world of Garbage Collectors.\n\nToday we take an in-depth look into caching, how it works, why it works, and understand why there would not be a significant performance improvement if we apply caching to our Mark and Sweep Garbage collector?\n\nOutline:\n\n00:00 Agenda\n03:09 Refresher about Mark and Sweep\n04:06 Why garbage collection needs to be fast?\n05:00 What is a cache?\n08:36 When would a cache improve performance?\n11:09 Hardware and Cache Prefetching\n14:50 Why caching would not improve Mark and Sweep's performance?\n19:14 A minor speed-up through caching",
    "img": "https://i.ytimg.com/vi/LkympMbgmCo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/1Ai794SGeIhn50H4HV/giphy.gif",
    "duration": "23:43",
    "view_count": 704,
    "like_count": 40,
    "comment_count": 6,
    "released_at": "2022-05-20",
    "gist": "Mark and Sweep GC is one of the most common garbage collection algorithms that has seen a massive adoption. The algorithm has two phases:\n\n- Mark Phase: we iterate through the object reference graph and mark all the reachable objects as live; and\n- Sweep Phase: we iterate through all the objects in the main memory and delete all the objects that are not marked live; thus cleaning up the garbage\n\n## We need Garbage Collectors to be fast\n\nGarbage collectors need to be fast because we want the CPU cycles to be used in running the core business logic of the user program and not cleaning up unused variables.\n\n## What is a Cache?\n\nA cache is anything that stores the data so that future requests are served faster. The data we can cache can be\n\n- results from the previous computation\n- a copy of the data from a slower storage\n\n## When does caching improve performance?\n\nCaching improves the performance of an application only when the use case exhibits one of the following two behaviors\n\n### Temporal Locality\n\nA recently accessed memory location is likely to be accessed again.\n\nBy caching the data, we can thus serve the information from the cache instead of doing an expensive lookup or computation.\n\n### Spatial Locality\n\nIf a location is recently accessed, the location adjacent to that is likely to be accessed soon.\n\nBy caching the data, we can thus serve the information from the cache instead of going through slower storage like disk or main memory.\n\n## Hardware and Cache Prefetching\n\nTo leverage caching, modern hardware pre-fetches the data, likely to be accessed, from the slower storage and caches it. This boosts the performance of the application.\n\nThere are two ways to pre-fetch:\n\n- hardware intelligently pre-fetches the data as per the access pattern\n- hardware exposing \"prefetch\" instruction leaving to the business logic to prefetch\n\n## Why caching would not improve GC performance?\n\nAs we established, for caching to improve the performance our use-case would need to exhibit either spatial or temporal locality. Do mark and sweep exhibit either of them?\n\n### Mark n Sweep and Temporal Locality\n\nMark and Sweep GC does not exhibit temporal locality given that we mark all the reachable objects in one iteration and in other iteration we go through the unreachables one and free them.\n\n### Mark n Sweep and Spatial Locality\n\nMark and Sweep GC does not exhibit spatial locality given we we do a DFS on object reference tree which hopes from one object too another in random order. So pre-fetching of memory locations would not help.",
    "notes_gd": "https://drive.google.com/file/d/1GpbYMPJPS78hOlN6KRagF58S7lvQfhh4/view?usp=sharing",
    "slug": "why-caching-would-not-speed-up-mark-and-sweep-gc"
  },
  {
    "id": 142,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "W6HANd8c9t4",
    "title": "An in-depth introduction to Blue Green Deployments",
    "description": "Deployments are a pain if we are unsure about our release changes. But sometimes even if we know our changes well, something weird could happen in the infra that would fail your deployment and put your infra in an inconsistent state.\n\nSo, is there a way to address this? What if we have a place to deploy our changes and validate them before they hit production; and have a quick way to roll back if something goes wrong?\n\nThis is the core idea behind a Blue-Green Deployment\n\nIn this video, we take an in-depth look into a blue-green deployment pattern, understand why we need them, what problem it addresses, learn how they are implemented, talk about its benefits and challenges, and conclude with some points to remember when you adopt this deployment pattern.\n\nOutline:\n\n00:00 Agenda\n03:13 An introduction to Blue-Green Deployments\n06:36 Why do we need Blue-Green Deployments\n09:12 How Blue-Green Deployment is implemented?\n13:26 Pros of having a Blue-Green Deployment\n19:49 Challenges in having a Blue-Green Deployment\n25:36 When to use Blue Green Deployments\n26:39 Key points to remember while adopting Blue Green Deployment",
    "img": "https://i.ytimg.com/vi/W6HANd8c9t4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oGRFnT4RyU4ayoUFi/giphy.gif",
    "duration": "28:55",
    "view_count": 1203,
    "like_count": 56,
    "comment_count": 11,
    "released_at": "2022-05-18",
    "gist": "Blue Green Deployment is a deployment pattern that reduces the downtime during deployment by running two identical production setups called - Blue and Green.\n\nDuring deployment when we reboot the API servers there are chances that the incoming request fail because the server is unresponsive for a short period. Also, it might happen that the release had a major bug and we need a quick rollback.\n\nHow can we achieve both of them in one shot? The answer is Blue Green Deployment.\n\n## Implemention\n\nBlue Green deployment is implemented by having a separate fleet of infrastructure for the old version - Blue and the new version - Green. The new infrastructure is identical to the old one.\n\nThe deployment flow\n\n1. the new deployment artifact is tested and kept ready to be deployed\n2. a parallel infrastructure is set up identical to the existing\n3. the new version is deployed on the new fleet - Green\n4. the correctness of the setup is validated\n5. the proxy is re-configured to now forward 100% of traffic from the Blue (old) setup to the Green (new) setup\n6. a final sanity test is run on the new fleet\n7. the blue fleet is now shut down\n\n## Pros of Blue Green Deployment\n\n1. rollbacks are just a config change and hence quick\n2. downtime during deployment is minimal\n3. deployment is just a flip of a switch\n4. disaster recovery is simple given we already have the automation to build a parallel setup\n5. deployments can now happen during the working hours\n6. debugging a failed deployment is simple as we have the infrastructure with the debug information handy\n\n## Possible challenges\n\n1. during the deployment the infrastructure cost shoots 2x\n2. the stateful application would need to rebuild the state on new servers\n3. the database would have to be shared between the fleets\n4. any schema migration on the database needs to be backward and forward compatible\n5. the API responses have to be forward and backward compatible\n6. setting up this deployment strategy for the first time is difficult\n\n## When to use Blue Green Deployment?\n\n- when you need zero downtime deployment\n- your infrastructure can tolerate 100% traffic switch\n- you can bear the 2x cost of infrastructure during deployment\n\n## Points to remember\n\n- have a solid automation test suite to validate the correctness\n- ensure forward and backward compatibility of API and schema changes\n- infra cost will shoot up hence minimizing the time for which you are running 2x infra",
    "notes_gd": "https://drive.google.com/file/d/1jSowz0IW8kD4Fjrv2fsE-ygHVTaZto1d/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-blue-green-deployments"
  },
  {
    "id": 141,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "nnseeKxovaM",
    "title": "An in-depth introduction to Canary Deployments",
    "description": "Deployments are stressful; what if something goes wrong? What if you forgot to handle an edge case that was also missed during the unit test, integration test, or an internal QA iteration.\n\nPutting such code into production can take down your entire infrastructure and could cause a massive outage. In order or handle such a situation gracefully and provide us with an early warning about something's wrong we have Canary Deployment.\n\nIn this video, we take an in-depth look into canary deployments, learn why canary deployments are called canary deployments, and understand how they are actually implemented, talk about the pros and cons of this deployment pattern, and conclude with a one really solid use case where you absolutely need them.\n\nOutline:\n\n00:00 Agenda\n03:05 Introduction to Canary Deployment\n06:06 Why Canary Deployments are called Canary Deployments?\n08:04 How to implement Canary Deployments?\n10:03 Pros of having Canary Deployments\n16:21 How to pick servers and users for a rollout?\n19:08 Cons of having Canary Deployments\n21:25 When we absolutely need Canary Deployment",
    "img": "https://i.ytimg.com/vi/nnseeKxovaM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NAVQDibk6Sesg/giphy.gif",
    "duration": "23:53",
    "view_count": 1874,
    "like_count": 112,
    "comment_count": 13,
    "released_at": "2022-05-16",
    "gist": "Canary Deployments is a deployment pattern that rolls out the changes to a limited set of users before doing it for 100%.\n\nWe compare the vitals side-by-side from the old setup and the canary servers to ensure everything is as expected. If all is okay, then we incrementally roll out to a wider audience. If not, we immediately roll back our changes from the canaries.\n\nCanary Deployment thus acts as an Early Warning Indicator to prevent a potential outage.\n\n## Why canary deployment is named canary deployment?\n\nIn 1920, coal miners used to carry caged canaries with them. If the gases in the mines were highly toxic the canaries would die and that alerted the miners to evacuate immediately, thus saving their lives.\n\nIn canary deployment, the canary servers are the caged canaries that alert us when anything goes wrong.\n\n## Implementing canary deployment\n\nCanary deployments are implemented through a setup where a few servers serve the newer version while the reset serves the old version.\n\nA router (load balancer / API gateway) is placed in front of the setup and it routes some traffic to the new fleet while the other requests continue to go to the old one.\n\n## Pros of Canary Deployment\n\n- we test our changes on real traffic\n- rollbacks are much faster\n- if something's wrong only a fraction of users are affected\n- zero downtime deployments\n- we can gradually roll out the changes to users\n- we can power A/B Testing\n\n## Cons of Canary Deployment\n\n- engineers will get habituated to testing things in production\n- a little complex setup\n- a parallel monitoring setup is required to compare vitals side-by-side\n\n## Selecting users/servers for canary deployment?\n\nThe selection is use-case specific, but the common strategies are:\n\n- geographical selection to power regional roll-out\n- create specific user cohorts eg: beta users\n- random selection\n\n## When we absolutely need Canary Deployments\n\nSay you own the Auth service that is written in Java and you chose to re-write it in - Golang. When taking it to production, you would NOT want to make a direct 100% roll-out given that the new codebase might have a lot of bugs.\n\nThis is where canary is super-helpful when we a fraction of servers serving requests from Golang server while others from the existing setup. We now forward 5% traffic to the new ones and observe how it reacts.\n\nOnce we have enough confidence in the newer setup, we increase the roll-out fraction to 15%, 50%, 75%, and eventually 100%. Canary setup thus gives us a seamless transition from our old server to a newer one.",
    "notes_gd": "https://drive.google.com/file/d/1JJD_Pa9AkUvhaZ7Dzwd4sQiGdC8nPn5t/view?usp=sharing",
    "slug": "an-in-depth-introduction-to-canary-deployments"
  },
  {
    "id": 140,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "eRndYq8iTio",
    "title": "Introduction to RPC - Remote Procedure Calls",
    "description": "One of the most interesting things that revived itself after a decade is Remote Procedure Calls, fondly called as RPCs; and they are widely adopted to do inter-service communication over the network.\n\nThe core highlight that sets RPCs apart is that they are designed to make the network call look just like a local function call. It does this by abstracting out all the complexities like serialization, deserialization, and transport.\n\nIn this video, let's take an in-depth look into what RPC is, where it fits, what are stubs, how communication happens between the services, and conclude by going through the advantages and disadvantages of using RPCs.\n\nOutline:\n\n00:00 Agenda\n03:01 Introduction to Inter-Service Communication\n03:43 Why RPCs were conceptualized?\n08:50 What is RPC?\n11:42 What are stubs in RPC?\n17:07 Interface Definition and Stub Generation\n22:09 Communication in RPC\n22:57 Advantages of using RPC\n29:16 Concerns while using RPC\n\nhttps://github.com/arpitbbhayani/grpc-advcalc",
    "img": "https://i.ytimg.com/vi/eRndYq8iTio/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/100iCPYcAQx3WM/giphy.gif",
    "duration": "33:5",
    "view_count": 2170,
    "like_count": 133,
    "comment_count": 18,
    "released_at": "2022-05-13",
    "gist": "What are Remote Procedure Calls? How were they conceptualized? and Why are people adopting them? Here's why \ud83d\udc47\u200d\n\nThe two services need to communicate with each other so as to get things done. The most common way to do it today is to make a REST call over HTTP to the other service.\n\nWith this approach, every service needs to write logic to make HTTP call to the other service and handle things like failures, retries, compression, and security. Can we not abstract these complexities in some way?\n\nRPCs were conceptualized to solve this problem.\n\n## Remote Procedure Calls\n\nRPCs are designed to make remote network calls look and feel like local procedures. They abstract out all the complexities of remote invocations like Marshalling, Unmarshalling, Compressions, Retries, Security, etc.\n\nRPCs achieve this level of coherence using Stubs that sit in between the two services and convert incoming and outgoing packets into native objects.\n\n## Stubs\n\nStubs are the common piece of auto-generated code that defines the interface, in a given language, exposed by the server, and used by the client to consume the data.\n\nThe interface is defined in a common language like Protobuf and holds the information about functions that the server exposes and the request response object types. A generator is shipped with the RPC runtime that would take this interface and generate code in the target language.\n\nFor example, if the Auth service is written in Golang, the generator would generate a working code with the interface along with the transport details. This way, we can solely focus on writing the business logic and not worry about the network or other repetitive things.\n\n## Communication RPC\n\nRPC can use any transport protocol for communication - Raw TCP, UDP, HTTP 1.1, or even HTTP 2. The transport is just a way through which the marshaled information will be sent across systems; and depending on the features an RPC runtime plans to support, an appropriate protocol will be chosen.\n\n## Advantages of using RPC\n\n- easy to use\n- strong API contract\n- remote invocations are just local function calls\n- cross language communication is a breeze\n- mundane tasks like retries, compression, etc are abstracted\n- get performance out-of-the-box - streaming, connection pool\n- security is just a plug\n- no need to write client libraries, they can be auto-generated\n\n## Concerns while adopting RPC\n\n- stubs need to be re-generated whenever the signature changes\n- testing RPC is n trivial for beginners\n- getting started can be a little challenging\n- browser support for RPC is pretty limited",
    "notes_gd": "https://drive.google.com/file/d/1UiyrR6YbvzWa_yTzXlSIL-lvaS23eMED/view?usp=sharing",
    "slug": "introduction-to-rpc-remote-procedure-calls"
  },
  {
    "id": 139,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "HiwOx-W1TIA",
    "title": "Designing Workflows in Microservices - Orchestration vs Choreography",
    "description": "In a microservices architecture there will always arise a need to design workflows; for example: when on an e-commerce website someone places an order, we need to send an email confirmation to the user, notify the seller to keep the shipment ready, and also assign a logistic delivery partner so that the package is delivered to the user.\n\nModeling these workflows is a challenge as it requires multiple microservices to coordinate. So, how can we implement them? There are two high-level architecture patterns to implement workflows, and they are - Orchestration and Choreography. In this video, we take a detailed look into the two patterns and see what they are, how they are implemented, and which one to use when?\n\nOutline:\n\n00:00 Agenda\n03:02 Introduction to Workflows in Microservices\n03:55 Orchestration\n06:28 Choreography\n09:51 When to use Orchestration, and when to use Choreography",
    "img": "https://i.ytimg.com/vi/HiwOx-W1TIA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/nMq17xl0Ls0xO/giphy.gif",
    "duration": "17:26",
    "view_count": 2112,
    "like_count": 133,
    "comment_count": 5,
    "released_at": "2022-05-11",
    "gist": "Say we are building an e-commerce website and upon every purchase made we need to send a confirmation email to the user, notify the seller to keep the shipment ready and assign a logistic delivery partner to deliver the package to the user. So, how do we implement this?\n\nTwo high-level architecture patterns help us achieve this\n\n- Orchestration\n- Choreography\n\n## Orchestration\n\nOrchestration is the simplest way to model workflows. The core idea of the Orchestration pattern is to keep the decision logic centralized and have a single brain in the system.\n\nIn our example, the Orders service can be that brain, and when the order is placed the order service talks to Notification, Seller, and Logistics services and get the necessary things done. The communication between them is synchronous and the Orders service acts as the coordinator.\n\nThe workflow as part of our example is a one-level simple workflow but in the real world, these workflows could become extremely complex and the Orders service would be needing to handle the coordination.\n\n## Choreography\n\nThe core idea of the Choreography pattern is to keep the decision logic distributed and let each service decide when needs to be done upon an event. It thus laid the foundation for Event Driven Architecture.\n\nIn our example, when the order is placed the Orders service will simply emit an event to which all the involved services subscribe. Upon receiving an event, the services will react accordingly and do what they are supposed to.\n\nAll the 4 involved services are thus totally decoupled and independent; making this a truly distributed and decentralized architecture\n\n## Orchestration vs Choreography\n\nMost model systems are inclined towards Choreography as it gives some amazing benefits\n\n- loose coupling: services involved are decoupled\n- extensibility: extending the functionality is simple and natural\n- flexibility: search service owns its own decision on the next steps\n- robustness: if one service is down, it does not affect others\n\nObservability might become a challenge here; given that we need to track each service, action it took, and completion of it.\n\nAlthough people prefer choreography, it does not make Orchestration bad. Orchestration has its advantages and can be used in modeling services that are involved transactionally.\n\nFor example, sending OTP during login is best modeled synchronous instead of doing it async. Another example is when we want to render recommended items the Recommendation service talks to relevant services to enrich the information before sending it to the user.",
    "notes_gd": "https://drive.google.com/file/d/1h-YVs2toYWW0qnRKGoPbPdlkpDgp9M9m/view?usp=sharing",
    "slug": "designing-workflows-in-microservices-orchestration-vs-choreography"
  },
  {
    "id": 138,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "82Xywy74kfE",
    "title": "Dissecting GitHub Outage - Downtime due to ALTER TABLE",
    "description": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nGitHub had a major outage and it all started with a schema migration. The outage affected their core services like GitHub actions, API requests, pull requests, and many more. Today, we dissect this outage and do an intense deep dive to extract 5 amazing insights. We also see how they very smartly mitigated the outage along with a potential long-term fix. \n\nOutline:\n\n00:00 Agenda\n02:57 Introduction\n03:23 Insight 1: Schema Migrations can take weeks to complete\n05:48 Insight 2: How schema are altered when the table is huge\n08:52 Insight 3: Deadlocks on Read Replicas\n11:20 Insight 4: Separate Replica fleet for internal read traffic\n13:59 Insight 5: Database failures cascade\n18:13 Mitigation Strategy\n29:28 Lont-term Fix\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/82Xywy74kfE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/nrXif9YExO9EI/giphy.gif",
    "duration": "36:44",
    "view_count": 1906,
    "like_count": 98,
    "comment_count": 24,
    "released_at": "2022-05-09",
    "gist": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nIt happened to GitHub on 27th November 2021 when most of their services were down because of a large schema migration.\n\n## Why did the outage happen?\n\nGitHub ran a migration on a massive MySQL table and it made their replicas enter deadlock and crash. Here are the 5 insights about their architecture\n\n### Insight 1: Schema migration can take weeks to complete\n\nSchema migrations are intense operations as in most cases require you to copy the entire table with the new schema. Hence it might take a schema migration weeks to complete.\n\n### Insight 2: The last step of migration is RENAME\n\nTo protect the database from not taking excessive locks during the migration, we create an empty ghost table from the main table, apply migration, copy the data, and then rename the table. This reduces the locks we need for the migration.\n\n### Insight 3: Read Replicas can have deadlocks\n\nThe writes happening through the replication job and the production read load can create deadlocks on the read replicas.\n\n### Insight 4: Have a separate fleet of replicas for internal traffic\n\nHave a separate fleet of Read Replicas for internal workflows ex: analytics, etc. This way, any internal load will not affect the production load.\n\n### Insight 5: Database failures cascade\n\nWhen a replica fails, the load on healthy one's increases; which may them down and hence the cascading effect.\n\n## Mitigation\n\nThe outage happened because there were not enough read replicas to handle the load, hence in order to mitigate it the way out was to add more read replicas. GitHub team very smartly promoted the replicas used for internal workloads to handle production.\n\nAlthough the move was smart, it did not mitigate the outage because the incoming load was so high that the new replicas added also started crashing.\n\n### Data Integrity over Availability\n\nTo ensure that the data integrity is not compromised because of repeated crashes, GitHub took a call and let the Read traffic fail. They took the replica out of the production fleet, gave it time to complete the migration, and then added it back.\n\nThis way all the replicas got the time they needed to complete the schema migration. It took some but the issue was completely mitigated.\n\n## Long-term fix\n\nVertical Partitioning is a long-term fix for this problem. The idea is to create smaller databases that hold related tables; ex: all tables related to Repositories can go in one DB. This allows migration to quickly complete and during an outage, only the involved functionalities will be affected.",
    "notes_gd": "https://drive.google.com/file/d/14jdP8o2wFZYL0iFtsCbqKr3jEX0QstaY/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-alter-table"
  },
  {
    "id": 137,
    "topic": {
      "id": 0,
      "uid": "garbage-collection",
      "name": "Garbage Collection",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3MvruOP09U1bWmcjxLIMwR",
      "bgcolor": "#F0E6FF",
      "themecolor": "#A66AFF"
    },
    "yt_video_id": "lhrRwjVPXPo",
    "title": "Tricolor Abstraction to build concurrent Garbage Collectors",
    "description": "A basic Mark-and-Sweep garbage collection algorithm operates in Stop-the-World mode, which means the program execution pauses while the GC runs. So, can we write a GC that runs concurrently with the program and does not need to always stop the world?\n\nIn this video, we take a look into something foundational called the Tricolour Invariant that enables us to build concurrent garbage collectors with low pause times. The concept we discuss is something that was contributed by Dijkstra, famously known for his Shortest Path algorithm.\n\nOutline:\n\n00:00 Agenda\n03:06 Recap of Mark and Sweep Garbage Collection\n03:56 States of an object during GC\n06:04 Tricolour Abstraction\n09:07 How does Tricolor Abstraction make things better?\n12:46 The Garbage Collection Flow with the Coloured Sets\n15:06 Why did we do this at all?",
    "img": "https://i.ytimg.com/vi/lhrRwjVPXPo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT5LMylSZz8lg53jC8/giphy.gif",
    "duration": "19:52",
    "view_count": 758,
    "like_count": 40,
    "comment_count": 5,
    "released_at": "2022-05-06",
    "gist": "The Mark and Sweep garbage collection algorithm is a simple DFS traversal with two phases - Mark and Sweep. In the mark phase, it marks all the objects that are reachable from the root, and the Sweep phase clears off all the unreachable ones.\n\nThis crude algorithm is slow and requires a program pause which means everything stops when the GC is cleaning up. This affects the performance and throughput of the program.\n\nSo, can we write a GC that runs concurrently with the program and does not need to always stop the world?\n\nThe foundation of concurrent Garbage Collector is based on a concept called Tricolour Abstraction which was developed by Dijkstra and Lamport, known for their work on core algorithms and distributed systems.\n\n## States of an object\n\nWhile tracing the objects across the heap we can see that each object can be in one of the 3 states: unprocessed, processing, and processed; and this becomes our three colors\n\nWhite: unprocessed objects\nGrey: visited but whose children are yet to be visited\nBlack: done processing\n\n### The garbage collection flow\n\nEvery node starts white. Once we see an object we color it Grey and once we are done visiting its children we mark it Black. This way we have 3 sets of objects: white, great, and black, and the objects move from white to grey and from grey to black.\n\nA key thing to note here is that we will never have an object that moves directly from white to black; i.e. there will never be an edge that connects one black and one white node.\n\n### How does Tricoloration make it better?\n\nBecause a black node is never connected to a white node, we ensure correctness i.e. a live object will never be cleaned up.\n\nOur GC flow can now be simplified as\n\n- pick the object from the grey set\n- color all the children of the node grey\n- move the grey object to the black\n\nrepeat the flow until the grey set is empty. Once done, we can just visit the white set, which now contains all the unreachable objects, and clean them up.\n\n### Speedup\n\nNow that we segregated the objects into sets we can ensure a quick cleanup by putting more threads at work on the grey set. It is hard to make a crude DFS concurrent and structuring it as sets make implementation much simpler.\n\nWe make our system reactive by keeping an eye on the grey set and triggering GC as soon as it hits some threshold.\n\nThis method of garbage collection is called \"on-the-fly\" which runs concurrently with the program and mutates the color of the objects as part of program execution and does not wait for a separate GC cycle. This reduces the load on GC and minimizes the pause.",
    "notes_gd": "https://drive.google.com/file/d/1k6y-oTQ3i46VUKs58qHRxK6QuG87z1-5/view?usp=sharing",
    "slug": "tricolor-abstraction-to-build-concurrent-garbage-collectors"
  },
  {
    "id": 136,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "ewUw0sUxHI4",
    "title": "Synchronous and Asynchronous Communication between Microservices",
    "description": "How should two microservices talk to each other? Picking the right communication pattern is super-important as a good decision will ensure a great user experience and scalability while a bad one will ruin the party.\n\nThere are overall two categories of communication: Synchronous and Asynchronous; In this video, we in-depth discuss what synchronous communication is and how it is done, what asynchronous communication is and how it is done, the advantages and disadvantages of both of them, and most importantly understand how to decide which one to opt for with some real-world examples.\n\nOutline:\n\n00:00 Agenda\n03:08 Need for Communication between Microservices\n05:10 Synchronous Communication\n08:17 Advantages of Synchronous Communication\n09:07 Disadvantages of Synchronous Communication\n15:58 When to use Synchronous Communication\n18:40 Asynchronous Communication\n23:01 Advantages of Asynchronous Communication\n31:41 Disadvantages of Asynchronous Communication\n34:39 When to use Asynchronous Communication",
    "img": "https://i.ytimg.com/vi/ewUw0sUxHI4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3orif4qbRS6WjGJ2zC/giphy.gif",
    "duration": "40:10",
    "view_count": 2471,
    "like_count": 140,
    "comment_count": 14,
    "released_at": "2022-05-04",
    "gist": "Say, we are building a Social Network and anytime someone reacts to your post, you need to be notified. So, how should the Reaction service talk to the Notification service to send out a notification?\n\nThe communication would be much simpler and reliable, just a function call if it was a monolith; but things become tricky as we go distributed.\n\nMicroservices need to talk to each other to exchange information and get things done; and there are two categories of communication patterns - Synchronous and Asynchronous.\n\n## Synchronous Communication\n\nCommunication is synchronous when one service sends a request to another service and waits for the response before proceeding further.\n\nThe most common implementation of Sync communication is over HTTP using protocols like REST, GraphQL, and gRPC.\n\n### Advantages of Synchronous Communication\n\n- It is simple and intuitive\n- Communication happens in realtime\n\n### Disadvantages of Synchronous Communication\n\n- Caller is blocked until the response is received\n- Servers need to be pro-actively provisioned for peaks\n- There is a risk of cascading failures\n- The participating services are strongly coupled\n\n### When to use Synchronous Communication\n\n- When you cannot proceed without a response from the other service\n- When you want real-time responses\n- When it takes less time to compute and respond\n\n## Asynchronous Communication\n\nThe communication is asynchronous when the one service sends a request to another service and does NOT wait for the response; instead, it continues with its own execution.\n\nAsync communication is most commonly implemented using a message broker like RabbitMQ, SQS, Kafka, Kinesis, etc.\n\n### Advantages of Asynchronous Communication\n\n- Services do not need to wait for the response and can move on\n- Services can handle surges and spikes better\n- Servers do not need to be proactively provisioned\n- No extra network hop due to Load Balancer\n- No request drop due to target service being overwhelmed\n- Better control over failures and retires is possible\n- Services are truly decoupled\n\n### Disadvantages of Asynchronous Communication\n\n- Eventual consistency\n- Broker could become a SPoF\n- It is harder to track the flow of the message between services\n\n### When to use Asynchronous Communication\n\n- When delay in processing is okay\n- When the job at hand is long-running and takes time to execute\n- When multiple services need to react to the same event\n- When it is okay for the processing to fail and you are allowed to retry",
    "notes_gd": "https://drive.google.com/file/d/16T1TszFP0yXXxFWAk9wQnzii5JIeo5O2/view?usp=sharing",
    "slug": "synchronous-and-asynchronous-communication-between-microservices"
  },
  {
    "id": 135,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "oiZH5U_a0pg",
    "title": "Introduction to Serverless Computing and Architecture",
    "description": "Serverless Computing is one of the hottest topics of discussion today, but the term \"serverless\" is slightly misleading and it does not mean that your code will not need a server to run. We have to be extremely cautious while deciding on adopting serverless for our use case, but it is not something that fits all the use cases.\n\nIn this video, we talk about what serverless computing is, see why it was built in the first place, learn about 5 real-world use-cases that become super-efficient with serverless architecture, understand the advantages and more importantly, the disadvantages of adopting it, and conclude with acknowledging when to use and when not to use this computation pattern.\n\nOutline:\n\n00:00 Agenda\n03:01 Need and the idea of the Serverless Computing\n11:16 Usecase 1: Chatbots\n13:57 Usecase 2: Online Judge\n16:27 Usecase 3: Vending Machines\n18:00 Usecase 4: CRON Jobs\n19:54 Usecase 5: Batch and Stream Processing\n22:16 Advantages of Serverless Computing and Architecture\n26:21 Disadvantages of Serverless Computing and Architecture\n31:37 When NOT to use Serverless\n34:00 When to use Serverless",
    "img": "https://i.ytimg.com/vi/oiZH5U_a0pg/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/5k1VkABjw5wQq0PNEU/giphy.gif",
    "duration": "36:23",
    "view_count": 2082,
    "like_count": 125,
    "comment_count": 17,
    "released_at": "2022-05-02",
    "gist": "Serverless is a cost-efficient way to host your APIs and it forms the crux of systems like Chatbots and Online Judge.\n\nServerless does not mean that your code will not run on the server; it means that you do not manage, maintain, access, or scale the server your code is running on.\n\nThe traditional way to host APIs is by spinning up a server with some RAM, and CPU. Say the resources make your server handle 1000 RPS, but you are getting 1000 RPS only 1% of the time which means for the other 99% you are overprovisioned.\n\nSo, what if there was an Infrastructure that\n\n- scales up and down as per the traffic\n- is billed per execution\n- is self-managed maintained and fault-tolerant\n\nThese requirements gave rise to Serverless Computing.\n\n## Real-world applications\n\n### Chatbot\n\nSay, we build a Slack chatbot that responds with the Holiday list when someone messages `holidays` . The traffic for this utility is going to be insignificant, and keeping a server running the whole time is a waste. This is best modeled on Serverless which is invoked on receiving a message.\n\n### Online Judge\n\nEvery submission can be evaluated on a serverless function and results can be updated in a database. Serverless gives you isolation out of the box and keeps the cost to a bare minimum. It would also seamlessly handle the surge in submissions.\n\n### Vending Machine\n\nUpon purchase, the Vending machine would need to update the main database, and the APIs for that could be hosted on Serverless. Given the traffic is low and bursty, Serverless would help us keep the cost down.\n\n### Scheduled DB Backups\n\nSchedule daily DB backups on the Serverless function instead of running a separate crontab server just to trigger the backup.\n\n### Batch and Stream Processing\n\nUse serverless and invoke the function every time a message is pushed on the broker making the system reactive instead of poll-based.\n\n## Advantages\n\n- No need to manage and scale the infra\n- The cost is 0 when you do not get any traffic\n- Scale is out of the box; so no capacity planning is needed\n\n## Disadvantages\n\n- Takes time to serve the first request as the underlying infra might boot up\n- The execution has a max timeout, so your job should complete within the limit\n- Debugging is a challenge\n- You are locked in on the vendor you chose\n\n## When NOT to use Serverless\n\n- Load, usage, and traffic pattern is consistent\n- Execution will go beyond the max timeout\n- You need multi-tenancy\n\n## When to use Serverless\n\n- Quick build, prototype, and deploy the changes\n- Usecase is lightweight\n- Traffic is bursty",
    "notes_gd": "https://drive.google.com/file/d/1sZShE0r41XcFa2gEPW1RS_YTaR3tC-zH/view?usp=sharing",
    "slug": "introduction-to-serverless-computing-and-architecture"
  },
  {
    "id": 134,
    "topic": {
      "id": 0,
      "uid": "garbage-collection",
      "name": "Garbage Collection",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3MvruOP09U1bWmcjxLIMwR",
      "bgcolor": "#F0E6FF",
      "themecolor": "#A66AFF"
    },
    "yt_video_id": "4qLf0FJMyf0",
    "title": "Mark and Sweep Garbage Collection Algorithm",
    "description": "Garbage Collection has to be one of the most interesting topics of discussion out there. In the previous videos, we took a look at why programming languages need an automatic garbage collection and what are the key metrics and characteristics we look at while picking one.\n\nIn this video, we take a detailed look into one of the most common GC algorithms out there called Mark-and-Sweep. We will talk about the two key phases of the algorithms, also dive into the important details and nuances of it, and take a look at two super-interesting optimizations that would algorithm a massive boost to the performance.\n\nOutline:\n\n00:00 Agenda\n02:57 Introduction to the Mark and Sweep Algorithm\n06:12 Mutator Threads, Collector Threads, and assumptions\n09:30 When is Garbage Collector invoked?\n13:11 Phase 1: Prepare the root list\n14:35 Phase 2: Mark roots, proceed, and a super optimization\n17:19 Phase 3: Mark and DFS Traversal\n19:22 Phase 4: Sweep and a super optimization",
    "img": "https://i.ytimg.com/vi/4qLf0FJMyf0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NV4cSrRYXXwfUcYnua/giphy.gif",
    "duration": "23:43",
    "view_count": 964,
    "like_count": 54,
    "comment_count": 13,
    "released_at": "2022-04-29",
    "gist": "The Mark-and-Sweep garbage collection algorithm is one of the most common and widely adopted garbage collection algorithms out there. It leverages the power of Graph data structure along with DFS to power the cleanup.\n\nAlmost all programming languages support allocating objects in heap and referring them via pointers to another object. This reference creates a small graph of all the references linked to the one root node.\n\nThe root node could be a global variable (object) or something allocated on the thread stack, and anything and everything referenced from that becomes the child nodes and the process continues.\n\n### Indirect Collection Algorithm\n\nThe mark-and-Sweep garbage collection algorithm is an indirect collection algorithm, which means it does not have any direct information about the garbage, instead, it identifies the garbage by eliminating everything LIVE.\n\n## When is the GC triggered?\n\nThe garbage collection is triggered when the runtime environment is unable to allocate any new object on the heap. When the language tries to fire `new` and it is unable to allocate space, it first triggers a quick garbage collection and then tries to re-allocate; if not successful again it throws an OutOfMemory exception, and in most cases, it is a fatal error.\n\n## Mark-and-Sweep Algorithm\n\nAlthough the algorithm is primarily split into two phases Mark and Sweep; to build a better understanding we split it into 4.\n\n### Prepare the root list\n\nThe first step of this algorithm is to extract and prepare the root list, and the roots could be global variables or variables referenced in the thread stack. These become the seed objects on which we then trigger a DFS.\n\n### Mark the roots and proceed\n\nOnce we identified all the roots, we mark all the roots as `LIVE` and proceed with the Mark phase. A super optimization that some implementations apply is to invoke the Mark phase for every root as it would help us keep the stack smaller.\n\n### Mark\n\nThe mark phase is just a Depth First Search traversal on one root node and the idea is to mark every node as `LIVE` that is reachable from the root node. Once all the nodes are marked, we can conclude that the nodes that remain unmarked are garbage; and the ones to be cleaned up.\n\n### Sweep\n\nThe nodes left unmarked are the garbage and hence in the sweep phase, the GC iterates through all the objects and frees the unmarked objects, and resets the marked object to prepare them for the next cycle.",
    "notes_gd": "https://drive.google.com/file/d/1_sCJpp4EQlk0GoAzsJEp06JYWEboAnom/view?usp=sharing",
    "slug": "mark-and-sweep-garbage-collection-algorithm"
  },
  {
    "id": 133,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "tV11trlimLk",
    "title": "Shared Database Pattern in Microservices",
    "description": "Microservices need to communicate with each other. Communication between them is always about getting or updating data that is owned by the other service. What if a service gets direct access to all the data it wants? This is the simplest way for the microservices to communicate with each other, and this pattern is called Sharing the Database. The core idea here is to let anyone who needs the data from a service or wants to update something, can directly talk to its database - no middlemen needed.\n\nAlthough most people think it is the wrong way of communication, we should not discard it completely. In this video, we talk about what this architecture pattern is, the 4 challenges associated with it, see ways to mitigate them, and understand when and where it could be beneficial for us to share the database rather than going through a middleman.\n\nOutline:\n\n00:00 Agenda\n03:13 Introduction\n04:14 Advantages of a sharing a database\n06:55 Challenge 1: External parties getting internal details\n10:30 Challenge 2: Replicating business logic\n13:31 Challenge 3: Risk of data corruption and deletion\n14:50 Challenge 4: Abusing the shared database\n16:27 Should we share the database then?",
    "img": "https://i.ytimg.com/vi/tV11trlimLk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/fxwvlpoM7NwP7i4jFB/giphy.gif",
    "duration": "21:21",
    "view_count": 2442,
    "like_count": 115,
    "comment_count": 22,
    "released_at": "2022-04-27",
    "gist": "Microservices need to communicate with each other, and one such way of doing it is through a shared database.\n\nFor example: While building a multi-user blogging application, say we have a Blogs service that manages all the blogs-related information and we have an Analytics service that takes care of all the analytics like Likes, Shares, Views, etc.\n\nAnalytics service updates the information asynchronously directly in the blog's database; eg: total_views that happened on the blog. This can be easily achieved by sharing the database between Blogs and Analytics, and this pattern is the Shared Database pattern.\n\n### Advantages of sharing the database\n\n- the simplest way of integration\n- no middleman involved\n- no latency overhead\n- quick development time\n\n## Challenges with Shared Database\n\nThere are 4 challenges to using this pattern\n\n### External parties know internal details\n\nBy sharing the database across services, an external party (Analytics) would get to know the internal details of the Blogs service; eg: deletion practice, schema, etc.\n\nThis leads to a very tight coupling between the services; which then restrains the maintainability and performance of the system. For example, whenever the Blogs service changes the schema, the Analytics Service would have to be informed about the change.\n\n### Sharing the database is sharing the logic\n\nTo compute some information we need to query a set of tables; and say, this information is required by the Blogs, Analytics, and Recommendation service.\n\nThe business logic to compute the information has to be replicated across all the 3 services. Any change in the logic needs to be made across all the services.\n\n### Risk of data corruption and deletion\n\nThere is a risk that one of the services might corrupt or delete some data given that the database is shared between the services.\n\n### Abusing the shared database\n\nOne service firing expensive queries on the database will affect the performance of other services sharing the same database.\n\n## When to share a database?\n\nA shared database pattern is helpful when you are seeking quick development time. Although it is not the best practice, sharing the database does reduce the development effort by a massive margin.\n\nSharing the database is also seen where it is inconvenient to have a middleman for the communication; for example: sending a notification to a million followers of a person is simple when the Relationship database is shared with the notification fan-out service; instead of iterating the millions of followers through some middleman API.",
    "notes_gd": "https://drive.google.com/file/d/1ql0chRVpcjgV4Fv_MJTRaXbtIZ3QJwcI/view?usp=sharing",
    "slug": "shared-database-pattern-in-microservices"
  },
  {
    "id": 131,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "wXvljefXyEo",
    "title": "Database Sharding and Partitioning",
    "description": "Sharding and partitioning come in very handy when we want to scale our systems. These concepts operate on the database and help us improve the overall throughput and availability of the system.\n\nIn this video, we take a detailed look into how a database is scaled and evolved through different stages, what sharding and partitioning are, understand the difference between them, see at which stage should we introduce this complexity, and a few advantages and disadvantages of adopting them.\n\nOutline:\n\n00:00 Introduction and Agenda\n03:05 How a database is progressively scaled?\n08:10 Scaling beyond the limit of vertical scaling\n11:57 Sharding vs Partitioning\n12:43 Example of Data Partitioning\n17:15 Sharding and Partitioning together\n20:20 Advantages and Disadvantages of Sharding and Partitioning",
    "img": "https://i.ytimg.com/vi/wXvljefXyEo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/9u8GF7MuhdvS8/giphy.gif",
    "duration": "23:53",
    "view_count": 2836,
    "like_count": 180,
    "comment_count": 19,
    "released_at": "2022-04-25",
    "gist": "Sharding and partitioning come in very handy when we want to scale our systems. Let's talk about these concepts in detail.\n\n## How is the database scaled?\n\nA database server is just a database process (like MySQL, MongoDB) running on a virtual server like EC2. Now when we put our database in production it starts getting from real good traction, say 100 writes per second (WPS).\n\n### Steady user growth\n\nSay, your product started getting some traction, and we find that the database is not able to handle the load, we scale it up by adding more CPU, RAM, and Disk to the server. This way we are now handling 200 WPS.\n\n### More read traffic  \n\nIf we see nor reads then can also choose to add a Read Replica and divert some of the read traffic to this node, while the master node can take in 200 WPS.\n\n### Viral Growth\n\nSay, your product went viral and you now got 5x more load which means now you have to handle 1000 WPS. To achieve this you again scale it up vertically and handle the desired load.\n\n### Insane growth\n\nSay, you now cracked the PMF and are getting some really solid traction and need to handle 1500 WPS, and when you visit the database console you found out that it is not possible to vertically scale your database any further, so how do you handle 1500 WPS?\n\nThis is where the horizontal scaling comes into the picture.\n\n## Scaling the database horizontally\n\nWe know one database server can handle 1000 WPS, but we need to handle 1500 WPS, so we split the data into half and split it across two databases such that each database owns half of the data and all the writes for that data goes to that particular instance.\n\nThis way each server will get 750 WPS, which it can very easily handle, and owns 50% of the data. Thus by adding more database servers we handled 1500 WPS (more than what a single machine could handle)\n\n## Sharding and Partitioning\n\nEach database server in the above architecture is called a Shard while the data is said to be partitioned. Overall, a database is sharded and the data is partitioned.\n\n### Partitioned data on shards\n\nIt is possible to have more partitions and fewer shards and in that case, each shard will own multiple partitions. Say, we have 100GB of data and it is split into 5 partitions and we have 2 shards. One shard will be responsible for 3 partitions while the other for 2.\n\n## Advantages and Disadvantages\n\n### Advantages of Sharding\n\n- handle more reads and writes\n- increases overall storage capacity\n- overall high availability\n\n### Disadvantages of Sharding\n\n- sharding is operationally complex\n- cross-shard queries are super-expensive",
    "notes_gd": "https://drive.google.com/file/d/14RqKYjN2pgqYTaVB1DYlH4WjZ0A8XQ02/view?usp=sharing",
    "slug": "database-sharding-and-partitioning"
  },
  {
    "id": 132,
    "topic": {
      "id": 0,
      "uid": "garbage-collection",
      "name": "Garbage Collection",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3MvruOP09U1bWmcjxLIMwR",
      "bgcolor": "#F0E6FF",
      "themecolor": "#A66AFF"
    },
    "yt_video_id": "IojMqbegejk",
    "title": "How to pick a garbage collector?",
    "description": "\"Best\" Garbage Collector is a myth.\n\nIf you are building your Garbage Collector or trying to pick the best for your use case, 7 parameters would help you objectively make the decision. These parameters define a Garbage Collector and determine its performance.\n\nIn the previous video we talked about why languages need an automatic garbage collection - primarily because engineers are unreliable, and in this one, we talk about the 7 key metrics and characteristics of a Garbage Collector that can help us compare and judge which one is better than the other for a specific workload;\n \nSomething all senior engineers spend their time on to get the performance out of their systems.\n\nOutline:\n\n00:00 Introduction\n03:13 7 Characteristics and Metrics of a Garbage Collector\n06:53 Safety\n08:21 Throughput\n11:12 Completeness\n12:52 Pause Time\n17:44 Space Overhead\n19:58 Language-Specific Optimizations\n22:27 Scalability",
    "img": "https://i.ytimg.com/vi/IojMqbegejk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Za7ZWOU0su5BC/giphy.gif",
    "duration": "26:37",
    "view_count": 867,
    "like_count": 43,
    "comment_count": 1,
    "released_at": "2022-04-22",
    "gist": "\"Best\" Garbage Collector is a myth.\n\nIf you are ever writing your Garbage Collector or trying to pick one for your workload, there are 7 metrics that you should be evaluating a GC algorithm on.\n\nNo one garbage collector can be the best across all 7 properties and it was found that any GC algorithm is at least 15% better than other algorithms on at least one of the characteristics. Hence it all boils down to the needs of your specific workload to pick one over the other. The key characteristics are\n\n## Safety\n\nA garbage collector is safe when it never reclaims the space of a LIVE object and always cleans up only the dead objects.\n\nAlthough this looks like an obvious requirement, some GC algorithms claim space of LIVE objects just to gain that extra ounce of performance.\n\n## Throughput\n\nA garbage collector should be as little time cleaning up the garbage as possible; this way it would ensure that the CPU is spent on doing actual work and not just cleaning up the mess.\n\nMost garbage collectors hence run small cycles frequently and a major cycle does deep cleaning once a while. This way they maximize the overall throughput and ensure we spend more time doing actual work.\n\n## Completeness\n\nA garbage collector is said to be complete when it eventually reclaims all the garbage from the heap.\n\nIt is not desirable to do a complete clean-up every time the GC is executed, but eventually, a GC should guarantee that the garbage is cleaned up ensuring zero memory leaks.\n\n## Pause Time\n\nSome garbage collectors pause the program execution during the cleanup and this induces a \"pause\". Long pauses affect the throughput of the system and may lead to unpredictable outcomes; so a GC is designed and tuned to minimize the pause time.\n\nThe garbage collector needs to pause the execution because it needs to either run defragmentation where the heap objects are shuffled freeing up larger contiguous memory segments.\n\n## Space overhead\n\nGarbage collectors require auxiliary data structures to track objects efficiently and the memory required to do so is pure overhead. An efficient GC should have this space overhead as low as possible allowing sufficient memory for the program execution.\n\n## Language Specific Optimizations\n\nMost GC algorithms are generic but when bundled with the programing language the GC can exploit the language patterns and object allocation nuances. So, it is important to pick the GC that can leverage these details and make its execution as efficient as possible.\n\nFor example, in some programming languages, GC runs in constant time by exploiting how objects are allocated on the heap.\n\n## Scalability\n\nMost GC are efficient in cleaning up a small chunk of memory, but a scalable GC would run efficiently even on a server with large RAM. Similarly, a GC should be able to leverage multiple CPU cores, if available, to speed up the execution.",
    "notes_gd": "https://drive.google.com/file/d/1eHwPYsx-k61JoLCz4FudN4DFZMAe-_3j/view?usp=sharing",
    "slug": "how-to-pick-a-garbage-collector"
  },
  {
    "id": 129,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "nfkdKHcKxbE",
    "title": "How to scope a microservice?",
    "description": "Microservices are extremely tempting and you will always feel like writing a new service for every problem at hand. You might build a service with very fine-grained responsibilities or you can build one that covers a big spectrum. So, what is the best approach? How should you decide?\n\nIn this video, we talk about ways to model and scope a microservice such that the architecture remains robust and flexible; and to achieve this we use the two key guiding concepts - Loose Coupling and High Cohesion.\n\nOutline:\n\n00:00 What is the problem with Microservice?\n03:05 Why do we love building microservices?\n04:04 What happens if we do not scope our services well?\n05:52 Two key guiding principles to scope a microservice\n07:19 Loose Coupling\n12:15 High Cohesion",
    "img": "https://i.ytimg.com/vi/nfkdKHcKxbE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/fKvTY11icTAZIPCIQb/giphy.gif",
    "duration": "19:3",
    "view_count": 1588,
    "like_count": 82,
    "comment_count": 9,
    "released_at": "2022-04-20",
    "gist": "It is always exciting to create new microservices as it gives us so many things to look forward to- a fresh codebase, a new tech stack, or even maybe a clean CICD setup. But does this mean we should create as many microservices as possible?\n\nWhenever we decide to create a new microservice, it is very important to understand its scope of it. If you create a new service for every utility then you are effectively creating a mesh of network calls that is prone to a cascading failure. If your scope is too big, it would lead to the classic problem of a monolithic codebase.\n\nThere are a couple of guiding principles that would help us with scoping of microservice.\n\n## Loose Coupling\n\nServices are loosely coupled if changes made in one service do not require a change in other. This is the core ideology behind microservices as well, but while designing a system we tend to forget it.\n\nSay, we have an Orders service and a Logistics service. These services are loosely coupled when they do not share anything in common and are communicating with each other via API contracts.\n\nTo achieve loose coupling, make your microservices expose as little information as possible. The other service should just know how to consume the data and that is it. No internals, no extra details.\n\n## High Cohesion\n\nThe principle of High Cohesion says that the related behavior should sit together as part of one service while the unrelated ones should be separate. This would encourage services to be operating independently.\n\nIf the Orders service also owns the customer data then when the changes are deployed in one might affect the other module. So the scope of testing before taking things to production increases.\n\nIf there is a very strong coupling between the services then it may also happen that the changes in one lead to deploy a few other services- all at the same time. Deploying multiple services at the same time is very risky; because one glitch and the almost entire product is down.\n\nHence it is not favorable for heterogeneous components to be part of the same service. Keep it crisp and short; and while designing try to keep services loosely coupled and split it to a level where the unrelated components are split up.",
    "notes_gd": "https://drive.google.com/file/d/1_P8YVcw7uwr0wfs2V6W-1gpOwnoG2Zdf/view?usp=sharing",
    "slug": "how-to-scope-a-microservice"
  },
  {
    "id": 106,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "CW4gVlU0xtU",
    "title": "Why, where, and when should we throttle or rate limit?",
    "description": "It is a common belief that a rate limiter is always external and is designed to prevent our systems from being abused by the external world, but this is not true. In this video, we understand what throttling is, why we need it in the first place and 5 use cases where external and internal rate limiters are super useful.\n\nOutline:\n00:00 Introduction\n02:56 What is Throttling?\n03:37 What rate limiter does when it gets a surge of requests?\n06:39 Why do we need a rate limiter?\n10:45 Usecase 1: Preventing catastrophic DDoS Attack\n12:20 Usecase 2: Gracefully handling a surge in legitimate users\n13:46 Usecase 3: Multi-tiered limits\n15:42 Usecase 4: Not overusing an expensive vendor\n16:48 Usecase 5: Streamlining deletes to protect an unprotected database",
    "img": "https://i.ytimg.com/vi/CW4gVlU0xtU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT5LMuVtaVYI03uXsc/giphy.gif",
    "duration": "19:5",
    "view_count": 1458,
    "like_count": 96,
    "comment_count": 16,
    "released_at": "2022-04-18",
    "gist": "## What is throttling?\n\nThrottling is a technique that ensures that the flow of the data or the requests being sent at the target machine/service/sub-system can be consumed at an acceptable rate.\n\nIt is a defensive measure and 3 possible reactions could be\n\n- slowing down the incoming requests\n- rejecting the surplus requests\n- ignoring the surplus requests\n\n## Why do we need throttling in the first place?\n\n- to prevent system abuse\n- to allow the amount of traffic we could handle\n- control the consumption cost\n- prevent cascading failures leading to a massive outage\n\n## Real-world use-cases for throttling\n\n### To prevent catastrophic DDoS attack\n\nWhen your service is under a DDoS attack the rate limiter acts as your first line of defense that could prevent the surplus request from reaching your system. It would only allow the requests to go through at the configured rate.\n\n### To gracefully handle a surge of users\n\nIt is possible that your product goes viral and now you are seeing a genuine surge in users. Upon getting a genuine surge in users, the stateful components like databases and caches crash which takes down the entire site.\n\nRate limiter in this case will help in preventing the entire site from going down; although some users would see some error, like 429- Too many requests- your product will continue to seamlessly work for the other set of users.\n\n### Multi-tiered limits\n\nSay, you are running a CICD company and offer 3 tiers of pricing- Tier 1 offers 200 minutes of build time, Tier 2 offers 1000 mins while Tier 3 offers unlimited build time. An internal rate limiter can keep track of the build times consumed by a customer and reject the requests once the limit is hit.\n\n### Ensure you are not over-consuming\n\nSay, we are consuming a super expensive third-party API and we want to ensure that we are not using it beyond a certain number otherwise the cost will shoot up. An internal rate limiter can keep a check on this to ensure the surplus request does not go through.\n\n### Not overwhelming an unprotected system\n\nHard deleting from a database is an expensive operation. If we are deleting a huge number of rows from the DB it may severely affect the performance of the DB and hence it is best done in a staggered way. An internal rate limiter can help us streamline the writing by spreading them uniformly across time.",
    "notes_gd": "https://drive.google.com/file/d/11lTCiIbRk0aRlEaebjSI1mU6g_hTAway/view?usp=sharing",
    "slug": "why-where-and-when-should-we-throttle-or-rate-limit"
  },
  {
    "id": 115,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "xa-hMF8gku0",
    "title": "An engineering deep-dive into Atlassian's Mega Outage of April 2022",
    "description": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. In this video, we will do an engineering deep dive into this outage trying to understand their engineering systems and practices.\n\nWe extract 6 key insights into how their engineering systems are built, their backup and restoration strategies, and most importantly why is it taking them so long to recover the data.\n\nDisclaimer: I do not have any insider information about this and the views are pure speculation.\n\nOutline:\n\n00:00 Impact of the outage\n03:56 Insight 1: Incremental Backup Strategy\n06:38 Why did the Atlassian outage happen?\n07:30 Insight 2: Progressive Rollout Strategy\n10:57 Insight 3: Soft Deletes vs Hard Deletes\n14:28 Insight 4: Synchronous Replication for High Availability\n17:47 Insight 5: Immutable backups for point-in-time recovery\n21:04 Insight 6: Nearly multi-tenant architecture\n23:30 Why is it taking time for Atlassian to recover the deleted data?\n\nOutage Report: https://www.atlassian.com/engineering/april-2022-outage-update",
    "img": "https://i.ytimg.com/vi/xa-hMF8gku0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NTur7XlVDUdqM/giphy.gif",
    "duration": "31:24",
    "view_count": 4659,
    "like_count": 241,
    "comment_count": 37,
    "released_at": "2022-04-15",
    "gist": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. Let's dissect the outage and understand its nuances of it.\n\nDisclaimer: I do not have any insider information and the views are pure speculation.\n\n## Insight 1: Data loss up to 5 minutes\n\nBecause some customers reported a data loss of up to 5 minutes before the incident, it shows that the persistent backup incrementally every 5 minutes. The backup typically happens through Change Data Capture which operates right over the database.\n\n## Insight 2: Rolling release of products\n\nAtlassian rolls out features to a subset of the users and then incrementally rolls them to others. This strategy of incremental rollout gives companies and teams a chance to test the waters on a subset and then roll out to the rest.\n\n## Insight 3: Mark vs Permanent Deletion\n\nThe script that Atlassian ran to delete had both the options- Mark of Deletion and Permanent deletion.\n\nMark for deletion: is soft delete i.e. marking is_deleted to true.\nPermanent deletion: hard delete i.e. firing DELETE query\n\nWhy do companies need permanent deletion? for compliance because GDPR gives users a Right to be Forgotten\n\n## Insight 4: Synchronous Replication\n\nTo maintain high availability they have synchronous standby replicas which means that the writes happening needs to succeed on both the databases before it is acknowledged back to the user. This ensures that the data is crash-proof.\n\n## Insight 5: Immutable Backups\n\nThe backup is made immutable and stored on S3 in some serialized format. This immutable backup allows Atlassian to recover data at any point in time while being super cost-efficient at the same time.\n\n## Insight 6: Their architecture is not truly a Multi-tenant Architecture\n\nIn a true multi-tenant architecture, every customer gets its fragment of infra- right from DB, to brokers, to servers. But at Atlassian, multiple customers share the same infra components. Companies typically do this to cut down on their infrastructure cost.\n\n## Why is it taking a long time to restore?\n\nBecause data of multiple customers reside in the same database when the DB was backed up the data (rows and tables) were backed up as is; implying that the backup also had data from multiple customers.\n\nNow to restore the intermingled rows of a customer, the entire backup needs to be loaded into a database and then the rows of specific customers need to be restored. This process is extremely time-consuming.",
    "notes_gd": "https://drive.google.com/file/d/1Bp4huBCx-wkG6kCQ5raSujdKID4RLU_y/view?usp=sharing",
    "slug": "an-engineering-deep-dive-into-atlassian-s-mega-outage-of-april-2022"
  },
  {
    "id": 105,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "1r9bPisYaOQ",
    "title": "How to approach System Design?",
    "description": "System Design is tricky but it does not have to be difficult - be it a technical discussion at your workplace or your next big interview. In this video, I share the two approaches that I have been using to design scalable systems in the last 10 years of my career. I will also share the 3 key pointers to remember while designing any system that would help you keep your discussion crisp and focused.\n\nOutline:\n00:00 Introduction\n02:41 What is System Design?\n06:02 The Spiral Approach to System Design\n07:27 The Incremental MVP Approach to System Design\n09:47 Key Pointers to remember during System Design",
    "img": "https://i.ytimg.com/vi/1r9bPisYaOQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/kQ3FSVoJrkYWk/giphy.gif",
    "duration": "13:55",
    "view_count": 2526,
    "like_count": 105,
    "comment_count": 6,
    "released_at": "2022-04-13",
    "gist": "System Design is tricky but it does not have to be difficult- be it a technical discussion at your workplace or your next big interview. Let's talk about how to approach System Design. Something I compiled from 10 years of my career.\n\n## What is System Design?\n\nSystem Design is all about translating and solving customer needs and business requirements into something tangible. The output system could be an application, a microservice, a library, or even hardware.\n\nThere are a couple of approaches that we can use to design any system out there. Picking one over the other depends on the company you work for and the flexibility it provides.\n\n## The Spiral Approach\n\nThe Spiral Approach pans like a spiral in which you start with some core that you are comfortable with (database, communication protocol, queue. etc) and build your system around it. Every single component you add to the design is something that you are pretty confident about and can proceed with the added complexities.\n\nFor example:\n\n- start with the database\n- then add LB and more servers\n- then add a queue for async processing\n- then other services\n- then add synchronous HTTP based communication between them\n\n## The Incremental MVP Approach\n\nIn the Incremental MVP-based approach we with a Day 0 design and then see how each component behaves at scale by dry-running it; after identifying the bottlenecks you fix them and re-iterate. You stop the iteration once you are happy with the final product. This kind of approach is typically seen in startups where they do not want to invest in architecture and quickly roll out features.\n\nFor example:\n\n- start with Day 0 architecture of users, API servers, and DB\n- then you add LB and more API servers\n- then you add Read Replica on DB to support more reads\n- then you split the service into a couple of microservices\n- then you partition the DB to handle more scale\n\n## 3 key pointers while designing systems\n\n- Every system is infinitely buildable, hence fence it well\n- Seek clarifications from your seniors\n- Ask critical questions that challenge the design decisions\n",
    "notes_gd": "https://drive.google.com/file/d/185a6688TxLDLXlrDfn2l4ODLSR2m1xLL/view?usp=sharing",
    "slug": "how-to-approach-system-design"
  },
  {
    "id": 130,
    "topic": {
      "id": 0,
      "uid": "garbage-collection",
      "name": "Garbage Collection",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3MvruOP09U1bWmcjxLIMwR",
      "bgcolor": "#F0E6FF",
      "themecolor": "#A66AFF"
    },
    "yt_video_id": "jcMxuLZCcqU",
    "title": "Why do programming languages need automatic garbage collection?",
    "description": "We know how important Garbage Collection is for any programming language. So, today we explore why programming languages need automatic garbage collection in the first place?\n\nIn this video, we understand - the basics of memory management, the need to allocate objects on the heap, the constructs of explicit deallocation, what happens when we do not do our garbage collection well, and why we need automatic garbage collection.\n\nOutline:\n\n00:00 Basics of Memory Management\n04:39 Why do we need heap allocations?\n06:59 Explicit Deallocation constructs\n09:26 Memory leak - when we do not delete an allocated object\n11:00 Dangling pointer - when we dereference an already freed object\n15:08 Why we need automatic garbage collection",
    "img": "https://i.ytimg.com/vi/jcMxuLZCcqU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xT0xehfxlptGeeIvEk/giphy.gif",
    "duration": "16:55",
    "view_count": 1552,
    "like_count": 105,
    "comment_count": 14,
    "released_at": "2022-04-11",
    "gist": "Our programs need memory, typically in the form of variables and objects, to do their job. The objects are either allocated on Stack or Heap.\n\n## Stack allocated objects\n\nA locally declared variable \"int a = 10;\" is allocated on the stack i.e. the stack frame of the function call and hence when the function returns the stack frame is popped, making the variable non-existent. Hence variables allocated on Stack do not need to be freed explicitly.\n\n## Heap allocated objects\n\nA variable allocated on the heap is typically done through functions like the \"new\" or \"malloc\". The object space allocated for such entities is in RAM and they outlive the function scope and execution, and hence they need to be explicitly freed as we are done with it.\n\n## Why do we need a Heap?\n\nObjects assigned on Heap need to be garbage collected, but why do we need the heap in the first place? There are 3 main reasons:\n\n- We cannot grow your stack-allocated objects dynamically,\n- We need dynamically growing objects like Arrays, LinkedList, Trees\n- We might need objects that could be larger than what Stack can fit in\n- We might need to share the same object across multiple threads\n- We do not want our functions to copy and pass bulk objects\n\n## Garbage Collection: Explicit De-allocation\n\nPrimitive programming languages like C and C++ do not have their garbage collection instead expect the developer to not only allocate the object but also deallocate it explicitly. Hence we see the functions like \"malloc\" and \"free\".\n\nThe objects we allocate using \"malloc\" will continue to exist unless they are reclaimed using \"free\". The explicit need to \"Free-ing\" the allocated object is called Explicit Deallocation.\n\nAlthough cleaning up the mess we created is a good idea, it is not reliable that we rely on the engineers and developers to always free the objects they allocated. Hence this gives rise to the need for automatic cleanup of unused variables- automatic garbage collection.\n\nThe two key side-effects of not cleaning up the unused objects we allocate are\n\n- Memory Leak: Leading to an eventual process crash\n- Dangling Pointer: Program behaving unpredictably\n\nHence, to reduce human error, and make the process more reliable and performant the runtimes of the programming languages implement their automatic garbage collection.",
    "notes_gd": "https://drive.google.com/file/d/1vdsTC6j4eDJFzcTOhhIM0JQOR2fW1l61/view?usp=sharing",
    "slug": "why-do-programming-languages-need-automatic-garbage-collection"
  },
  {
    "id": 119,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "JPj6mhVLQN0",
    "title": "Advantages of adopting a microservices-based architecture",
    "description": "Microservices are great, and the overall microservices-based architecture has some key advantages. In this video, we talk about what are microservices, the key advantages of using a microservices-based architecture, and understand how to fence service and define its set of responsibilities.\n\nOutline:\n\n00:00 What are microservices?\n01:53 Key advantages of adopting a microservices-based architecture\n09:48 How to fence a microservice?",
    "img": "https://i.ytimg.com/vi/JPj6mhVLQN0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/lKXEBR8m1jWso/giphy.gif",
    "duration": "14:18",
    "view_count": 3205,
    "like_count": 192,
    "comment_count": 14,
    "released_at": "2022-04-08",
    "gist": "Microservices are small, autonomous, harmonic subsystems that work together to solve the bigger problem.\n\nThe core idea of microservices is Divide and Conquer. We break the big problem into smaller sub-problems, and solve each of the sub-problem optimally, enabling us to solve the bigger problem well.\n\nWhy Microservices?\n\n## Codebase grows over time\n\nThe product evolves and new features are added to it and that bloats up the codebase. It becomes difficult for multiple teams to coordinate and collaborate on a humungous codebase. One team breaking one module can take down the entire product.\n\n## Scaling is predictable\n\nWith microservices, scalability becomes predictable; you can linearly amplify the infrastructure requirements of individual microservices and be predictable in handling the load.\n\n## Teams become autonomous\n\nWith each team responsible for a set of microservices they can take charge of their tech stack and design decisions. These decisions will be best for their problem statement and can ensure that they are solving them the best way possible.\n\n## Fault Tolerance\n\nIf one microservice is down, it may lead to a partial outage of the product affecting a small fragment of the systems; while other components remain unaffected and can continue to service the traffic.\n\n## Upgrades are simpler\n\nSo long as a microservice adheres to the API contract, the team that owns it can upgrade the tech stack, architecture, and DB seamlessly.",
    "notes_gd": "https://drive.google.com/file/d/1lK2e3me09VNz51DWDunI5xDmlgLnOvfC/view?usp=sharing",
    "slug": "advantages-of-adopting-a-microservices-based-architecture"
  },
  {
    "id": 102,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "m6DtqSb1BDM",
    "title": "Implementing Idempotence in a Payments Microservice",
    "description": "Idempotence is an extremely critical property that we must consider while implementing an API or designing a microservice. The situation becomes even more critical when the money is involved - ensuring no matter how many times the user or internal service retries, the amount is transferred just once between the two users in one transaction.\n\nThis video looks at idempotence, why there is even a need for it, and, more importantly, one common implementation approach commonly observed in payments services.\n\nOutline:\n\n00:00 What is Idempotence?\n02:32 Examples where Idempotence is relevant\n04:06 Why do we even need to retry?\n07:18 Implementation Approach 1: Do not retry\n09:45 Implementation Approach 1: Check and Update",
    "img": "https://i.ytimg.com/vi/m6DtqSb1BDM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/j3x5hjUoXIesM/giphy.gif",
    "duration": "16:36",
    "view_count": 2506,
    "like_count": 163,
    "comment_count": 21,
    "released_at": "2022-04-06",
    "gist": "Idempotence is executing the same action multiple times, but the result is as if the operation was applied just once.\n\nExample: Double tapping a post multiple times on Instagram does not increase the like count. It just increases the first time and by 1.\n\nIdempotence becomes extremely critical when money is involved. If A wants to transfer money to B, the transfer should happen just once. If due for any reason, the payment is implicitly retried, the funds will be deducted twice, which is unacceptable.\n\n### Why would a transaction repeat?\n\nBefore we talk about idempotence, it is important to understand why it would repeat in the first place.\n\nConsider a situation where the payments service initiated a payment with a Payment Gateway, the money got deducted, but the payments service did not get the response. This would make the Payments service retry the API call, which would lead to a double deduction.\n\n## Implementing idempotence\n\nCheck and Update: Weave everything with a single ID.\n\nThe idea is to retry only after checking if the payment is processed or not. But how do we do this? The implementation is pretty simple- a global payment ID that weaves all the services and parties together.\n\nThe flow is:\n\n1. Payments service calls the PG and generates a unique Payment ID\n2. Payments service passes this ID to the end-user and all involved services\n3. Payments service initiates the payment with Payment Gateway with this ID specifying the transfer between A and B\n4. If there are any failures, the Payment service retries and in that request specifies the Payment ID\n5. Using the payment ID, the payment gateway checks if the transfer was indeed done or not and would transfer only when it was not done\n\nAlthough we talked about the Payments service here, this approach of implementing idempotence is pretty common across all the use cases. The core idea is to have a single ID (acting as the Idempotence Key) weaving all the involved services and parties together.",
    "notes_gd": "https://drive.google.com/file/d/1Zyt8qN11IiAZJKrdan4wi1c5J6n_eAyU/view?usp=sharing",
    "slug": "implementing-idempotence-in-a-payments-microservice"
  },
  {
    "id": 103,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "8S4k7k_f9Sk",
    "title": "Sending payload in an HTTP GET request",
    "description": "Can we send data in an HTTP GET request? Most people think, No. The truth is, we can send the data in the request payload of an HTTP GET request so long as our webserver can understand it.\n\nIn this video, we go through the HTTP 1.1 specification and see what it says about the GET requests, write a simple Flask application to see that we can indeed process the payload of a GET request if we want to, and, more importantly, go through a real-world example where it was essential to send data in the request payload.\n\nOutline:\n\n00:00 HTTP GET Request\n01:53 What does HTTP 1.1 specification say?\n05:38 Request payload in Python Flask\n07:18 ElasticSearch using request payload for search\n10:40 When to use HTTP request payload in a GET request",
    "img": "https://i.ytimg.com/vi/8S4k7k_f9Sk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l2Je53k7wnV8sQZOg/giphy.gif",
    "duration": "12:",
    "view_count": 3556,
    "like_count": 141,
    "comment_count": 24,
    "released_at": "2022-04-04",
    "gist": "It is a common myth that we could not pass the request body in the HTTP GET request. HTTP 1.1 specification neither enforces nor suggests this behavior.\n\nThis means it is up to implementing the application web servers- Flask, uWSGI, etc.- to see if it parses the request body in the HTTP GET request. To do this, just check the request object you would be getting in your favorite framework.\n\n### What can we do with this information?\n\nSay you are modeling an analytics service like Google Analytics in which you are exposing an endpoint that returns you the data point depending on the requirements. The requirements specified here could be a large, complex JSON.\n\nPassing this query in the URL of the GET request as a query param is not convenient as it would require you to serialize and escape the JSON string before passing.\n\nThis is a perfect use case where the complex JSON query can be passed as a request body in the HTTP GET request, giving a good user experience.\n\n### So, does any popular tool uses this convention?\n\nYes. ElasticSearch- one of the most popular search utilities, uses this convention.\n\nThe search endpoint of ElasticSearch is a GET endpoint where the complex search queries in JSON format are sent in the request payload.\n",
    "notes_gd": "https://drive.google.com/file/d/1JwVEh9EG0ZGts-VePXNlIE1e8kivdHbM/view?usp=sharing",
    "slug": "sending-payload-in-an-http-get-request"
  },
  {
    "id": 114,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "6oJaZbQKnJE",
    "title": "Dissecting Google Maps Outage: Bad Rollout and Cascading Failures",
    "description": "Google Maps had a global outage on 18th March 2022, during which the end-users were not able to use Directions, Navigation, or Google Maps in general. The outage happened because of a bad rollout, and it lasted more than 2 hours 30 minutes. During the outage, users complained to have seen Gray tiles implying that the map/direction was neither getting initialized nor working.\n\nIn this video, we dissect the Google Maps outage and understand what actually happened, how they mitigated it, and, more importantly, understand ways to prevent such an outage and build a robust way of handling cascading failures.\n\nOutline:\n\n00:00 Introducing the outage and impact\n02:07 Root cause\n08:36 Cascading Failures\n12:22 Remediation\n13:45 Preventive measures\n\nIncident Report: https://issuetracker.google.com/issues/225361510",
    "img": "https://i.ytimg.com/vi/6oJaZbQKnJE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xTiTnyogFXV3Khw6Xe/giphy.gif",
    "duration": "20:17",
    "view_count": 1243,
    "like_count": 78,
    "comment_count": 8,
    "released_at": "2022-04-01",
    "gist": "On the 18th of March, 2022, Google Maps faced a major outage affecting millions of people for a couple of hours. The outage happened due to a bad deployment.\n\nAlthough a bad deployment does not sound too bad, the situation worsens when there are cascading failures if 3 services have a synchronous dependency, forming a chain-like A -> B -> C.\n\nIf A goes down, it will have some impact on B, and if the impact is big enough, we might see B going down as well; and extending it, we might see C going down.\n\nThis is exactly what happened in this Google Outage. Some services had a bad deployment, and they started crashing. Tile Rendering service depended on it, and the Tile rendering service went down because of retries.\n\nThe Direction SDK, Navigation SDK directly invoked the Tile rendering service for rendering the maps, which didn't work, causing a big outage.\n\n## How to remediate a bad deployment?\n\nRollback as soon as possible.\n\n## Preventing cascading outages\n\n- Reject requests when the server is exhausted\n- Tune the performance of the webserver and networking stack of the server\n- Monitor the server resource consumption and set alerts\n- Add circuit breakers wherever possible.\n",
    "notes_gd": "https://drive.google.com/file/d/10yi5K2xluTA9d7RNrorDDKU_-Mi3uHKZ/view?usp=sharing",
    "slug": "dissecting-google-maps-outage-bad-rollout-and-cascading-failures"
  },
  {
    "id": 116,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF"
    },
    "yt_video_id": "oMhESvU87jM",
    "title": "Implementing Distributed Transactions using Two Phase Commit Protocol",
    "description": "Previously, we built a theoretical foundation of Distributed Transaction using the Two-Phase Commit protocol. In this video, we implement the Distributed Transaction locally and mimic the food delivery system locally. While implementing we understand how to make the individual operations atomic and the entire distributed transaction atomic. We address resource contention while guaranteeing a consistent user experience.\n\nOutline:\n\n00:00 Revising the Two-Phase Commit\n07:35 Designing Database Schema\n11:40 Defining API Endpoints\n12:24 High-Level Architecture and Request Flow\n19:55 No inconsistent data - Atomicity\n24:14 Code walkthrough",
    "img": "https://i.ytimg.com/vi/oMhESvU87jM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/RbDKaczqWovIugyJmW/giphy.gif",
    "duration": "39:24",
    "view_count": 2223,
    "like_count": 103,
    "comment_count": 22,
    "released_at": "2022-03-30",
    "gist": "Distributed Transactions are not theoretical; they are very well used in many systems. An example of it is 10-min food/grocery delivery.\n\nPreviously we went through the theoretical foundation for the Two-phase commit protocol; in this one let's spend some time going through the implementation detail and a few things to remember while implementing a distributed transaction.\n\n> The UX we want is: Users should see orders placed only when we have one food item and a delivery agent available to deliver.\n\nA key feature we want from our databases (storage layer) is atomicity. Our storage layer can choose to provide it through atomic operations or full-fledged transactions.\n\nWe will have 3 microservices: Order, Store, and Delivery.\n\nAn important design decision: The store services have food, and every food has packets that can be purchased and assigned. Hence, instead of just playing with the count, we will play with the granular food packets while ordering.\n\n## Phase 1: Reservation\n\nOrder service calls the reservation API exposed on the store and the delivery services. The individual services reserve the food packet (of the ordered food) and a delivery agent atomically (exclusive lock or atomic operation).\n\nUpon reservation, the food packet and the agent become unavailable for any other transaction.\n\n## Phase 2: Assignment\n\nOrder service then calls the store and delivery services to atomically assign the reserved food packet and the delivery agent to the order. Upon success assigning both to the order, the order is marked as successful, and the order service returns a 200 OK to the user.\n\nThe end-user will only see \"Order Placed\" when the food packet is assigned, and the delivery agent is assigned to the order. So, all 4 API calls should succeed for the order to be successfully placed.\n\nNegative cases:\n\n- If any reservation fails, the user will see \"Order Not Placed\"\n- If the reservation is made but assigning fails, the user will see \"Order Not Placed\"\n- If there is any transient issue in any service during the assignment phase, APIs will be retried by the order service to complete the order.\n- To not have a perpetual reservation, every reserved packet and delivery agent will have an expiration timer that will be large enough to cover transient outages.\n\nThus, in any case, an end-user will never experience a moment where we say that the order is placed, but it cannot be fulfilled in the backend.",
    "notes_gd": "https://drive.google.com/file/d/18q2ELr9n6GCemKbJ0aS7q7NyF7wX1kL9/view?usp=sharing",
    "slug": "implementing-distributed-transactions-using-two-phase-commit-protocol"
  },
  {
    "id": 117,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF"
    },
    "yt_video_id": "7FgU1D4EnpQ",
    "title": "Distributed Transactions: Two-Phase Commit Protocol",
    "description": "Distributed Transactions are tough and intimidating. It is hard to guarantee atomicity across microservices given the network delays, resource contention, and unreliable services.\n\nIn this video, we discuss and take a detailed look into Distributed Transactions, understand why they are needed with a real-world example of Zomato's 10-minute food delivery, and build our understanding of the workings of the Two-Phase Commit protocol.\n\nOutline:\n\n00:00 Why Distributed Transactions\n03:44 Atomicity in Distributed Transactions\n06:47 Two-Phase Commit Protocol for Distributed Transactions\n18:29 Advantages and Disadvantages of Two-Phase Commit",
    "img": "https://i.ytimg.com/vi/7FgU1D4EnpQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6wO5b4A4Mx965tWE/giphy.gif",
    "duration": "21:21",
    "view_count": 4578,
    "like_count": 197,
    "comment_count": 28,
    "released_at": "2022-03-28",
    "gist": "Distributed Transactions are essential to have strong consistency in a distributed setup.\n\nAn example could be as simple as a 10-min food/grocery delivery- where to guarantee a 10-min delivery, you can only accept orders when there are goods available in the dark store, and a delivery agent is available to deliver the goods.\n\nThis is a classic case of Distributed Transaction where you need a guarantee of atomicity and consistency across two different services. In a distributed setup, we can achieve it using an algorithm called Two-phase Commit.\n\nThe core idea of 2PC is: Split the transaction into two phases: Reservation and Assignment.\n\n## Phase 1: Reservation\n\nThe Order service will first talk to store service to reserve food items and delivery service to reserve a delivery partner. When the food or delivery partner is reserved, they are not notified. By reserving them, we are just making them unavailable for everyone else.\n\nIf the order service fails to reserve any of these, we roll back the reservation and abort the transaction informing the user that the order is not placed. Reservation comes with a timer, which means if we cannot assign a reserved food item to order in \"n\" minutes, we will be releasing the reservation, making them available for other transactions.\n\nWe move forward to the Commit phase only when the order service reserves both- a food item and a delivery agent.\n\n## Phase 2: Commit\n\nIn the Commit phase, the order services reach out to the store service and the delivery service to assign the food and agent to the order. Because the food and the agent were reserved, no other transaction could see it, and hence with a simple assignment, we can get the reserved food and agent assigned to an order.\n\nUpon this assignment, the store and the delivery agent are notified about the order and proceed with their respective duties.\n\nWe retry a few times if any of the assignments fail (which could happen only if the service goes down). If we still cannot get the assignment done, we inform the user that the order cannot be placed.\n\nThe order is placed only after the food item, and the delivery agent is assigned to the order.",
    "notes_gd": "https://drive.google.com/file/d/18WDFAstffIe_vGbtTz_CS117XhvDcBx3/view?usp=sharing",
    "slug": "distributed-transactions-two-phase-commit-protocol"
  },
  {
    "id": 107,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5"
    },
    "yt_video_id": "xELqRiovEcI",
    "title": "What are Embedded Databases?",
    "description": "Embedded databases are coupled with the application they are part of and operate in a confined space. They are designed to solve one problem for their niche very well. In this video, we take an introductory look into this amazing class of databases, understand the core reason why they exist, talk about a few popular ones, and understand a few use cases.\n\nOutline:\n00:00 Server-based Databases\n02:32 Embedded Databases\n06:35 Popular Embedded Databases\n10:39 Applications of Embedded Databases",
    "img": "https://i.ytimg.com/vi/xELqRiovEcI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/uk3R91z24SoUZ27WTh/giphy.gif",
    "duration": "15:57",
    "view_count": 2140,
    "like_count": 95,
    "comment_count": 18,
    "released_at": "2022-03-25",
    "gist": "Traditional databases like MySQL, Postgres, MongoDB run on their server on a specific port. Anyone who wants to talk to the database can directly connect and talk.\n\nEmbedded Databases are different from these traditional databases, and they operate in their own confined space within a process. There is no separate process for the database.\n\nNo one can directly connect to this database, unlike how we do it with MySQL and other databases. The role and the use of the embedded database are limited to the process it is confined to.\n\n## Popular embedded databases are\n\n- SQLite: an embedded SQL-like database\n- LevelDB: on disk KV store by Google\n- RocksDB: on disk KV store optimized for performance\n- Berkeley DB: KV store with ACID, Replication, and Locking\n\nAn embedded database is always designed to solve one niche really well.\n\n## Application of Embedded Databases\n\nEvery modern browser uses an embedded database called IndexedDB to store browsing history and other configuration settings locally. The browser is confined to a machine, and the IndexedDB is contained in the browser; there is no separate process to connect to.\n\nEvery Android phone has support for SQLite database that we can use to store any information like game scores, stats, and information locally on the phone.\n\nThe core idea: When we need to store and query data that could be confined within a space and does not need to be centralized, we choose to use an Embedded Database.",
    "notes_gd": "https://drive.google.com/file/d/1_iXh0rCmGVZJj5CLWP7gJ4YzAP-yIiGb/view?usp=sharing",
    "slug": "what-are-embedded-databases"
  },
  {
    "id": 113,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "ZFRAFTn0cQ0",
    "title": "Dissecting GitHub Outage: ID column reaching the max value 2147483647",
    "description": "GitHub experience an outage on 5th May 2020 on a few of their internal services and it happened because a table had an auto-incrementing integer ID and the column reached its maximum value possible 2147483647. In this video, we dissect what happened, mimic the situation locally and see what could have happened, and look at possible ways to mitigate and prevent a situation like this.\n\nOutline:\n\n00:00 Outage walkthrough\n02:48 Mimicking the situation locally\n10:13 MySQL AUTO_INCREMENT behavior\n12:37 Preventive measures\n14:25 Approach 1 for mitigating the issue\n18:40 Approach 2 for mitigating the issue\n\nReferences:\n - https://github.com/arpitbbhayani/mysql-maxint\n - https://github.blog/2020-07-08-introducing-the-github-availability-report/\n - https://dev.mysql.com/doc/refman/8.0/en/integer-types.html\n - https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html\n - https://www.linkedin.com/pulse/so-you-hit-2147483647-heath-dutton-/\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/ZFRAFTn0cQ0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/UDU4oUJIHDJgQ/giphy.gif",
    "duration": "24:34",
    "view_count": 1898,
    "like_count": 157,
    "comment_count": 33,
    "released_at": "2022-03-23",
    "gist": "On the 5th of May, 2020, GitHub experienced an outage because of this very reason. One of their shared table having an auto-incrementing ID column hits its max limit. Let's see what could have been done in such a situation.\n\n## What's the next value after MAX int?\n\nGitHub used 4 bytes signed integer as their ID column, which means the value can go from `-2147483648` to `2147483647`. So, now when the ID column hits `2147483647` and tries to get the next value, it gets the same value again, i.e., `2147483647`.\n\nFor MySQL, `2147483647` + `1` = `2147483647`\n\nSo, when it tries to insert the row with ID `2147483647`, it gets the *Duplicate Key Error* given that a row already exists with the same ID.\n\n## How to mitigate the issue?\n\nA situation like this is extremely critical given that the database is not allowing us to insert any row in the table. This typically results in a major downtime of a few hours, and it depends on the amount of data in the table. There are a couple of ways to mitigate the issue.\n\n### Approach 1: Alter the table and increase the width of the column\n\nQuickly fire the `ALTER` table and change the data type of the ID column to `UNSIGNED INT` or `BIGINT`. Depending on the data size, an ALTER query like this will take a few hours to a few days to execute. Hence this approach is suitable only when the table size is small.\n\n### Approach 2: Swap the table\n\nThe idea here is to create an empty table with the same schema but a larger ID range that starts from `2147483648`. Then rename this new table to the old one and start accepting writes. Then slowly migrate the data from the old table to this new one. This approach can be used when you can live without the data for a few days.\n\n### Get warned before the storm\n\nAlthough mitigation is great, it is better to place a monitoring system that raises an alert when the ID reaches 70% of its range. So, write a simple DB monitoring service that periodically checks this by firing a query on the database.",
    "notes_gd": "https://drive.google.com/file/d/13rNEWXwIdNkNcP2gSQQvBF8czUBpF47y/view?usp=sharing",
    "slug": "dissecting-github-outage-id-column-reaching-the-max-value-2147483647"
  },
  {
    "id": 108,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5"
    },
    "yt_video_id": "wI4hKwl1Cn4",
    "title": "How does the database guarantee reliability using write-ahead logging?",
    "description": "Any persistent database needs to guarantee reliability. No matter how big or small the changes are, they should survive any reboots, OS, or hardware crashes once they are committed. All the persistent databases use a write-ahead logging technique to guarantee such reliability while not affecting the performance.\n\nIn this video, we talk about write-ahead logging, how it ensures reliability, a few solid advantages of using it, one of them being a massive database performance boost, and how the log files are structured on the disk.\n\nOutline:\n00:00 What happens on Commit\n05:57 Write-ahead Logging\n08:44 Advantages of having a Write-ahead Logging\n14:29 Data Integrity in WAL files\n16:40 Write-ahead Logging Internals\n\nRelated Videos:\nHow indexes make a database read faster: https://www.youtube.com/watch?v=3G293is403I",
    "img": "https://i.ytimg.com/vi/wI4hKwl1Cn4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l4pTqyJ8XMhLZ3ScE/giphy.gif",
    "duration": "22:6",
    "view_count": 2140,
    "like_count": 112,
    "comment_count": 7,
    "released_at": "2022-03-21",
    "gist": "Any persistent database needs to guarantee the reliability, implying that any update/delete fired on the database is reliably stored on the disk. The alterations on the data should not be affected by power loss, OS failure, or hardware failure.\n\nThe changes to the data once committed should do to non-volatile storage like Disk making them outlive any outage or crash. Although the flows seem simple enough to say that- hey let's just flush the changes to the disk; it is a little more complicated than that.\n\n## Disk writes are complicated\n\nWhen we perform the disk write, the changes are not directly written to the disk sectors, instead, it goes through a variety of buffers like RAM, OS Cache, Disk Cache, and then finally to the Disk sectors. So, if the changes are in any of these intermediate caches and the process crashes, our changes are lost.\n\nSo, while guaranteeing reliability we have to skip all of these caches and flush our changes as quickly as possible on the disk sectors. But if we do that, we are impacting the throughput of the system given how expensive disk writes are.\n\n## Write-ahead Logging\n\nWrite-ahead logging is a standard way to ensure data integrity and reliability. Any changes made on the database are first logged in an append-only file called write-ahead Log or Commit Log. Then the actual blocks having the data (row, document) on the disk are updated.\n\nIf we update any row or a document of a database, updating the data on disk is a much slower process because it would require on-disk data structures to rebalance, indexes to be updated, and so much more. So, if we skip the OS cache, and other buffers, and directly update the rows to the disk every time; it will hamper the overall throughput and performance of the database.\n\nHence, instead of synchronously flushing the row updates to the disk, we synchronously just flush the operation (PUT, DEL) in a write-ahead log file leading to just one extra disk write. The step will guarantee reliability and durability as although we do not have the row changes not flushed but at least the operation is flushed. This way if we need to replay the changes, in case of a crash, we can simply iterate through this simple file of operations and recover the database.\n\nOnce the operation is logged, the database can do its routine work and make the changes to the actual row or document data through the OS cache and other buffers. This way we guarantee reliability and durability in as minimal of a time as possible while not affecting the throughput much.\n\n### Advantages of using WAL\n\nThe advantages of using WAL are\n\n- we can skip flushing the data to the disk on every update\n- significantly reduce the number of disk writes\n- we can recover the data in case of a data loss\n- we can have point-in-time snapshots\n\n### Data integrity in WAL\n\nWAL also needs to ensure that any operation flushed in the log is not corrupted and hence it maintains its integrity using a CRC-32 and flushes it on the disk with every entry. This CRC is checked during reading the entry from the disk, if it does not match the DB throws an error.",
    "notes_gd": "https://drive.google.com/file/d/1VC77CEEYLvlFaXpKsb3Q_e0JvbbryyU0/view?usp=sharing",
    "slug": "how-does-the-database-guarantee-reliability-using-write-ahead-logging"
  },
  {
    "id": 118,
    "topic": {
      "id": 0,
      "uid": "microservices",
      "name": "Designing \u03bc-services",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT0ug8eizS71G6LZb6-4FAFt",
      "bgcolor": "#FFFAE6",
      "themecolor": "#CAA200"
    },
    "yt_video_id": "Hxja4crycBg",
    "title": "Handling timeouts in a microservice architecture",
    "description": "Handling timeout well is extremely critical as it makes your distributed system robust and ensures you provide a consistent user experience by adhering to SLA guarantees. In this video, we discover how a synchronous dependency on a microservice leads to long delays becoming a big problem, understand how timeout addresses the concern, and discuss 5 approaches to handle service timeouts.\n\nOutline:\n00:00 Why is handling timeout critical?\n01:13 Synchronous communication and timeouts\n05:39 A rule of thumb: Timeout\n07:52 Approach 1: Ignore the timeout\n10:28 Approach 2: Configure and use defaults\n11:27 Approach 3: Retry when timeout\n16:36 Approach 4: Retry only when needed\n20:06 Approach 5: Rearchitect and remove synchronous dependency",
    "img": "https://i.ytimg.com/vi/Hxja4crycBg/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oriO4kSYahYQr6e1a/giphy.gif",
    "duration": "23:38",
    "view_count": 2931,
    "like_count": 180,
    "comment_count": 21,
    "released_at": "2022-03-18",
    "gist": "Microservices give us the flexibility to pick the best tech stack to solve the problem optimally. But one thing that ruins the real thrill is Timeouts.\n\nSay we have a blogging website where a user can search for blogs. The request comes to the Search service, and it finds the most relevant blogs for the query.\n\nIn the response, a field called `total_views` should hold the total number of views the blog received in its lifetime. The search services should talk to the Analytics service synchronously to get the data. This synchronous dependency is the root of all evil.\n\nThe core problem: Delays can be arbitrarily large\n\nBecause the delay depending on service can be arbitrarily large, we know how long to wait for the response. We for sure cannot wait forever, and hence we introduce Timeout. Every time the Search service invokes the Analytics service, it starts a timer, and if it does not get a response in the stipulated time, it timeout and moves on.\n\nThere are 5 approaches to handling timeouts.\n\n- Approach 1: Ignore the timeout and move on\n- Approach 2: Use some default value if you timed out\n- Approach 3: Retry the request\n- Approach 4: Retry only when needed\n- Approach 5: Re-architect and make synchronous dependency an async one",
    "notes_gd": "https://drive.google.com/file/d/1GjObZ3xpLFxDEOO3EGRCj0Pq8bWLixjU/view?usp=sharing",
    "slug": "handling-timeouts-in-a-microservice-architecture"
  },
  {
    "id": 109,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5"
    },
    "yt_video_id": "3G293is403I",
    "title": "How do indexes make databases read faster?",
    "description": "In this video, we discuss how indexes make a database operate faster. While discussing that, we dive deep into how the data is read from the disk, how indexes are structured, serialized, and stored on the disk, and finally, how exactly data is quickly read using the right set of indexes.\n\nOutline:\n\n00:00 How is a table stored on the disk?\n03:14 Reading bytes from the disk\n05:21 Reading the entire table from the disk\n08:33 Evaluating a simple query without an index\n10:00 Basics of Database Indexes\n13:24 Evaluating a simple query with index",
    "img": "https://i.ytimg.com/vi/3G293is403I/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/WoWm8YzFQJg5i/giphy.gif",
    "duration": "23:25",
    "view_count": 3902,
    "like_count": 282,
    "comment_count": 42,
    "released_at": "2022-03-16",
    "gist": "The database is just a collection of records. These records are serialized and stored on the disk. The way the records are serialized and stored on the disk depends on the database engine.  \n  \n## How does the database read from the disk?  \nA disk is always read in Blocks, and a block is typically 8kb big. When any disk IO happens, even requesting one byte, the entire block is read in memory, and the byte is returned from it.  \n  \nSay we have a \"users\" table with 100 rows, with each row being 200B long. If the block size is 600B, we can read 600/200 = 3 rows in one disk read.  \n  \nTo find all users with age = 23, we will have to read the entire table row by row and filter out the relevant documents; we will have to read the entire table. In one shot, we read 3 rows so that it would take 100/3 = 34 disk/block reads to answer the query.  \n  \n## Let's see how indexes make this faster.  \n  \nAn index is a small referential table with two columns: the indexed value and the row ID. So an index on the age column will have two columns age (4 bytes) and row ID (4 bytes).  \n  \nEach entry in the index is 8 bytes long, and the total size of the index will be 100 * 8 = 800 bytes. When stored on the disk, the index will require 2 disk blocks.  \n  \nWhen we want to get users with age == 23, we will first read the entire index, taking 2 disk IOs and filtering out relevant row IDs. Then make disk reads for the relevant rows from the main table. Thus we read 2 blocks to load the index and only relevant blocks have the actual data.  \n  \nIn our example, it comes out to be 2 + 2 = 4 block reads. So, we get an 8x boost in performance using indexes.  \n  \nNote: This is a very crude example of how fetch happens with indexes; there are a lot of optimizations that I have not talked about.  ",
    "notes_gd": "https://drive.google.com/file/d/1wDDOc3rdsZIdZEEe50E_61wi-1iMHu2G/view?usp=sharing",
    "slug": "how-do-indexes-make-databases-read-faster"
  },
  {
    "id": 110,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5"
    },
    "yt_video_id": "UT_TVldzA64",
    "title": "How to handle database outages?",
    "description": "In this video, we talk about why a database goes down, what happens when the database is down, a few short-term solutions to minimize the downtime, and a few long-term solutions that you should be doing to ensure that your database does not go down again.\n\nOutline:\n\n00:00 Why a database goes down?\n06:10 What happens when a DB is down?\n09:46 Short-term solutions to get your DB up\n17:33 Long-term solutions to fix the database",
    "img": "https://i.ytimg.com/vi/UT_TVldzA64/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/lVBtp4SRW6rvDHf1b6/giphy.gif",
    "duration": "30:38",
    "view_count": 2263,
    "like_count": 152,
    "comment_count": 19,
    "released_at": "2022-03-14",
    "gist": "### Why a database goes down?\nAn unexpected heavy load on your database can lead to a process crash or a massive slowdown.\n\nBefore jumping to the potential short-term and long-term solutions, ensure you monitor the database well. CPU, Memory, Disk, and Connections are being closely monitored.\n\n## Short term solutions\n\n- Kill the queries that have been running for a long time\n- Quickly scale up your database if you have been seeing a consistent heavy usage\n- Check if the recent deployment is the culprit; if so, revert asap\n- Reboot the database will calm the storm and buy you some time\n\n## Long term solutions\n\n- Ensure the right set of indexes is in place\n- Tune your database default parameters to gain optimal performance\n- Check for the notorious N+1 Queries\n- Upgrade the database version to get the best that DB can offer\n- Evaluate the need for Horizontal scaling using Replicas and Sharding",
    "notes_gd": "https://drive.google.com/file/d/1Q6YokLBvmfW1Tw1mpndOfx-I2NyRGVG0/view?usp=sharing",
    "slug": "how-to-handle-database-outages"
  },
  {
    "id": 112,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "h5hXivWws7k",
    "title": "Dissecting Spotify's Global Outage - March 8, 2022",
    "description": "Incident Report: Spotify Outage on March 8: https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/\nGoogle Cloud Traffic Director Outage: https://status.cloud.google.com/incidents/LuGcJVjNTeC5Sb9pSJ9o\nJava gRPC Client Bug: https://github.com/grpc/grpc-java/issues/8950\n\nIn this video, we dissect the reasons behind Spotify's Global Outage and try to understand its architecture, and learn from critical things to remember while architecting a system.",
    "img": "https://i.ytimg.com/vi/h5hXivWws7k/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26FKWlRgIL3JOIkGA/giphy.gif",
    "duration": "26:",
    "view_count": 3164,
    "like_count": 187,
    "comment_count": 32,
    "released_at": "2022-03-12",
    "gist": "",
    "notes_gd": "",
    "slug": "dissecting-spotify-s-global-outage-march-8-2022"
  },
  {
    "id": 128,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "Q8luYnxiFFE",
    "title": "How \"is\" operator is implemented in python?",
    "description": "In this video, we find out the internals of \"is\" operator in Python. We go through the CPython source code and answer how it efficiently implements the \"is\" operator.\n\n\ninteger optimizations in python: https://www.youtube.com/watch?v=6mhXGEXRXG0\nstring optimizations (interning) in python: https://www.youtube.com/watch?v=QpGK69LzfpY\nshort circuit evaluations in python: https://www.youtube.com/watch?v=zz2Lu5ht_jA\n\n\n\n# The Honest Python\n\nThe Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.",
    "img": "https://i.ytimg.com/vi/Q8luYnxiFFE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l1KXrCk0QZ8WljxqU/giphy.gif",
    "duration": "19:12",
    "view_count": 480,
    "like_count": 27,
    "comment_count": 6,
    "released_at": "2021-05-31",
    "gist": "",
    "notes_gd": "",
    "slug": "how-is-operator-is-implemented-in-python"
  },
  {
    "id": 127,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "IGLnxdmJu2c",
    "title": "How id() function is implemented in python?",
    "description": "How is `id` function implemented in Python?\n\nIn this video, we explore the internals of `id` function in Python and find out how it is actually implemented.\n\nHow Python implements super-long integers: https://arpitbhayani.me/blogs/super-long-integers\n\n# The Honest Python\n\nThe Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.",
    "img": "https://i.ytimg.com/vi/IGLnxdmJu2c/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/whzQo9wK8OuoiDtAnz/giphy.gif",
    "duration": "10:46",
    "view_count": 309,
    "like_count": 16,
    "comment_count": 0,
    "released_at": "2021-05-24",
    "gist": "",
    "notes_gd": "",
    "slug": "how-id-function-is-implemented-in-python"
  },
  {
    "id": 126,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "u4tfIy--4GM",
    "title": "How python compares a float and an int objects?",
    "description": "How python internally performs the comparison of a float and an integer value? We find this out in this video.\n\n\nComparing a float to an int is a fascinating problem, especially for Python. Comparing numbers involve comparing magnitudes across different types; plus things become even more complex considering Python supports infinitely long number.\n\nChapters:\n0:00 Introduction to number comparison\n\n1:14 Disassembling the code\n2:00 Understanding COMPARE_OP\n2:54 Exploring RichCompare\n7:16 Understanding the core logic\n\n\n# The Honest Python\n\nThe Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.",
    "img": "https://i.ytimg.com/vi/u4tfIy--4GM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l0ExiSoCkhCfSm94k/giphy.gif",
    "duration": "20:34",
    "view_count": 277,
    "like_count": 10,
    "comment_count": 1,
    "released_at": "2021-05-13",
    "gist": "",
    "notes_gd": "",
    "slug": "how-python-compares-a-float-and-an-int-objects"
  },
  {
    "id": 125,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "6mhXGEXRXG0",
    "title": "How python optimizes integers?",
    "description": "In this video, we answer, How python optimizes integers? Does it cache them? We go through the CPython source code to find out the exact internals of it.\n\n\nChapters:\n0:00 Introduction to integer optimization\n1:59 Going through the CPython code and tracing the function\n2:41 Going through PyLong_FromLong\n3:39 Understanding IS_SMALL_INT\n5:13 Understanding get_small_int\n8:47 How Python initializes the cache of integers?\n12:07 Where all Python is reusing small integers?\n14:45 Why Python caches integers?\n\n\nLink to essays and articles:\nhttps://arpitbhayani.me/blogs/python-caches-integers\nhttps://arpitbhayani.me/blogs/super-long-integers\n\n\n# The Honest Python\n\nThe Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.",
    "img": "https://i.ytimg.com/vi/6mhXGEXRXG0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/sgFDIjII6GtmE/giphy.gif",
    "duration": "15:40",
    "view_count": 263,
    "like_count": 24,
    "comment_count": 2,
    "released_at": "2021-05-06",
    "gist": "",
    "notes_gd": "",
    "slug": "how-python-optimizes-integers"
  },
  {
    "id": 101,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "dTPh3KcKPYo",
    "title": "Scaling Taxonomy Service and Database",
    "description": "In this video, we do the high-level design of Udemy's Taxonomy Service and see how to scale databases in general.\n\nOutline:\n\n0:00 Recap Taxonomy DB Design\n0:57 Bird's eye view\n2:30 Within the Taxonomy Service\n3:50 Scaling the relational database\n5:32 High-Level Architecture\n7:20 Join my System Design Course\n7:57 Like, Share, and Subscribe",
    "img": "https://i.ytimg.com/vi/dTPh3KcKPYo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/YxlUxrYGw2w9y/giphy.gif",
    "duration": "8:38",
    "view_count": 979,
    "like_count": 29,
    "comment_count": 1,
    "released_at": "2021-05-02",
    "gist": "",
    "notes_gd": "",
    "slug": "scaling-taxonomy-service-and-database"
  },
  {
    "id": 124,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "zz2Lu5ht_jA",
    "title": "How python implements chained comparison operators?",
    "description": "In this essay, we explore how python evaluates chained comparison operators. We dive deep into the code and answer the following questions\n\n- how Python evaluates chained comparison operators?\n- how it differs from C-like evaluation?\n- how Python implements short-circuiting?\n\nIn the process, we also alter the code to make Python evaluate such expressions C-like.\n\nDetailed Essay: https://arpitbhayani.me/blogs/chained-operators-python\n\nChapters:\n0:00 Chained Comparison Operators\n1:32 How Python and C differ in evaluation\n6:34 Disassembling the code\n8:00 Instruction by Instruction walkthrough\n15:42 Short-circuit evaluation\n17:27 Tracing and understanding the CPython code\n17:40 What makes Python-like evaluation the way it is\n24:22 Altering the code to make it a C-like evaluation\n26:19 Disassembling the altered code\n27:13 Instruction by Instruction walkthrough\n28:40 Concluding\n30:18 Like, Share, and Subscribe\n\n# The Honest Python\n\nThe Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.",
    "img": "https://i.ytimg.com/vi/zz2Lu5ht_jA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/MFabj1E9mgUsqwVWHu/giphy.gif",
    "duration": "30:45",
    "view_count": 222,
    "like_count": 15,
    "comment_count": 0,
    "released_at": "2021-04-28",
    "gist": "",
    "notes_gd": "",
    "slug": "how-python-implements-chained-comparison-operators"
  },
  {
    "id": 100,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "4_jlmX_oB94",
    "title": "Designing Taxonomy on SQL",
    "description": "In this video, we design the Taxonomy of Udemy on top of a SQL-based Relational DB. We learn how to model any taxonomy on relational DB, why to choose SQL over NoSQL, designing the schema, deciding Indexes, and writing very interesting SQL queries.\n\nLink to the article: https://arpitbhayani.me/blogs/udemy-sql-taxonomy\n\nChapters:\n0:00 Udemy's Taxonomy\n1:40 A bad Database Design for Taxonomy\n3:29 Good Database Design for Taxonomy\n7:34 Is Relational DB a good choice to store taxonomy?\n13:04 Deciding Indexes\n15:26 Get topic by ID\n15:56 Get topic path\n23:00 Get child-topics\n25:05 Get Top Taxonomy in Single Query\n31:05 Summarizing Indexes\n31:52 Fun exercise to explore SQL\n33:55 What next in System Design",
    "img": "https://i.ytimg.com/vi/4_jlmX_oB94/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/6XuA2WMKsgqS4/giphy.gif",
    "duration": "35:15",
    "view_count": 1983,
    "like_count": 79,
    "comment_count": 8,
    "released_at": "2021-04-19",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-taxonomy-on-sql"
  },
  {
    "id": 123,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "QpGK69LzfpY",
    "title": "How python optimizes strings using String Interning?",
    "description": "In this video, we dive deep into CPython and find out how Python optimizes strings. The optimization we will be going through is called String Interning which is like caching the strings - using the same reference and not creating a new instance every time.\n\n\nLink to my String Interning article: https://arpitbhayani.me/blogs/string-interning\nCode changes we made in the video: https://github.com/arpitbbhayani/cpython/pull/9\n\n\nChapters:\n0:00 What is String Interning?\n4:26 Tracing CPython function that interns strings\n5:36 Going through the PyUnicode_InternInPlace function\n18:45 Interned state stored in strings\n21:10 Interning does not work on all strings\n25:22 The catch with interning\n\n# The Honest Python\n\nThe Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.",
    "img": "https://i.ytimg.com/vi/QpGK69LzfpY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/lPiGDQQyhG9zEoGmuh/giphy.gif",
    "duration": "30:28",
    "view_count": 523,
    "like_count": 24,
    "comment_count": 3,
    "released_at": "2021-04-13",
    "gist": "",
    "notes_gd": "",
    "slug": "how-python-optimizes-strings-using-string-interning"
  },
  {
    "id": 111,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5"
    },
    "yt_video_id": "x1XmcuosZNk",
    "title": "5 million + random rows in less than 100 seconds using SQL",
    "description": "In this video, we generate 5 million + random rows in less than 100 seconds using just SQL. We mock the data for any taxonomy (Udemy's example taken). We use Joins and SQL tricks to amplify the rows and use them to ingest.\n\n\nLink to the gist: https://gist.github.com/arpitbbhayani/96a42c28d134871ebc11faad272b5349",
    "img": "https://i.ytimg.com/vi/x1XmcuosZNk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Gpu3skdN58ApO/giphy.gif",
    "duration": "26:3",
    "view_count": 2411,
    "like_count": 64,
    "comment_count": 20,
    "released_at": "2021-04-10",
    "gist": "",
    "notes_gd": "",
    "slug": "5-million-random-rows-in-less-than-100-seconds-using-sql"
  },
  {
    "id": 122,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "rWb7VYz1q1U",
    "title": "Changing python grammar to support standalone walrus assignments",
    "description": "In this video, we alter the Python grammar and make an invalid syntax valid. We alter grammar and allow the Walrus operator to be executed as a standalone statement similar to how the usual assignment statement works. \n\nThis video would pave way for budding Python developers to understand CPython internals.\n\nChapters:\n0:00 What is a walrus operator?\n1:20 Building a toy shell to understand Walrus\n6:27 A non-intuitive behavior\n8:15 Going through the grammar\n14:25 Altering the Walrus behavior\n16:55 Understanding why it worked\n\nLink to the article: https://arpitbhayani.me/blogs/the-weird-walrus\n\n# The Honest Python\n\nThe Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.",
    "img": "https://i.ytimg.com/vi/rWb7VYz1q1U/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/HxnMNAEQgHqEHcGKBb/giphy.gif",
    "duration": "22:22",
    "view_count": 609,
    "like_count": 32,
    "comment_count": 10,
    "released_at": "2021-04-08",
    "gist": "",
    "notes_gd": "",
    "slug": "changing-python-grammar-to-support-standalone-walrus-assignments"
  },
  {
    "id": 104,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "m_7xSIhxZL8",
    "title": "End-to-end Message Encryption",
    "description": "In this video, we find how to implement a very very simple version of end-to-end WhatsApp-like message encryption. This is not even 1% of how WhatsApp does it but it gives a fair amount of idea on how we could do it.\n\nHow do WhatsApp, Facebook, Signal actually do it? Answer: The Signal Protocol\n\nWe do it with a very simple Public Key Cryptography and a Digital Signature.",
    "img": "https://i.ytimg.com/vi/m_7xSIhxZL8/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/pYfYNJbEftHOfIpoBC/giphy.gif",
    "duration": "15:57",
    "view_count": 5136,
    "like_count": 221,
    "comment_count": 15,
    "released_at": "2021-04-05",
    "gist": "",
    "notes_gd": "",
    "slug": "end-to-end-message-encryption"
  },
  {
    "id": 121,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "k5isLG6vqss",
    "title": "Setting up cpython locally and making your first change",
    "description": "In this video, we set up CPython, build our binary, and make our first \"Hello, World!\" change.\n\nCPython Github Repository: https://github.com/python/cpython/\nCPython Setup Guide: https://devguide.python.org/setup/",
    "img": "https://i.ytimg.com/vi/k5isLG6vqss/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6Zt34440romOQWo8/giphy.gif",
    "duration": "6:32",
    "view_count": 1071,
    "like_count": 54,
    "comment_count": 3,
    "released_at": "2021-04-04",
    "gist": "",
    "notes_gd": "",
    "slug": "setting-up-cpython-locally-and-making-your-first-change"
  },
  {
    "id": 120,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "Python Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT12HU6v00VlcZ18ckWRGxXU",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662"
    },
    "yt_video_id": "E-3J8X5DulY",
    "title": "Series Introduction: The Honest Python",
    "description": "The Honest Python is a series in which we dissect certain features, behaviors, and not-so-obvious outputs by diving deeper into the CPython source code and if possible altering it to get a much clearer understanding.\n\nWe will find answers to questions like\n\n  - How python implements super-long integers?\n  - What kind of algorithms power this language and its operations?\n  - How Garbage Collection is implemented?\n  - How are types like Lists, Sets, and Dictionaries are implemented?\n\nIf this sounds exciting, hop along with me on this journey and give a subscription to this channel.",
    "img": "https://i.ytimg.com/vi/E-3J8X5DulY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/H0uLRCd8JIhRS/giphy.gif",
    "duration": "40",
    "view_count": 1063,
    "like_count": 41,
    "comment_count": 3,
    "released_at": "2021-04-03",
    "gist": "",
    "notes_gd": "",
    "slug": "series-introduction-the-honest-python"
  },
  {
    "id": 99,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT24ynKXkPQwxl9gyVLLGsBO",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00"
    },
    "yt_video_id": "kIP8L-CSl2Y",
    "title": "Designing Notifications Service for Instagram",
    "description": "This video is a snippet from my System Design Masterclass and in it, we are discussing How Instagram Scales its notification systems. The primary challenge of such a system is doing a real fast fanout and we discuss how to do this very efficiently.",
    "img": "https://i.ytimg.com/vi/kIP8L-CSl2Y/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/huyZxIJvtqVeRp7QcS/giphy.gif",
    "duration": "37:18",
    "view_count": 20049,
    "like_count": 481,
    "comment_count": 48,
    "released_at": "2021-04-01",
    "gist": "",
    "notes_gd": "",
    "slug": "designing-notifications-service-for-instagram"
  }
]