Kademlia - a P2P Distributed Hash Table

Kademlia a P2P Distributed Hash Table pure - To get information about peers , a node in the BitTorrent network talks to Tracker . central still Having a entity is prone to attacks and failures so can we do p2p network without Tracker ? , a pure , kademlia say have set , we a gigantic that node of KV pairs one cannot store or handle . Hence we have to distribute Hence it is called a Distributed Hash Table 1 How do distribute ? . we KY ? 2 . How would a node know how to find a ? 3. How to gracefully handle nodes joining / leaving 4. How to do this without central ? a entity

Representation Every node (machine ) participating gets a unique 160 b 12013 ) ID . The unique ID can be f h explicitly assigned > > - For P2P derived Node IP implicitly - The data that is stored the h across > g- > network is also hashed and identified key 160 b by ID KY pair to BitTorrent * This is a generic DHT , nothing specific In the context BitTorrent the that of . only thing changes is the kind [ reachable stored the node of information peers ) on . n' Ownership , "2 k key 141 hag Node Nes hn ,, r , The node that is closest to the key , ka owns the key. * not a ring Ki C- Nj / dlh ki , h Nj ) is minimum for V-j

Distance metric " " In order to need distance metric find the closest nude we a that quantifies the closeness For any non - euclidean geometry Requirement from a distance metric dlx.nl O F distance to 1. = N self = 0 2. d IX. y ) > 0 if x y try distance to other is + ve 3- dbe y) , + dly , 2) due 2) , triangle inequality b/w 2 shortest distance 1 ' ' is a N 2 the Keio points y u y straight line connecting them For hero nodes / keys in our kademlia distribution , the distance metric is dbl iy ) = X y satisfies all the 3 requirements 1. dbe.nl K A 0 160 bit = = Bitwise OR of 2. doe y ) = K y > 0 interpreted , IDs , as 3. dbl Y ) 1- d( Yi 21 integer , = (x y ) (y 2) = 2 = d( Hi 2)

Visualizing Distance Node / key ID Bit Representation For simplification say we work 11 11 , Nz 15 with 4- bit ID 6 0110 ka i.c. nodes and key are given Nz 5 0101 4- bit IDs 13 1101 KB d( Ka Nz) , = 0110 1111 = 1001=9 d( KA , Nz ) = 0110 0101 = 0011 =3 Hence , key ka is owned by node Nz the bits that are same A pattern we see is that the would OR too common the smaller the distance so , IDs sharing the prefix , " " closer same prefix are . Hence , we visualize this as a tries " ( Nz KB ) " we can see how , are close I " " 0 and ( Nz , Ka ) are close . I 0 I 0 I 0 I 0 I 0 0 I I 0 I 0 I 0 I 0 I 0 I 0 I 0 I me NL KB KANZ

Instead of creating a complete I 0 the binary tree , we create ' o o paths as needed . , Instead complete I 0 O ' of creating Nz it till it path carve cue Ni KB ' KA N2 minimally disambiguate KD Kc Ni 15 1111 6 0110 Kc 8 1000 KA 5 0101 k 9 1001 Nz 13 1101 I 0001 KB Nz Routing Given that there is no central entity to hold the all the nodes addresses of HOW Would one node access the KY on the other NL NZ > . ?? . . . . O Ki V1 kqvg % K2 V2 Ks V5 GET kg 1<3 V3 KG V6

network would need to Every node in the j keep track of nodes and hope a few , L they keep track of others , and so on . Ksi V5 Eventually we would have covered the entire network Peer nodes that each node keep track be Random of cannot . as we need guaranteed convergence quickly . < what should be So , our routing strategy , that ensures core Idea knows at least : Every node one node in each subtree that it is not part of . Routing table of node NL should have contact in the 4 subtrees 1 1,00 , 011 , 0101 I 0 0 , I 0 I 0 0 I 0 , I 0 I 0 I 0 I 0 I 0 ' NI I O I 0 ^ a ^ a

If every node in the network keeps track of at least one node in each subtree , we would converge to the desired node in log n L L O 08 L O r 1 I 0 0 & Say , Nes 10000 ) wants to reach Nz 11111 ) that it does not have a direct connection with . so , it will leverage intermediate nodes the table in routing Nil 0000 ) NA 11000 ) NB ( 1101 ) Nz ( 1111 ) would have would have would have one entry one entry one entry for subtree for subtree 111 for subtree I 11. _ . . . . . . . _ Thus we see how OR based distance metric ensure we , would always converge lueithout ever digressing )

Thus each node only has to keep track ip node id of port small subset nodes and the routing a of takes to the care of converging target node. Communication happens over UDP and routing table holds node id Lip udp port , > node has few contact in As the routing converges when every a it every subtree that is not part of . the problem statement reduces to making fault tolerant K buckets - each subtree holds k entries Every node . for , subtree _ prefix node id ip Udp port 1 : 0101

K bucket time last Each - is sorted by seen prefix node most at the tail recently seen a typical K is 20 , ie . for each subtree , each most node holds 20 Contacts recent . table Updating routing when a node receives any message from other node in the network , it updates k buket with the node id its appropriate - . the tail 1. entry is always added at 2. entry is always created at the tail the K bucket is if - full least node ( at the head ) node pings the recently seen - then evict and insert new node at the tail if no respond , discarded node if respond , new node is 2 first is moved to the tail . It is observed . that if a node is online for a long time . it would continue to remain online in the future this K bucket - algorithm exploits .

Communication Interface Every node part of kademlia exposes 4 RPC it is online PING : Probes a node to see if FIND NODE - : The node returns < ip port . , node id > for the K nodes it knows about that are closer to the node requested Mz - - - - - - - - - - ( Nz) Na . FIND NODE _ . . N1 > . < k nodes closer to Nz FIND - VALUE : It is like FIND NODE _ but the machine holds the key , it would return the stored value . the intermediate nodes do not forward the request Note : return the they just nodes through which we could reach the target . The lookup continues until we reach the target and complete the desired action .

STORE : Instructs a node to store < kill > pair To store a 4k v7 node locates K closest nodes pair a - . , and sends them STORE RPC Each node re - publishes to keep the k.ro alive in the network . Original publisher republishes hours and nodes store 24 every them with 24 hour expiration The implementation per the * of STORE varies as use - case / } single - copy multiple copies - Torrent have different may expiration / no expiration implementation than digital reads / write responsibilities certificates . Performance optimization with LRU eviction , the chain cache the Ku pair throughout would if a node goes down . neighbouring nodes have the k.ie pairs already