so we always hear great things about microservices, but every few months every senior engineer gets a feeling, hey, can we not go back to a simpler time and have monolithic architecture again? in this video, let's understand the top 10 challenges that comes with adopting microservices. today, we would spend time talking about not just engineering challenges, but also organizational level challenges that we all should be aware of and address while we adopt micro services. but before we move forward, i'd like to talk to you about a course on system design that i have been running for over a year now. the course is a cohort based course, which means i won't be rambling a solution and it will not be a monologue. instead, a small, focused group of 50- 60 engineers every cohort will be brainstorming systems and designing it together. this way, we build a solid system and learn from each other's experiences. the course to date is enrolled by 600 plus engineers, spanning nine cohorts and 10 countries. engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say. the coolest part about the course is the depth we go into and the breadth we cover. we cover topics ranging from real-time text communication for slack to designing our own toy load balancer, to greek business live text commentary to doing impressions counting at scale for any advertisement business. in all we would cover roughly 28 questions and the detailed curriculum, split week by week, can be found on the course page, which is linked in the description down below. so if you're looking to learn system design from the first principles, you will love this course. i have two offerings for you. the first one is the live cohort discourse which you see on the left side, and the second one is the recorded course which you can see on the right side. the live code base course happens every two months and it will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to binge learn system design, i would highly recommend you going for the recorded one. otherwise, the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page at pittmarty dot me slash master class and i would highly recommend you to check that out. i put the link of the course in the description down below. so if you are interested to learn system design, go for it. check out the link in the description down below and i hope to see you in my next cohort. thanks. so we always talk about the benefits that we get out of adopting micro services and but, to be really honest, grass is not always green. right, benefits are there, benefits are well documented, but what about challenges? there are so many challenges that comes with adopting micro services. let's talk about them one by one. so the first key challenge that comes with adopting microservice is managing them. so with monolith you have just one thing to manage, but with micro services you have hundreds, if not thousands, of services. and as the number of micro services increase, the complexity of your infrastructure increases and defining a scope of service also becomes critical. like, hey, your micro service should not be too small. others, you will have far too many micro services and it should not be too large, otherwise it would go into this macro monolithic architecture. so that is where, whenever a proposal of a new micro service is given, it needs to be a well thought through thing. it cannot be just: hey, our team wants to build a micro service, let's do it. a lot of people should be involved into making the decision: hey, do we really need a service for this? or, if we do need what should be the scope of this very important point? otherwise, you are just creating chaos over chaos by building services over services not really recommended. and with micro services it is very strong tendency. like things would go out of hand very quickly, like you like, if you give free hand to everyone to create microsoft. like random micro services would be created in inner infra, which would overall increase your uh, your uh, your overall latency of your api request, given that there will be so much of inter-service communication and all. but it's, it's very it's something that could go very easily out of hand. so that's why there needs it needs to be a well thought through thing rather than just thinking, hey, let's adopt micro services. people should be putting in thought and should be a well planned decision. also, adding to this point is with when it comes to managing micro services, you would always feel, or every team would feel, an urge: hey, let me build this from scratch. hey, this would be so fun too, because it's very like most two links out there. it's very fun to build them. so that's where it is counter-intuitive that you- or rather, it is very important to fight your urge to create new tools or to rebuild existing tools like you should not be spending time creating your own ci, cd, use jenkins instead, right? so understanding this is very important. otherwise, instead of focusing on solving the problem that you intended to in the first place, you would be spending time building those tools unnecessarily. second key challenge that comes in, and this is a very super important one, it's monitoring and logging. when you have so many services or so many infrastructure components, they are bound to have blind spots. like you might not know a particular service even existed after a few years, or you might not know a dependency to a service exist even today, and that happens. that happens. so with a good monitoring setup, you should be able to exactly pinpoint the overall dependency on the services that they have above each other, and it's very important to do so. always try to like blind spots would come in with time, but we have to be very of the fact that we have to keep them at the bare minimum right and blind spot, not just with respect to an existence of a service or existing or existing of a dependency. but it is also around that every infra component- your api servers, databases, services, communication protocol and whatnot- should be monitored, because if you are not monitoring them, you will not place alerts. how would you get to know if something went down? plus, how would you know if you are utilizing an infrastructure? so monitoring plays a very key role, and much more than that with microservices. given that your request would go from one microservice to another, to another to another. stringing them together is very important. otherwise how would you debug a problem that your end user is facing? let's say your end user is facing a particular problem and that api call that end user made talks to seven different micro services in different order. so how would you know which request did your user like, was that particular users request and how it went through all of the systems? so this is the classic problem of distributed tracing you need to have. it might not be like immediate need, but to not have blind spots and to be able to debug your outages better or your user complaints better, you need to have a distributed tracing setup, something like zipkin zipkin- you can google about it. it's a very nice open source tool, that sub that powers distributed tracing, and the idea is to have a constant or a request id that would flow across micro services and you would be able to trace out exactly what happened, with that request id becoming, uh, basically becoming a bastion, so it becoming that singular point that would string all of the services together. and it's again this becomes complexity that we add. when we adopt micros doing a console output, every log goes into the same place. there is no request, uh, no need of distributor tracing, and all and life is very simple there. but with micro services you have to do this because one service would fork multiple and then would go to next and what not, and then you will get the final response. the next challenge is the next technical challenge is service discovery. so, with hundreds of micro services in place, spread across thousands of servers, how would you know for a particular to get a particular thing done, with server to talk to, or which service to talk to, or rather, you know which service of doctor, but with server to talk to, right? so it seems like a very simple problem at first and hey, hey, i'll just. i know, doctor, i might have an ip address or a sub domain to which i will connect to, but it's not that simple at scale. things fail. that's, that's a. that's a thing that you should always remember so, with service discovery, uh, you need to have some way through which your services would be able to discover other services. like you can just say, hey, i want to talk to payment service, but how would it talk to payment services- something that a service discovery module should be thinking about or should be taken care of? right, it might. so there are three classic ways to do it. we'll not go into technical details of it here- in future we will definitely touch up on that. but first is by having a central service registry which you are registering all the services and how to connect to them. second is having a load balancer based conventional discovery, where every service has a set of servers and basically there is a load balancer in front of it. you talk to load balance and the load balancer talks to, obviously, forwards the request to one of their servers and you get things done. and third is service mesh, where you might use something like an istio in or or say, ny, in order to discover: hey, this is the exact container, this is the exact server that i would want to go to. rather than going through a load balancer, try to reduce a network. hop right three classic ways to do it, not going to technical details. i would highly encourage you to google about all of this three to understand them in detail. but you get, digest will anyway touch up on that in detail, right? so service discovery- big problem at scale. uh, a lot of companies, uh, they, they, they find it difficult, or they, or a lot of companies, would see this as their bottleneck or their challenge. that comes in that how do we do efficient service discovery? otherwise it would increase your overall or your overall latency. the next technical challenge, something that most companies overlook, is authentication authorization. that authentication is for end user, right, end user to talk to your product. what i am talking about here is authentication and authorization between services, because what happens is, let's say, you opened up a notification service. anyone who wants to send notification would talk to the notification service, whatever. what if a developer goes rogue and it tries to put random messages into the queue that your notification service was consuming and it starts to send random notifications to your end user would look very bad on the reputation of the organization. that is where even your services, needs to have an authentication in an authorization layer so that no other service, even intentionally or even accidentally, abuses another service. right, a way to implement this would be having a simple authentication, having a central authentication of our authorization server that issues jwt to occur, allowing you to communicate to other services for a short time and then the token is reissued- something very simple but very effective. you need to do it, uh, because of like, if you want to go with no trust policy, that we don't trust anyone and have security as your default way to approach things, should be thinking about this. the next one, again a very common problem, again a technical challenge, is configuration management. every service that you build needs to talk to a sas offering or a database or a cache or a queue. there are so many secrets to take care of. these configurations needs to be stored somewhere. now imagine each service trying to store its own configuration. one way. let's check it in in the code base. that has its own different problem because of secret scanning and all. you should not be checking in secrets into your code base, but that is one way to do it. other way, let each service store its configuration in a secure location, for example. you might want to store it in database. but where would you date store the database credential? right, because that is now becoming the secret for you. so that is where what us microservice might be urged to do is to have their own configuration management system. but if every service tries to build their own configuration management system, again a lot of waste of time should not be doing that. so that's where, when you adopt micro services, you would have to think about a central configuration management system where all the configurations are stored. configurations, as in it, could be secrets, uh, like database credentials, uh, your cache credentials, and all. it could be normal configuration, like normal constants, for example, the number of posts we would always use in pagination. the default value could be 10. it could be override by by, by passing a parameter to it. right, those sort of configuration. but there has to be a place where a service would be able to fetch configurations from store configurations from, change the configurations on the fly without having to restart their processes. okay, the next challenge is more of an organizational challenge. it says there is no going back. so what happens is when you are making this decision of moving from monolith to micro services. this has to be a very thought through decision because once you break your monolith into micro services, it's hard to go back to monolith because, first of all, take diversity. so what you would have done is, let's say, you split out payment service and you build it in golang because it hates very performant, it's very concurrent, let me use that. but a monolith is written in python. how would you merge it? you cannot right, because you have chosen different stack for different services. it's very hard for you to go back. there would be so much of code needs to be rewritten. second is: team have- uh, so team have tasted- i've misspelled it- but team have tasted autonomy. so what happens is they have become autonomous with respect to adding features to their services, taking them to production. if you make them go back to monolith, where there is so much of inter-team communication and dependency that comes in, they would have like they would find a very hard time to readjust themselves to having so much of dependency. uh, even before thinking of a deployment, like, let's say, they have to deploy just one line of chain but they have to wait for other changes to be there because they are already pushed to master. they are waiting for your changes to hit master and then all of those changes together will be pushed to production, something that we, not like engineers, would find it very difficult to go back. or after the tested autonomous uh like they being autonomous in deploying and taking their own decisions, waiting for approval would reduce your uh ship or your uh your speed of delivery or the way you are shipping features. it would slow it down a bit. the third thing here is people have adopted to new tools and processes, like when you we just saw uh, whenever we are adopting microservices, there are so many new tools that we have to bring in, for example, tool for distributed tracing, rule for monitoring and logging, centralized logging, secret management and whatnot. and you might have also chosen to go into kubernetes and containers, because now you have micro services, so why, to put it on raw ec2 instances, let kubernetes handle it. so there are so many new tools and processes that the teams have uh adopted. it would become very hard to unlearn them right. so more of an organizational challenge, not a pure tech challenge, but it's important to consider that. hey, when you are making this decision of moving to micro services, it's very, very, very, very hard to come back. the next one, a little technical and a little non-technical, is fault tolerance. when you have so many components- uh, because microservices, architecture and all you have so many components, so there are more ways to fail because when, like if you think about if, on an average, a server goes down- hypothetical example, if a server goes down once every 100 days and if, let's say, you have 100 servers, so on an average, one server will go down every day- right, pure math. so if you think about it at scale, when you are managing hundreds and hundreds, 500 or even thousand servers, this comes in very uh, or this becomes very critical- that every once a while some other service would be going down. so outages are inevitable. you can't do much about it, but it is very important to ensure that outage in one service does not affect other. it should not be the case that, hey, a service phase outage and your entire infrared store. it's a very poor architecture, to be really honest. so that is where, whenever you are modeling micro services, think about modeling it loosely coupled, where, even if one service goes down, it does not affect other, and try to have, instead of synchronous, dependency across services. try to make it a synchronous, try to make it even driven. so when you make asynchronous dependency, typically using message brokers, where once, let's say, post is published, i put a message, that message is consumed by search service and search indexes the post in in the search engine. right, this is a synchronous dependency. so try to model as much as you can with asynchronous dependency. next point, again organizational kind of technical. it's internal and external testing. testing becomes super complex when it comes to micro services because a good testing environment is always isolated right whenever they would want to, or whenever your qa team or automated test needs to run. a separate environment needs to be spun up totally independent of what anything else is going on, so that features and everything else can be tested well. and with microservices architecture, let's say you have five micro services and every time i test super suit, you would have to spin up those five micro services with some infra provisioning to to get that thing done. and it is very costly. first of all it is costly, so most of the organizations don't do it. second, it's a big engineering challenge where how easily can you spin up all of your micro services in a standalone environment? bit of an engineering challenge. but your qa team or your uh or yeah, basically, your qa team needs them to test it. well, so how do you strike a balance? and it's very difficult, uh, especially when you are stuck with a legacy architecture that you cannot, uh, move out of. you do any reason, but you still have your steel, are in process of migrating to micro services. so bit of engineering challenge, bit of organizational challenge, but something that we all should be thinking of while we adopt micro services. next challenge is you have to be very counterintuitive uh to your approach of development and think with failures in mind. so assume that your code breaks or your system breaks after every single line of code and solve that problem. so what happens if, after this line is executed, my code failed or my process crashed? every single link, after every single line, you should be thinking about this exact same thing. so thinking, hey, everything went off. hey, this process failed, that process failed. when you do that, you would be designing a very robust architecture which is fault tolerant, fault tolerant with respect to isolation, we saw. but this is more about how do you ensure that you maximize the uptime of your service and also consider the failures that happen in other services. we know that failures are, or outages are, inevitable outage in one service should not be affecting other services. but if you are dependent on that service synchronously, then you should be the one who should be handling it, right? so assume that after every single line of code, everything is breaking and solve it. solve it and reiterate. that would help you design a very solid, solid, solid, solid, uh distributed system. the next one is, and the final one, rather dependency management, pure tech side. but dependency management is a nightmare. so dependency management is when you have so many services, uh, uh, in your infra or to drive your product, ah, depend on. managing dependency across them is very tough. for example, there are three kinds of. there are typically three kinds of dependencies that i have typically seen. first is service dependency, where one service synchronously depends on another service. if this service goes down, it takes the other service with it. so with synchronous dependency, when two services are dependent on each other, if one going, if one goes down, the other is affected, right? so this has a potential of triggering cascading failures, leading to a complete outage eventually, right? so how do you manage service dependency that? hey, i am dependent on these two services to get my responsive. what? if they are down, i will be affected, right, something that we need to be very off. second is library or module level dependency. so let's say you break your service, or or you break your monolith into multiple microservices. you found a common utility that needs to be shared between the two. so what you do? instead of duplicating your code, you create a common library and add it as your dependency. now what if that library needs to be upgraded and adding new features, or you found a bug in the library to solve it? now what would happen if you're not following a proper versioning policy for your, or proper versioning for your library, or your backward compatibility for the library? these services would be affected. so whenever you are making changes in any of the dependent or in any of the depending services, the services on which are libraries on which others depend on, would have to ensure that it is backward compatible. number one and second: you have to ensure that you are using- at least you are using- proper versioning so that one micro service can upgrade the package while other stays on the previous version. right, you have to ensure that. otherwise, if you are not doing that, you would be in a situation where you would say all the two, the both of the services needs to be deployed at the exact same time when we upgrade the micro, uh, when we upgrade the package. all the three things should go out together. a very poor way of having your micro service or having you try to untangle yourself, but every time you think about it you are tangling yourself even more, and that happens. so always whenever you are having or introducing such dependency, ensure that you are as decoupled as possible. have proper versioning for your libraries and modules uh. ensure that they are always backward compatible, no matter what. the third dependency and the final thing we discuss is data dependency on services. like there are cases where your services rely on data coming from other services. it's not a synchronous dependency. it is an asynchronous dependency, but you are directly relying on the data that other service would generate. for example, you would want to serve recommendations to your end user, but your recommendation service is yet to generate a recommendation, so you cannot forward the request, or you cannot send recommendation email to other services or or to your end users, right, so you are waiting on the data that needs to be generated by some other service. now it's not just data, but imagine that the structure of data changes over time. you are dependent on that. so, again, similar to how we have uh versioning, we, how we have backward compatibility for libraries and modules, similarly we need to have versioning or backward compatibility for the data that is generated from the other service, so that other services are not affected. so, like, output of recommendation service, consumed by five different services, right, any change in data pattern or the way data is structured in uh, your recommendation service, will affect every single one of the depending services. so that is where, if such dependency exists, you need to ensure that your data, that a data format, is also backward compatible, maybe through versioning or maybe through uh, ensuring that the data, the structure of the data, doesn't change or, if it changes, it is backward compatible. right, this three dependencies, very critical we all should think about. but again, although sound, this might sound ki or microsis is so complex, no, it has its wonderful set of advantages that you get out of it and, at scale, micro services become inevitable, right, and that's why everyone adopted. but these are the points that you should always be thinking about. that, hey, in order to design a good micro services based architecture, you need to think of all of these 10 possible challenges that comes in with it. nice, yeah, so, yeah, that's all. that's all for this one. i hope you like this video and it added a lot of value. now you would start thinking as senior engineers, how to think and what all things to consider, even right. so i hope it added value again. if you like this video, give this video a thumbs up. if you guys like the channel, give this channel a sub. i post three in-depth engineering videos every week and i'll see you in the next one. thanks, saturn. [Music] you.