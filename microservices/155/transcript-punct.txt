so we all love creating micro services, but what if every team creates their service in their own unique way and using their own conventions? it would create a massive chaos of practices, protocols, frameworks and conventions. this propels us to have some standardization on how we build a new micro service, and hence in this video, we talk about why it is important to standardize some aspects of micro service, how enforcing a few practices would help us in the long run, and look at three most important verticals that you should definitely standardize while adopting microservices. but before we move forward, i'd like to talk to you about a course on system design that i have been running for over a year now. the course is a cohort discourse, which means i won't be rambling a solution and it will not be a monologue. instead, a small, focused group of 50- 60 engineers every cohort will be brainstorming systems and designing it together. this way, we build a solid system and learn from each other's experiences. the course to date is enrolled by 600 plus engineers spanning nine cohorts and 10 countries. engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say. the coolest part about the course is the depth we go into and the breath we cover. we cover topics raging from real-time text communication for slack to designing our own toy load balancer, to quick buzzes, live text commentary to doing impressions counting at scale for any advertisement business. in all we would cover roughly 28 questions and the detailed curriculum- uh split week by week, can be found on the course page, which is linked in the description down below. so if you're looking to learn system design from the first principles, you will love this course. i have two offerings for you. the first one is the live cohort discourse which you see on the left side, and the second one is the recorded course, which you can see on the right side. the live code base course happens every two months and it will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to binge learn system design, i would highly recommend you going for the recorded one. otherwise, the live court is where you can participate and discuss things live with me and the entire cohort and amplify your learnings. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page at pitt binary dot me slash master class and i would highly recommend you to check that out. i put the link of the course in the description down below. so if you are interested to learn system design, go for it. check out the link in the description down below and i hope to see you in my next cohort. thanks, so we all love creating micro services, but there has to be a way that standardizes on how we can build a microservice. for example, let's say an organization is very heavy on python and goal line right. if we give full autonomy to every demon picking their favorite language, what would happen someday? something would say, hey, i want to use scala for this. or i want to use, let's say hypothetically, which i want to do- is fortran for this unnecessary, unnecessary addition of tech stack or languages or frameworks into our organization. is is painful, because what if you don't have? or what if you don't find ingenious? let's say, the current set of engineers know that stuff, know that language, know that framework, but it would be harder to find engineers in today's day and age. let's say, who knows rust very well? they might have basic idea, but they might not know in depth of it. so then, if these engineers leave, how would we find engineers who are capable of building products in that particular stack? so this is one key reason on why we need to have some sort of a limited white listed frameworks and languages that we can use right by while we pick- and obviously this is not about discouraging an organization to not pick a new language, but it has- but there has to be a reason. there has to be a very solid reason on why are we picking that language. would we be able to find engineers who know that language very well if the current team leaves? just a small example on why standardization is important. another thing to elaborate on is if we don't have a way to, or if we don't have standardization in, let's say, the way we are collecting the data from our servers and applications and whatnot. what if there is a production outage and each service is having their own way of logging things? it would be very hard for- uh, for us to have a central logging infrastructure that analyzes any and every log and keeps us handy for us during an outage, because if each team is following their own conventions, it would be harder for your central logging system to process it, and so that is where it is very important to have standardized way of logging things, standardized way of accessing things, standardized way of building things, right? so this is where a term called a good service would come in. so, whenever you are building a microservice or taking it to production, what is the definition of a good service for you? right, obviously, any running process could qualify to be a micro service. but is that micro service what you want in your, as part of your ecosystem, right? so then there has to be a set of criteria that any and every service anyone builds needs to adhere to. if they follow that criteria, or if they complete that criteria, that would qualify to be a good micro service, right? so that is where a simple three- or if we ensure that we follow these three things, or if we ensure that these three things are met, we can say that, hey, this is a good service, right? so these three things are specifically for an organization, so it's up to an organization. on how they define manageability, observability and debug ability- not even sure if that is a word, but you basically get digest, right. so a service definitely needs to be manageable, not just with current engineers, but with future engineers, that we would bring it manageable with respect to people, understanding how to scale it. for example, your srn devops team needs to know how to fire fight any outage that happens with that service. okay, observable with respect to you, knowing on what's happening in a service and not having their own unique conventions to do it. there has to be a standard way through which we are ensuring our services are observable. then debuggable: it's hard to debug any service. for example, if no one in the organization knows rust and only a couple of engineers know, only they would be capable of debugging it. what if both of them are on leave? and if there is a major bug that is reported in that service, who would be debugging that right? debugging not just with respect to programming language but also with respect to how it interacts with other system. for example, some languages out of the box now support http 2 and maybe a persistent display, just to make it simply, just to simplify, a persistent tcp connections while making http requests. so if you are using that, how would someone know, like some other ninja might just be: hey, this is requested this response. he or she might not have an idea about it always creating a persistent tcp connection. so, always knowing all of this fact, we have to decide: hey, these are the particular tech stack or ways through which we would build a service, so that it makes it easier for anyone and everyone part of your organization to see what exactly would be happening. right, so that standardization always helps. so then, if a developer wants to switch teams, he or she would not have to go through a long on-boarding process because most of the things are there. he or she knows how to access laws here. she knows how to see the key metrics and how to debug basic stuff. all he or she needs to prepare on is the language and the tech stack and the framework that they are using. right? but then you'll say: hey, pete, you were talking in the in the previous few videos, talking about autonomy, and every micro services is needs to be autonomous and is allowed to make their own decisions. yes, it's true, but autonomous does not mean that you are allowing any and everything. right, there has to be a set of things that you want them to adopt. you want them. hey, pick from these three or these five things. don't just randomly bring in any new things in your tech stack. we are not sure if we would be even supporting that in the long term, or we would be even able to find engineers who know that stuff. a lot of uh, a lot of non-technical or non-engineering based uh factors come in. when we talk about uh, white listing a particular set of thing or allowing a particular set of things in an organization. it's not a random decision. so that's where we are not. when we adopt standardization, we are not snatching away autonomy, right. what we are trying to do here is to ensure that anyone and everyone who adopts things or builds things, they are on the same plate and most of the foundational things are covered. then it all remains on how that particular language or text stack happens. so having a standardized, having a standardization across things, helps us keep our entire system coherent and uniform. right. switching teams becomes easy, debugging becomes easy and you know what you are signing up for, that there will be not random thing coming into your text. right now let's talk about three pillars, or three important word pillars, but three important verticals that we should definitely standardize on. we'll start with the most most famous one: monitoring. so it is very essential to know on how your services interact with each other or how a request originating from a user is interacting with different services, eventually getting responded back to the user right. so we need to have a way through which we can visualize and see what's happening and, in case of an outage, in case of a bug, we are able to trace those things to the particular service and then debug it right. having observability or having a good monitoring and alerting setup on one service is easy, but we need to have an end-to-end view so that we understand how those systems are behaving and what is the choke point in the entire flow. so there has to be a standard way through which we are tracing it. maybe use zipkin, maybe use aws x-ray, any distributed tracing tool. have one standard distributor tracing tool that would solve this problem for you, so that every single engineer in the org exactly knows where to look for when he or she wants to debug a distributed bug. so, apart from this, we also need to know, with respect to monitoring, how is every single server of us in an infrastructure is doing, how is every single service is also doing? so what do we want to have? we want to measure the cpu, the memory, the disk, the network consumption of a particular server. from a service perspective. we want to measure the health checks, the periodic health checks of that service, the request counter- 2 x x, 5 x x, 3 x x happening on the surface, right. so we need a standardized way. we need a standardized way to gather these metrics and put it into a database for us to set up alerting on top of that. so typically what you do is you would use a tech or you would use tools like prometheus graphite, new, really data doc, right. so promote this graphite, open source, self host neural link data dog- paid, offering that you would. you can go for so gathering all the metrics from all the servers, all the services, and basically collecting it into one of this possible text. again, there are many more, but just just listing out this for prometheus graphite, new, relic and data dog and then they put it in the dpn. you have this uniform way of querying any server's metrics or anything. for example, you'd want to check, hey, for this server, what was the cpu usage during this time window? you'll be able to get it from this 1db and db, as it would not directly access the db, but you would have a nice interface to query on that. but the idea being every single thing put together, put into this one central place, standardized way of doing it and putting it into this one db for everyone else to query it right. some metrics that you would want to standardize that we should be collecting, and it's mostly infrastructure configuration or your common library framework configuration that you can configure like like that you can alter. first thing is collecting server metrics on informative cpu ram disk network. collecting logs like application log, process application log, as in your application or how how is request coming and what's happening and what's not. then your process logs, so multiple processes or multiple critical processes running into your machine. you need to gather their locks and put it into a central place. then user logs on how user is interacting with your system. if you are having those user specific logs that you are filing, maybe your end user logs or even your operating system user logs on what ssh combat data, user fire, right then ssh logs, operating system locks. all of that you can collect and ingest it into your central monitoring system. then from the application side matrix, it could be the 2xx, 3xx, so basically 200 or 200 plus- sorry, greater than 200- http status code, 3xx, 4xx, 5xx, that you get uh response time so that you can compute what's the average response time, mean response time, p99 response time and all the request code on number, on how many requests are you getting on your services and the number of servers that a particular service is huddling. so these are some basic metrics that, out of the box, every single micro service should be throwing into your uh, into a central monitoring system. in most cases not everyone needs to monitor it, but that's a standardization, needs to be there on when your service is deployed. you would have all of this metrics out of the box that you don't have to worry about. but having a centralized or having a standardized way of accessing it is a key. the second vertical i want to talk about is interfaces. so interfaces, as the word implies, how would to service talk to each other and then how would your end user talk to one of your service? so this is a very open space. there are so many protocols and frameworks that we can use in order to achieve this. for example, i can use http rest, i can use grpc, i can use soa, i can use so many things in order to have this communication from server to server or user to server, but then you cannot allow anyone and everyone to pick any and every framework or tool or protocol to do it. there has to be a few ones that you allow people to choose from, right. so that is where having a standardized way to do it is extremely important. not just this, but even within that, within a particular, let's say, we pick rest. within that, how would we define routes? is there a way to define dots? for example, the route should always contain plural, it cannot have conjugate words and whatnot. so it's up to you on what kind of rules that you would want to impose, otherwise people would create random routes and then there would be no standardization whatsoever, right? so if you are going for a rest based thing, all the routes should adhere to the rest standards, otherwise it should not be pushed, something that you need to enforce, and there has to be a standardized way to do it so that it makes life, so it makes things predictable for every engineer in the organization to understand. hey, this is exactly how a trout should be. they would not have to unlearn and relearn a lot of stuff. right, then? how to name end points, singular plural, right? how to paginate documents? are you going for a limit offset based pagination or a token based pagination or a cursor based pagination, right. so there has to be a singular way of paginating things so that or at least a couple of ways of being, but that has to be standardized enough that it does not uh or a developer does not have to go through a lot of process, notice and how the pagination is working and you can create a common layer. that would make your life easier whenever you would want to pay genetics. next thing is about about versioning. like there are so many ways to version your apis. i'm not even talking about package versioning, but just api versioning. that, hey, are you prefixing your route with the v1, v2, v3 or with a version, or are you suffixing it with a version? or are you always creating a sub domain for it, like just rand, like just example that i'm giving you that possible way that i've seen people uh creating routes right or basically doing versioning on their api routes right. so there has to be a single way, like we cannot have profile service using approach one while payment service using approach two. there has to be a single way: that which the entire organization is creating or is versioning that apis. the next one is about uh connection timeouts or how, when two servers or two service communicate with each other or user communicates with the server, there will always be a timeout and this timer needs to be set like. there is to be a standardized way through which we are configuring timeouts and when the timeouts happen. there has to be us a common way how which we are retrying the- uh, the request right, and when we are retrying. are we using exponential backup or what? which algorithm are we using when we are trying to retry a failed request? your connection timeout should not be too small or should not be too large. it has to be configurable, but there has to be a limit that, hey, from this to this, only you can configure the value. then, when we are making http request, what should be the payload type like? should it always be json or should it always be xml? should always be raw text? it's up to you, but there has to be a standardized way of doing it so that- and in most cases it would be json. but having a standardized way gives clarity for any engine, any and every engineer in your organization, on how to interact with it, and you can obviously not just engineer side of things but also automation side of things that you exactly know, that whenever a payload would come in, it would always be json, for example. it would make life so simple that you can abstract out the complexity of passing bodies and creating json at every client right by just having this standardization or convention in there. and the final vertical i want to talk about is tolerance. so tolerance is very important in order to ensure that our infrastructure is not going down or a service or a product is not going down. so when we have a micro services based architecture, a lot of service will interact with a lot of other services, right. so there we have to ensure that if one service bombards another service with a lot of request, right, that should not take down any other service, or even that service, for that matter. so that is where every its responsibility of every service to shield itself, right. a few strategies that would help you do this, or that would help you gain a good level of tolerance, are, first of all, ration the number of calls that you get from each service. for example, if you are, let's say, if you are- owner of profile service, what you have to ensure that, no matter how many requests you get, you are not going down. so that is where, if any service is making excessive request to profile service, you would have to rate, limit it, right. so ration the number of calls that a service can make to you- for example, 300 calls per second- will not support any more than that from a particular service. this way, you are ensuring that you are not going down, and if you are not going down, any service that depends on you does not go down. for example, some service or there would not be just one service calling the profile service to get profile information of a user, there'd be many such service. if one service bombards a lot of request and takes on a profile service, every other thing gets affected, right, so we'd have to ensure that this does not happen. next is it's not just about rationing the incoming request to you, but also, as a good citizen, you have to ensure that you are not also bombarding someone else or some other service to, or to a very large level. for example, you want to ensure that you are not bombarding the notification service to send millions of notifications every minute, right, because that might take down notification service, giving out a bad performance. so that's where a bad experience. so that is where you'd want to not just shield yourself from the incoming threat, but also from you would want to ensure that you are not creating chaos in someone else's life. the next is the next two points are very critical. it gives you this ability to turn on and off a particular uh invocation, for example. have a way through which you can dynamically say that hey, i want to cut off any incoming request from this service so that if, let's say, profile service is getting bombarded by uh, let's say a notification service for some reason, and when it is doing that, you would want to venture on the fly that, hey, stop getting any incoming request from this service. have that ability to cut it off completely and without. obviously, when i say on the fly, it implies you should not be having to do another deployment to do it, just a button press or just a configuration change and you are basically cutting this service off from any incoming request from any other from a particular service similar to this. have a dynamic configuration that would help you cut off any outgoing call stores. you will say: but outgoing, how would it matter? imagine you are profile server or you are, let's say, notification service and you are invoking profile service to get that information. what if profile service is done? so when profile service is done, it's not that your request would immediately fail. the http request would be invoked. it would wait up until the connection timeout happens and then it would fail. so if your connection timeout is 10 second, it would take you 10 seconds for it to detect. hey, something is wrong, and then it would time out and what not right. so it would put unnecessary load onto this system because so for those large time windows, your tcp connection is kept open unnecessarily. so that is where have an ability that you know that if a particular downstream service is down, you are immediately cutting it off, so that you don't even make a call to that service and send a default response to your user, right? so these four abilities are very important in order to define, for, in order to have a good fault tolerance level for your service, right? nice, so, yeah, those were the three verticals that i wanted to talk about. that definitely needs standardization. if you do, if you have it, this would cover 80 percent of your use case. otherwise, there are many more standardizations, but these are basic, high level standardizations that we should all be doing, and this would fair. this would give you a good edge over most most uh practices out there. by doing twenty percent of effort, you are covering eighty percent of it, right, nice? so, yeah, that's it. that's it for this video. i hope i added value and again, if you like this video, give this video a thumbs up. if you guys like the channel, give this channel a sub. i post three in-depth engineering videos every week and i'll see you in the next one. thanks, [Music].