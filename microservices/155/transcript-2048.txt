so we all love creating micro services, but what if every team creates their service in their own unique way and using their own conventions?
this propels us to have some standardization on how we build a new micro service, and hence in this video, we talk about why it is important to standardize some aspects of micro service, how enforcing a few practices would help us in the long run, and look at three most important verticals that you should definitely standardize while adopting microservices.
the first one is the live cohort discourse which you see on the left side, and the second one is the recorded course, which you can see on the right side.
thanks, so we all love creating micro services, but there has to be a way that standardizes on how we can build a microservice.
or i want to use, let's say hypothetically, which i want to do- is fortran for this unnecessary, unnecessary addition of tech stack or languages or frameworks into our organization.
so this is one key reason on why we need to have some sort of a limited white listed frameworks and languages that we can use right by while we pick- and obviously this is not about discouraging an organization to not pick a new language, but it has- but there has to be a reason.
would we be able to find engineers who know that language very well if the current team leaves?
another thing to elaborate on is if we don't have a way to, or if we don't have standardization in, let's say, the way we are collecting the data from our servers and applications and whatnot.
what if there is a production outage and each service is having their own way of logging things?
it would be very hard for- uh, for us to have a central logging infrastructure that analyzes any and every log and keeps us handy for us during an outage, because if each team is following their own conventions, it would be harder for your central logging system to process it, and so that is where it is very important to have standardized way of logging things, standardized way of accessing things, standardized way of building things, right?
right, obviously, any running process could qualify to be a micro service.
but is that micro service what you want in your, as part of your ecosystem, right?
if they follow that criteria, or if they complete that criteria, that would qualify to be a good micro service, right?
so that is where a simple three- or if we ensure that we follow these three things, or if we ensure that these three things are met, we can say that, hey, this is a good service, right?
on how they define manageability, observability and debug ability- not even sure if that is a word, but you basically get digest, right.
so a service definitely needs to be manageable, not just with current engineers, but with future engineers, that we would bring it manageable with respect to people, understanding how to scale it.
for example, your srn devops team needs to know how to fire fight any outage that happens with that service.
okay, observable with respect to you, knowing on what's happening in a service and not having their own unique conventions to do it.
there has to be a standard way through which we are ensuring our services are observable.
for example, some languages out of the box now support http 2 and maybe a persistent display, just to make it simply, just to simplify, a persistent tcp connections while making http requests.
so if you are using that, how would someone know, like some other ninja might just be: hey, this is requested this response.
so, always knowing all of this fact, we have to decide: hey, these are the particular tech stack or ways through which we would build a service, so that it makes it easier for anyone and everyone part of your organization to see what exactly would be happening.
but then you'll say: hey, pete, you were talking in the in the previous few videos, talking about autonomy, and every micro services is needs to be autonomous and is allowed to make their own decisions.
right, there has to be a set of things that you want them to adopt.
switching teams becomes easy, debugging becomes easy and you know what you are signing up for, that there will be not random thing coming into your text.
right now let's talk about three pillars, or three important word pillars, but three important verticals that we should definitely standardize on.
so it is very essential to know on how your services interact with each other or how a request originating from a user is interacting with different services, eventually getting responded back to the user right.
so we need to have a way through which we can visualize and see what's happening and, in case of an outage, in case of a bug, we are able to trace those things to the particular service and then debug it right.
having observability or having a good monitoring and alerting setup on one service is easy, but we need to have an end-to-end view so that we understand how those systems are behaving and what is the choke point in the entire flow.
have one standard distributor tracing tool that would solve this problem for you, so that every single engineer in the org exactly knows where to look for when he or she wants to debug a distributed bug.
so, apart from this, we also need to know, with respect to monitoring, how is every single server of us in an infrastructure is doing, how is every single service is also doing?
we want to measure the health checks, the periodic health checks of that service, the request counter- 2 x x, 5 x x, 3 x x happening on the surface, right.
so we need a standardized way.
we need a standardized way to gather these metrics and put it into a database for us to set up alerting on top of that.
so typically what you do is you would use a tech or you would use tools like prometheus graphite, new, really data doc, right.
you can go for so gathering all the metrics from all the servers, all the services, and basically collecting it into one of this possible text.
for example, you'd want to check, hey, for this server, what was the cpu usage during this time window?
but the idea being every single thing put together, put into this one central place, standardized way of doing it and putting it into this one db for everyone else to query it right.
some metrics that you would want to standardize that we should be collecting, and it's mostly infrastructure configuration or your common library framework configuration that you can configure like like that you can alter.
collecting logs like application log, process application log, as in your application or how how is request coming and what's happening and what's not.
so these are some basic metrics that, out of the box, every single micro service should be throwing into your uh, into a central monitoring system.
but having a centralized or having a standardized way of accessing it is a key.
for example, i can use http rest, i can use grpc, i can use soa, i can use so many things in order to have this communication from server to server or user to server, but then you cannot allow anyone and everyone to pick any and every framework or tool or protocol to do it.
so it's up to you on what kind of rules that you would want to impose, otherwise people would create random routes and then there would be no standardization whatsoever, right?
so if you are going for a rest based thing, all the routes should adhere to the rest standards, otherwise it should not be pushed, something that you need to enforce, and there has to be a standardized way to do it so that it makes life, so it makes things predictable for every engineer in the organization to understand.
so there has to be a single way, like we cannot have profile service using approach one while payment service using approach two.
there has to be a single way: that which the entire organization is creating or is versioning that apis.
the next one is about uh connection timeouts or how, when two servers or two service communicate with each other or user communicates with the server, there will always be a timeout and this timer needs to be set like.
there is to be a standardized way through which we are configuring timeouts and when the timeouts happen.
there has to be us a common way how which we are retrying the- uh, the request right, and when we are retrying.
it's up to you, but there has to be a standardized way of doing it so that- and in most cases it would be json.
but having a standardized way gives clarity for any engine, any and every engineer in your organization, on how to interact with it, and you can obviously not just engineer side of things but also automation side of things that you exactly know, that whenever a payload would come in, it would always be json, for example.
it would make life so simple that you can abstract out the complexity of passing bodies and creating json at every client right by just having this standardization or convention in there.
so there we have to ensure that if one service bombards another service with a lot of request, right, that should not take down any other service, or even that service, for that matter.
so that is where every its responsibility of every service to shield itself, right.
for example, if you are, let's say, if you are- owner of profile service, what you have to ensure that, no matter how many requests you get, you are not going down.
so that is where, if any service is making excessive request to profile service, you would have to rate, limit it, right.
this way, you are ensuring that you are not going down, and if you are not going down, any service that depends on you does not go down.
if one service bombards a lot of request and takes on a profile service, every other thing gets affected, right, so we'd have to ensure that this does not happen.
next is it's not just about rationing the incoming request to you, but also, as a good citizen, you have to ensure that you are not also bombarding someone else or some other service to, or to a very large level.
for example, you want to ensure that you are not bombarding the notification service to send millions of notifications every minute, right, because that might take down notification service, giving out a bad performance.
obviously, when i say on the fly, it implies you should not be having to do another deployment to do it, just a button press or just a configuration change and you are basically cutting this service off from any incoming request from any other from a particular service similar to this.
so when profile service is done, it's not that your request would immediately fail.
so that is where have an ability that you know that if a particular downstream service is down, you are immediately cutting it off, so that you don't even make a call to that service and send a default response to your user, right?
so these four abilities are very important in order to define, for, in order to have a good fault tolerance level for your service, right?