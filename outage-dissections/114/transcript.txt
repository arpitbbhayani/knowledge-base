so on 18th of march 2022 google maps had a major global outage that lasted for over two and a half hours this is the incident report of it and in this video we will dissect this particular outage and as an outcome of this will understand how to design a robust micro services based architecture right so let's jump right into the issue what happened is uh on 18th of march google maps platform api experienced elevated error rates which means which basically means in terms of http request a lot of 5x6 and the api setup included was google map javascript api static api android ios navigation sdk gaming services so typically everything around google maps was down for that two and a half hours the main culprit or the main thing that got impacted was the map style api was impacted for over 3 hours and 45 minutes so tile api renders the tile on your google map which has this rich information so that tile was not rendering which means that what what did your end user see during this outage your end user during this outage saw what they saw great isles right so because the tile api service was down for an end user what did they see they see a lot of gray tiles which means the information on the map was not loading which means the map initialization was failing and and it was all gray because that might have been the default value that map uses to render basically whenever it does not have anything so during this outage what happened is 65 percent error rate that's insane amount of error rate directions api p99 response time hit 8.5 seconds and 8 500 milliseconds so that's that's pretty insane amount of hit that uh google maps took during that outage and 37 error rate in navigation so someone using google map for navigation 37 of the requests were resulting in errors so given the scale of it you would see how how major of an incident was it right so the root cause of this incident the root cause of this incident is actually a feature rollout in some service they have not named the service there uh there happened a feature rollout in a particular and obviously every single day some of the other developers ships the code and some feature got rolled out in some service and that feature and that particular rollout might had some bug or something in it because of which it exhausted the allocated resources on the server so obviously though your service runs on a server server has memory disk cpu maybe some shared database and whatnot it exhausted because of the bug that it had it exhausted the resources they have not specified which resource but obviously it could be one of these four so shared uh shared stateful thing or cpu or disk or memory and when something exhaust like when your server exhausts its resources and as in your service wants more resource but it is not there let's say service needs more ram but it is not there what happens your process crashes so that's exactly what happened there also the service crashed so as soon as the new feature was rolled out because it has some bug in it it exhausted all of the allocated resources and then it crashed so every time the service was getting up it it immediately consumed all the resources and it crashed coming back up resources so the services of the the processes of this particular service that they have not named continuously kept crashing right now what was uh what what happened because of that some service the core tile rendering service which renders which renders the tiles on the back end and sends it as a response that service had a synchronous dependency had a synchronous dependency on this service on which the new feature was rolled out because it had a synchronous dependency and this service had an outage what happened your tile rendering service started getting timeouts because this service was not responding let's say your tile rendering service whenever uh it was making a call to some service and we know that in a micro services based architecture whenever there is a synchronous dependency between two services it is always great to add time out to it so let's say you say that hey tile rendering service whenever it invokes the api of the sum service needs to get response within two seconds if it does not get it would result in a timeout so that's what time that's what tile rendering service did and because this service was down the api call made by this trial rendering service always resulted in a timeout because it always resulted in a timeout there were tile training service like any other good good micro service design does it retried right so it it kept on retrying for that until the time out of this service hits from the user side or or from the customer side it hit it was it was retrying so let's say they hypothetically it retried for three times and then it killed off he i'm not getting any response let me just send a default response to the user right so because tile rendering services resulted in timeouts because of synchronous dependency on a crash service your tile rendering service started throwing errors right i'll give you an example of how this happens so let's say your tile rendering service has a timeout of sorry your uh when your tile rendering service is talking to the sum service has a timeout of two seconds and with three retries right so it would have to it would make a call to some service wait for two seconds to get a response and then retry how many times three times so the time at which your tile rendering service like for anyone who is invoking the tile rendering service for that to get an error how much time would it take two plus two plus two six seconds so two seconds timeout between these service is equal to six second time out or six second so it would take six seconds for the customer of or someone who is invoking the style ending service to get an error because style rendering service is internally doing this retry so it all adds up it is all cumulative uh thing that would happen because of the timeouts and the retries that you have placed to make your system robust so now your tile rendering service started throwing timeouts and because of timeouts uh because of retries it started throwing errors after it after it exhausted all the retries and why the errors happened obviously that it was not able to get it but with that retries in place the internal memory queues of tile rendering services reached capacity so they might be using so for what do we use internal memory cues it could be a small buffer for processing the retried request it could be a it could be a partially baked tcp connection pool maybe some sort of async behavior that they might want to do within multiple threads could be anything they have not specified it but obviously whenever you do retry it has an impact on the code that you need to always validate on what would happen when you go into that state of of infinite rate rise or or very large number of rate rates so because of retries your tile the tile rendering service went uh they it basically reached its capacity and it started crashing because server ran out of memory that's what i said as soon as it just server runs out of memory the process that is running it crashes and because your tile rendering service crashes it it increased like because it crashes so that's then it started throwing what 503 basically uh service temporarily unavailable and when server runs out of memory what happens so the the request that are partially accepted right they would see increased latency and the increased latency are not just because memory is also is not just because memory is less but also because of this synchronous dependency that it had as i said two seconds of timeout over here with three retries is equal to three is equal to six seconds time out for anyone to realize hey this is download this is impacted so for every request you would see increased latency because of retries and timeout and eventually the request getting rejected by 503 so this is what elevated error rates happened then the next part failure cascade so you saw how an outage in one service had an impact on another service because of it this service crashed now the services that depend on this is are with service the end user service the maps sdk the gaming sdk uh the directions sdk right all of those service they have they they depend on the tile rendering service synchronously so and who invokes maps sdk there are either end users like us who invoke map sdk through our mobile app to talk to that or some b2b customer who might be using sdk to do some some sort of like like they like to to to process their own request or use google's data for their own needs so because style rendering service had an impact sdk when it tried to hit the particular endpoint it started it started getting increased response time it started getting time outs it started so that's where it got that gree uh great i'll for your as part of an end user so there might be some users who saw elevated error rates while some user who saw it like completely down right because there might be some request that still got accepted took 10 seconds for execution the user was waiting for the response while some connections were not even accepted so user immediately saw an error out there right here the key part is the customer the the end user customer this is the most interesting part of this outage that because of the maps sdk and navigation sdk directly depending on the style rendering service the external clients were retrying as in your device my device internally it was retrying to get the tile information from the google maps api so because of that retry the traffic went 10x the traffic on the api server of the style rendering service went 10x because earlier if everything was working fine you would have gotten response in two seconds or one second now it is taking six seconds for it to give you that error so because it's all bulking it up so the total number of eps the total amount concurrently requires that it is handling short up to 10x of the normal traffic which means that it is it is putting your service and in a it's it's basically kind of mimicking a denial of service attack not an explicit attack but an implicit denial of service attack because of the retries that was being done by the end users and that's going to happen right because it's very hard to differentiate between what a data lab service attack is and what like what's a genuine traffic and what's and basically what's a malicious traffic so because it was a very sudden spike or very sudden jump in the number of requests this a lot of retries put this service on uh you know you know in a dos attack like and because of they doing retries in order to get that information but that service got overwhelmed by the number of requests that happened and it basically turned it it basically triggered an entire outage like an entire crash an entire set of cascading crashes right and that's exactly what happens when you have synchronous dependency as part of your micro services architecture so here there was the synchronous dependency from user to map sdk you can't do much about it from s map sdk to tile from tile to some service because of retries which was which happened uh which grew the traffic to the 10x of the normal levels your service went into a denial of service so it thought that it was a dinner service attack and it's and it started rejecting the request so when this outage happened the first thing the first limitation thing that anyone would do is roll back like we know that if any latest deployment happened and that uh and we know that some deployment happened the first thing that you do is you if you can roll back the deployment you do it right away that's exactly what they did so they roll back the uh deployment but the because the the the time for which the rollback was happening the the time for which the during that time the synchronous communication happening between other services was still showing the cascaded error so the errors on the main service the the the some service got down it got recovered but because of the cascading failures that happened it took some time for other services to recover so let's say if my first service recovered in let's say 30 minutes it took one hour or one or twenty minutes for other services to recover because people were retrying on that sdks were trying on that and what not right so it takes time whenever there is synchronous dependence and whenever there is this cascaded failure it's not just the recovery of the of the core or of the root cause which is important but also the recovery of all the synchronously dependent component is important right so it took some time for them to recover and obviously after two after two hours everything got recovered error rates were so basically the error rates dropped down and what not and everything was peaceful so then as part of prevention this is where i loved this thing about uh this particular outage on what on how google is planning uh to to to basically prevent uh the outages happening because of the same reason so the first prevention technique what they're saying is they would want to reject the request when the server is at full capacity right so they mentioned this in the in the outage document that they would want to re reject the request when the server is operating at the full request full capacity which means graceful degradation so instead of accepting the request making a call to trying to render it it's always better that if you know that your server is full do not accept the uh do not accept the request at all and instead immediately return an error to the user that hey i am overwhelmed right now for example 429 basically rate limit excited or something that suggests the user that hey um i am overwhelmed i am not going to accept this request unless i have the capacity to do it so whenever you are putting something into production see how your system would behave at scale when it gets a lot of inbound request then second is very interesting where it says that optimize the bounds of server queues the in-memory server queues of the tile request that got bulk up they said that they would be doing some some some optimizing of bounds of server queue as per my guess it would be around tcp tuning as in what the number of backlog tcp connections that they would have web server turing on the number of concurrent connection your web server would handle and in memory process buffer tuning so if they have allocated a buffer with 1000 as a capacity maybe they would want to increase or decrease it depending on the uh depending on the dissection that they do internally the next part is exhaustive monitoring obviously it's always better to get aggressively notified about something so whenever there is a significant jump in the resource consumption you would have to be you would have to be notified about it so very exhaustive monitoring on all the services whenever there is an abrupt jump of resource consumption it's not the request that has come in because you might have very small chunk of request or request that could be served quickly but monitoring of your server resources if it goes if it if basically there is a sudden jump in it typically due to memory leak or some bad rollout you should be notified right away rather than your after your service has gone down you get notified it's better to get notified beforehand only and and the final thing the more important one is circuit breaking this i love this part about google on how they said that they said that they would block they would find a way to block internal traffic and continue to serve customers which means that on a service that went down let's say your tile rendering service that got impacted a lot of sdks a lot of internal service might be dependent on it right like it was not just end users like us who directly depend on the tile rendering service but also some internal service so what they said is they would want to find a way through which they can cut off the traffic from the internal services so that end users like us can you can can continue to use google maps uh seamlessly so that shows customer observation of google on how they are willing to like and obviously this is not just basically cutting off all the internal services but obviously they would they would classify it into some that we that these are critical and these are not by cutting off the traffic that came on to came from the internal traffic while serving the end users is something that is a very solid way of ensuring your end users sees a minimal impact while our internal services suffer to some extent right so always whenever there is a synchronous dependency whenever there is this inter service dependency try to classify which all are the critical components that needs to serve and which are something that it's okay for them to to take a larger hit right and by doing this what happens if you cut off the traffic from your internal service the the number of requests being made onto the service would go would drop down drastically having more bandwidth to serve your end user facing request so thinking about your customers whenever you are architecting your microservices is an excellent way of doing it classification of services is something that every single one should be doing while designing a micro service architecture and have circuit breakers in place what circuit breakers would do is it's like a switch where you might when as soon as you turn on your your the switch what would happen is like the traffic from this internal service would would stop coming so in this context that would happen right so that's uh about it uh this is exactly what is written in this document i'll link uh the link the outage link or in the description down below feel free to go through it it's a very simple one they have just like the things that i explained it's exactly what they've written i just tried to read between the lines to understand what could have happened into this but uh that's some pretty solid thing that they have uh done like i i love this particular part where they would want to block the internal traffic to their backend servers just to ensure that your end user is not impacted this clearly so shows solid customer obsession so yeah that's in the and this is one diagram that they added into this thing where you can see where you can clearly see the number of errors that it got basically the overall error rate it started bulking up bulking of bulking up this is where they applied the mitigation and then it suddenly dropped down and everything recovered after that right so that's typically this particular outage document again i'll link this document in the description down below so yeah that's it for this one google map having an outage not a good thing but it actually helps us learn a lot of a lot of nuances around designing solid robust microservices based architecture so i hope uh you folks learned something new today on designing a robust microsis architecture and uh that's it for me for this video so if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sub i make three engineering videos every week and i'll see in the next one thanks a ton