Importance of localizing failures

Dissecting GitHub Outage Localized Failures What happened ? Failure Actions or delay in queued jobs for GitHub happened because of some infrastructure error in SOL layer Database failure affected Authentication and communication between different microservices for Github Actions What this tells us ? El - GitHub actions has multiple microservices shared database through shared and they communicate a DB across multiple eg : one would pick the job and update DB microservices ! ! other would execute and update theDB - Zero trust btw micro services The database / service also drove authentication unauthorized not made ensuring request are even btw services

But , if a database is down . shouldn't there be an automatic failover ? Yes usually . the databases are configured - -1 with auto failover "" R ie . if master goes down , a replica is promoted Orchestrator But this did not happen Because DB was down ! telemetry did not show that orchestrator also needs a source to find Master is down and failover is needed time to determine root and mitigate Hence it took a long cause Once the root cause was identified they . would have done a manual failover rebooted the machine * Refer other Outage dissections to learn about and hacks ! possible ways of mitigation

term long - fixes is . Change the automation and ensure it understands this failure Better } failure detection Minimizing impact Better automated failover 2. localize failures Microservices should be loosely coupled Outage in one component / service cannot take down entire sub - system . Hence GitHub team would invest in ensuring loose coupling that does not , so outage in one affect others let be loosely coupled f) they . and the communication is A SYNC through @ or tolerant APIs 0