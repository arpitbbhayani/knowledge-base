on 8th of march. uh, spotify had a massive outage because of which all the users of spotify were logged out of their applications and when they retried logging in back, they were unable to do so. the first set of speculations that came was: maybe their authentication service is done or some of the related component is down, but because we are not able to log in, we can't use their services. so that was the first set of speculations that people had about it. but today, on 11th of march, they released their incident report of that very incident. i have it open right here. so in this video, what we'll do is we'll try to dissect on what exactly happened, why that thing happened and what was the fix that they made. through this, we would touch upon a lot of very interesting nuances on what happens when a company operates at scale. what are different infra components that need to talk to each other to ensure that everything works well, right, okay, so let's jump into this. uh, okay, so they say that on march 8, they experienced a global outage triggered by issue in a cloud hosted service discovery system used at spotify. so this service discovery is the keyword over here, because what this implies is: spotify has a lot of micro services and for one micro service to discover other micro services, they're using something to discover the services, right? so, uh, a few example of this is: one is ny, another one is istio. so basically, there are a few few, few very solid solutions out there, but this shows that at scale, when company operates, they are using something for service discovery. okay, they're made aware of the issue. this outage affected users and we apologize for insurance cost. our service is not fully required. good, okay, so now what exactly happened? the spotify back end consists of multiple microservices that communicate with each other. for microservices to be able to find each other, we utilize multiple service discovery technologies. this is a critical component. why? because what they have written is they utilize multiple service discovery technologies, so it implies that they are not just relying on one way to discover their services. they always. they have a fallback plan, as in what if one of the service discovery element goes down? they can fall back to other studies. so this one word here is very critical: like any time you are architecting anything, always ensure that you have a fallback plan on what happens if that component goes down. okay, okay, most of our services are using dns based service recovery system. we will be talking about. what is dns statistical system in in some time. most of their services use dns based service service discovery system. however, some of our services use xds, so xds is an open protocol on how service discovery needs to happen. so all popular service discovery solutions out there they adhere to this particular set of specifications which makes them agnostic. they use xds based traffic control plane discovery system called traffic director. so traffic director is a managed offering, uh, which is provided by gcp, basically google cloud platform. so spotify uses traffic director as their control plane. now let's spend some time into understanding what these things are, right, so i've just created a few basic set of nodes helping us to understand what. what basically could have happened, okay, so, uh, what happened with spotify global outage on eighth of march? user logged out. they were not able to log in back. initial set of speculations were: or service down, which even i posted. even i thought that maybe our service is down because, like basic, what else if? if we are not able to log in, right? so, uh, other possible reason could be that only their cash store or maybe their or or maybe their authentication token store might be down, which could have led to that outage. but when we go through the dissection we realize that no, it's not related to that, but rather it was an outage on the service discovery side. so now what exactly happens with service discovery is, let's say, you have four services, uh, let's say we're talking about spotify here. only the first service is streaming service, which takes care of, uh, streaming music to users. we have authentication service, we have search service and we have recommendation service. so for each service to talk to each other, right, obviously, like when a request comes in, only that service might not have all the relevant information to basically give out to, to be basically given out as a response, so that service would be dependent on other peripheral services in order to, in order to basically fulfill the request coming in from the user. so that's where one service needs to talk to some other service to get some data. so that is where micro services need to talk to each other- synchronously in some cases and asynchronously in others. when microservice wants to talk to each other synchronously, they might use some way to do it. so a popular way to do it is: let's say, you expose an http endpoint, like every services expose their http endpoints and they directly talk to them, like, for example, um, so this is a very standard way of doing it. so, uh, which is using url of the service. like, every service has a specific url, let's say searchspotifycom, or basically authspotifycom or something like that, and every service has a load balancer behind that. there is a multiple set of servers whom they can connect to, and all so here, one way to do it is to have these url endpoints configured on all of these services, right, and they know whom to talk to, how to reach out to them- mostly rest based access, what you might want to define over here. so, when you do that, that's a very traditional way of doing it, but there is a modern way of doing it which is very popular when you have hundreds and thousands of micro services. uh, we will talk about the advantages and disorders in just a couple of minutes. so, service discovery- the popular way of doing it- is called as a service mesh. so what is a service mesh is, service mesh is a way through which multiple services can talk to each other, such that the services only need to focus on the business logic side of things, while all the heavy lifting around- for example, tracing, load balancing, scaling, security, retry, circuit breaking- is all offloaded to a side car. now this side car is is is a peripheral service, a small process which is running in parallel to your, your, your process, which handles your primary service and this side car. what it does is like it would do all the heavy lifting. for example, if your authentication service wants to talk to your streaming service, so authentication service will not directly talk to your streaming service. instead, what it will do, authentication service will talk to the side car running very close to itself, like, for example, if this is one machine, you'll have one process that runs your authentication business logic. you'll have another process that runs your sidecar, right? so authentication service will talk to the sidecar and will say: hey, sidecar, i want to talk to streaming service. so sitecar will talk to the sidecar of the streaming service and then this sidecar would then forward a request to the process that handles the streaming part of it. right? so direct communication between the two- authentication and streaming- is now the communication between the corresponding side cars. now what is the advantage of using this, this, this sort of complicated thing, like because load balancer was working fine? why do you want to add such such sort of complexities in your architecture. a very simple reason that justifies this thing is that every service needs to take care of: hey, if i'm talking to other service, what if that other service does not give me a response in one in, in, let's say, five seconds? i want to have time out. what if i do not get response? i may want to retry, uh, making that request again, right? what if i don't want? let's say, if my service is internal, open to all, what if i don't want? like, as a service, let's say i am the owner of a search service and i don't want anyone from the streaming service to directly talk to me, like these sort of security things i want to handle like. so every, like, every service needs to rewrite all of the basic peripheral things, like all of those things, on its own. so, apart from business logic that that service needs to handle, it would also need to handle uh, uh, tracing, load balancing, scaling, security, retry, circuit breaking and whatnot, right? so that is where what this new sort of new sort of architecture pattern suggests is to have a side car which offloads, which basically takes the burden of your micro service, and all of this common things is is handled over here, right. so this is where solutions like this is where solutions like envoy, solutions like envoy and sto actually comes into the play. so what they do is they run as a side car they have, they. they basically take care of all the peripheral things so that authentication service can simply take care of authentication. streaming service can simply take care of streaming. so logging, distributor tracing and whatnot is all offloaded to sidecar, right? so how the communication happens is that the authentication service wants to talk to streaming service. authentication service sends, basically makes a request to the sidecar running on that instance. this sidecar knows somehow, somehow it knows where should i like, like which side car should i go to that handle streaming part? so this sidecar will talk to the sidecar of the streaming service and this then talks to uh and then it basically uh forwards the request to the the process on the same machine which handles the streaming part of it. now how would they know on? you know where those systems are or where those services reside, or basically what's the ip. so here the idea of a control plane comes in. so this control plane is the one that holds all configurations, all the configurations that you can imagine, like how much traffic should go there, what's the security practices, what's the timeout, what's return, what's not. all of that is handled by this control k, right, and this control plane, like every single service, uh, like for the- uh, basically, nystu and whatnot- everyone comes with its own set of control plan, that that you can leverage out there. so this control plane is the one that holds all that information. and anytime, let's say, you spill up a new instance of, let's say, another instance of streaming service, right? so what would happen is when you add one more instance of streaming service, that instance would have one process for streaming service and one process for a sidecar for that streaming service. so when this machine boots up, it would send the data to the control panel. hey, i am the new machine in the town i handle uh streaming and this is my ip address. so they register themselves to the control plane. so control plane will hold all the information, key virtual machines i have, what all processes are there or what sort of micro service or what sort of business logic is being written on that particular instance, so that it knows if there is comes a request, who should i basically forward it to. so all of this information is sent to the control plane, right. so whenever a new, uh whenever a new service is uh coming up, now when authentication service- let's say authentication service- wants to talk to streaming service for some reason, so what would happen is: uh, how would the sidecar of authentication service know whom to reach out to? so this is where the control plane continuously pushes the updates to the side cars. so control playing card job is that whenever a new- like whenever it gets a new service up on a new service thing, that registers itself. the configuration is stored on the control plane and it is then sent to the to all the side cars out there. so this way, all side cars are always in sync and they say that, hey, i know whom to reach out to whenever is there. so they have a local copy of that configuration and whatnot. okay, so this is how all sidecar get that information. so that's where control plane is a very critical component in uh. whenever we talk about a service mesh architecture: okay. so now coming to spotify, now we know what service mesh is. uh, we have basic understanding of what dns based routing is, where every service has a specific url and all. so now spotify uses what spotify uses: a service mesh for service discovery for some services, while they use dns based routing for other services. so now, let's say, this is some. this, like authentication and streaming, might be some services who would be using a service mesh. well, there could be some sort of legacy services. let's say one of my legacy services, let's say payments on payments. what i have is i don't have this fancy sidecar setup, but i have a simple load balancer whose do whose internal domain is, let's say, paymentsspotifycom. behind that load balancer, i have multiple instances running right and each of them handling that same payments logic. so anyone who wants to talk to payment logic can directly reach out to paymentsspotifycom rather than going through sidecar and other. so this is a very static url that maps to. so from your any service who wants to talk to that, they can directly talk to that by hitting that particular url, right? so there were few services who were on dns based, few services who were based on this uh service mesh. so what spotify does is for this handling of this control plane. so what they use, they use gcp's traffic director. so instead of using something proprietary or something like that. so they they went for, they went for a managed solution out there. they said, hey, let me start using gcp's traffic director in order to manage the control plane that i have, right? so then, what happened? on march 8, 2022? gcp's traffic director had an outage, so the outage was not directly on the spotify side. we as an end user. we saw the outage on the on the spotify site, but actually the outage happened on gcp's traffic director. so what happened when? uh, so what happened when? gcp's direct trafficker had a? so it was not a major outage on it, it was a very minor outage. so they released some configuration which affected users having load balancers talking to this control plane and some hybrid architecture. so it affected those set of users. so gcp is yet to provide an update. so, as of now, gcp is yet to provide an update on that. and, apart from that, a very interesting thing was that there was also a bug in java's grpc client library that sent the error which originated as part of name resolver directly to the channel. so, like typically, when you send an error or, let's say, when your depending service has some error, you and you and you get that error. so you have to wrap this error and and add some context to it. for example, from where does this error generated. what was the error? some specific, uh, some specific error code of your service. so let's say, if you are dependent on a payment service and if basically there is an error which is thrown from the payment service, that's a, basically payment not completed. so you would want to wrap this error and say: keep payment failure as a code, and then you might want to add some message to it which says that, hey, the exception that i'm handling over here and i'm passing forward is generated from the payment service. right, java's grpc library for this error didn't do that. so instead of saying that, so when the name resolver failed, instead of saying it is error from the name resolver and it is unavailable at the moment, it sent a generic error saying not found because error was not explicit. it will go through the source code on what exactly changed there, right so? uh, this is typically a good practice whenever you are writing a proxy sort of layer that, like someone makes a call to you and you make a call to someone else and you and that c get some error, so b needs to handle. uh, we need to wrap and add some context to it so that a knows what might have gone wrong. right. so this is, in general, a good error uh handling practice which java grpc library did not follow at that time. so here, what was the fix that spotify applied? spotify applied something that was suggested by gcp team where gcp team said that try to move your workload out of google traffic director and into something which does not use that. so that's where spotify rolled out a configuration where they used where they, like we had. they had some of their services that were running on a traffic director, some of the services that were using uh, dns based routing. so they started to roll out a configuration which means, basically, they did a redeployment with some configuration chain where they said, hey, instead of using a traffic director or basically, instead of using service mesh, let's immediately like, just to mitigate this issue, let's just switch to dns based routing, because that was working fine, right. so they quickly did it in two, two and a half hours and then the services came back up, right. so let's see what else do this like. let's go through the code on and and see what. what exactly was the outage and because there are a few very interesting nuances here on, okay, uh, so we are till here on march 8: uh, google traffic director experienced in outage. this in coordination with a bargain in a grpc's java client library. so let's see what this google cloud status dashboard says about that outage. so they say that uh elevated http 5xx for some number of customers with load balancers on traffic director managed backends. so people who were using so services, who were using traffic director managed backends, they saw elevated 5x6 due to a recent rollout that happened. so here they say: oh, workaround, huh. so as soon as they identify here, they identified the issue: uh, 8th of march, 1236 pst. the workhorn that they immediately proposed is customers with traffic director managing their invoice should consider moving to back ends that are not traffic director managed. so as a very quick work around google, uh, basically the gcp team suggested to immediately move the traffic from uh google traffic director, uh managed backend to dns to anything that does not use that. they did not specify another. but because spotify had already partial workloads on dns and they knew how to do it, they immediately switched back to a dns based routing right and they still didn't have etf back then. but now their issue is resolved. there is still a detailed support that needs to be given to that uh, which is still pending a very detailed report that that they hopefully should be releasing out soon. okay, so a google traffic director broke, which took down spotify. spotify immediately moved the traffic out of gcp's traffic director and started using dns based routing. okay, and then it also talked about a bug in uh java's uh- grpc library. now see what, uh, what this book says. it's a very interesting thing, uh. so someone pointed out, if a watch in excess client fails, xds name resolver will propagate the error directly to the channel without adding context, uh and without setting proper status. so this is the line of code that had that issue. here you can see that error was something that was generated. here you can see the error was something that was uh generated as part of, as part of a subsequent network call or a subsequent call, and this error was was verbatim, like as is. it was propagated to channel. so what happened? because of this, the, the, the code that invoked this thing, did not know what went wrong, because if you would have known that only only client failed due to some reason, but everything else was working, you might have, you might have handled that error in a different way. like you might just say: hey, this must be a transient issue, let me retry, or this must be an issue which i can ignore, right? so that's where this proper wrapping up error, adding some context to it, is very important. so, as part of a fix in the library, what they said is, instead of just passing the error vibrating, they added the context. now what did they add they? instead of saying just error, they, they created an error object of type unavailable with cause, as in errorgetcost. so they wrapped the entire error and passed it as an unavailable error rather than a default not found error with description: this unable to load lds. xds server returned this and samsung some data right. and here they have. uh added it at other place as well. then here they added the overall uh, their uh in as part of that unit testing thing. they added they basically explicitly handled this error. so this is the code of java's grpc client library. it has nothing to do with spotify, it was something that they uh discovered at all. so this is where the change happened and it was released at version 1.45.0. so two things got highlighted here: gcp had an outage java jrpc client library, so both in sync kind of uh messed up spotify, but- and obviously it was not just a spotify who got affected. there would be a lot of customers at the same time leveraging google cloud, uh, car or traffic director, uh, they all were affected by that. and this happened in usc, swan region. so people, so all the customers using that region were affected. okay, so what did you do? as soon as the problem was discovered, a spotify team rolled out a configuration change to revert our affected systems back to use our dns based service discovery and saw it recover gradually and this is what they gave out as a timeline. some cloud provider broke, affected. this thing, plus a very interesting thing around proper error handling strategies and what could have happened, like if errors were, if errors would have been properly contexted and wrapped, other people, like people who are using that client, could have handled it explicitly like: why not? okay, so, uh, now, obviously all the systems are back and thing, but here we kind of understand, or we kind of understand the importance of we heavily relying on something. uh, let's say here, in this case, spotify was heavily relying on gcp, which is okay, obviously, there, obviously, the job of gcp is to handle it. but as an engineer, uh, we should always, we always be thinking about having a fallback mechanism here we can beautifully see on: uh, where is that? huh? so spotify used multiple service uh service discovery technologies. so just imagine if, if spotify did not have a dns like, if they did not have any services that were using dns based routing, what could have happened? it would have taken them far long to meeting at the issue, or they could have just simply sat back and saying, hey, let the gcp folks fix in, but up until then my services weren't working. this impacts business, right? so from a business perspective, it is always important to ensure that, no matter if your cloud provider fails, you should always like the time for you to mitigate the issue and get the systems up and running should be as low as possible. having fallbacks critically important, right? so this teaches us that. okay, nice, uh, that's all from me for this one. uh, i hope it. i hope i made sense. uh, for this, but uh, uh, for uh. for more such videos, feel free to subscribe to my channel. uh, i'm basically planning to post a lot of videos regularly. so, yeah, uh, that's it from me this time. uh, see you again next day. bye.