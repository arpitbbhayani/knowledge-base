a large chunk of users saw elevated error rates while using github.
so what happens is companies run eb experiment in order to decide which works the best- a or b.
now here, like, for example, if you want to change a color of a button.
let's say you'd want to change the shape of the button on the website.
let's say you have it triangle, now you want to change it to a square- right, it's a.
so what you do is, instead of directly rolling out your changes with respect to a square button to all of your users, what you do is you run an ab experiment.
so what happens is you randomly pick a set of users and you randomly show 50 of them- the default behavior, so no alterations for them- and 50 the new behavior, and then you measure the key vitals of your system.
for example, if you are doing a eb experimentation for this particular button use case, how, how likely are people to click on this button would be a key metric that you would be chasing.
what everyone does, a b experiments, small tweaks in ui, small functionality changes, and they measure how good that, uh, how good that rollout is, and if it is really good, they would allow it to 100 of the people, otherwise they would not right.
a b experimentation for ui introduced an unknown dependency on the presence of a specific dynamically generated file that is served by a separate application.
in any organization, there is a a b service which basically does the measurement of how good a particular variation is right.
it gets all the events that it wants to do that processing- and is the one that decides which user gets what- the default behavior or the new behavior, how aba experiments happen.
at github it looked something like this: we're still not sure, we don't have a lot of idea, but from that two lines what i could deduce is they have something called as a conflict generator whose job is to generate configuration that would power an av experiment.
for example, if i would want to change on the flight, change some parameters, for example, change the color of this button from blue to red, shape from triangle to square, this could be dynamic configuration, which it might be picking from this conflict generator and, because this is a dynamic, information of how the interface should behave.
right, we still don't know what was the eb experiment that was run, so pure speculation.
not right, but in any case, the flow, what happens over here is whenever your servers, typically your front end servers- uh, and this specifically happens at github, might not be true for all organizations- so what they do is they load this dynamic configuration from here, from this conflict generator.
so what happens is this service is responsible for for dynamically generating a configuration file which is sent to all of the front-end servers which they use to then power an av experiment and instrument.
this means that, due to some issue in deployment or something, a deployment was triggered and which obviously this had to generate a lot of files when the request came in.
so anytime the front-end server boot up, it would made a request to config generator, it would have to generate that file and return it over here and then it would start serving the a, b, uh, experiment to the users.
this service was overwhelmed because the file failed to be generated on significant proportion of application deployment due to a high retrieval rate being rate limited by the upstream application.
so what happened is, when a lot of request came in to this config generator, this became overwhelmed.
so what happened is the users who were enrolled in that experiment, they faced error rates because the front-end did not know what to render to which user, right.
so because of this- and this typically happened- because conflict generator rate limited, if it would not have rate limited, obviously you should always rate limit, but if it would not have limited or if had an elevated rate limiting configuration, then it would have sustained, right?
because conflict generator rate limited, the front-end servers did not get the file.
so users were not shown the correct variation, right, and this happened.
this happens a lot, right?
so conflict generator- it limited, it caused that issue.
here we, what we understand is the rate limiting is very essential, but the limits specifically for internal service should be tuned very well, very well, otherwise it would cause outages like this.
so configuration for abn multivariate experiments will be cached internally, right?
this clearly shows that configuration for abn multivariate experiment, the file contained the configuration to run a- b experiment will be cached internally to ensure successful propagation to dependencies.
instead of every time generating a dynamically in, instead of every time dynamically generating the configuration file, they would be generating it once and caching it internally so that the load on the service is reduced.
so with the existing flow that github had, the servers had to call this config generator to generate files and this was all synchronous communication happening.
in a synchronous call it would be generating the configuration files and caching it so that it can just basically percolate the file downstream, right.
it would be pre-generated and it would just be sending the file upon request.
it is very essential even- and this is a good thing- that a rate limiter is employed, even for an internal service.
right, because it is very important, very important to rate limit.
we saw in in previous videos how important rate limiters are, uh, so that our system and things don't go down and no one abuses each other service.
what i feel is github should have- although they would have had this internally, but github should have classified their services into multiple tiers.
and tier 3 services- if it goes down for a little longer, it's okay, right?