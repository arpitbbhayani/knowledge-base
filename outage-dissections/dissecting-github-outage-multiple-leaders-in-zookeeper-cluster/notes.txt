GitHub Outage: Chaos in the Zookeeper Cluster

Dissecting GitHub Outage The second leader problem Zookeeper and Kafka ( TI T2 T' T, F Tu T' -13 T2 T2 -14 g- q 1- 3 T2 Broker 1 Broker 2 Broker 3 Broken 4 1. Controller Election : leader / Follower for all partitions . 2k ensures that if leader goes down . Follower becomes the leader 2. Cluster Membership : Managing which nodes are part of cluster 3. Topic configuration : list of topics # partitions , for each topic . location of replica . leader node location , and many more 4. ACL and Quota : who is allowed to read / write and how much * Newer version of Kafka Zookeeper is an extremely important does not rely in Zookeeper component for Kafka . It is the Brain that holds the most important info

Version upgrade . Loo Keeper routine maintenance Patch 0s patch . Security , Nodes of Loo Keeper cluster needs to be upgraded hence new nodes are added and then old ones are removed " " Neue nodes too to zookeper cluster were added quickly " " when nodes added it new were quickly , resulted in another leader Election autonomous When node is added to the Zookeeper is near - . a new lot of added , cluster it tries to self - discover . If a new nodes are ' ' leader and thought they they could not discover leaderless are and hence triggered a leader Electron

Split Brain Broker Broker Broker in the cluster A Single Kafka connected to the newly formed Zookeeper Controller * when clients are cluster and elected itself as connecting to Zookeeper 1 controller for a topic ] for Kafka Details , they are getting Conflicting Information This led to failures of writes , until the clients discover , some ceerihs would have happened through new node controller .

Recovery Zookeeper auto - detects this inconsistency over - time and auto heals - we can also manually take actions to fix it . ↓ killing the second logical cluster ? what affected The Kafka So , did they lose any Cluster where this happened . jobs ? No ! handled internal background jobs Fallback Queues A standard architecture requires you to have Dead letter Queues . Idea : if unable to write message in the main queue , we put the same message in DLQ Which are then later processed Kafka client > 0 O O O ' DLQ

For write a high ingestion . Kafka DLR would overload 0 0 . client > But with retries and one 0 0 ' time it could be processed processing , DLQ quickly ↑ Secondary Job System Important learning I. Having a DLQ is a must 2. Consumers should be idempotent 3. Clients and consumers should have retries 4. Automate cluster provision with jitter