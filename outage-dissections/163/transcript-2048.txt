so outrageous are inevitable, but we should design our architecture such that if a component is down, it should not lead to a complete outage.
a partial outage affecting few services is fine, but a complete outage is catastrophic.
in this video, we dissect yet another github outage, gather few interesting insights about that microservices architecture, spend some time discussing and understanding the importance of having a smaller blast radius and talk about a foolproof way of achieving that.
but before we move forward, i'd like to talk to you about a course on system design that i have been running for over a year now.
engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say.
we cover topics ranging from real-time text communication for slack to designing our own toy load balancer, to quick buzzes, live text commentary to doing impressions counting at scale for any advertisement business.
the first one is the live cohort discourse which you see on the left side, and the second one is the recorded course which you can see on the right side.
the live code base course happens every two months and it will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is.
so, like always, we start with the incident report and sure it goes so what it says.
by the way, most of the outages that i've seen of github are related to action service.
but, yeah, some abnormal level of errors affecting action service.
this incident resulted in the failure or delay of some cute jobs for a period of time.
so, as you all know, action service of github is primarily powering the github actions in which, like whenever a commit happens, you have to trigger a workflow and all of that happens.
and this is a very important point around designing an architecture.
so that is where, whenever we are designing- uh, most of the architecture, think about: can we do it asynchronous way?
we identified that the issue was caused by an infrastructure error in our sql database layer- again, something to do with database, the most brittle component in every single architecture out there.
okay, the database failure impacted one of the core microservices that facilitates authentication and communication between action microservices.
so what it says is something happened on the authentic on one micro services, or, sorry, on one micro service part of its actions microservice that handled authentication and communication.
one of them that we saw in the previous outage was around github actions premium service runner, something around that, right.
so there is not one micro service powering github actions.
there are multiple components that power github actions all together and they all share a database.
either they share a database because- see, the outage happened because there was an infrastructure error in the sql database layer, right, which means that either all of them, all of the actions microservices- shared that common database and were directly dependent on it or indirectly dependent on it through synchronous way.
so somehow every single one of this was linked to this one database and if that was affected, it would affect every single service out there.
all the people think that, hey, we should not have shared database of, or, or sorry, or we should.
we should have a database for every micro service.
so sharing a database looks like an empty pattern, and it is indeed, but a lot of people adopt it.
so this is a very common way to do it and looks like github, if not all services had was sharing a database, but at least the synchronous dependency.
the second key point that we talked about: that the service that got affected handled authentication right.
why you think, uh, services or micro services would need authentication, like they're all part of your infrastructure.
most companies, when they design architecture, it's always zero trust architecture, which means even if two services talk to each other, they would not trust.
like: imagine you having a notification service.
so that is where, in most cases, the way microservices are designed, the design with zero trust, right.
they ensure that even when a service wants to communicate to any other service, that needs to be authenticated and authorized to do that.
and the service that got affected in this particular outage was the one that handled communication.
it's all speculation, but you get the gist right: it handle authentication and communication.
we know that the outage happened on the database layer, but we also know that, let's say, for database master is down, a replica is auto promoted, replica is promoted to be the master.
so then this really should not have been an outage, because in most cases, automatic failovers are configured right.
in normal circumstances, automated processes would detect that the database was unhealthy and failover with minimal or no customer impact, because this is a very regular exercise where master goes down and a replica is promoted to be a master.
hardly few rights fail in the span of half a second or a second, but nothing major happens.
okay, but before we jump into why it did not, let's spend some time on how it happens.
so database failover is very common to happen.
it is not some magical thing that happens that automatically replica will be promoted.
so there has to be someone who is monitoring if my database becomes unhealthy so that he can promote other node to become the master.
in this case, the failure pattern was not recognized by the automated processes and telemetry did not show any issues with the database, resulting in a longer time to determine the root cause and complete mitigation efforts.
so there is a classic case where your database is down, or but your butter telemetry is not showing it, and this is very common, very, no, not very common, but this is common.
so here the orchestrator was not able to identify that the master was stopped, so failover did not happen.
because failover did not happen and because the telemetry also did not show that the database was having an impact- and very common this kind of thing- because it could not show something's wrong with into the database.
how would engineer even come to know gary databases down right?
because the first thing when an outage happens, people look at telemetry.
the database was done but the telemetry was not showing it right.
they did not specified in the necessary report, but there would be two possible ways to do it right.
promote the replica to be the new master and reconfigure your database very easy.
reboot rights would fail, but in any case it is failing because there is an outage, so that's fine.
but in case you would want to dive deeper into other ways of doing it, i've discussed a few more approaches in other live section videos.
he uh, the telemetry did not show any issue and the database resulted in the longer time time uh on determine the root cause.
okay, to help avoid this class of failures in the future, we are updating the automation process in a sql database layer to improve error detection and failover.
everyone says this and this is something which becomes a step zero after the, after the issue is mitigated, to ensure that you do not see the outage because of the same reason again, and that is a very important thing to do anytime when you face an outage, right?
so, as part of the long-term strategy to ensure that the artist doesn't happen for the same reason, they would need to do is, uh, understand this failure better, right?
first of all, they have to detect the failure, because this time, database was down but they were not able to figure it out.
second, they need better automated failover where they, when they proactively know that database is down very quickly, they should be able to failover.
so, going through or re-auditing the scripts, re-auditing the authentic uh, the automation is very important to ensure that if similar thing happens in the future, it is not, it does not take a relatively longer time to debug it.
furthermore, we are continuing to invest in localizing failures to minimize the scope of impact resulting from infrastructure errors.
so here we saw how impact on one database cause entire github's actions to be affected.
right, because that critical database handled uh, authentication communication so synchronous.
right, and this is what tells us importance of localizing features, localizing failures.
so failures are bound to happen, but we all should be thinking about how can we ensure that our failures are localized.
when i say localized implies that if a particular micro service is experiencing a failure- or, let's say, database experienced a failure, only that micro service should be affected.
there should be a default response code that you are throwing out or just absorbing the failures or something around that, depending on the use case that you have, you would.
you should be designing your architecture such that whenever something goes down, it just affects the bare minimum infrastructure components or micro service components whatsoever.
it's really to be honest, it's easy to see, but it's very use case specific on what your, on what you are building, how you are building, what your organization's engineering majority is, and then you would be able to employ the corresponding architectural pattern so that your failures are localized.
so when you have an outage and if something breaks, the things that are affected by it is the blaster it's like when you, when a bomb blast, what all things are affected.
similar to that whenever the outages happen.
let's say, database went down, what is the blast radius?
so when you keep the blast radius at the bare minimum, so not a lot of services get affected, so you would see an outage, but it would be a very partial outage where only one service is getting affected.
a very good way to do this is to have a synchronous dependency.
a very typical way to look at this is: blast radius is larger when the dependence is synchronous and it is smaller when the dependencies are synchronous.
so try to model the communication between your services asynchronously, as in use: message brokers, message streams like sqs, kafka, rabbit, mq- to do the communication between microservices.
right things that can be done in an asynchronous way.
try to model it to do it in an asynchronous way, keep them loosely coupled right, and it totally depends on your use case.
it is very dependent on your use case, how you are modeling it, but this should be your, your thing, that you do on day zero, right?
can i broker, basically, can i break it down into multiple smaller services such that if one of them goes down, i would have a smaller blast areas?
but in most cases, if you think about this one point, can i do this communication in asynchronous way would automatically make it loosely coupled right.
so think about this whenever modeling any architecture modeling and inter-service communication.
if you start with this, you would automatically start to design loosely coupled micro services which inherently would have a very small blast radius, and when it has a smaller blast radius, you would always see partial outages, not mega outages like this.
this is what i'd want to convey: the importance of having loose coupling and to ensure that you have blast radio or basically your failures are localized, they don't propagate across the system.
right, very, very, very important when you design microservices based architecture.