so outrageous are inevitable, but we should design our architecture such that if a component is down, it should not lead to a complete outage. a partial outage affecting few services is fine, but a complete outage is catastrophic. it is easy to say this, but really hard to implement. in this video, we dissect yet another github outage, gather few interesting insights about that microservices architecture, spend some time discussing and understanding the importance of having a smaller blast radius and talk about a foolproof way of achieving that. but before we move forward, i'd like to talk to you about a course on system design that i have been running for over a year now. the course is a cohort based course, which means i won't be rambling a solution and it will not be a monologue. instead, a small, focused group of 50- 60 engineers every cohort will be brainstorming systems and designing it together. this way, we build a solid system and learn from each other's experiences. the course to date is enrolled by 600 plus engineers, spanning nine cohorts and ten countries. engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say. the coolest part about the course is the depth we go into and the breadth we cover. we cover topics ranging from real-time text communication for slack to designing our own toy load balancer, to quick buzzes, live text commentary to doing impressions counting at scale for any advertisement business. in all we would cover roughly 28 questions and the detailed curriculum, uh split, week by week, can be found on the course page, which is linked in the description down below. so if you're looking to learn system design from the first principles, you will love this course. i have two offerings for you. the first one is the live cohort discourse which you see on the left side, and the second one is the recorded course which you can see on the right side. the live code base course happens every two months and it will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to binge learn system design, i would highly recommend you going for the recorded one. otherwise, the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page at pitbi dot me slash master class and i would highly recommend you to check that out. i put the link of the course in the description down below. so if you are interested to learn system design, go for it. check out the link in the description down below and i hope to see you in my next. thanks. so, like always, we start with the incident report and sure it goes so what it says. on january 28, uh, 4 am, utc- our services are our service monitors- detected abnormal levels of errors affecting action service. by the way, most of the outages that i've seen of github are related to action service. my best case is because it is very high throughput service. it is very proven for outages. a lot of them have been around action service. but, yeah, some abnormal level of errors affecting action service. this incident resulted in the failure or delay of some cute jobs for a period of time. so, as you all know, action service of github is primarily powering the github actions in which, like whenever a commit happens, you have to trigger a workflow and all of that happens. so a lot of jobs are asynchronous. so because of this incident, there was a failure or delay of some cute job for a period of time. that's fine. jobs that were cured during the incident were ran successfully after the issue was resolved. this is a very important point to note. so here, even though their service was down when it got back up, everything, like the job that were queued, they ran like they were put to a completion. and this is a very important point around designing an architecture. so if this was not asynchronous? so here we can clearly see that this was an asynchronous workflow where most of the things were done asynchronously. so your queue was acting as a buffer, right, or a message broker was acting as a buffer. and if that was not there, then if this was modeled in a synchronous way, the right field is right filled and the data is lost right. so that is where, whenever we are designing- uh, most of the architecture, think about: can we do it asynchronous way? because that would give you fault tolerance out of the box. okay, so what happened? we identified that the issue was caused by an infrastructure error in our sql database layer- again, something to do with database, the most brittle component in every single architecture out there. but that's why. okay, the database failure impacted one of the core microservices that facilitates authentication and communication between action microservices. so this gives us a very nice sneak peek of their architecture. so what it says is something happened on the authentic on one micro services, or, sorry, on one micro service part of its actions microservice that handled authentication and communication. so this tells us to very interestingly. the first interesting thing that it tells us is that github actions- although most of the folks would model it as a singleton microservice that powers github actions, but internally github actions itself consists of multiple microservices. one of them that we saw in the previous outage was around github actions premium service runner, something around that, right. so there is not one micro service powering github actions. there are multiple components that power github actions all together and they all share a database. either they share a database because- see, the outage happened because there was an infrastructure error in the sql database layer, right, which means that either all of them, all of the actions microservices- shared that common database and were directly dependent on it or indirectly dependent on it through synchronous way. so somehow every single one of this was linked to this one database and if that was affected, it would affect every single service out there. and shared database is a speculation. they have not specified it, but it's a very common practice. all the people think that, hey, we should not have shared database of, or, or sorry, or we should. we should have a database for every micro service. but it is infeasible with respect to cost. if you have, if you do not have, shared database, a lot of network latency will shoot up and you would incur a lot of cost. in most cases, that is not needed. so sharing a database looks like an empty pattern, and it is indeed, but a lot of people adopt it. right and there is nothing wrong with it, so long as if outage in one does not let your entire infrastructure go down, right. so this is a very common way to do it and looks like github, if not all services had was sharing a database, but at least the synchronous dependency. so, either directly dependent on the database or indirectly through a synchronous dependency, were dependent on the on this database. the second key point that we talked about: that the service that got affected handled authentication right. and why? why you think, uh, services or micro services would need authentication, like they're all part of your infrastructure. why would they need to have authentication in place? so this authentication is not for end user. this is microservice to microservices based authentication like: why so? most companies, when they design architecture, it's always zero trust architecture, which means even if two services talk to each other, they would not trust. like: imagine you having a notification service. it is not having any, any authentication of its own. imagine some random service gone rogue or an engineer gone rogue in a different team then started, uh, invoking apis of this notification service, which started sending messages to your end users. very poor experience, right? so that is where, in most cases, the way microservices are designed, the design with zero trust, right. with zero trust. they ensure that even when a service wants to communicate to any other service, that needs to be authenticated and authorized to do that. and the service that got affected in this particular outage was the one that handled communication. so communication is typically shared communication and authentication. obviously they would not reveal the details. it's all speculation, but you get the gist right: it handle authentication and communication. okay. so what next? we know that the outage happened on the database layer, but we also know that, let's say, for database master is down, a replica is auto promoted, replica is promoted to be the master. so then this really should not have been an outage, because in most cases, automatic failovers are configured right. so that's exactly what github said. in normal circumstances, automated processes would detect that the database was unhealthy and failover with minimal or no customer impact, because this is a very regular exercise where master goes down and a replica is promoted to be a master. hardly few rights fail in the span of half a second or a second, but nothing major happens. it's a very standard operating procedure. but this was a normal circumstance where this should have happened but it did not. okay, but before we jump into why it did not, let's spend some time on how it happens. so database failover is very common to happen. so you have a master node, you have a replica, master goes down and replica becomes a master. but how? it is not some magical thing that happens that automatically replica will be promoted. so there is typically an orchestrator sitting who is monitoring when the master is down. so there has to be someone who is monitoring if my database becomes unhealthy so that he can promote other node to become the master. who is doing that? that is orchestra. but how would orchestrator do it? it would rely on some metrics. it would rely on some telemetry. it would rely on some data that would tell it. hey, master is down. let me promote a replica to be the new master. someone has to do it. it is not magical. it would automatically happen, but, as we know, this did not happen, so this did not happen what it says. in this case, the failure pattern was not recognized by the automated processes and telemetry did not show any issues with the database, resulting in a longer time to determine the root cause and complete mitigation efforts. so there is a classic case where your database is down, or but your butter telemetry is not showing it, and this is very common, very, no, not very common, but this is common. this is indeed common, where the automation that you may have written might not be capable of detecting this failure, because it might be trained on some particular patterns, but not on this pattern, and it might not. it might be very possible that the entire database is not down, only a fragment of it is done, or some particular kind of tables are locked, or not everything, uh, or it is not a very obvious outage, outage on the database, right? so this is very common, right? so here the orchestrator was not able to identify that the master was stopped, so failover did not happen. because failover did not happen and because the telemetry also did not show that the database was having an impact- and very common this kind of thing- because it could not show something's wrong with into the database. how would engineer even come to know gary databases down right? so it required them longer time to determine the root cause, because up until the time they would reach to the database- hey, is there a problem with the database or not. a lot of steps needs to be validated or to ensure that, hey, downtime is not because of this. because the first thing when an outage happens, people look at telemetry. when they look at elementary, they eric the database is doing fine, let me debug at other place. but that did not happen. the database was done but the telemetry was not showing it right. okay, so this happened. so what did they do? it took time for them to determine the root cause. so what's the next step? they did not specified in the necessary report, but there would be two possible ways to do it right. uh, but but uh, there are one very elaborated way also, but two very common way to do it. first of all, do a manual failover- you know that the master is down- manually. promote the replica to be the new master and reconfigure your database very easy. hardly takes 15- 20 seconds to do it both. on most cases that is just a script that you need to run and would do the failover case number one. case number two is reboot the machine. so if you know that the master is down due to transient failure, reboot the machine during that. reboot rights would fail, but in any case it is failing because there is an outage, so that's fine. after few minutes everything would be back up and normal. right. but in case you would want to dive deeper into other ways of doing it, i've discussed a few more approaches in other live section videos. i would highly recommend you to check that out right. so that's what github also talked about. he uh, the telemetry did not show any issue and the database resulted in the longer time time uh on determine the root cause. it took longer time for them to do so. okay, to help avoid this class of failures in the future, we are updating the automation process in a sql database layer to improve error detection and failover. everyone says this and this is something which becomes a step zero after the, after the issue is mitigated, to ensure that you do not see the outage because of the same reason again, and that is a very important thing to do anytime when you face an outage, right? so that would be the first thing. so, as part of the long-term strategy to ensure that the artist doesn't happen for the same reason, they would need to do is, uh, understand this failure better, right? first of all, they have to detect the failure, because this time, database was down but they were not able to figure it out. so they need better failure detection. second, they need better automated failover where they, when they proactively know that database is down very quickly, they should be able to failover. so, going through or re-auditing the scripts, re-auditing the authentic uh, the automation is very important to ensure that if similar thing happens in the future, it is not, it does not take a relatively longer time to debug it. it should be pretty quick, okay. furthermore, we are content. this is a very important point. furthermore, we are continuing to invest in localizing failures to minimize the scope of impact resulting from infrastructure errors. this is an important point. so here we saw how impact on one database cause entire github's actions to be affected. right, because that critical database handled uh, authentication communication so synchronous. uh, so synchronous dependency, director, indirect or everything got affected. right, and this is what tells us importance of localizing features, localizing failures. so failures are bound to happen, but we all should be thinking about how can we ensure that our failures are localized. when i say localized implies that if a particular micro service is experiencing a failure- or, let's say, database experienced a failure, only that micro service should be affected. everything else should not go down. there should be default values to handle. there should be a default response code that you are throwing out or just absorbing the failures or something around that, depending on the use case that you have, you would. you should be designing your architecture such that whenever something goes down, it just affects the bare minimum infrastructure components or micro service components whatsoever. and this is very important. it's really to be honest, it's easy to see, but it's very use case specific on what your, on what you are building, how you are building, what your organization's engineering majority is, and then you would be able to employ the corresponding architectural pattern so that your failures are localized. but this is basically called as blast radius. so when you have an outage and if something breaks, the things that are affected by it is the blaster it's like when you, when a bomb blast, what all things are affected. so how big is the blast radius, right it is, is it five kilometers, ten kilometers, fifteen kilometers, right? so? similar to that whenever the outages happen. let's say, database went down, what is the blast radius? so when a component goes down, how many component it takes with it. so this is what we have: to keep it at bare minimum, right? so when you keep the blast radius at the bare minimum, so not a lot of services get affected, so you would see an outage, but it would be a very partial outage where only one service is getting affected. but how do you do this? a very good way to do this is to have a synchronous dependency. so blast area is typical. a very typical way to look at this is: blast radius is larger when the dependence is synchronous and it is smaller when the dependencies are synchronous. so try to model the communication between your services asynchronously, as in use: message brokers, message streams like sqs, kafka, rabbit, mq- to do the communication between microservices. do it synchronously only where it is very, extremely essential. right things that can be done in an asynchronous way. try to model it to do it in an asynchronous way, keep them loosely coupled right, and it totally depends on your use case. there is no golden way or there is no silver bullet to this. it is very dependent on your use case, how you are modeling it, but this should be your, your thing, that you do on day zero, right? if you are going for a micro services arc, uh, always think: are these services loosely coupled? am i, am i burdening a single service with a lot of synchronous requests or am i giving a lot of responsibility to this one service? can i broker, basically, can i break it down into multiple smaller services such that if one of them goes down, i would have a smaller blast areas? and also, what if? when you are communicating to any service, think about what if that service is down? how are you handling, how are you gracefully handling the failure on this service? right? very important to think about whenever you are doing it. but in most cases, if you think about this one point, can i do this communication in asynchronous way would automatically make it loosely coupled right. so think about this whenever modeling any architecture modeling and inter-service communication. think about: can i glue them with a message broker or a message stream? that's it. if you start with this. this is a very good starting point. if you start with this, you would automatically start to design loosely coupled micro services which inherently would have a very small blast radius, and when it has a smaller blast radius, you would always see partial outages, not mega outages like this. right, and yeah, that's it. that's it for this video. this is what i'd want to convey: the importance of having loose coupling and to ensure that you have blast radio or basically your failures are localized, they don't propagate across the system. right, very, very, very important when you design microservices based architecture. i'm really glad that github uh wrote about it. get abroad about that. they are so open about their outages because it helps us learn. it helps us learn on importance of these small practices that most people just just, they just basically take it for granted unnecessarily. don't do that. it's really important. okay, okay, amazing. that's it. that's it for this video. if you guys like this video, give this video a thumbs up. if folks like this channel, give this channel a sub. i post three in-depth engineering videos every week and i'll see in the next one. thanks again. [Music].