so imagine you're trying to create a new github repository and the api call is failing- failing for 53 minutes. this happened with github in april 2021 when, for 53 minutes, people were unable to create new repositories. upon investigation, they found out that the root cause was scanning secrets. two seemingly different use cases took down one of the most important apis for github. this has to be one of the most amusing outages that have seen in recent times. so in this video, we dissect this outage, understand the root cause of it, look at the importance of secret scanning and conclude with building an understanding on how github mitigated this issue. but before we move forward, i'd like to talk to you about a course on system design that i have been running for over a year now. the course is a cohort based course, which means i won't be rambling a solution and it will not be a monologue. instead, a small, focused group of 50- 60 engineers every cohort will be brainstorming systems and designing it together. this way, we build a solid system and learn from each other's experiences. the course to date is enrolled by 600 plus engineers spanning 9 cohorts and 10 countries, engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say. the coolest part about the course is the depth we go into and the breath we cover. we cover topics raging from real-time text communication for slack to designing our own toy load balancer, to creek buses, live text commentary to doing impressions counting at scale for any advertisement business. in all we would cover roughly 28 questions and the detailed curriculum- uh split week by week, can be found on the course page, which is linked in the description down below. so if you're looking to learn system design from the first principles, you will love this course. i have two offerings for you. the first one is the live cohort discourse, which you see on the left side, and the second one is the recorded course, which you can see on the right side. the live cover based course happens every two months and it will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to binge learn system design, i would highly recommend you going for the recorded one. otherwise, the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings. the decision is totally up to you. the course details, prerequisites, testimonials can be found on the course page at binary dot me slash master class and i would highly recommend you to check that out. i put the link of the course in the description down below. so if you are interested to learn system design, go for it. check out the link in the description down below and i hope to see you in my next cohort. thanks, so this is the incident report for this particular outage, where the folks from github says that our service monitors detected an elevated error rate when using api request, which resulted in degraded state of availability for repository creation, clearly states that repository creation flow was broken upon. upon further investigation, we identified that the issue was caused by a bug from a recent data migration in a data migration to isolate secret scanning tables. now let's take a look at what exactly is secret scanning. so what is secret scanning? we as developers, or whenever we write any micro service, we connect to a lot of components. for example, a database, a cache or elasticsearch cluster may be some sas offering maybe aws to to do something with their infra. so in order to communicate with this system, we have to have authentication with it. for example, with aws we need api, we need uh, api access scan secret key. with respect to database, we need username, password and db endpoint. for sas we might need uh a token, or for inter-service communication we might need a token for authentication. so all of these things needs to be part of your code- may not be our code, but at least part of your service, using which they would be successfully be able to communicate with that other system. now, the most basic way of putting this information would be in your settings file order constraints file, right, so where you might have a key called api access key and where you are actually specifying are actually your, your actual api access key. so what happens with that is, if you are checking in this file with your code, this secret stays permanently in the github repository. you will say, hey, but my github repository is private. what would happen? what if github has a leak? what if github itself has a data breach? right, then your private repositories are out in the public, right, accidentally made public because of the database that happened on github. so then your access key, secret key, every single secret, is out in the market. so someone having access to your aws. api gains secret key can do so much with your infrastructure, right? so, which is where you have to be very cautious and not check in any secret in the repository, no matter what, and this often often leads to data breaches. for example, just one api key and one access screen security for aws being leaked can lead to an attacker getting into infrastructure, accessing the database and getting the data out. user pi information is out there in public and which is a big, big, big security threat, and they typically do this to get some ransom. so you'd lose money, you'd lose reputation, you'd lose your brand and what not. so data breaches are very like. they affect you more than anything else. or, second is they can do infrastructure abuse. for example, let's say, someone getting an access to an infrared may not have access to a database because you have made your database pretty secure, but they will create their own set of easy to instances and start mining bitcoins. who says they can't? with the? when they have access to your infrastructure, they can do whatever they like. you would not even know, and an attacker getting into your infrastructure can start mining bitcoins, can start using it for maybe some terrorism purpose. you are under scrutiny, then right, so it happens, it happens. so data breaches, infrastructure abuse- very common, very common exploitations that you would see, and the root cause of most of this thing is because of secrets being pushed into the code. somehow that repository made public, maybe by github, maybe by you, someone randomly doing that, or even if it is part of your history- then someone having access to a code base can very easily extract, because it's plain text in which you have written. so extraction itself is very easy. so that is where what github does is: github notifies us about it, it does periodic checks on the repositories and tells us if we have committed any secrets or not. the way it does it, with simple entropy matching algorithm. it's very simple to implement, but it does it periodically on every single repository that is there on github and just notifies the owner of the repository that hey, you might have committed a secret, right? so here what github would have had is they would have had a secret scanner service whose job would be to basically scan all the secrets, like periodically go through the repositories and scan the secrets that are there and in turn notify the user about it. so obviously, a secret, uh, a secret scatter service would need to have a database, and this database would be the one in which they would be putting that information, that these are the repositories within these repositories, these are the secrets that it found so, and this information would be used by some api to tell to the user that, hey, these are the potential secrets that you might have checked it into your source code, right? so what next? in data migration, to isolate our secret scanning tables into their own cluster, a bug was discovered that broke the ability of application to successfully write to secret scanning database. what is this? let's talk about it. so now let's talk about the repository creation flow. when a new repository is created, someone has to tell that this is the new repository which is created and start scanning it. the way it looks like from the outage document, the way it looks like, the way github is doing it, is that whenever a repository is created, the normal repository creation flow- where a database entry in the main repository service or something would have been made apart from that, it makes an entry into your secret scanner db, stating that, hey, this is the new repository which you have to start scanning, right, and a secret scanning service would read its database, go through that information and then start scanning secrets in that particular repository. so what exactly changed here? your outage happens because something changed. so what change and what triggered this outage? so they said in the report that secret scanning table was moved into its own cluster. so it says that the data migration to isolate secret scarring tables into their own clusters and some bug was discovered. so we'll come to that in a minute. so secret scanning table was moved to its own cluster, which means that there would be initially a common database in which they would have had all the tables. one of those tables would be for secret schedule and during the repository creation, right from marking or right from creating the repository, making an entry in the database for repository, they would have had a table in which they would be making an entry. for now this repository needs to be scanned periodically. so then let's say i have four tables. just an example, just a visual example of that. let's say i have four tables in my database node. their migration would have kept the three tables as this and move the fourth table which belongs, which is the, which is the table that holds, which all database needs to be scanned- what's the configuration about it and what not that table into its own cluster. so moving this table from their old database and separating and out- this way, this they would have typically done to handle large scale. right, so giving its own uh cluster to work with so that it doesn't interfere with existing database or existing tables with respect to uh, with respect to cpu consumption or memory consumption, right, so moving this table into a separate database of its own. so then what? what exactly happened? they said that in the report it said that they discovered a bug that broke the ability of application to successfully write to secret scanning database. this is super interesting. so it says that bug broke the ability of the application to successfully write to a secret scanning database. why is this interesting? we would typically think that in a pure micro services based architecture, you would have a repository service, you would have a secret scanning service. secret scanning service would have its own database. but obviously, organizations- number one- organizations- does not have microservices architecture from day zero. they all start with a monolith and then they migrate to microservices, which means that there would come a transition phase, or rather, there would come a phase where everything exist in this one database and you try to fork it out right. second is you would think that, hey, when i have services, it would be an api call and that would take care of writing it to the database. but here what we see is it says that application was unable to write successfully in this database, which means your repository service itself directly connects to the database and writes an entry to that earlier. all the tables are part of what database? so it was easy for the repository service to write it into the, to make an entry into the database, but this failed after the data migration, which means when this secret scanning table was taken out of this database and put into a separate database, this started failing because your application, which means, let's say, a repository service, was not able to write it to this database. now there could be hundreds of reasons for this to not happen. right, they have not specified why it did not happen, but few possible reasons could be: there might be a network configuration which does not whitelist a particular database to be connected from a particular application. it could be your database string would be would be weird. or you might have some random network security policies that might have been imposed, or some genuine issue with your database because of which your migration failed, but you didn't know about it. there could be a ton of reasons for this to not happen and they have not specified it anywhere in the report side. these are my guesses on what would have happened. right, and one very important point is what they say is repository creation had, like the incident revealed, an unknown dependency that repository creation had upon secret scanning, which makes a call for every repository created, which means upon repository creation. every time a new repository is created, an entry is made into this table. and it was an unknown dependency. imagine, at the scale of github, they having blind spots, they having blind spots about hey, who is doing what, and that happens. that happens in every organization, or that there are blind spots. the degree of blind spots matter: in some cases it is very small, in some cases it's huge, but blind spots are there. this was that one blind spot that github didn't know existed. everything was working fine, but they didn't know during the data migration that a, this, y to be something that would be affected. so repository service directly writing on to the database was an unknown dependency. and this happens to the best of us. no, like part and parcel of your engineering career. uh, you'll somewhere, like even after working in organization for three years, five years, seven years, we'll discuss something. hey, was this? these two systems were actually connected. we didn't even know, and it and it was working just fine. so that happens. so, because of which your repository creation was failing? because now your repository service, where it was trying to create an entry into the database, was not working due to some bug- they have not specified what bug, network issue, something, something, something would have happened, but it was not able to write it- because of which your repository creation was failing. what does this say? this says that the call was synchronous. if it was asynchronous then repository connection would not have failed. but because it failed, the call was singles, which means user gave request to api, to repositories, to create a repository, repository service. in the request call itself was making an entry into the database. so when this failed, your entire api request or entire response failed, which resulted into a 5xx and repository not getting created, right. so this shows a synchronous dependency on something that should have been asynchronous, took down, uh, the website, or basically took down this particular feature, not the entire, but this particular feature. so how did they mitigate? due to this dependency, reposition was blocked. until we roll back the migration. so in most cases you would see that organization, instead of rolling out a fix. the first intuition that they get is: let's quickly roll out. oh, let's quickly roll back, because you know that the rollback would definitely work. if the rollback is simpler and could happen quickly, you can minimize the time for the outage. imagine, for one hour, people not able to create their github repository. it's a big, big, big deal. so that's where, uh, rolling back your data migration is something that most people would have done immediately. and and they have amended our mitigation process to include revised steps for remediation, which means automatic remediation in case something like this happens. in case similar happens have been open, our application code has been updated to remove the dependency on secret scanning for creation of repositories. this shows that they would have either changed it from synchronous to asynchronous, which means that their application code has been updated to remove the dependency on secret scanning for the creation of repositories, which means they would have done it asynchronously. so, upon repository creation, they might have sent an event into kafka or sq or some some message broker and would have consumed it asynchronously. hey, now we can enter into this database rather than doing it synchronously in that request call so that the repository creation happens and as part of its reactive strategy, it would go and create that entry into your secret scanning database. so how did they mitigate it? they- in in the incident report we saw this- simply mentioned that they roll back the data migration. so this is a pure speculation. that's few things that you need to know about that or you could have done over here. so as soon as the data would have been migrated, the creation, the api, uh, the repository creation, would have failed and they quickly moved back. moved back as in they migrated the table and they migrated it back into the old tv could be one, but because they are like it seems very weird that you have to re-migrate the table to the old thing, it they might just have changed the network configuration as well, because the old table would have been there in the old database, because they would not have immediately deleted it. they would have had it in the old database and they would have just changed the configuration to start pointing it to the old database, uh, immediately, so that it starts to work. pure speculation, but if there would have been more information about it, we would have dive deeper into how they would have mitigated this plan. but in general, whenever an outage happens, the first intuition of an organization is to see if it could be rolled back. if it is easy to roll back, every organization would, would prefer doing a roll back immediately when something goes down because it's the most, uh, most trustworthy solution out there and otherwise they would have, uh, tried to push a fix, maybe a network change, maybe a configuration change and whatnot. and it purely is the context in which the outage happened. right, obviously, pure speculation. i would not want to comment a lot because we don't know what their internal architecture or what exactly their mitigation plan was, but your speculation out of it. this is what you could have or you should be doing when you see an outage in your organization. right, think of rollback first and then fix later. if it's rollback worthy, do that. roll back immediately. otherwise, push up, fix, right, nice this. why is this? why this outage was very amazing? because repository creation, secret scanning- who would have thought these two components talk to each other? second application: directly writing it into the database. you would see in microsoft architecture. i'll make a call to this service and that service would write to its own database. here. there was no micro service out there, it was just that same code. writing it into two separate places, one of them, failing, took down the entire repository creation. so moving from synchronous to asynchronous, and one key rule to uh always remember is things that should not be done synchronously, should never be done in single two, should never be done synchronously. make it asynchronous wherever possible. make your architecture as reactive as possible. this would help you keep your things robust, have a smaller blast radius, but if something goes down, if basically something goes wrong, only that part is affected and not everything else. so, yeah, that's it. that's it for this outage. i love this outage. uh, glad, glad, uh glad. github. uh wrote about it in an incident report. no matter how small the reveal was, but it was still worthy to know how different components react, and you won't even understand or you won't even know about an unknown dependency that exists in your infrared. so, yeah, that's part and parcel. nice, that's it. that's it for this video. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel a sub. i post three in-depth engineering videos every week and i'll see in the next one. thanks a ton.