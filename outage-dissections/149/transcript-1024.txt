thanks, so this is the incident report for this particular outage, where the folks from github says that our service monitors detected an elevated error rate when using api request, which resulted in degraded state of availability for repository creation, clearly states that repository creation flow was broken upon.
so what happens with that is, if you are checking in this file with your code, this secret stays permanently in the github repository.
right, then your private repositories are out in the public, right, accidentally made public because of the database that happened on github.
for example, just one api key and one access screen security for aws being leaked can lead to an attacker getting into infrastructure, accessing the database and getting the data out.
it's very simple to implement, but it does it periodically on every single repository that is there on github and just notifies the owner of the repository that hey, you might have committed a secret, right?
so here what github would have had is they would have had a secret scanner service whose job would be to basically scan all the secrets, like periodically go through the repositories and scan the secrets that are there and in turn notify the user about it.
so obviously, a secret, uh, a secret scatter service would need to have a database, and this database would be the one in which they would be putting that information, that these are the repositories within these repositories, these are the secrets that it found so, and this information would be used by some api to tell to the user that, hey, these are the potential secrets that you might have checked it into your source code, right?
in data migration, to isolate our secret scanning tables into their own cluster, a bug was discovered that broke the ability of application to successfully write to secret scanning database.
the way it looks like from the outage document, the way it looks like, the way github is doing it, is that whenever a repository is created, the normal repository creation flow- where a database entry in the main repository service or something would have been made apart from that, it makes an entry into your secret scanner db, stating that, hey, this is the new repository which you have to start scanning, right, and a secret scanning service would read its database, go through that information and then start scanning secrets in that particular repository.
so it says that the data migration to isolate secret scarring tables into their own clusters and some bug was discovered.
so secret scanning table was moved to its own cluster, which means that there would be initially a common database in which they would have had all the tables.
they said that in the report it said that they discovered a bug that broke the ability of application to successfully write to secret scanning database.
so it says that bug broke the ability of the application to successfully write to a secret scanning database.
we would typically think that in a pure micro services based architecture, you would have a repository service, you would have a secret scanning service.
secret scanning service would have its own database.
second is you would think that, hey, when i have services, it would be an api call and that would take care of writing it to the database.
but here what we see is it says that application was unable to write successfully in this database, which means your repository service itself directly connects to the database and writes an entry to that earlier.
so it was easy for the repository service to write it into the, to make an entry into the database, but this failed after the data migration, which means when this secret scanning table was taken out of this database and put into a separate database, this started failing because your application, which means, let's say, a repository service, was not able to write it to this database.
right, they have not specified why it did not happen, but few possible reasons could be: there might be a network configuration which does not whitelist a particular database to be connected from a particular application.
right, and one very important point is what they say is repository creation had, like the incident revealed, an unknown dependency that repository creation had upon secret scanning, which makes a call for every repository created, which means upon repository creation.
so repository service directly writing on to the database was an unknown dependency.
because now your repository service, where it was trying to create an entry into the database, was not working due to some bug- they have not specified what bug, network issue, something, something, something would have happened, but it was not able to write it- because of which your repository creation was failing.
in case similar happens have been open, our application code has been updated to remove the dependency on secret scanning for creation of repositories.
this shows that they would have either changed it from synchronous to asynchronous, which means that their application code has been updated to remove the dependency on secret scanning for the creation of repositories, which means they would have done it asynchronously.
hey, now we can enter into this database rather than doing it synchronously in that request call so that the repository creation happens and as part of its reactive strategy, it would go and create that entry into your secret scanning database.
so as soon as the data would have been migrated, the creation, the api, uh, the repository creation, would have failed and they quickly moved back.
moved back as in they migrated the table and they migrated it back into the old tv could be one, but because they are like it seems very weird that you have to re-migrate the table to the old thing, it they might just have changed the network configuration as well, because the old table would have been there in the old database, because they would not have immediately deleted it.
because repository creation, secret scanning- who would have thought these two components talk to each other?