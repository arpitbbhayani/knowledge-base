[
  {
    "id": 138,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "82Xywy74kfE",
    "title": "Dissecting GitHub Outage - Downtime due to ALTER TABLE",
    "description": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nGitHub had a major outage and it all started with a schema migration. The outage affected their core services like GitHub actions, API requests, pull requests, and many more. Today, we dissect this outage and do an intense deep dive to extract 5 amazing insights. We also see how they very smartly mitigated the outage along with a potential long-term fix. \n\nOutline:\n\n00:00 Agenda\n02:57 Introduction\n03:23 Insight 1: Schema Migrations can take weeks to complete\n05:48 Insight 2: How schema are altered when the table is huge\n08:52 Insight 3: Deadlocks on Read Replicas\n11:20 Insight 4: Separate Replica fleet for internal read traffic\n13:59 Insight 5: Database failures cascade\n18:13 Mitigation Strategy\n29:28 Lont-term Fix\n\nOutage report: https://github.blog/2021-12-01-github-availability-report-november-2021/",
    "img": "https://i.ytimg.com/vi/82Xywy74kfE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/nrXif9YExO9EI/giphy.gif",
    "duration": "36:44",
    "view_count": 1526,
    "like_count": 78,
    "comment_count": 11,
    "released_at": "2022-05-09",
    "gist": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nIt happened to GitHub on 27th November 2021 when most of their services were down because of a large schema migration.\n\n# Why did the outage happen?\n\nGitHub ran a migration on a massive MySQL table and it made their replicas enter deadlock and crash. Here are the 5 insights about their architecture\n\n## Insight 1: Schema migration can take weeks to complete\n\nSchema migrations are intense operations as in most cases require you to copy the entire table with the new schema. Hence it might take a schema migration weeks to complete.\n\n## Insight 2: The last step of migration is RENAME\n\nTo protect the database from not taking excessive locks during the migration, we create an empty ghost table from the main table, apply migration, copy the data, and then rename the table. This reduces the locks we need for the migration.\n\n## Insight 3: Read Replicas can have deadlocks\n\nThe writes happening through the replication job and the production read load can create deadlocks on the read replicas.\n\n## Insight 4: Have a separate fleet of replicas for internal traffic\n\nHave a separate fleet of Read Replicas for internal workflows ex: analytics, etc. This way, any internal load will not affect the production load.\n\n## Insight 5: Database failures cascade\n\nWhen a replica fails, the load on healthy one's increases; which may them down and hence the cascading effect.\n\n# Mitigation\n\nThe outage happened because there were not enough read replicas to handle the load, hence in order to mitigate it the way out was to add more read replicas. GitHub team very smartly promoted the replicas used for internal workloads to handle production.\n\nAlthough the move was smart, it did not mitigate the outage because the incoming load was so high that the new replicas added also started crashing.\n\n## Data Integrity over Availability\n\nTo ensure that the data integrity is not compromised because of repeated crashes, GitHub took a call and let the Read traffic fail. They took the replica out of the production fleet, gave it time to complete the migration, and then added it back.\n\nThis way all the replicas got the time they needed to complete the schema migration. It took some but the issue was completely mitigated.\n\n# Long-term fix\n\nVertical Partitioning is a long-term fix for this problem. The idea is to create smaller databases that hold related tables; ex: all tables related to Repositories can go in one DB. This allows migration to quickly complete and during an outage, only the involved functionalities will be affected.",
    "notes_gd": "https://drive.google.com/file/d/14jdP8o2wFZYL0iFtsCbqKr3jEX0QstaY/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-alter-table"
  },
  {
    "id": 115,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "xa-hMF8gku0",
    "title": "An engineering deep-dive into Atlassian's Mega Outage of April 2022",
    "description": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. In this video, we will do an engineering deep dive into this outage trying to understand their engineering systems and practices.\n\nWe extract 6 key insights into how their engineering systems are built, their backup and restoration strategies, and most importantly why is it taking them so long to recover the data.\n\nDisclaimer: I do not have any insider information about this and the views are pure speculation.\n\nOutline:\n\n00:00 Impact of the outage\n03:56 Insight 1: Incremental Backup Strategy\n06:38 Why did the Atlassian outage happen?\n07:30 Insight 2: Progressive Rollout Strategy\n10:57 Insight 3: Soft Deletes vs Hard Deletes\n14:28 Insight 4: Synchronous Replication for High Availability\n17:47 Insight 5: Immutable backups for point-in-time recovery\n21:04 Insight 6: Nearly multi-tenant architecture\n23:30 Why is it taking time for Atlassian to recover the deleted data?\n\nOutage Report: https://www.atlassian.com/engineering/april-2022-outage-update",
    "img": "https://i.ytimg.com/vi/xa-hMF8gku0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NTur7XlVDUdqM/giphy.gif",
    "duration": "31:24",
    "view_count": 4157,
    "like_count": 219,
    "comment_count": 36,
    "released_at": "2022-04-15",
    "gist": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. Let's dissect the outage and understand its nuances of it.\n\nDisclaimer: I do not have any insider information and the views are pure speculation.\n\n## Insight 1: Data loss up to 5 minutes\n\nBecause some customers reported a data loss of up to 5 minutes before the incident, it shows that the persistent backup incrementally every 5 minutes. The backup typically happens through Change Data Capture which operates right over the database.\n\n## Insight 2: Rolling release of products\n\nAtlassian rolls out features to a subset of the users and then incrementally rolls them to others. This strategy of incremental rollout gives companies and teams a chance to test the waters on a subset and then roll out to the rest.\n\n## Insight 3: Mark vs Permanent Deletion\n\nThe script that Atlassian ran to delete had both the options- Mark of Deletion and Permanent deletion.\n\nMark for deletion: is soft delete i.e. marking is_deleted to true.\nPermanent deletion: hard delete i.e. firing DELETE query\n\nWhy do companies need permanent deletion? for compliance because GDPR gives users a Right to be Forgotten\n\n## Insight 4: Synchronous Replication\n\nTo maintain high availability they have synchronous standby replicas which means that the writes happening needs to succeed on both the databases before it is acknowledged back to the user. This ensures that the data is crash-proof.\n\n## Insight 5: Immutable Backups\n\nThe backup is made immutable and stored on S3 in some serialized format. This immutable backup allows Atlassian to recover data at any point in time while being super cost-efficient at the same time.\n\n## Insight 6: Their architecture is not truly a Multi-tenant Architecture\n\nIn a true multi-tenant architecture, every customer gets its fragment of infra- right from DB, to brokers, to servers. But at Atlassian, multiple customers share the same infra components. Companies typically do this to cut down on their infrastructure cost.\n\n## Why is it taking a long time to restore?\n\nBecause data of multiple customers reside in the same database when the DB was backed up the data (rows and tables) were backed up as is; implying that the backup also had data from multiple customers.\n\nNow to restore the intermingled rows of a customer, the entire backup needs to be loaded into a database and then the rows of specific customers need to be restored. This process is extremely time-consuming.",
    "notes_gd": "https://drive.google.com/file/d/1Bp4huBCx-wkG6kCQ5raSujdKID4RLU_y/view?usp=sharing",
    "slug": "an-engineering-deep-dive-into-atlassian-s-mega-outage-of-april-2022"
  },
  {
    "id": 114,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "6oJaZbQKnJE",
    "title": "Dissecting Google Maps Outage: Bad Rollout and Cascading Failures",
    "description": "Google Maps had a global outage on 18th March 2022, during which the end-users were not able to use Directions, Navigation, or Google Maps in general. The outage happened because of a bad rollout, and it lasted more than 2 hours 30 minutes. During the outage, users complained to have seen Gray tiles implying that the map/direction was neither getting initialized nor working.\n\nIn this video, we dissect the Google Maps outage and understand what actually happened, how they mitigated it, and, more importantly, understand ways to prevent such an outage and build a robust way of handling cascading failures.\n\nOutline:\n\n00:00 Introducing the outage and impact\n02:07 Root cause\n08:36 Cascading Failures\n12:22 Remediation\n13:45 Preventive measures\n\nIncident Report: https://issuetracker.google.com/issues/225361510",
    "img": "https://i.ytimg.com/vi/6oJaZbQKnJE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xTiTnyogFXV3Khw6Xe/giphy.gif",
    "duration": "20:17",
    "view_count": 1075,
    "like_count": 67,
    "comment_count": 8,
    "released_at": "2022-04-01",
    "gist": "On the 18th of March, 2022, Google Maps faced a major outage affecting millions of people for a couple of hours. The outage happened due to a bad deployment.\n\nAlthough a bad deployment does not sound too bad, the situation worsens when there are cascading failures if 3 services have a synchronous dependency, forming a chain-like A -> B -> C.\n\nIf A goes down, it will have some impact on B, and if the impact is big enough, we might see B going down as well; and extending it, we might see C going down.\n\nThis is exactly what happened in this Google Outage. Some services had a bad deployment, and they started crashing. Tile Rendering service depended on it, and the Tile rendering service went down because of retries.\n\nThe Direction SDK, Navigation SDK directly invoked the Tile rendering service for rendering the maps, which didn't work, causing a big outage.\n\n## How to remediate a bad deployment?\n\nRollback as soon as possible.\n\n## Preventing cascading outages\n\n- Reject requests when the server is exhausted\n- Tune the performance of the webserver and networking stack of the server\n- Monitor the server resource consumption and set alerts\n- Add circuit breakers wherever possible.\n\n",
    "notes_gd": "https://drive.google.com/file/d/10yi5K2xluTA9d7RNrorDDKU_-Mi3uHKZ/view?usp=sharing",
    "slug": "dissecting-google-maps-outage-bad-rollout-and-cascading-failures"
  },
  {
    "id": 113,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "ZFRAFTn0cQ0",
    "title": "Dissecting GitHub Outage: ID column reaching the max value 2147483647",
    "description": "GitHub experience an outage on 5th May 2020 on a few of their internal services and it happened because a table had an auto-incrementing integer ID and the column reached its maximum value possible 2147483647. In this video, we dissect what happened, mimic the situation locally and see what could have happened, and look at possible ways to mitigate and prevent a situation like this.\n\nOutline:\n\n00:00 Outage walkthrough\n02:48 Mimicking the situation locally\n10:13 MySQL AUTO_INCREMENT behavior\n12:37 Preventive measures\n14:25 Approach 1 for mitigating the issue\n18:40 Approach 2 for mitigating the issue\n\nReferences:\n - https://github.com/arpitbbhayani/mysql-maxint\n - https://github.blog/2020-07-08-introducing-the-github-availability-report/\n - https://dev.mysql.com/doc/refman/8.0/en/integer-types.html\n - https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html\n - https://www.linkedin.com/pulse/so-you-hit-2147483647-heath-dutton-/",
    "img": "https://i.ytimg.com/vi/ZFRAFTn0cQ0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/UDU4oUJIHDJgQ/giphy.gif",
    "duration": "24:34",
    "view_count": 1630,
    "like_count": 142,
    "comment_count": 32,
    "released_at": "2022-03-23",
    "gist": "On the 5th of May, 2020, GitHub experienced an outage because of this very reason. One of their shared table having an auto-incrementing ID column hits its max limit. Let's see what could have been done in such a situation.\n\n# What's the next value after MAX int?\n\nGitHub used 4 bytes signed integer as their ID column, which means the value can go from `-2147483648` to `2147483647`. So, now when the ID column hits `2147483647` and tries to get the next value, it gets the same value again, i.e., `2147483647`.\n\nFor MySQL, `2147483647` + `1` = `2147483647`\n\nSo, when it tries to insert the row with ID `2147483647`, it gets the *Duplicate Key Error* given that a row already exists with the same ID.\n\n# How to mitigate the issue?\n\nA situation like this is extremely critical given that the database is not allowing us to insert any row in the table. This typically results in a major downtime of a few hours, and it depends on the amount of data in the table. There are a couple of ways to mitigate the issue.\n\n## Approach 1: Alter the table and increase the width of the column\n\nQuickly fire the `ALTER` table and change the data type of the ID column to `UNSIGNED INT` or `BIGINT`. Depending on the data size, an ALTER query like this will take a few hours to a few days to execute. Hence this approach is suitable only when the table size is small.\n\n## Approach 2: Swap the table\n\nThe idea here is to create an empty table with the same schema but a larger ID range that starts from `2147483648`. Then rename this new table to the old one and start accepting writes. Then slowly migrate the data from the old table to this new one. This approach can be used when you can live without the data for a few days.\n\n## Get warned before the storm\n\nAlthough mitigation is great, it is better to place a monitoring system that raises an alert when the ID reaches 70% of its range. So, write a simple DB monitoring service that periodically checks this by firing a query on the database.",
    "notes_gd": "https://drive.google.com/file/d/13rNEWXwIdNkNcP2gSQQvBF8czUBpF47y/view?usp=sharing",
    "slug": "dissecting-github-outage-id-column-reaching-the-max-value-2147483647"
  },
  {
    "id": 112,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "h5hXivWws7k",
    "title": "Dissecting Spotify's Global Outage - March 8, 2022",
    "description": "Incident Report: Spotify Outage on March 8: https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/\nGoogle Cloud Traffic Director Outage: https://status.cloud.google.com/incidents/LuGcJVjNTeC5Sb9pSJ9o\nJava gRPC Client Bug: https://github.com/grpc/grpc-java/issues/8950\n\nIn this video, we dissect the reasons behind Spotify's Global Outage and try to understand its architecture, and learn from critical things to remember while architecting a system.",
    "img": "https://i.ytimg.com/vi/h5hXivWws7k/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26FKWlRgIL3JOIkGA/giphy.gif",
    "duration": "26:",
    "view_count": 2773,
    "like_count": 169,
    "comment_count": 29,
    "released_at": "2022-03-12",
    "gist": "",
    "notes_gd": "",
    "slug": "dissecting-spotify-s-global-outage-march-8-2022"
  }
]