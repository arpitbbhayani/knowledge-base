[
  {
    "id": 164,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "LeT_s-UFw-U",
    "title": "Dissecting GitHub Outage and things to do after that",
    "description": "Outages happen and in such a tense situation, the main priority is to get the system back up, but is that it? Is everything done when the service is up and running?\n\nSo today let's spend some time talking about the aftermath of an outage. There are many things to take care of once the outage is mitigated and in this video, we dissect a GitHub outage, understand what happened, and look at a set of common practices that we follow to ensure closure.\n\nOutline:\n\n00:00 Agenda\n02:31 The outage and mitigation\n07:23 Solve data inconsistency\n09:51 Invalidate the cache\n11:03 Setup alerting\n13:53 Take preventive measures\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/LeT_s-UFw-U/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6MbhbYBsqTrbP2qQ/giphy.gif",
    "duration": "16:24",
    "view_count": 417,
    "like_count": 22,
    "comment_count": 5,
    "released_at": "2022-07-08",
    "gist": "Getting the service up is P0 during the outage, but that is not all. There are a few other things that we need to take care of once the issue is mitigated.\n\n## Resolve Inconcistencies\n\nDuring the outage, the business logic or database must have crashed while handling in-flight requests; hence there is a very high chance of the data becoming inconsistent.\n\nFor example: if the process crashed while transferring money from one account to another; it is possible that money got deducted from one account but did not credit to another.\n\nHence once the outage is mitigated, the first thing to be done is to ensure that the data does not remain in an inconsistent state. Achieving this depends on the usecase at hand and the tech stack involved.\n\n## Invalidate the Cache\n\nAnother place that needs attention is cache invalidation. If the outage sustains for a long time, there is a chance that some of the entries in the cache are not valid anymore.\n\nHence, depending on the usecase, it is advised to go through the cache entries and delete the invalid once ensuring the end-user sees a globally consistent view of the data.\n\n## Audit alerting and monitoring\n\nOnce the outage is mitigated and the inconsistencies are resolved, it is better if we take a look into existing alerting and monitoring practices so that we ensure that we get alerted at the right time.\n\nIn most cases, you would find that a few things could always be improved when it comes to monitoring the right things, and hence an outage is an opportunity to get alerting and monitoring improvements prioritized.\n\n## Take preventive measures\n\nEvery outage has a root cause, and it is super-important to fix the root cause to ensure that the over never happens again for the same reason. This requires us to go in-depth and take preventive measures to ensure complete closure.\n\nFor example, re-auditing all the INT32 columns and moving them to INT64 to ensure that we never repeat the outage due to integer overflow.",
    "notes_gd": "https://drive.google.com/file/d/1pDHYlKlQsTB_oDvqG9mpsvfKkHWDnkRG/view?usp=sharing",
    "slug": "dissecting-github-outage-and-things-to-do-after-that"
  },
  {
    "id": 163,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "Of3FS2qDM28",
    "title": "Dissecting GitHub Outage - Why should we localize failures?",
    "description": "Outages are inevitable; but we should design our architecture such that if a component is down, it should not lead to a complete outage. It is easy to say this, but hard to implement.\n\nIn this video, we dissect yet another GitHub outage, gather a few interesting insights about their Microservices architecture, and spend some time discussing and understanding the importance of having a smaller blast radius and a fool-proof way of achieving that.\n\nOutline:\n\n00:00 Agenda\n02:36 What happened\n04:30 Root Cause\n05:14 Insights about their architecture\n08:50 Master failover did not happen\n13:40 Long-term fixes and Localizing failures\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/Of3FS2qDM28/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3oEduUNILoKY1sciqI/giphy.gif",
    "duration": "20:33",
    "view_count": 392,
    "like_count": 30,
    "comment_count": 4,
    "released_at": "2022-07-06",
    "gist": "Outages are inevitable; but we should design our architecture and ensure that if a component is down, it should not lead to a complete outage.\n\n## What happened with GitHub?\n\nGitHub saw a lot of failures with their Actions service and this led to delays in queued jobs from being processed. The root cause was some infrastructure error in the SQL layer.\n\n## Insights about their architecture\n\nA couple of insights about their architecture\n\n### Synchronous dependency\n\nAlthough GitHub Actions look like a single feature, internally it consists of multiple microservices. Some of these, have a synchronous dependency on the database. Because of this, when the DB had a hiccup, the entire Actions feature was hindered.\n\n### Zero trust communication\n\nThe service that was most affected in this outage handled communication; why would services need authentication? they are all internal the infrastructure?\n\nMicroservices talk. The communication needs to be protected with auth so that any engineer/service gone rogue cannot abuse the system in any capacity. Only authenticated and authorized services are allowed to take action.\n\n## What about automatic failover?\n\nGiven that the outage happened on the database layer, why did the database did not auto-recover? it is a standard procedure and configuration that would have just promoted a replica to be the new master.\n\nAlthough it is a common config, during this outage the metrics did not show any issue with the database, and hence the auto-failover was never triggered. It took a long time to even understand the root cause and then start mitigation.\n\n## Long-Term Fixes\n\n### Update the automation scripts\n\nThe automation that reads the telemetry and decides to do a failover needs to be updated so that such failures are detected and action is taken.\n\n### Localizing failures\n\nAn important long-term change that needs to be driven is to localize the failure. In this outage, we learned how a hiccup in one database/service causes downtime of all dependent Microservices. This shouldn't have happened as the Microservices are supposed to solve this very problem.\n\nA good way to ensure that the blast radius of the outage is minima; is by ensuring the failures are localized, implying, that when a service is down, only the service is affected while everything else is functioning perfectly fine.\n\nA common approach to getting this loose coupling is by powering inter-service communication through the asynchronous medium instead of synchronous API calls. Thus, if something breaks, we could fix it and continue to process the messages.",
    "notes_gd": "https://drive.google.com/file/d/1V_nuSi4KDsuTD_p0beIWoBDmRN9T5d5V/view?usp=sharing",
    "slug": "dissecting-github-outage-why-should-we-localize-failures"
  },
  {
    "id": 161,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "bycFzB6yrK0",
    "title": "Dissecting GitHub Outage - Multiple Leaders in Zookeeper Cluster",
    "description": "Distributed Systems are prone to problems that seem very obscure. GitHub had an outage because a set of nodes in the Zookeeper cluster ran an election and elected a second leader. How obscure is that?\n\nIn this video, we dissect a GitHub outage that would tell us how weird is the world of Distributed Systems. We also talk about Zookeeper and its importance for managing a Kafka cluster. We see what happened during the outage, and how GitHub was able to mitigate it with ZERO data loss and a brilliant fallback strategy.\n\nOutline:\n\n00:00 Agenda\n02:37 Zookeeper and Kafka\n05:27 What happened\n06:25 Bootstrapping Zookeeper Node and Leader Election\n09:06 Broker connecting to the new cluster\n14:05 Recovery\n15:55 Dead Letter Queue and Fallback Strategy\n18:07 Key Learnings\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/bycFzB6yrK0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l4pTi9xLGNINzfyXm/giphy.gif",
    "duration": "21:9",
    "view_count": 936,
    "like_count": 50,
    "comment_count": 13,
    "released_at": "2022-07-01",
    "gist": "The split-brain problem in Distributed Systems is not theoretical. GitHub had an outage because their Zookeeper cluster ended up having two leaders, leading to writes to Kafka failing.\n\nZookeeper is an essential component for Kafka as the clients connect to get information about the brokers and cluster internally using it to manage its state.\n\n## What happened?\n\nDuring scheduled maintenance the Zookeeper nodes were getting upgraded/patched and during this time, many new nodes were added \"too quickly\" to the zookeeper cluster.\n\nWith a lot of new nodes added \"too quickly\", they were unable to self-discover or understand the topology and the way the bootstrap code of Zookeeper is written, they thought the cluster was leaderless. This made them trigger a Leader Election.\n\nGiven that a lot of new nodes were added, they formed the majority and elected a new leader. These nodes formed a logical second cluster operating independently. Thus the cluster is now having a split-leadership problem.\n\n## Broker Connected\n\nOne of the Kafka broker (node) connected to the second cluster and found that no other nodes are present (because second zookeeper cluster had no entries) and hence elected itself as the controller.\n\nWhen the Kafka clients (producers) were trying to connect to the Kafka cluster they got conflicting information which led to the 10% of writes failing.\n\nIf the number of brokers connecting to the new cluster were more, then it could have led to even data consistency issues. But because only one node connected, the impact was minimal.\n\n## Recovery\n\nThe zookeeper cluster would have auto-healed but it would have taken a long time to converge, and hence a quick way to fix this is to manually update the zookeeper entries and configure it to have a single leader.\n\nTo keep things clean, the nodes that were part of second zookeeper cluster could have been deleted as well.\n\n## Ensuring zero data loss\n\nEven though 10% of write requests failed, why did it not lead to a data loss? the secret is Dead Letter Queue.\n\nIt is a very standard architectural pattern that ensures zero data loss even when the message broker (queue) crashes. The idea is to have a secondary queuing system that you can push messages to if the write on the primary fails.\n\nAll the messages that client tried to write to Kafka failed, but they were persisted in DLQ which they processed later.\n\n## Key Learnings\n\n- Make consumers idempotent\n- Automate cluster provisioning\n- Have a DLQ for all critical queueing systems\n- Have retries with exponential back-offs on clients",
    "notes_gd": "https://drive.google.com/file/d/1ut8trVZ5IF4hB6amfKpZSzPJl_8eRiCk/view?usp=sharing",
    "slug": "dissecting-github-outage-multiple-leaders-in-zookeeper-cluster"
  },
  {
    "id": 160,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "4mVJQJbw6Vw",
    "title": "GitHub Outage  - How databases are managed in production",
    "description": "So, how are databases managed in production? When the master goes down, how a replica is chosen and promoted to be the new master? Is this a manual step or something that can be automated?\n\nIn this video, we dissect yet another GitHub outage and apart from understanding what happened, we would spend a significant amount of time learning how databases are managed in production, and the kind of tools companies use to ensure optimal DB performance and High Availability.\n\nOutline:\n\n00:00 Agenda\n02:39 What happened?\n03:20 ProxySQL and why it is used in production?\n09:14 Orchestrator and what it does?\n14:12 Anti-flapping policy of orchestrator\n18:10 Root cause and mitigation\n19:46 Key takeaways\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/4mVJQJbw6Vw/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/1vZcbntQ1jVcSA78gw/giphy.gif",
    "duration": "23:41",
    "view_count": 990,
    "like_count": 73,
    "comment_count": 13,
    "released_at": "2022-06-29",
    "gist": "Managing databases in production is not easy and it does require a lot of tooling to keep it up and running. GitHub had an outage that gives us a glimpse of the toolings they use in production. So, here's what happened\n\n## Incident Summary\n\nThe GitHub team updated the ProxySQL and a week later a master node in a critical database cluster crashed. A replica was promoted as the new master, and it also crashed. The team tried to manually recover, but the manually promoted replica also crashed.\n\nFinally, the GitHub team did a full revert - reverted all the code changes, and also downgraded the ProxySQL version. Post this, things become normal and the master did not crash.\n\n## ProxySQL\n\nProxySQL is a database proxy that sits between the servers and the database cluster. The server seamlessly connects to the ProxySQL and fires the usual SQL queries and the proxy forwards it to the data node as per the configuration.\n\n### Why do we need ProxySQL\n\n- better connection management\n- as a cache for SQL query responses\n- a gatekeeper to handle security and routing\n\n## Orchestrator\n\nOrchestrator is a MySQL topology management tool that helps us achieve High Availability on a MySQL cluster. It keeps an eye on all the nodes and takes corrective actions whenever something goes wrong.\n\nWe configure Orchestrator to keep an eye on the master node and as the node crashers, it promotes a replica to become the new master. Given that all of this happens automatically, it just takes a few seconds for the cluster to recover from the master crash.\n\n### Anti-flapping policy\n\nA very common cascading failure happens when the master fails and a replica is promoted to be the new master. Due to the high load, say the new master also crashed. The cycle thus continues until all nodes crash leading to a complete outage.\n\nThe anti-flapping policy of Orchestrator prevents this complete outage by not promoting replica to master until the cool-off period ends. Once the replica is promoted to be the new master, Orchestrator does not promote another replica until the cool-off period ends.\n\nAlthough the master is down, this anti-flapping policy ensures that we are at least partially functional and can continue serving some reads. Along with this, we see only a small subset of nodes are thrown in the fire and hence have fewer data nodes to recover.\n\n## Mitigation\n\nTo mitigate the issue, the GitHub team\n\n- reverted the ProxySQL version\n- reverted the code that required an upgraded version of ProxySQL\n\nWith this full revert, the master node stopped crashing and things become normal again.",
    "notes_gd": "https://drive.google.com/file/d/1Fqad5kpmlvrNMFXSojlOvBwOAOu7c2nd/view?usp=sharing",
    "slug": "github-outage-how-databases-are-managed-in-production"
  },
  {
    "id": 158,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "VPZo8cO1HbI",
    "title": "Dissecting GitHub Outage - Downtime due to Rate Limiter",
    "description": "Rate limiters are supposed to avoid downtimes, but have you ever heard that a rate limiter caused a downtime? This happened with GitHub, where a big chunk of their users saw elevated error rates.\n\nIn this quick incident dissection, let's take a look at a high-level overview of how GitHub does their A/B experiments, how a low-level decision led to this incident for a large chunk of users, and conclude with some key things we all should learn from this outage.\n\nOutline:\n\n00:00 Agenda\n02:32 What happened?\n02:47 What is A/B Experimentation?\n05:33 A/B Experimentation at GitHub and what failed\n11:12 Mitigation and Long-term Fix\n12:04 Key Takeaways\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/VPZo8cO1HbI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/5XIH3sxd50onu/giphy.gif",
    "duration": "16:11",
    "view_count": 916,
    "like_count": 44,
    "comment_count": 9,
    "released_at": "2022-06-24",
    "gist": "Rate Limiters are supposed to avoid downtimes, but what if they turn out to be the root cause of a major outage?\n\nA large chunk of GitHub users saw elevated error rates and this happened after deploying their A/B Experimentation service. So, what went wrong? but before that let's understand what is A/B experimentation\n\n## What is A/B Experimentation?\n\nIt is hard to decide which UI is better and hence before rolling out any critical UI change to all the users, a company tests it through an A/B experiment.\n\nA set of users are chosen at random and a fraction of them are shown the new variation while others are shown the old one. Key vitals and metrics of the features are measured and compared to decide if the variation is indeed an improvement.\n\nIf the metrics are positive and significantly better then the new variation is rolled out to 100% of the users. This way companies ensure that the features that are rolled out are genuine improvements in the product.\n\n## A/B Testing at GitHub\n\nEvery server that needs to participate in any A/B experiment fetches a configuration file that is dynamically generated using, say, Config Generator service.\n\nThe configuration allows granular controls for the A/B experiment and holds critical information that shapes experimentation. When any server requests for a config file, the request hits the config service and it, in turn, generates the file and sends it back to the user.\n\n## What failed?\n\nBecause a lot of requests were made to the Configuration Service, the rate limiting module of the service started throttling and it prevented the configuration file to be generated and sent to the servers.\n\nThis affected the users who were part of this experiment and they saw elevated error rates as the frontend did not have the necessary information it required to power the experiment.\n\n## Mitigation and Long-term Fix\n\nAs quick mitigation, the GitHub team disabled the dependency on the dynamically generated file and it restored the services to normal.\n\nTo ensure the outage would not happen due to the same reason, the Config Generator service would generate and cache the configuration files so that when a request comes, the file could be served directly from the cache instead of generating on the fly which was time consuming.\n\n## Key Takeaways\n\n- avoid sync dependencies between services and prefer async\n- classify the services by severity tiers and run periodic audits of tier-1 services to ensure they are architected well and there are no blindspots",
    "notes_gd": "https://drive.google.com/file/d/17xmYQ9tXQfhMUc85uYfkA880VH1ANEKH/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-rate-limiter"
  },
  {
    "id": 157,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "ZirBDq1JwpY",
    "title": "Dissecting GitHub Outage - Master failover failed",
    "description": "Companies announce their planned maintenance, what happens during that? Could something go wrong while running maintenance?\n\nGitHub team was switching their Master databases from one node to another; while doing this something went wrong and the new database crashed. This led to data divergence and a production incident that lasted over 5 hours.\n\nIn this video, we dissect this incident and understand what happens during planned maintenance, what went wrong with GitHub, how GitHub mitigated it, and understand some really cool things about switching databases and solving data divergence.\n\nOutline:\n\n00:00 Agenda\n02:42 What happened?\n03:29 Scaling reads with Read Replicas\n04:40 Planned Database Maintenance\n10:08 Database crashed and quick mitigation\n11:44 Data Divergence between two masters\n13:54 Remediating Data Divergence\n18:23 Read Replica taking time to spin up\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/ZirBDq1JwpY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/ZsVhGBa2QB7U31BnPo/giphy.gif",
    "duration": "22:30",
    "view_count": 785,
    "like_count": 30,
    "comment_count": 16,
    "released_at": "2022-06-22",
    "gist": "Master failover failed for GitHub leading to a 5-hour long incident, let's see what happened.\n\n## Incident Summary\n\nFor five hours, GitHub users observed delays in data being visible on the interface and API after it was written on the database. This happened during the maintenance when they were switching the Master DB.\n\n## Planned Maintenance\n\nPlanned maintenance is a popular way for companies to take a small downtime and execute all the maintenance activities. Some activities for which we do plan database maintenance are\n\n- applying security patches\n- apply version upgrades\n- parameter tuning\n- hardware replacement\n- periodic reboots\n\nA popular activity during database maintenance is to switch the Master node i.e. shift the traffic coming from the master node to a new node so that we could patch the old instance.\n\nFor a very short duration, when the config switch is happening, the database would be unavailable leading to a small outage; and this is expected behavior.\n\n## Database Crash\n\nDuring the failover, when the traffic was moved to the new database, the `mysqld` process crashed. This led to incoming writes failing. To quickly mitigate the issue, the team moved the traffic to the old database. This solved the issue and the site was up and running.\n\n## Something interesting happened\n\nThe new database before crashing served the write traffic for 6 seconds. So, after the crash when the traffic was redirected to the old database, it did not have the data that was written in that 6 seconds window.\n\nThis is a huge concern, as it would lead to bad UX, and in the worst case consistency failures. So, how to remediate this issue?\n\n## Remediating master failovers\n\nIn order to remediate this, we take the help of the Write Ahead Log or Commit log of the database. Whenever we do a failover, we always keep track of the `BINLOG` coordinates.\n\nOnce we moved the traffic to the old database, all we have to do is iterate through the BINLOG and apply all the changes that happened on the new database post the noted coordinate on the old database.\n\nThis would re-create or modify the exact data that was written to the new database on the old database, leading to zero data loss or consistency breach.\n\n## Cleaning up the mess\n\nTypically when we have such a failover, it is better that we restore the read replicas and hence GitHub team rotated all the replicas. Creating a read replica takes time, given the scale of GitHub.\n\nIt took them 4 hours to set up replicas and 1 hour to re-configure the cluster hence for over 5 hours the incident was affecting the users.",
    "notes_gd": "https://drive.google.com/file/d/1uzicMXBufPqkPTzrxyQOS6zn1x6WURwi/view?usp=sharing",
    "slug": "dissecting-github-outage-master-failover-failed"
  },
  {
    "id": 152,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "9GHh_U-ud9U",
    "title": "Dissecting GitHub Outage - Downtime they thought was avoided",
    "description": "Has it ever happened to you that you anticipated that something would go wrong, you pro-actively fixed it, but it still went wrong? This exact thing happened to GitHub on March 1, 2021, in a situation for which they were prepared for 6 months. This is a very amusing incident as it teaches us a lot about the blind spots that exist in every system out there.\n\nIn this video, we talk about an outage that GitHub faced with their Actions service. and take an in-depth look into what happened, why it actually happened, and what GitHub did to prepare for this very situation, but it still went wrong.\n\nOutline:\n\n00:00 Agenda\n02:46 What happened?\n05:12 Table Exploded with Data and IT reached its limit\n07:03 GitHub's Anticipation and What went wrong?\n12:27 Mitigation\n14:04 Impact on the Search Service\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/9GHh_U-ud9U/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3xz2BLBOt13X9AgjEA/giphy.gif",
    "duration": "17:44",
    "view_count": 433,
    "like_count": 24,
    "comment_count": 9,
    "released_at": "2022-06-10",
    "gist": "GitHub thought they avoided an outage by fixing a possible root cause 6 months in advance, but fate had different plans.\n\n## Check Suites and Workflows\n\nWhen we push any changes to any GitHub repository there are some checks that run. We can see them on our Pull Request and it basically prevents us from merging the PR until all checks are successful. We can also add custom checks on our own to the workflow.\n\nAn entry is made in the database for every execution of the check suite. This is a high frequency that would lead to heavy ingestion in the database table. A side effect it would be that the auto-incrementing ID column, which is typically a 32-bit integer would exhaust leading to the writes getting failed.\n\n## GitHub's Anticipation\n\nGitHub anticipated this situation 6 months in advance and they altered the column from 32-bit integers to 64-bit integers ensuring that even when the ID range exhausts the 32-bit limit it would lead to downtime.\n\nBut still, the team faced an outage, how? what happened?\n\n## What exactly happened?\n\nThe service was able to create entries in the database about the check suite execution as the database supported 64-bit integers, but there was one external dependency that unmarshalled JSON strings to native objects which only supported 32-bit integers.\n\nThe service was responsible for pulling the jobs from the database and putting it in the queue to be picked up by executors. This service depended on the library and hence it was unable to execute the checks. This led to all the checks remaining in the pending state during the course of this outage.\n\n## Impact on the Search service\n\nThe search service was also impacted by this as the indexing used queue as the source. Since the newer jobs were not put in the queue, they were not indexed in the search cluster (eg: ElasticSearch), and hence when the user searched, they could not find the latest checks and workflows.\n\n## Mitigation\n\nIn order to mitigate the issue, the GitHub team released a code fix. Speculation: they would have updated the library version that would support 64-bit integers, or they might have quickly forked and patched it with the changes, or they might have written some ad-hoc job that temporarily pulled the jobs and put them in the queue.\n\nThis incident shows that no matter how big the company gets and how prepared are you for an extreme event, there would always be some blind spots in the system that would bite us back.",
    "notes_gd": "https://drive.google.com/file/d/1J9wbS2sK5BrW5Ftgdz2tmPrt86tAhQaA/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-they-thought-was-avoided"
  },
  {
    "id": 150,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "df2QgLW0QC4",
    "title": "Dissecting GitHub Outage Downtime due to creating an Index",
    "description": "GitHub wanted to optimize their SQL query performance, and they had to reverse a database index. Instead of getting a performance boost, thy incurred a downtime of more than 60 minutes. Imagine the state of the team who wanted to do good, but were stuck in this fix.\n\nThis outage gives us a super-in-depth insight into MySQL and its indexing structure. It is indeed fascinating to see the kind of optimizations engineers have to make while operating at scale.\n\nIn this video, let's dissect this outage and understand what happened, why GitHub had to flip their index, how things went wrong and affected end users like us, and conclude with 3 key takeaways from this outage.\n\nOutline:\n\n00:00 Agenda\n02:47 What happened?\n03:52 Need of Reversing an Index\n07:21 What could go wrong during and after Indexing?\n14:52 Cascading Failures\n17:16 Mitigation and Key Takeaways\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/df2QgLW0QC4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26FxBCxSr8V7sHWdG/giphy.gif",
    "duration": "22:49",
    "view_count": 861,
    "like_count": 57,
    "comment_count": 12,
    "released_at": "2022-06-06",
    "gist": "Imagine you created an index on a table and instead of boosting the performance, it lead to an outage \ud83e\udd26\u200d\u2642\ufe0f GitHub ran a migration to reverse an index and it lead to a 60 mins outage.\n\nNote: The example we have taken is pure speculation, the official incident report had minimal information about the outage. But the write-up will make you aware of possible challenges that might come during such situations.\n\n## Reverse an index\n\nReversing the order of the index is done when we have a multi-column index and the query requires different sorting orders on them; for example, `DESC` on `date` and `ASC` on `user_id`.\n\nFor a query to be optimally executed on the database, we would need an index that physically stores the index ordered by the `date` in the descending order and `user_id` in the ascending order.\n\nMySQL by default stores any index in `ASC` order and hence, GitHub had to run the migration to reverse the order of the index and gain a boost.\n\n## What could go wrong?\n\nA reverse index would require a Full Table Scan during the creation putting a load on the database. Also, upon changing the order of the index, it is possible we overlook another query that is more frequent but optimal with the old order.\n\nThe database does its best to create an optimal execution plan and it might not use the reverse index we just created. We can solve this by specifying Index Hints like `USE INDEX` and `FORCE INDEX`, ensuring that it uses our index to evaluate the query.\n\n## Cascading Effect\n\nBecause one of the queries was doing a Full Table Scan, it put a load on the database which had a cascading effect on the service eventually propagating to the end user. All the intermediate services will timeout giving a degraded experience to the user.\n\n## Key Takeaways\n\n### Never blindly trust ORM\n\nORMs are designed to make our lives simpler but they might not generate the most optimal queries, and hence it is always better to periodically audit the queries and ensure they are optimal.\n\nPoorly generated queries will put a load on the database choking the entire performance.\n\n### Check the query execution plan\n\nWhile updating a query or changing a schema always check the query execution plan. We can get the execution plan for any query using the `EXPLAIN` statement.\n\nThe diff in the plan would give tell us if any of our queries would perform a full table scan.\n\n### Audit the queries and indexes\n\nKeep an inventory of the queries we fire and the indexes it uses during execution. So, whenever we change any index, we can quickly run an audit and ensure zero regressions.",
    "notes_gd": "https://drive.google.com/file/d/1JrtyE2wt9ikic3iOTy36NYKxEPESMt4a/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-creating-an-index"
  },
  {
    "id": 149,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "48YZzGi7QMk",
    "title": "Dissecting GitHub Outage - Repository Creation Failed",
    "description": "Imagine you trying to create a new GitHub repository and it call is failing, failing for 53 minutes. This happened with GitHub in April 2021 when for 53 minutes people were unable to create any new repositories. Upon investigation, they found out that the root was scanning secrets. Two seemingly different usecases took down one of the most important APIs.\n\nThis has to be one of the most amusing outages that I have seen in recent times.  In this video, we dissect this outage, understand the root cause of it, look at the importance of secret scanning, and conclude with an understanding of their mitigation process.\n\nOutline:\n\n00:00 Agenda\n02:49 What happened?\n03:20 Secret scanning and its importance\n08:28 Repository Creation Flow\n09:15 What lead to an outage?\n17:26 Outage mitigation\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/48YZzGi7QMk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/VIo556t5920j07cCR4/giphy.gif",
    "duration": "20:59",
    "view_count": 498,
    "like_count": 25,
    "comment_count": 7,
    "released_at": "2022-06-03",
    "gist": "Just imagine you trying to create a repository on GitHub and it is not working, and this happened to GitHub in April 2021 when their users were not able to create a new repository.\n\nThe root cause for this outage was something that seems unrelated - Scanning Secrets. The root cause makes this outage super interesting to dissect.\n\n## What is Secret Scanning?\n\nOur API servers need to talk to peripheral components like Databases, Cache, SaaS services, etc. This communication involves some sort of authentication and authorization through auth tokens, passwords, or secret keys.\n\nDevelopers tend to commit the secrets in the settings/constant files and push them to GitHub. What if the repository content gets leaked? What if GitHub itself has a data breach and the attacker gets access to the private repositories?\n\nIf the secrets like AWS access keys, auth tokens, and DB passwords are leaked and the attacker can then get the dump of the data and ask for a ransom. Or they may even abuse the infrastructure to perform some illegal activities or mine cryptocurrencies.\n\nHence, GitHub periodically runs a job that checks all the repositories for any secrets that are committed and warns the user about it.\n\n## Repository Creation Flow\n\nWhen a repository is created an entry is made into the Secret Scanning table which is then used by a job that scans for potential secrets and notifies the owner.\n\n## What led to the outage?\n\nThe GitHub team ran a data migration in which they moved the Secret Scanning table from a common database to its own cluster allowing it to scale independently.\n\nGitHub team was unaware of this dependency! and hence after the migration of the table happened to a different database the creation of a new repository started failing to lead to this outage. It is interesting to see such mature products having blindspots.\n\n## How did GitHub mitigate it?\n\nThe mitigation strategy of GitHub was to roll back the migration. Although it is unclear from the incident report on what exactly they did but there are a few speculations\n\n1. they could have recopied the table quickly to the old database\n2. whitelisted the database so that applications could connect\n3. old table would have been intact and hence they would have just renamed and made it active again.\n\nAgain, it is pure speculation given we do not have any insider information nor they specified in the report. It would have been fun to have gone through their actual mitigation steps. We could have learned so much, but nonetheless, we did learn a few interesting insights from this outage.",
    "notes_gd": "https://drive.google.com/file/d/1KtNWAH5YHC9-qVxvbxLTXF3u5yGV7l-t/view?usp=sharing",
    "slug": "dissecting-github-outage-repository-creation-failed"
  },
  {
    "id": 144,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "iqapVyfoFqc",
    "title": "Dissecting GitHub Outage: Downtime due to an Edge Case",
    "description": "In August 2021, GitHub experienced an outage where their MySQL Master database went into a degraded state. Upon investigation, they found out it was due to an edge case. So, how can an edge case take down a database?\n\nIn this video, we understand what happened, take a fictional example of how an edge case can put extra load on the database, and conclude with an exciting way to make our SQL queries fool-proof.\n\nOutline:\n\n00:00 Agenda\n00:27 Outage Overview\n07:38 What could have happened?\n08:51 A fictional example of an edge case taking down DB\n17:18 How to avoid edge cases on SQL queries\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/iqapVyfoFqc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/243UHkbIOZJGj7FZ84/giphy.gif",
    "duration": "21:32",
    "view_count": 1155,
    "like_count": 56,
    "comment_count": 12,
    "released_at": "2022-05-23",
    "gist": "An edge case took down GitHub \ud83e\udd2f\n\nGitHub experienced an outage where their MySQL database went into a degraded state. Upon investigation, it was found out that the outage happened because of an edge case. So, how can an edge case take down a database?\n\n## What happened?\n\nThe outage happened because of an edge case which lead to the generation of an inefficient SQL query that was executed very frequently on the database. The database was thus put under a massive load which eventually made it crash leading to an outage.\n\n### Could retry have helped?\n\nAutomatic retries always help in recovering from a transient issue. During this outage, retries made things worse. Automatic retries added the load on the database that was already under stress.\n\n## Fictional Example\n\nNow, we take a look at a fictional example where an edge case could potentially take down a DB.\n\nSay, we have an API that returns the number of commits made by a user in the last `n` days. The way, this API could be implemented is to get the `start_date` as an integer through the query parameter, and the API server could then fire a SQL query like\n\n```\nSELECT count(id) FROM commits\nWHERE user_id = 123 AND start_time > start_time\n```\n\nIn order to fire the query, we convert the string `start_time` to an integer, create the query, and then fire it. In the regular case, we get the correct input and then compute the number of commits and respond.\n\nBut as an edge case, what if we do not get the query parameter or we get a non-integer value; then depending on the language at hand we may actually use the default integer value like `0` as our `start_time`.\n\nThere is a very high chance of this happening when we are using Golang which uses `0` as the default integer value. In such a case, the query that gets executed would be\n\n```\nSELECT count(id) FROM commits\nWHERE user_id = 123 AND start_time > 0\n```\n\n  \nThe above query when executed iterates through all the rows of the table for a particular user, instead of the rows for the last 7 days; making it super inefficient and expensive. The above query would put a huge load on the database and a frequent invocation can actually take down the entire database.\n\n### Ways to avoid such situations\n\n1. Always sanitize the input before executing the query\n2. Put guard rails that prevent you from iterating the entire table. For example: putting `LIMIT 1000` would have made you iterate over 1000 rows in the worst case.",
    "notes_gd": "https://drive.google.com/file/d/1PI302Cvutu8LJKmZu7fiIVOAqRhiSMek/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-an-edge-case"
  },
  {
    "id": 138,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "82Xywy74kfE",
    "title": "Dissecting GitHub Outage - Downtime due to ALTER TABLE",
    "description": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nGitHub had a major outage and it all started with a schema migration. The outage affected their core services like GitHub actions, API requests, pull requests, and many more. Today, we dissect this outage and do an intense deep dive to extract 5 amazing insights. We also see how they very smartly mitigated the outage along with a potential long-term fix. \n\nOutline:\n\n00:00 Agenda\n02:57 Introduction\n03:23 Insight 1: Schema Migrations can take weeks to complete\n05:48 Insight 2: How schema are altered when the table is huge\n08:52 Insight 3: Deadlocks on Read Replicas\n11:20 Insight 4: Separate Replica fleet for internal read traffic\n13:59 Insight 5: Database failures cascade\n18:13 Mitigation Strategy\n29:28 Lont-term Fix\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/82Xywy74kfE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/nrXif9YExO9EI/giphy.gif",
    "duration": "36:44",
    "view_count": 1914,
    "like_count": 99,
    "comment_count": 24,
    "released_at": "2022-05-09",
    "gist": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nIt happened to GitHub on 27th November 2021 when most of their services were down because of a large schema migration.\n\n## Why did the outage happen?\n\nGitHub ran a migration on a massive MySQL table and it made their replicas enter deadlock and crash. Here are the 5 insights about their architecture\n\n### Insight 1: Schema migration can take weeks to complete\n\nSchema migrations are intense operations as in most cases require you to copy the entire table with the new schema. Hence it might take a schema migration weeks to complete.\n\n### Insight 2: The last step of migration is RENAME\n\nTo protect the database from not taking excessive locks during the migration, we create an empty ghost table from the main table, apply migration, copy the data, and then rename the table. This reduces the locks we need for the migration.\n\n### Insight 3: Read Replicas can have deadlocks\n\nThe writes happening through the replication job and the production read load can create deadlocks on the read replicas.\n\n### Insight 4: Have a separate fleet of replicas for internal traffic\n\nHave a separate fleet of Read Replicas for internal workflows ex: analytics, etc. This way, any internal load will not affect the production load.\n\n### Insight 5: Database failures cascade\n\nWhen a replica fails, the load on healthy one's increases; which may them down and hence the cascading effect.\n\n## Mitigation\n\nThe outage happened because there were not enough read replicas to handle the load, hence in order to mitigate it the way out was to add more read replicas. GitHub team very smartly promoted the replicas used for internal workloads to handle production.\n\nAlthough the move was smart, it did not mitigate the outage because the incoming load was so high that the new replicas added also started crashing.\n\n### Data Integrity over Availability\n\nTo ensure that the data integrity is not compromised because of repeated crashes, GitHub took a call and let the Read traffic fail. They took the replica out of the production fleet, gave it time to complete the migration, and then added it back.\n\nThis way all the replicas got the time they needed to complete the schema migration. It took some but the issue was completely mitigated.\n\n## Long-term fix\n\nVertical Partitioning is a long-term fix for this problem. The idea is to create smaller databases that hold related tables; ex: all tables related to Repositories can go in one DB. This allows migration to quickly complete and during an outage, only the involved functionalities will be affected.",
    "notes_gd": "https://drive.google.com/file/d/14jdP8o2wFZYL0iFtsCbqKr3jEX0QstaY/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-alter-table"
  },
  {
    "id": 115,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "xa-hMF8gku0",
    "title": "An engineering deep-dive into Atlassian's Mega Outage of April 2022",
    "description": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. In this video, we will do an engineering deep dive into this outage trying to understand their engineering systems and practices.\n\nWe extract 6 key insights into how their engineering systems are built, their backup and restoration strategies, and most importantly why is it taking them so long to recover the data.\n\nDisclaimer: I do not have any insider information about this and the views are pure speculation.\n\nOutline:\n\n00:00 Impact of the outage\n03:56 Insight 1: Incremental Backup Strategy\n06:38 Why did the Atlassian outage happen?\n07:30 Insight 2: Progressive Rollout Strategy\n10:57 Insight 3: Soft Deletes vs Hard Deletes\n14:28 Insight 4: Synchronous Replication for High Availability\n17:47 Insight 5: Immutable backups for point-in-time recovery\n21:04 Insight 6: Nearly multi-tenant architecture\n23:30 Why is it taking time for Atlassian to recover the deleted data?\n\nOutage Report: https://www.atlassian.com/engineering/april-2022-outage-update",
    "img": "https://i.ytimg.com/vi/xa-hMF8gku0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NTur7XlVDUdqM/giphy.gif",
    "duration": "31:24",
    "view_count": 4669,
    "like_count": 242,
    "comment_count": 37,
    "released_at": "2022-04-15",
    "gist": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. Let's dissect the outage and understand its nuances of it.\n\nDisclaimer: I do not have any insider information and the views are pure speculation.\n\n## Insight 1: Data loss up to 5 minutes\n\nBecause some customers reported a data loss of up to 5 minutes before the incident, it shows that the persistent backup incrementally every 5 minutes. The backup typically happens through Change Data Capture which operates right over the database.\n\n## Insight 2: Rolling release of products\n\nAtlassian rolls out features to a subset of the users and then incrementally rolls them to others. This strategy of incremental rollout gives companies and teams a chance to test the waters on a subset and then roll out to the rest.\n\n## Insight 3: Mark vs Permanent Deletion\n\nThe script that Atlassian ran to delete had both the options- Mark of Deletion and Permanent deletion.\n\nMark for deletion: is soft delete i.e. marking is_deleted to true.\nPermanent deletion: hard delete i.e. firing DELETE query\n\nWhy do companies need permanent deletion? for compliance because GDPR gives users a Right to be Forgotten\n\n## Insight 4: Synchronous Replication\n\nTo maintain high availability they have synchronous standby replicas which means that the writes happening needs to succeed on both the databases before it is acknowledged back to the user. This ensures that the data is crash-proof.\n\n## Insight 5: Immutable Backups\n\nThe backup is made immutable and stored on S3 in some serialized format. This immutable backup allows Atlassian to recover data at any point in time while being super cost-efficient at the same time.\n\n## Insight 6: Their architecture is not truly a Multi-tenant Architecture\n\nIn a true multi-tenant architecture, every customer gets its fragment of infra- right from DB, to brokers, to servers. But at Atlassian, multiple customers share the same infra components. Companies typically do this to cut down on their infrastructure cost.\n\n## Why is it taking a long time to restore?\n\nBecause data of multiple customers reside in the same database when the DB was backed up the data (rows and tables) were backed up as is; implying that the backup also had data from multiple customers.\n\nNow to restore the intermingled rows of a customer, the entire backup needs to be loaded into a database and then the rows of specific customers need to be restored. This process is extremely time-consuming.",
    "notes_gd": "https://drive.google.com/file/d/1Bp4huBCx-wkG6kCQ5raSujdKID4RLU_y/view?usp=sharing",
    "slug": "an-engineering-deep-dive-into-atlassian-s-mega-outage-of-april-2022"
  },
  {
    "id": 114,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "6oJaZbQKnJE",
    "title": "Dissecting Google Maps Outage: Bad Rollout and Cascading Failures",
    "description": "Google Maps had a global outage on 18th March 2022, during which the end-users were not able to use Directions, Navigation, or Google Maps in general. The outage happened because of a bad rollout, and it lasted more than 2 hours 30 minutes. During the outage, users complained to have seen Gray tiles implying that the map/direction was neither getting initialized nor working.\n\nIn this video, we dissect the Google Maps outage and understand what actually happened, how they mitigated it, and, more importantly, understand ways to prevent such an outage and build a robust way of handling cascading failures.\n\nOutline:\n\n00:00 Introducing the outage and impact\n02:07 Root cause\n08:36 Cascading Failures\n12:22 Remediation\n13:45 Preventive measures\n\nIncident Report: https://issuetracker.google.com/issues/225361510",
    "img": "https://i.ytimg.com/vi/6oJaZbQKnJE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xTiTnyogFXV3Khw6Xe/giphy.gif",
    "duration": "20:17",
    "view_count": 1252,
    "like_count": 78,
    "comment_count": 8,
    "released_at": "2022-04-01",
    "gist": "On the 18th of March, 2022, Google Maps faced a major outage affecting millions of people for a couple of hours. The outage happened due to a bad deployment.\n\nAlthough a bad deployment does not sound too bad, the situation worsens when there are cascading failures if 3 services have a synchronous dependency, forming a chain-like A -> B -> C.\n\nIf A goes down, it will have some impact on B, and if the impact is big enough, we might see B going down as well; and extending it, we might see C going down.\n\nThis is exactly what happened in this Google Outage. Some services had a bad deployment, and they started crashing. Tile Rendering service depended on it, and the Tile rendering service went down because of retries.\n\nThe Direction SDK, Navigation SDK directly invoked the Tile rendering service for rendering the maps, which didn't work, causing a big outage.\n\n## How to remediate a bad deployment?\n\nRollback as soon as possible.\n\n## Preventing cascading outages\n\n- Reject requests when the server is exhausted\n- Tune the performance of the webserver and networking stack of the server\n- Monitor the server resource consumption and set alerts\n- Add circuit breakers wherever possible.\n",
    "notes_gd": "https://drive.google.com/file/d/10yi5K2xluTA9d7RNrorDDKU_-Mi3uHKZ/view?usp=sharing",
    "slug": "dissecting-google-maps-outage-bad-rollout-and-cascading-failures"
  },
  {
    "id": 113,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "ZFRAFTn0cQ0",
    "title": "Dissecting GitHub Outage: ID column reaching the max value 2147483647",
    "description": "GitHub experience an outage on 5th May 2020 on a few of their internal services and it happened because a table had an auto-incrementing integer ID and the column reached its maximum value possible 2147483647. In this video, we dissect what happened, mimic the situation locally and see what could have happened, and look at possible ways to mitigate and prevent a situation like this.\n\nOutline:\n\n00:00 Outage walkthrough\n02:48 Mimicking the situation locally\n10:13 MySQL AUTO_INCREMENT behavior\n12:37 Preventive measures\n14:25 Approach 1 for mitigating the issue\n18:40 Approach 2 for mitigating the issue\n\nReferences:\n - https://github.com/arpitbbhayani/mysql-maxint\n - https://github.blog/2020-07-08-introducing-the-github-availability-report/\n - https://dev.mysql.com/doc/refman/8.0/en/integer-types.html\n - https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html\n - https://www.linkedin.com/pulse/so-you-hit-2147483647-heath-dutton-/\n\nCheck out the free course covering all GitHub outages \u2192  https://courses.arpitbhayani.me/github-outage-dissections/",
    "img": "https://i.ytimg.com/vi/ZFRAFTn0cQ0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/UDU4oUJIHDJgQ/giphy.gif",
    "duration": "24:34",
    "view_count": 1910,
    "like_count": 158,
    "comment_count": 33,
    "released_at": "2022-03-23",
    "gist": "On the 5th of May, 2020, GitHub experienced an outage because of this very reason. One of their shared table having an auto-incrementing ID column hits its max limit. Let's see what could have been done in such a situation.\n\n## What's the next value after MAX int?\n\nGitHub used 4 bytes signed integer as their ID column, which means the value can go from `-2147483648` to `2147483647`. So, now when the ID column hits `2147483647` and tries to get the next value, it gets the same value again, i.e., `2147483647`.\n\nFor MySQL, `2147483647` + `1` = `2147483647`\n\nSo, when it tries to insert the row with ID `2147483647`, it gets the *Duplicate Key Error* given that a row already exists with the same ID.\n\n## How to mitigate the issue?\n\nA situation like this is extremely critical given that the database is not allowing us to insert any row in the table. This typically results in a major downtime of a few hours, and it depends on the amount of data in the table. There are a couple of ways to mitigate the issue.\n\n### Approach 1: Alter the table and increase the width of the column\n\nQuickly fire the `ALTER` table and change the data type of the ID column to `UNSIGNED INT` or `BIGINT`. Depending on the data size, an ALTER query like this will take a few hours to a few days to execute. Hence this approach is suitable only when the table size is small.\n\n### Approach 2: Swap the table\n\nThe idea here is to create an empty table with the same schema but a larger ID range that starts from `2147483648`. Then rename this new table to the old one and start accepting writes. Then slowly migrate the data from the old table to this new one. This approach can be used when you can live without the data for a few days.\n\n### Get warned before the storm\n\nAlthough mitigation is great, it is better to place a monitoring system that raises an alert when the ID reaches 70% of its range. So, write a simple DB monitoring service that periodically checks this by firing a query on the database.",
    "notes_gd": "https://drive.google.com/file/d/13rNEWXwIdNkNcP2gSQQvBF8czUBpF47y/view?usp=sharing",
    "slug": "dissecting-github-outage-id-column-reaching-the-max-value-2147483647"
  },
  {
    "id": 112,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "h5hXivWws7k",
    "title": "Dissecting Spotify's Global Outage - March 8, 2022",
    "description": "Incident Report: Spotify Outage on March 8: https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/\nGoogle Cloud Traffic Director Outage: https://status.cloud.google.com/incidents/LuGcJVjNTeC5Sb9pSJ9o\nJava gRPC Client Bug: https://github.com/grpc/grpc-java/issues/8950\n\nIn this video, we dissect the reasons behind Spotify's Global Outage and try to understand its architecture, and learn from critical things to remember while architecting a system.",
    "img": "https://i.ytimg.com/vi/h5hXivWws7k/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26FKWlRgIL3JOIkGA/giphy.gif",
    "duration": "26:",
    "view_count": 3184,
    "like_count": 190,
    "comment_count": 32,
    "released_at": "2022-03-12",
    "gist": "",
    "notes_gd": "",
    "slug": "dissecting-spotify-s-global-outage-march-8-2022"
  }
]