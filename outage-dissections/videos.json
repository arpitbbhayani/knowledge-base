[
  {
    "id": 150,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "df2QgLW0QC4",
    "title": "Dissecting GitHub Outage Downtime due to creating an Index",
    "description": "GitHub wanted to optimize their SQL query performance, and they had to reverse a database index. Instead of getting a performance boost, thy incurred a downtime of more than 60 minutes. Imagine the state of the team who wanted to do good, but were stuck in this fix.\n\nThis outage gives us a super-in-depth insight into MySQL and its indexing structure. It is indeed fascinating to see the kind of optimizations engineers have to make while operating at scale.\n\nIn this video, let's dissect this outage and understand what happened, why GitHub had to flip their index, how things went wrong and affected end users like us, and conclude with 3 key takeaways from this outage.\n\nOutline:\n\n00:00 Agenda\n02:47 What happened?\n03:52 Need of Reversing an Index\n07:21 What could go wrong during and after Indexing?\n14:52 Cascading Failures\n17:16 Mitigation and Key Takeaways",
    "img": "https://i.ytimg.com/vi/df2QgLW0QC4/mqdefault.jpg",
    "gif": null,
    "duration": "22:49",
    "view_count": 312,
    "like_count": 21,
    "comment_count": 12,
    "released_at": "2022-06-06",
    "gist": "",
    "notes_gd": "",
    "slug": "dissecting-github-outage-downtime-due-to-creating-an-index"
  },
  {
    "id": 149,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "48YZzGi7QMk",
    "title": "Dissecting GitHub Outage - Repository Creation Failed",
    "description": "Imagine you trying to create a new GitHub repository and it call is failing, failing for 53 minutes. This happened with GitHub in April 2021 when for 53 minutes people were unable to create any new repositories. Upon investigation, they found out that the root was scanning secrets. Two seemingly different usecases took down one of the most important APIs.\n\nThis has to be one of the most amusing outages that I have seen in recent times.  In this video, we dissect this outage, understand the root cause of it, look at the importance of secret scanning, and conclude with an understanding of their mitigation process.\n\nOutline:\n\n00:00 Agenda\n02:49 What happened?\n03:20 Secret scanning and its importance\n08:28 Repository Creation Flow\n09:15 What lead to an outage?\n17:26 Outage mitigation",
    "img": "https://i.ytimg.com/vi/48YZzGi7QMk/mqdefault.jpg",
    "gif": null,
    "duration": "20:59",
    "view_count": 348,
    "like_count": 18,
    "comment_count": 4,
    "released_at": "2022-06-03",
    "gist": "Just imagine you trying to create a repository on GitHub and it is not working, and this happened to GitHub in April 2021 when their users were not able to create a new repository.\n\nThe root cause for this outage was something that seems unrelated - Scanning Secrets. The root cause makes this outage super interesting to dissect.\n\n## What is Secret Scanning?\n\nOur API servers need to talk to peripheral components like Databases, Cache, and some SaaS services, etc. This communication involves some sort of authentication and authorization through auth token, passwords, or secret keys.\n\nDevelopers tend to commit the secrets in the settings/constant files and push them to GitHub. What if the repository content gets leaked? What if GitHub itself has a data breach and the attacker gets access to the private repositories?\n\nIf the secrets like AWS access keys, auth tokens, and DB passwords are leaked and the attacker can then get the dump of the data and ask for a ransom. Or they may even abuse the infrastructure to perform some illegal activities or mine cryptocurrencies.\n\nHence, GitHub periodically runs a job that checks all the repositories for any secrets that are committed and warns the user about it.\n\n## Repository Creation Flow\n\nWhen a repository is first created an entry is made into Secret Scanning table which is then used by a job that scans for potential secrets and notifies the owner.\n\n## What led to the outage?\n\nGitHub team ran a data migration in which they moved the Secret Scanning table from a common database to its own cluster allowing it to scale independently.\n\nGitHub team was unaware about this dependency! and hence after the migration of table happened to the different database the creation of new repository started failing leading to this outage. It is interesting to see such mature products having blindspots.\n\n## How did GitHub mitigate it?\n\nThe mitigation strategy of GitHub was to rollback the migration. Althrough it is unclear from the incident report on what exactly they did but there are a few speculations\n\n1. they could have recopied the table quickly to old database\n2. whitelisted the database so that applications could connect\n3. old table would have been intact and hence they would have just renamed and made it active again.\n\nAgain, it is pure speculation given we do not have any insider information nor they specified in the report. It would have been fun to have gone through their actual mitigation steps. We could have learned so much, but nonetheless we did learn a few interesting insights from this outage.",
    "notes_gd": "https://drive.google.com/file/d/1KtNWAH5YHC9-qVxvbxLTXF3u5yGV7l-t/view?usp=sharing",
    "slug": "dissecting-github-outage-repository-creation-failed"
  },
  {
    "id": 144,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "iqapVyfoFqc",
    "title": "Dissecting GitHub Outage: Downtime due to an Edge Case",
    "description": "In August 2021, GitHub experienced an outage where their MySQL Master database went into a degraded state. Upon investigation, they found out it was due to an edge case. So, how can an edge case take down a database?\n\nIn this video, we understand what happened, take a fictional example of how an edge case can put extra load on the database, and conclude with an exciting way to make our SQL queries fool-proof.\n\nOutline:\n\n00:00 Agenda\n00:27 Outage Overview\n07:38 What could have happened?\n08:51 A fictional example of an edge case taking down DB\n17:18 How to avoid edge cases on SQL queries",
    "img": "https://i.ytimg.com/vi/iqapVyfoFqc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/243UHkbIOZJGj7FZ84/giphy.gif",
    "duration": "21:32",
    "view_count": 1048,
    "like_count": 51,
    "comment_count": 9,
    "released_at": "2022-05-23",
    "gist": "An edge case took down GitHub \ud83e\udd2f\n\nGitHub experienced an outage where their MySQL database went into a degraded state. Upon investigation, it was found out that the outage happened because of an edge case. So, how can an edge case take down a database?\n\n## What happened?\n\nThe outage happened because of an edge case which lead to the generation of an inefficient SQL query that was executed very frequently on the database. The database was thus put under a massive load which eventually made it crash leading to an outage.\n\n### Could retry have helped?\n\nAutomatic retries always help in recovering from a transient issue. During this outage, retries made things worse. Automatic retries added the load on the database that was already under stress.\n\n## Fictional Example\n\nNow, we take a look at a fictional example where an edge case could potentially take down a DB.\n\nSay, we have an API that returns the number of commits made by a user in the last `n` days. The way, this API could be implemented is to get the `start_date` as an integer through the query parameter, and the API server could then fire a SQL query like\n\n```\nSELECT count(id) FROM commits\nWHERE user_id = 123 AND start_time > start_time\n```\n\nIn order to fire the query, we convert the string `start_time` to an integer, create the query, and then fire it. In the regular case, we get the correct input and then compute the number of commits and respond.\n\nBut as an edge case, what if we do not get the query parameter or we get a non-integer value; then depending on the language at hand we may actually use the default integer value like `0` as our `start_time`.\n\nThere is a very high chance of this happening when we are using Golang which uses `0` as the default integer value. In such a case, the query that gets executed would be\n\n```\nSELECT count(id) FROM commits\nWHERE user_id = 123 AND start_time > 0\n```\n\n  \nThe above query when executed iterates through all the rows of the table for a particular user, instead of the rows for the last 7 days; making it super inefficient and expensive. The above query would put a huge load on the database and a frequent invocation can actually take down the entire database.\n\n### Ways to avoid such situations\n\n1. Always sanitize the input before executing the query\n2. Put guard rails that prevent you from iterating the entire table. For example: putting `LIMIT 1000` would have made you iterate over 1000 rows in the worst case.",
    "notes_gd": "https://drive.google.com/file/d/1PI302Cvutu8LJKmZu7fiIVOAqRhiSMek/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-an-edge-case"
  },
  {
    "id": 138,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "82Xywy74kfE",
    "title": "Dissecting GitHub Outage - Downtime due to ALTER TABLE",
    "description": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nGitHub had a major outage and it all started with a schema migration. The outage affected their core services like GitHub actions, API requests, pull requests, and many more. Today, we dissect this outage and do an intense deep dive to extract 5 amazing insights. We also see how they very smartly mitigated the outage along with a potential long-term fix. \n\nOutline:\n\n00:00 Agenda\n02:57 Introduction\n03:23 Insight 1: Schema Migrations can take weeks to complete\n05:48 Insight 2: How schema are altered when the table is huge\n08:52 Insight 3: Deadlocks on Read Replicas\n11:20 Insight 4: Separate Replica fleet for internal read traffic\n13:59 Insight 5: Database failures cascade\n18:13 Mitigation Strategy\n29:28 Lont-term Fix\n\nOutage report: https://github.blog/2021-12-01-github-availability-report-november-2021/",
    "img": "https://i.ytimg.com/vi/82Xywy74kfE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/nrXif9YExO9EI/giphy.gif",
    "duration": "36:44",
    "view_count": 1745,
    "like_count": 92,
    "comment_count": 22,
    "released_at": "2022-05-09",
    "gist": "Can an ALTER TABLE command take down your production? \ud83e\udd2f\n\nIt happened to GitHub on 27th November 2021 when most of their services were down because of a large schema migration.\n\n## Why did the outage happen?\n\nGitHub ran a migration on a massive MySQL table and it made their replicas enter deadlock and crash. Here are the 5 insights about their architecture\n\n### Insight 1: Schema migration can take weeks to complete\n\nSchema migrations are intense operations as in most cases require you to copy the entire table with the new schema. Hence it might take a schema migration weeks to complete.\n\n### Insight 2: The last step of migration is RENAME\n\nTo protect the database from not taking excessive locks during the migration, we create an empty ghost table from the main table, apply migration, copy the data, and then rename the table. This reduces the locks we need for the migration.\n\n### Insight 3: Read Replicas can have deadlocks\n\nThe writes happening through the replication job and the production read load can create deadlocks on the read replicas.\n\n### Insight 4: Have a separate fleet of replicas for internal traffic\n\nHave a separate fleet of Read Replicas for internal workflows ex: analytics, etc. This way, any internal load will not affect the production load.\n\n### Insight 5: Database failures cascade\n\nWhen a replica fails, the load on healthy one's increases; which may them down and hence the cascading effect.\n\n## Mitigation\n\nThe outage happened because there were not enough read replicas to handle the load, hence in order to mitigate it the way out was to add more read replicas. GitHub team very smartly promoted the replicas used for internal workloads to handle production.\n\nAlthough the move was smart, it did not mitigate the outage because the incoming load was so high that the new replicas added also started crashing.\n\n### Data Integrity over Availability\n\nTo ensure that the data integrity is not compromised because of repeated crashes, GitHub took a call and let the Read traffic fail. They took the replica out of the production fleet, gave it time to complete the migration, and then added it back.\n\nThis way all the replicas got the time they needed to complete the schema migration. It took some but the issue was completely mitigated.\n\n## Long-term fix\n\nVertical Partitioning is a long-term fix for this problem. The idea is to create smaller databases that hold related tables; ex: all tables related to Repositories can go in one DB. This allows migration to quickly complete and during an outage, only the involved functionalities will be affected.",
    "notes_gd": "https://drive.google.com/file/d/14jdP8o2wFZYL0iFtsCbqKr3jEX0QstaY/view?usp=sharing",
    "slug": "dissecting-github-outage-downtime-due-to-alter-table"
  },
  {
    "id": 115,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "xa-hMF8gku0",
    "title": "An engineering deep-dive into Atlassian's Mega Outage of April 2022",
    "description": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. In this video, we will do an engineering deep dive into this outage trying to understand their engineering systems and practices.\n\nWe extract 6 key insights into how their engineering systems are built, their backup and restoration strategies, and most importantly why is it taking them so long to recover the data.\n\nDisclaimer: I do not have any insider information about this and the views are pure speculation.\n\nOutline:\n\n00:00 Impact of the outage\n03:56 Insight 1: Incremental Backup Strategy\n06:38 Why did the Atlassian outage happen?\n07:30 Insight 2: Progressive Rollout Strategy\n10:57 Insight 3: Soft Deletes vs Hard Deletes\n14:28 Insight 4: Synchronous Replication for High Availability\n17:47 Insight 5: Immutable backups for point-in-time recovery\n21:04 Insight 6: Nearly multi-tenant architecture\n23:30 Why is it taking time for Atlassian to recover the deleted data?\n\nOutage Report: https://www.atlassian.com/engineering/april-2022-outage-update",
    "img": "https://i.ytimg.com/vi/xa-hMF8gku0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/NTur7XlVDUdqM/giphy.gif",
    "duration": "31:24",
    "view_count": 4412,
    "like_count": 231,
    "comment_count": 36,
    "released_at": "2022-04-15",
    "gist": "In April 2022, Atlassian suffered a major outage where they \"permanently\" deleted the data for 400 of their paying cloud customers, and will take them weeks to recover the data. Let's dissect the outage and understand its nuances of it.\n\nDisclaimer: I do not have any insider information and the views are pure speculation.\n\n## Insight 1: Data loss up to 5 minutes\n\nBecause some customers reported a data loss of up to 5 minutes before the incident, it shows that the persistent backup incrementally every 5 minutes. The backup typically happens through Change Data Capture which operates right over the database.\n\n## Insight 2: Rolling release of products\n\nAtlassian rolls out features to a subset of the users and then incrementally rolls them to others. This strategy of incremental rollout gives companies and teams a chance to test the waters on a subset and then roll out to the rest.\n\n## Insight 3: Mark vs Permanent Deletion\n\nThe script that Atlassian ran to delete had both the options- Mark of Deletion and Permanent deletion.\n\nMark for deletion: is soft delete i.e. marking is_deleted to true.\nPermanent deletion: hard delete i.e. firing DELETE query\n\nWhy do companies need permanent deletion? for compliance because GDPR gives users a Right to be Forgotten\n\n## Insight 4: Synchronous Replication\n\nTo maintain high availability they have synchronous standby replicas which means that the writes happening needs to succeed on both the databases before it is acknowledged back to the user. This ensures that the data is crash-proof.\n\n## Insight 5: Immutable Backups\n\nThe backup is made immutable and stored on S3 in some serialized format. This immutable backup allows Atlassian to recover data at any point in time while being super cost-efficient at the same time.\n\n## Insight 6: Their architecture is not truly a Multi-tenant Architecture\n\nIn a true multi-tenant architecture, every customer gets its fragment of infra- right from DB, to brokers, to servers. But at Atlassian, multiple customers share the same infra components. Companies typically do this to cut down on their infrastructure cost.\n\n## Why is it taking a long time to restore?\n\nBecause data of multiple customers reside in the same database when the DB was backed up the data (rows and tables) were backed up as is; implying that the backup also had data from multiple customers.\n\nNow to restore the intermingled rows of a customer, the entire backup needs to be loaded into a database and then the rows of specific customers need to be restored. This process is extremely time-consuming.",
    "notes_gd": "https://drive.google.com/file/d/1Bp4huBCx-wkG6kCQ5raSujdKID4RLU_y/view?usp=sharing",
    "slug": "an-engineering-deep-dive-into-atlassian-s-mega-outage-of-april-2022"
  },
  {
    "id": 114,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "6oJaZbQKnJE",
    "title": "Dissecting Google Maps Outage: Bad Rollout and Cascading Failures",
    "description": "Google Maps had a global outage on 18th March 2022, during which the end-users were not able to use Directions, Navigation, or Google Maps in general. The outage happened because of a bad rollout, and it lasted more than 2 hours 30 minutes. During the outage, users complained to have seen Gray tiles implying that the map/direction was neither getting initialized nor working.\n\nIn this video, we dissect the Google Maps outage and understand what actually happened, how they mitigated it, and, more importantly, understand ways to prevent such an outage and build a robust way of handling cascading failures.\n\nOutline:\n\n00:00 Introducing the outage and impact\n02:07 Root cause\n08:36 Cascading Failures\n12:22 Remediation\n13:45 Preventive measures\n\nIncident Report: https://issuetracker.google.com/issues/225361510",
    "img": "https://i.ytimg.com/vi/6oJaZbQKnJE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xTiTnyogFXV3Khw6Xe/giphy.gif",
    "duration": "20:17",
    "view_count": 1131,
    "like_count": 72,
    "comment_count": 8,
    "released_at": "2022-04-01",
    "gist": "On the 18th of March, 2022, Google Maps faced a major outage affecting millions of people for a couple of hours. The outage happened due to a bad deployment.\n\nAlthough a bad deployment does not sound too bad, the situation worsens when there are cascading failures if 3 services have a synchronous dependency, forming a chain-like A -> B -> C.\n\nIf A goes down, it will have some impact on B, and if the impact is big enough, we might see B going down as well; and extending it, we might see C going down.\n\nThis is exactly what happened in this Google Outage. Some services had a bad deployment, and they started crashing. Tile Rendering service depended on it, and the Tile rendering service went down because of retries.\n\nThe Direction SDK, Navigation SDK directly invoked the Tile rendering service for rendering the maps, which didn't work, causing a big outage.\n\n## How to remediate a bad deployment?\n\nRollback as soon as possible.\n\n## Preventing cascading outages\n\n- Reject requests when the server is exhausted\n- Tune the performance of the webserver and networking stack of the server\n- Monitor the server resource consumption and set alerts\n- Add circuit breakers wherever possible.\n",
    "notes_gd": "https://drive.google.com/file/d/10yi5K2xluTA9d7RNrorDDKU_-Mi3uHKZ/view?usp=sharing",
    "slug": "dissecting-google-maps-outage-bad-rollout-and-cascading-failures"
  },
  {
    "id": 113,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "ZFRAFTn0cQ0",
    "title": "Dissecting GitHub Outage: ID column reaching the max value 2147483647",
    "description": "GitHub experience an outage on 5th May 2020 on a few of their internal services and it happened because a table had an auto-incrementing integer ID and the column reached its maximum value possible 2147483647. In this video, we dissect what happened, mimic the situation locally and see what could have happened, and look at possible ways to mitigate and prevent a situation like this.\n\nOutline:\n\n00:00 Outage walkthrough\n02:48 Mimicking the situation locally\n10:13 MySQL AUTO_INCREMENT behavior\n12:37 Preventive measures\n14:25 Approach 1 for mitigating the issue\n18:40 Approach 2 for mitigating the issue\n\nReferences:\n - https://github.com/arpitbbhayani/mysql-maxint\n - https://github.blog/2020-07-08-introducing-the-github-availability-report/\n - https://dev.mysql.com/doc/refman/8.0/en/integer-types.html\n - https://dev.mysql.com/doc/refman/8.0/en/example-auto-increment.html\n - https://www.linkedin.com/pulse/so-you-hit-2147483647-heath-dutton-/",
    "img": "https://i.ytimg.com/vi/ZFRAFTn0cQ0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/UDU4oUJIHDJgQ/giphy.gif",
    "duration": "24:34",
    "view_count": 1739,
    "like_count": 147,
    "comment_count": 32,
    "released_at": "2022-03-23",
    "gist": "On the 5th of May, 2020, GitHub experienced an outage because of this very reason. One of their shared table having an auto-incrementing ID column hits its max limit. Let's see what could have been done in such a situation.\n\n## What's the next value after MAX int?\n\nGitHub used 4 bytes signed integer as their ID column, which means the value can go from `-2147483648` to `2147483647`. So, now when the ID column hits `2147483647` and tries to get the next value, it gets the same value again, i.e., `2147483647`.\n\nFor MySQL, `2147483647` + `1` = `2147483647`\n\nSo, when it tries to insert the row with ID `2147483647`, it gets the *Duplicate Key Error* given that a row already exists with the same ID.\n\n## How to mitigate the issue?\n\nA situation like this is extremely critical given that the database is not allowing us to insert any row in the table. This typically results in a major downtime of a few hours, and it depends on the amount of data in the table. There are a couple of ways to mitigate the issue.\n\n### Approach 1: Alter the table and increase the width of the column\n\nQuickly fire the `ALTER` table and change the data type of the ID column to `UNSIGNED INT` or `BIGINT`. Depending on the data size, an ALTER query like this will take a few hours to a few days to execute. Hence this approach is suitable only when the table size is small.\n\n### Approach 2: Swap the table\n\nThe idea here is to create an empty table with the same schema but a larger ID range that starts from `2147483648`. Then rename this new table to the old one and start accepting writes. Then slowly migrate the data from the old table to this new one. This approach can be used when you can live without the data for a few days.\n\n### Get warned before the storm\n\nAlthough mitigation is great, it is better to place a monitoring system that raises an alert when the ID reaches 70% of its range. So, write a simple DB monitoring service that periodically checks this by firing a query on the database.",
    "notes_gd": "https://drive.google.com/file/d/13rNEWXwIdNkNcP2gSQQvBF8czUBpF47y/view?usp=sharing",
    "slug": "dissecting-github-outage-id-column-reaching-the-max-value-2147483647"
  },
  {
    "id": 112,
    "topic": {
      "id": 0,
      "uid": "outage-dissections",
      "name": "Outage Dissections",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT3_Z97svMs6S2y7tv1PcUmc",
      "bgcolor": "#FFF1E7",
      "themecolor": "#FF8229"
    },
    "yt_video_id": "h5hXivWws7k",
    "title": "Dissecting Spotify's Global Outage - March 8, 2022",
    "description": "Incident Report: Spotify Outage on March 8: https://engineering.atspotify.com/2022/03/incident-report-spotify-outage-on-march-8/\nGoogle Cloud Traffic Director Outage: https://status.cloud.google.com/incidents/LuGcJVjNTeC5Sb9pSJ9o\nJava gRPC Client Bug: https://github.com/grpc/grpc-java/issues/8950\n\nIn this video, we dissect the reasons behind Spotify's Global Outage and try to understand its architecture, and learn from critical things to remember while architecting a system.",
    "img": "https://i.ytimg.com/vi/h5hXivWws7k/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26FKWlRgIL3JOIkGA/giphy.gif",
    "duration": "26:",
    "view_count": 2914,
    "like_count": 175,
    "comment_count": 29,
    "released_at": "2022-03-12",
    "gist": "",
    "notes_gd": "",
    "slug": "dissecting-spotify-s-global-outage-march-8-2022"
  }
]