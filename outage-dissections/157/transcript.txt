so github team was switching their master database from one node to another while doing this something went wrong and the new database crashed this led to data divergence and a production incident that lasted over five hours so what happened what was this incident all about how they mitigated it in this video we dissect this incident and understand what happens when a company announces a planned maintenance window what do companies do during that planned maintenance what went wrong with github how they sold it and ran some really cool things really cool things about switching databases and solving data divergence problem but before we move forward i'd like to talk to you about a course on system design that i have been running for over a year now the course is a cohort based course which means i won't be rambling a solution and it will not be a monologue instead a small focused group of 50 60 engineers every cohort will be brainstorming systems and designing it together this way we build a solid system and learn from each other's experiences the course to date is enrolled by 600 plus engineers spanning 9 cohorts and 10 countries engineers from companies like google microsoft github slack facebook tesla yelp flipkart dream 11 and many many many more have taken this course and have some wonderful things to say the coolest part about the course is the depth we go into and the breadth we cover we cover topics ranging from real-time text communication for slack to designing our own toy load balancer to creek buses live text commentary to doing impressions counting at scale for any advertisement business in all we would cover roughly 28 questions and the detailed curriculum split week by week can be found on the course page which is linked in the description down below so if you're looking to learn system design from the first principles you will love this course i have two offerings for you the first one is the live cohort business which you see on the left side and the second one is the recorded course which you can see on the right side the live cover based course happens every two months and it will go on for eight weeks while the recorded course contains the recordings from one of the past cohorts as is if you are in a hurry and want to binge learn system design i would highly recommend you going for the recorded one otherwise the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings the decision is totally up to you the course details prerequisites testimonials can be found on the course page at pittsby dot me slash master class and i would highly recommend you to check that out i put the link of the course in the description down below so if you are interested to learn system design go for it check out the link in the description down below and i hope to see you in my next cohort thanks so like always we start with the incident report and here it goes instead of reading it from the top this time we started from the end what it says for a period of approximately 5 hours users may have observed delays before data returned to their affected database cluster were visible in the web interface and api what this tells us this tells us a very interesting thing this tells us that user observing delays which means not data loss but delays in data being visible on the interface or api after it is being written in through their database this clearly shows this clearly shows that the reads are going somewhere else and the writes are going somewhere else so this indicates us this indicates us about having a master replica set setup in which the reads are going to read replica while the rights are going to master and there is some sort of asynchronous replication happening between them right so given that the data was written successfully it implies that master did not have any problem the the rights were getting accepted but there was a lag there was a lag from the time the write happened and if the user wanted to see those changes on the ui it took delays they did not mention how long it took but it took some delay a minute or two or five minutes it took some delay in a user seeing those same changes on their web interface or their apk so basically this typically tells us that the place we store is different from the place we read from so that's why you typically have a master replica set up where the rights go to the master and the reads go to the replica right and this is typically done by the organization in order to handle a large read load right and this is a very very very common uh scaling strategy that everyone adopts right so now let's move to the next part what they say is will now will start from top so during so we understood that the total outage or the or the total incident took about five hours we will circle back on why it took five hours right but now what we see is what exactly happened with github so what did they what they say is during a planned maintenance operation failing over a mysql primary instance we experienced a noble crash in mysql d process right we'll talk about it exactly this right okay hear me out so we always see on twitter where companies announce or they even drop you an email they say that hey we are going for a planned maintenance so what happens during plant maintenance so plan maintenance is of time window in which company tells its customer that hey we will be unavailable might be we will be unavailable during this short period of time let's say it's 10 minutes 50 minutes so during that time the site will be non-operational so site will be down no one would be able to do anything on that no reads no rights nothing right no one would be able to access application nothing nothing nothing it is all stop the world kind of things but first of all why do companies have to do that won't won't this uh uh wonders impact their customers it would won't this affect their revenue it would but then why do companies have to do it there are several reasons so here we talk specifically about database maintenance window so what happens during this database maintenance window so companies have to reboot their databases due to n number of reasons listing out five very popular reasons first is applying security patches right so database is also a piece of software it will also have bugs so typically your database vendors like mysql postgres they give you security patches so as in they do their version upgrade their minor version upgrade and they apply security patches and they request us to upgrade the software on our database where we have hosted right now when we want to install it would also have to refresh or we would also have to restart the process so during that restart during that restart the rights would fail the system would not work so that is where by taking a planned uh maintenance window during that maintenance window all of this thing is done so applying security patches to your database second is version upgrades apart from security patches there are other reasons why you might have to do version upgrades of your database maybe youtube later may be due to better performance you might see a better performance on a good version of your database so or on a higher version of your database maybe some optimization that the database vendors have rolled out so you might have to do version upgrade data as well third is parameter tuning so let's say you want to alter some parameter tune let's say you want to alter the cache configuration of your database so that it is performing faster let's say you want to change switch from one index type to another index type right so when you do that it is not an on the fly operation you can change the parameter on the fly but for it to take its effect your database had to reboot right so that can also be done in this plan maintenance window another reason is hardware replacement for example you want to replace the underlying hardware typically your cloud provider wants to replace an underlying hardware so it leverages that maintenance window in order to replace the underlying hardware right and this typically done because hardware goes through uh basically wear and tear and it would like s type we also see our laptops when we purchase a new laptop it works really well and then the performance starts to decline same thing happens with any hardware in the world even with the cloud provider so their servers their hardwares also go through wear and tear and that's where the cloud providers would have to move the database from one node to another and in general you might want to do periodic reboots just to ensure that in case there are any memory leaks into your database they are not eating up a lot of stuff typically this does not happen but doing a periodic reboot solves a lot of problem right so this is why companies do plan maintenance right so now we talk about we like in this uh plan maintenance window what github did is they switched their primary database from one machine to another so when i say that there is a plan maintenance under the window of let's say 10 minutes this does not mean for the entire 10 minutes of duration your website will be unavailable every company tries to minimize the plan matters window so they ask their customer for a big enough time but they try to wrap their thing in in a very small fraction of that time right so here what github did is this flipped their master database because they had to do some maintenance on the old one right standard procedure so what they do is uh they keep a second instance handy right and with just a configuration so they have an old master they bring up a new master exactly same as the old master with no data loss or with with with basically complete data being in sync and then with birth configuration change they flip from master one to master two or old from from old to new right and for this very short duration when this conflict flip is happening your database would become unavailable but nothing more nothing else right so this is all about plan maintenance window but then something interesting happened there so what github says is we experienced a novel crash in my sequel d process on the newly promoted my sequel primary server so when they were switching the master from old to new what happened is as soon as the new master got the traffic it crashed it was unable to process it it may be because of high load it may be because of misconfiguration maybe because of n number of reasons but the thing is the mysql d process crashed the normal mysql process that we even run on our server on our on our local machine that crashed which means they flipped the traffic to the new master and it crashed so what happened what what what is the problem in that so if that master crashed we already had an old instance right because we already had it because as soon as we flipped as soon as we flipped it to the new master it crashed so we can just flip it back and that's exactly what github update so what github says is to mitigate the impact of the crash we manually redirected traffic back to the original primary obviously make sense you already have the database nd which has all the data you just flip it back and it would work and that's pretty smart idea right you already have the database and you do it so they did exactly that as soon as this crashed this flipped out the traffic to the old master and it should have worked and it did but there is a catch the catch is very interesting the crash to my sequel primary had already served approximately six seconds of traffic specifically right traffic this is the problem now let's understand what what would have happened so here when they flipped from old master to new master that database worked for six seconds before it crashed so for six seconds the database took in all the rights because master is handling all the rights so all the right traffic that went to the new master got accepted so rights were successful right and then it crashed now if github flips from new database from new master to old master again what about the rights that happened during that six seconds what would be well that data is gone then right so then this is that that user saw right getting successful but your database does not have that data because you flipped it to the old database old database does not have it so some writes that went to the new database and we switched back what would happen then so just a small visual way to look at this is you have two masters a blue and a green all the old rights were anyway went to the blue master which is the old master then we did a failover then some rights happen on the new master right and then the machine crashed when the crash happened the team flipped to the old master so the newer rights are now going to old master so what about the rights that went to the new master we want these rights to be there in double master otherwise the user will say hey i saved it but it is not there or it might worse it might also create some sort of consistency issues right for eventually consistent system but this is very challenging only for that six seconds of time that it accepted the rights you have to do a lot of engineering to solve that problem so the current state of github so now in their outage report or in the incident report they have not mentioned how they solve it but let's dive deep and see if you ever come into this situation how would you solve it right okay so the current situation that we are in right now is we have two master nodes one old and one new newer rights are going to old master because your new master crashed after accepting the rights for six seconds right the new master is there the old master is there the newer rights are going to old master right and the old data is intact so all we have to do is whatever the new data was written in the uh sorry whatever the rights were written in the new database or the new master needs to be picked up and put it into old database but how do we do it if we if we have hundreds of people we cannot go to hundreds of people and find what was the basically what's the delta what to pick and what to put there that would be very slow and so what do we do it so this is where uh right ahead logging of database has come into the picture because my uh because github is using mysql these are called bin lock coordinates so what happens anytime and every time you fire a query you fire a query onto a database it is first logged into a bin log file or a write ahead log file which has some location called bin law coordinates and it has all the queries that you ever fired not read queries but update queries so update in update delete alter schema all of those queries are locked into a bin log file right so whenever always remember whenever companies do a failover from one node to another node any time and every time what they do is they keep a track of the bin lock coordinate that hey exactly after this bin log coordinate or exactly after this statement we did a failover they keep a track of this so they know exactly at which point they flipped the database so what would have happened is the new master when they spun up from the old master they would have taken the snapshot of the old then they spun up the new then they would have enabled the replication so that both of the data comes in sync and then they would have done the configuration switch right so while doing that configuration switch they would have kept a check at this point we are doing the switch so this is the bin lock coordinate and this is where i'm doing the switch so all the blue lines that you see are the old bin log entries and the green one are the new bin log entries these is there in the new master right this all this particular bin lock file is present in the new master that we have now what we have to do over here is because bin log has all the updates uh all the update statements that have been fired updates inserts deletes and all of that what we have to do is we have to pick all the updates that went into the bin lock file after that coordinator after the failover happened from the new database or from the new master and applied to the old master this is exactly how company does it and my guess is is exactly how github would have done it because they move the traffic back to the old master they want to ensure that the old data that is there or sorry the rides that went to the new master for that six seconds needs to be copied into the old one right because new master is not is not accepting any rights as of now the last n bin log statements would be the one that were for the new writes so it just have to start iterating from the bin law coordinate where the failover happened and read it to the end of the file and apply to the old database this way the new chunk of data that was pushed into the new master for six seconds would come to the old master right and this is exactly how companies do it bin logs is like bin log makes our life so simple right so right ahead logging very important for database bin logs very important for database and this is exactly how companies mitigate such situation right okay now let's move but if you remember github said that hey they had an incident blasted for five hours this seems like one second two second thing what what is this five hours all about so what typically happens is what github said well let's start with that and then i'll explain what exactly happened so at this point so when they switch back to the old master at this point a restore of replicas from the new primary was initiated which took approximately four hours with a further r for cluster reconfiguration to enable full read capacity i'll explain right so what happened whenever a failover happens like this or some outage happens like this right most companies they want to give it a fresh start right so we create a fresh set of replicas from the master that is now accepting the rights and rotate all other replicas so this way what happens is to give this a fresh start from the master that is currently serving we spin up new read replicas and slowly transfer the traffic to this new replicas right so now what happens is while doing this spinning up when you read replica takes time given the amount of data github has it takes very long time right so creating a new replica and obviously they don't just have one read replica they would have tens and 15 separate replicas so speeding up those many replicas takes time and some time to wait to do cluster configuration as in hey these are the set of read replicas these are right replicas let me go and change into my configuration about hey talk to this read replica and not talk to that other read replica and what not so these sort of changes require them one hour so creating read replicas and configuring it took them total five hours there was a scope of automation and they agreed on that and they uh acknowledged that hey we should have automated a bunch of stuff there but still configuring them takes time so it took them roughly five months to do that so during that five hours it was not entire site down it was just that reads were getting delayed and the reason reads were getting delayed or the cons or the cons or the rights that they made were getting delayed to be reflected on the ui this only this much thing happened and this purely happened because it took time for them to spin upright replicas some reads were going to old ones slowly the data was getting replicated and over that five hours they required that to went to full rate capacity right so partial reads were served for some users the replication lag would be just one minute for some it would be two minutes for some it would be five minutes but it was not that i would not see my rights for five hours or something right it took them five hours to get rid of all replication lag and go to full read capacity right so yeah that's the incident so whenever company says incident it does not typically imply the complete outage but more like hey something went wrong we acknowledge it and this is what we learned from it and would ensure that we are not making the same mistakes again so nice ah that's it that's it for this incident uh i hope you learned something new that was uh specifically about data diversity how to solve it if you're really very curious i would highly highly highly encourage you to read about bin lock bin lock coordinates how synchronous and asynchronous replication happens very simple my sequel documentation it's superb uh go go through that five six six sequel query that you have to fire and you are sorted for life right go through that try that out highly highly highly encourage you to do it so nice that's it that's it folks for this video uh if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sub i post three in-depth engineering videos every week and i'll see in the next one thanks a ton [Music] you