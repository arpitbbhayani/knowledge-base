so in august 2021 github experienced an outage where their mysql database went into a degraded state upon investigation they found out that it was due to an edge case so how can an edge case in your business logic can take down an entire database in this video we understand what happened and take a fictional example a total fictional example of how an edge case can put extra load onto the database so let's jump right into it so what i'll do is i'll start with the report this is august 2021 site availability report for github and in this what we look at is we look at the first incident so what it says that the incident was caused when one of our mysql primaries entered into degraded state so this was not a replica but a master but a master instance of primary which means rights will be affected right so if the master goes down because master is the one that is definitely taking in all the rights so if master is affected your rights will be affected so if there is any application that is interacting with your master database right away you need to be very careful that hey if that application is misbehaving with your database and your database goes down you are directly practicing the master database affecting a number of internal services internal services is fine right but this causes an impact to github.com services which means an internal database that was handling rights got affected which with a cascading effect took down the main github.com website so if you see how the even though you have microservices architecture you would see how these components are linked to each other and internal surveys or internal database getting affected by some internal service but main website getting affected so that is a con or that is a challenge of managing a distributed system you never know when some random machine gets affected and it takes down your entire website that's the beauty but basically that's the beauty of of basically running a massively scalable distributed system you never know which machine going down takes your infrastructure with it so which resulted in some users being unable to perform operations now this are end users like you and me they are unable to perform operations because the master database was down because of some internal service that's fine so investigation had identified an edge case in one of our most active applications so here the key thing is most active application sorry so the most active applications that we talk about is when an application is most active which means it is the one it is one application that is firing a large number of queries on your database so if there is an edge case into something that is very frequently invoked making a call onto the database it has the potential of taking a database down or your application service talk because it is most active so if let's say some service queries a database once every hour even if that is done it would not impact much but if something which is very high uh which is very frequently querying your data that's where if that goes down it takes everybody with it so we have to be very cautious that whenever we are writing a service that is very active or very frequently invoked we cannot potentially bear the cost of an edge case in that right so what it led to so there was an edge case agreed but what it led to it led to the creation or generation of poorly performing query capable of impacting the overall database capacity this is pretty serious because one edge case is leading to creation of a poor sql query which is then fired onto the database which means that whenever and obviously we know that if our queries are not optimal what would happen if our queries are not optimal your database will take large amount of time to execute it it would continue to use its capacity or ram and cpu to evaluate that that under optimize query which would take a large amount of time we should put additional load onto the database and because this application is a most active application which means that the number of queries that it would fire will also be large so a large number of queries sorry a large number of unoptimized queries being fired onto the database it would suddenly it would suddenly increase the cpu and the memory utilization of it eventually taking it down so with this you'd say but if that is there what if i uh what if i put a load onto the database but database would eventually execute the query right yes but it would take time for database to do it so it would be able to handle not a large but small number of quick small number of concurrent queries we should put in more load onto our api servers it would do retries because it failed and that would have massive impact onto your database because it is continuously getting bombarded by poor or unoptimized queries which is putting unnecessary load on to it even here they mentioned that this combined with like even generated poor performing queries but this combined with application retry and queueing logic so we always think retries and queueing logic are the way out to fix ourselves from the transient issue but that would not work or rather that works but that would put in in this case a more unnecessary load onto a database because let's say you wrote a query which is very poorly or a query was generated which is very poorly performing and let's say it takes 10 seconds for it because of this where it takes 10 seconds to execute and let's say you have a retro logic or you have a timeout of let's say 5 seconds and after five seconds you do a retry so you shorter query onto your database which takes things 10 seconds to execute you have a timeout of 5 seconds and then you are doing retry so that query gets executed in 10 seconds but in 5 seconds you shot another query because of your retry logic so you are constant and plus this is being an active application it is bombarding a large number of queries onto the database which means that your database is never getting a chance to automatically recover which is the worst part so if your database would have gotten a breathing space it could have recovered on its own but because of application retry logic a large number of poor queries are getting fired onto your database just so that like just so that you thought that it would automatically fix your transient tissue and get your things up and running but poor performing queries firing continuously on your database can take it down right in this incident the folks at github did not mention anything about what exactly happened or how it happened but this was the gist that they gave it was an edge case that took down the database so what i'll do now is we'll take a quick look into a fictional example on how an edge case can take down a database although we just talked about how there is an edge case which led to generation of poor performing queries but can we can we take a almost real example which is very close to github and see how it could have could have it's purely fictional how it could have done that right so what exactly happened is whenever the request came in the request came in it came to one of your api servers this api server was generating a mysql query required to be on which was getting fired onto this database sorry so user api api uh your api request came in api generated the mysql query and it fired onto the database if this resulted into a timeout your ap server automatically retried as part of its retry logic so here what happened is a b added retries to recover the transient issue right but it also led to shooting large number of queries onto a database so while your database was already executing heavy queries it got retries it means it goes it goes it got large number of queries so then your database never get a chance to recover which means the database was constantly in a degraded state so whenever an outage like this happens you typically cut down all the rights you give your database time to recover and then once it is back up you then again start moving requests to that so now again a disclaimer this is just an example nothing to do with github just i'm just trying to explain how an edge case can put huge amount of load onto a database so say we have commit stable the main commits table in which whenever we commit to any repository there should be one table that holds all the commit information right so let's say we have a commits table in which we have uh id message author id created the column created at is epoch milliseconds which means it's stored as an integer right and let's say for every uh for every user of github we are rendering a profile card like this in which you have photo name some bio information and 72 commits last week so basic analytics in which we are showing users that hey you made 72 commits last week now this being a profile card of a user this is very frequently fetched it's most active right so now whenever a profile is loaded of any user whenever you hover on profile of any user you get this information now what would happen now in order to render basic information name photo bio is part of your authentication table like you can very quickly get it or your profile you can very quickly get it but in order to get the number of comments you made in last seven days what you have to do this typically would come from your front end however let's say let's say the start date comes from your front end so that you decouple your back end and your front end environment so what you do is you fire a query in order to aggregate or in order to count the number of commits you made you fire a query like this select count id from commits where author id is equal to one to three and created add is greater than start date this start date over here is an integer because created at is an epoch milliseconds column and now let's assume that our service is written in golang it can happen with any language but i'm just taking any a very particular example of an edge case so let's say a service is written in golang and we get start date in the query parameter right so from your front end you would get request something like this https colon slash api.github.com commits and in querymeter and then basically query parameter we pass start at is equal to some epoch timestamp so from this epoch timestamp how many commits have been made right so we so for example on and why do we want to do that let's say on mobile screen you would also want to plot the graph on how your commits have been over last seven days on desktop you have larger space so let's say you want to go for 30 days on mobile you have smaller space let's say you have to go for seven days just hypothetical example so that is why it is important to send start date as a query parameter so now if i write a go link based code to solve this problem what i would do is because i am getting started in my query parameter i would have to extract that and start date oh sorry in any case whenever we get the query parameter they are all string right so what we have to do is we have to extract that query parameter convert it into an integer and then pass it into our sql query right so what i have code what our code would typically look like is in started you define an integer variable called start date you then try to convert your query parameter start date and store it into the start underscore date variable and what golang does is golang gives you a function called a to i which means string to integer it gives you a function that converts a string to an integer so if you give one as a string it would generate 1 as an integer if you give 100 as a string it would give you 100 as an integer so given that you invoke this function and you have assumed that the input that you will get will always be an integer or will always be uh will will will always look like an integer right so now what you do is you apply this function strco and v dot a to i and in bracket you pass in your query parameter that you got so what golag does is golang does not generate exception it's it returns an error so it would return the return value of this will be 2 will be two values the first value is the integer value which is which is what we want and second is the error value now you are pretty confident that hey it's a front end who is sending me the request now the front end would always give me a correct integer to start with it would give me the correct epoch timestamp to start with so you just ignored the error so you what you wrote is you wrote start underscore date comma underscore which means you are ignoring the error then colon equal to strconv dot a2y and in bracket you are passing the query parameter and then you are using the start underscore date which is now an integer into your sql query and trying to execute it so when you do you fire a query like this select count id from commits where author id is equal to 1 to 3 and created at is greater than start date this started being an epoch millisecond you are literally injecting this integer into the string and basically trying to evaluate the query now what happens is let's say there is an edge case where your front end did not pass the start date if your front end did not pass the start date or due to any reason your front end could not pass uh an integer value in the query parameter let's say it passed in one hyphen it was not a proper integer some bug some bug happened some release happened some front-end issue happened or uh there was something wrong with the calendar widget the version got upgraded something weird happened but because of which the starter that it is sending from the front end is either not passed because of some backward compatibility front end issue or is not an integer value so then what would happen is because of some misconfiguration the value that you are getting is either not an integer or you're not even getting it so now what would happen because a query parameter is not there start underscore date what would happen is your str conv dot a2i will throw an error but you did not handle that error because you ignored it you thought that it was it would be proper so then what happens in this case because you ignore the error the default value of an integer will be used so what's the default value of an integer in goal line 0. so because now it is using the default value the query that would be fired would look something like this select count id from commits where author id equal to one to three and created at is greater than zero your created call you are created at column is an integer which is storing epoch milliseconds and now it is doing and created at is greater than 0 which means it is literally for that person it is literally going through all the commits and aggregating it so this means that every time this query is invoked it has to go through all the rows or all the commits made by the user ever and do a count of it so that is extremely costly not just one execution but coming in bulk because this is a profile card if the request would come in very frequently so when at a when one of the most active applications is having an edge case which lets it to generate a poorly performing query it would put unnecessary load on to your database right because this is an expensive query it is continuously getting fired onto a database you don't get a response in five seconds so you retry you retry again and again this same query is being filed onto a database the database takes large amount of time to do it it does not get time to recover and puts it into a degraded state eventually taking it down so this means that unnecessary load is being put because of an edge case right so given this is a very common query very frequent query it will put a massive load onto a database so this is a fictional example of an edge case which is generated so now what is a key take away from this that whenever we are firing any query or whenever we know that something is very actively or some query will be very actively fired onto a database no matter how confident you are never miss out on any error never ignore any error never overlook an edge case assume everything ah assume everything that was given to you would be errored or would be viewed like let's say you are right or you are inserting a string have have a limit on that how big of a string can you insert onto your database don't just take in any input that is given to you with integers also do sanitization right before you invoke a query be as pessimistic as possible when writing application so that you make your system your process your product foolproof otherwise you'd never know when a weird thing happens or a backward compatibility issue happens in your front-end library taking down your entire back-end or sorry basically taking down your entire database and eventually your paying customers getting affected right so just just just a basic question on how we should be thinking about building something or or utilizing something without just actually putting in any thought always think about edge cases or how something could go wrong never gulp an exception always understand what's happening and what's not always see if hey what if this information is not given that information is not given how would my query perform like here this is a classic case where what if start date is not an integer then the default value will be used default value is zero so the worst case of that is if something is wrong because of an edge case i'm literally going through all the rows of my of my table or not all the all the rows of a user of in a table to do into account of it a simple way to have guard railed this particular error was that when we do select count andy from commits were authority one two three and created at greater than zero you could have like a simple guard rail even though you would have not handled greater than zero use case right whereas target is not given you could have at least 80 at max i would have i traded through 1000 commits so you can just apply limit 1000 so even if it is created at greater than zero it will still only go through 1000 commits it will not have to scan the entire all the rows of a user in a table so you are just trying to protect the cartridge like or you are just trying to install a guard rail that hey even if that is a even if i see an edge case i can protect myself or i can limit the iterations that i want to make on to my database although the number would not be correct you would you will catch a thousand plus commits in last week instead of showing exact number of commits in last week that is also a possible solution it's always keep your self on your toes on whenever you're writing your query how to structure it is there an edge case into this query can this query take down your database in most cases your answer would be no that hey how can this this this simple looking query take down my database but it happens it actually happens in real world so always think twice before implementing it and consider all hk easy for me to say that's what i said in put those guard rails and hey at max i will only iterate through 1000 rows but even in the edge case or even in the worst case i would not have to skim through all the rows of a table right these are some basic noises and obviously i just took a very fictional example to help you folks understand uh the importance of edge cases uh retries how a database can be taken down because of an f in edge case how poor how poor sql queries are generated this is purely a fictional example but just to uh but just to make you folks understand the gravity of situation on at scale things even a small hiccup can take down your entire infrastructure right that's it that's it for this one i hope you found it amusing if you learned something new uh if you did if you guys again like this video give this video a thumbs up if you guys like the channel give this channel a sub i post three in-depth engineering videos every week and i'll see you in the next one thanks a ton