so distribute systems are prone to problems that seems very very very obscure github at an outage because some nodes of a zookeeper cluster ran an election and elected a second leader i mean how obscure is that in this video we dissect a get about this that would tell us how weird the world of distributed systems is we would also talk about how important zookeeper is for managing a kafka cluster we would see what happened during the outage and how github was able to mitigate it with zero data loss and a brilliant fallback strategy but before we move forward i'd like to talk to you about a course on system design that i have been running for over a year now the course is a cohort based course which means i won't be rambling a solution and it will not be a monologue instead a small focused group of 50 60 engineers every cohort will be brainstorming systems and designing it together this way we build a solid system and learn from each other's experiences the course to date is enrolled by 600 plus engineers spending 9 cohorts and 10 countries engineers from companies like google microsoft github slack facebook tesla yelp flipkart dream 11 and many many many more have taken this course and have some wonderful things to say the coolest part about the course is the depth we go into and the breadth we cover we cover topics ranging from real-time text communication for slack to designing our own toy load balancer to quick buzz live text commentary to doing impressions counting at scale for any advertisement business in all we would cover roughly 28 questions and the detailed curriculum uh split week by week can be found on the course page which is linked in the description down below so if you're looking to learn system design from the first principles you will love this course i have two offerings for you the first one is the live cohort discourse which you see on the left side and the second one is the recorded course which you can see on the right side the live code based course happens every two months and it will go on for eight weeks while the recorded course contains the recordings from one of the past cohorts as is if you are in a hurry and want to binge learn system design i would highly recommend you going for the recorded one otherwise the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings the decision is totally up to you the course details prerequisites testimonials can be found on the course page at binary dot me slash master class and i would highly recommend you to check that out i put the link of the course in the description down below so if you are interested to learn system design go for it check out the link in the description down below and i hope to see you in my next cohort thanks so before we go to the incident report let's spend some time understanding the importance of zookeeper for a uh for a kafka cluster so kafka is a very popular message stream it has topics it has partitions uh each topic can have multiple or every topic can have multiple partitions and kafka is not a single node cluster there would be multiple nodes that are running kafka process and each node is called as a broker over here each partition of a particular topic has a leader and has a replica a broker can have a leader of one partition and a replica of another partition right and this is how across such cluster across this cluster it is able to sustain a high read load and a high right load so what zookeeper does zookeeper fulfills for his four key responsibilities for apache kafka first of all is controller election basically the leader election so here for every partition for every topic there is a node which is acting as a leader while other node is acting as a follower who does this leader election zookeeper who manages this information zookeeper if leader goes down someone else has to become a leader who does this zookeeper right so zookeeper is acting as a very nice brain that is doing this leader election for every topic for every partition in the cluster second indus cluster membership so now that there are multiple machines on which your kafka is running you need to know which machine is in which machine is out whenever machine joins a cluster whenever machine leaves the cluster abrupt failures and what not who manages that zookeeper third is topic configuration kafka has multiple topics each topic has multiple partitions each partition or a partition can be a leader or a replica where does this partition hold where should i forward the request to in order to send my data or in order to perform the rights or the reads all of this information stored in stored as part of topic configuration in apache kafka right then acls and quota like who is allowed to read from a partition who is allowed and how much is something which is managed by uh zookeeper so all of this information is stored in zookeeper so now here you see how important is zookeeper for managing uh apache kafka right so it is acting as a brain as a ring master to hold and manage the entire cluster for us the just disclaimer kafka from version 2.8 does not have a true dependency on zookeeper prior to that it had now they managed or they have built their own internal thing to manage the cluster so they have removed the dependence on zookeeper but uh it still holds true the concept uh the concept outage still holds true it might not be okay but it might be something else right the distributes the foundations remain the same right okay so now what we do is now we take a look now that we have a very solid foundation we take a look at the incident report so what incident report says while reprovisioning zookeeper nodes as part of routine upgrades right so every cluster needs to go throughout them these is what we are talking about zookeeper nodes not kafka nodes zookeeper nodes so while provisioning zookeeper nodes as part of routine upgrades new hosts were introduced too quickly which resulted in the election of a second leader let's spend some time understanding what exactly happened so zookeeper had a routine maintenance why do we need to do routine maintenance because version upgrade os patches security patches machine rotations right normal routine process that was going on so zookeeper nodes were getting changed right and when they were when this was happening a lot of new nodes were added right and when nodes were added too quickly what happened is they they ran another election algorithm so what happens by uh internally what happens whenever a node is added to a zookeeper cluster right what would happen is zookeeper is made to be near autonomous right so when a node is added when a node is added to a zookeeper cluster what it would try to do it would try to understand what the cluster looks like it would try to understand or it would try to do self-discovery on how it fits into the cluster right when a lot of new nodes were added into the cluster they could not find a leader or they could not discover a leader might be any reason they could not discover a leader and the majority of information that they found was like they seem to have majority of nodes not knowing where the leader is right and this is all written in code this is not some hypothetical thing that i'm talking about this is all written in code as part of the bootstrap code of any true distributed system so when a lot of nodes are added these nodes did not know where they were not able to quickly discover the leader so then what they did they triggered a leader election right and this is where what happened is and because there were too many nodes added too quickly leader election was successful they were able to elect a new leader the old leader was there they elected a new leader and the new setup and the newer set of replica or the newer set of nodes were became part of a second logical cluster it was not a physical case they were all part of this one uh cluster but within that they elected another leader so the the end state of this uh after they added nodes too quickly was that they had two logical clusters with two leaders and multiple replicas and this is i am talking about zookeeper cluster right so now we have two logical zookeeper clusters part of the same infrastructure right okay so now what happened and this is uh not so common but uh a true a distributor system will have this problem sometime in the future like it would bound to happen uh basically a second leader problem or a second brain problem is bound to happen right so this happened for zookeeper it happened because we added uh because the github team added a lot of nodes in a very short time right so leader election had to spun up and why i said had to because the nodes could not discover an existing leader quickly that very com not so common but a very possible situation right okay so then what happened so after they added this effectively introducing a logically distinct second zookeeper cluster where there should have been only one so now how they are operating is now we have two logical zookeeper cluster right now depending on where anyone connects to it would be part of that cluster correct so that's exactly what happened while the zookeeper host were in this state a single kafka broker in the cluster connected to the newly formed second zookeeper cluster and elected itself as a node controller okay brilliant so what happened there were a lot of brokers who were connected to old uh leader or they had information about old leader and all and old followers of zookeeper right a single broker a single broker got added or got connected to this new logical zookeeper cluster and when it got connected to this zookeeper cluster now this broker just spun up and this broker is also autonomous the bootstrap code of this broker would be what hey for this topic is there any leader it would check in the zookeeper cluster in the zookeeper cluster this cluster does not know it does not know the existence of other cluster right so this cluster would say no there is no leader for this topic this because hey i would be the leader and this is exactly what happens in with a split brain problem so when this broker got connected to this newly formed zookeeper it elected itself as a controller for a particular topic so now what happened or now this could be for a topic for multiple they have not specified it but it elected itself as a controller so now you would have multiple controllers in your zookeeper class uh in your kafka cluster for a same set of topics problem so now what would happen we already had two we already had two distinct logical zookeeper clusters now we are having two brokers or two brokers claiming to be a controller now what would happen when clients connect to uh when basically clients are connecting to zookeeper to get the information so whenever a write needs to happen to kafka let's say you're writing it to the kafka you have to talk to zookeeper to get the information on where to write on to which note do you want to write right so when they connect to zookeeper what they're getting is they're getting conflicting information so client it would require some time for the client to discover that they are getting conflicting state of information as part of response from zookeeper now how would they do it when you make a call to zookeeper cluster and get the information the zookeeper internally fans it out grabs it uh and then basically returns it to the client hey this is the consistent view of the information right in order to get that consistently of the information it would have to talk to a lot of stuff when it does that what would happen is the client is getting conflicting information as our client is getting conflicting information it would drop the rights right it would not proceed further because it knows that something's wrong right so the rights would start to fail there might be some rights and this is this did not happen with github but there might be some rights that would have gone to the new from the new broker to the new zookeeper cluster it would have told some other some brokers ip and would have gone to that but here only one broker was added right that was uh that's why github became lucky one broke only one broker got added to this new cluster so there was not data loss but if there were multiple brokers and they themselves elected themselves okay i'll be the uh i'll be the follower you'll be the leader and all if that would have happened then the rights would have been successful in that right but for some time duration until zookeeper hills itself there would be a state where your right where your cluster is sending inconsistent information and while it is sending that inconsistent information what would happen is your rights would start to fail and that's what happened so here what happened at this point there were two distinct kafka clusters that were serving conflicting cluster state information to the client this incorrect state caused right failures to approximately 10 percent of request to our background job service resulting in backup of jobs we'll talk about this so here what we clearly see is there were two distinct kafka clusters that were serving conflicting cluster state information to client this is where your client needs to be uh smart enough to understand that that i am getting a that i am getting a conflicting set of information so i should not be writing to this right in most cases you have to handle this right so what happened over here how did they even recover so there are two ways to do it zookeeper does a periodic uh basically does a periodic cleanup not really a cleanup but basically a periodic heal if it requires to do so so zookeeper can auto detect this inconsistency over time and it can oh and it can auto heal quite possible right but it takes some time to do this so instead if you don't want to wait for that long you can manually take actions to fix it for example killing the second logical cluster and how would you know that there is a second logical cluster zookeeper configuration because your rights are failing you would have gotten a pager duty alert you would see you would see key in the zookeeper configuration you can see totally see the two leaders are getting elected or two leaders are operating because of which the rights are failing you would be deleting those entries from zookeeper right and removing those nodes right this is how the recovery would happen wait for the auto hill to happen or otherwise take manual actions to fix it right so with this sort of recovery strategy what exactly happened what did github do there was some very fancy things that github was talking about so the incorrect state caused right failure to approximately 10 percent of the request to our background job service resulting in backup jobs as we migrated traffic and worker capacity to our secondary job processing system two keywords ten percent request fail and secondary job processing system so what typically happens so what got affected over here is an internal background jobs processing system right some internal jobs that they were running they stopped processing because rights were failing and what not so did they lose like this looks like a pure case of data loss because the rights was supposed to happen but rights failed so why wasn't there a data loss this is where a concept of fallback queues come in so whenever you are putting a message in a broker what you should be always doing is if writing that message to a broker fails have a provision of something called as a dead later queue it's a very standard nomenclature so dead letter q is what would hold the messages that got failed so if your client is writing to a message to kafka and if this fails due to some reason it would write the message to a dead litter queue it has to be a very cheap queue mostly a message broker not a message stream but what would you do if your client tries to write to kafka it does with some retries three or four retries and if it still is not successful it would write it to a dead later queue this becomes your secondary job processing system right your primary one is very ingested into kafka and there are workers consuming it but in case your right to the kafka fails have a fallback and this dead letter q is a fallback to do so right so this is what came in very handy because of this there was no background jobs were lost during this incident why and while we experience significant q backups for some systems the retry behavior in our clients and presence of redundant queuing system mitigated such issues right this clearly shows the importance of having a secondary job system or a secondary job processing system so your client was writing to kafka it failed it pushed through dead letter q right so there is no message loss whatsoever right and there are consumers these are the primary consumers which are reading from kafka that did not get any job because rights were failing so you are dead later consumer started processing it so there was no data loss but what would happen is because this is not as throughput as kafka a lot of things would pile up right so it would take some delay but no data loss very important right delay is okay for internal job processing system but not data loss so this is a very key design thing or the this is a very key design that you all should remember that having a fallback strategy is very very very important for every single thing so here you cannot assume that kafka is always working it can also fail so have a dead letter queue right okay so some important key takeaways to conclude this discussion first of all having a dead letter queue is a must so if you are relying only on a single queue don't do that what if that queue goes down what if that broker goes down right have a dead letter queue so that you have a secondary job processing system so that you don't have any data loss right second point is consumer should be idempotent right it's very important that your consumer the way you are writing your consumers for any job for any message processing system the consumer that you are writing has to be idempotent in nature if it is not idempotent then your data would become inconsistent ensure that it is item ported third your clients and consumers should have retrieves right your client should not just make one call to kafka and expect it to write there should be retries after three or four retries with exponential backup you can say hey now i'm unable to write to the kafka let me put it to dead literally so give yourself or give your client a chance to write or if there are any transcendent issues it would auto require it would your eventual right would succeed but if that fails after three or four retries you put it into a real queue right and the fourth one automatic cluster provisioning with the jitter so here what happened when github tried to do it or when github was adding node to zookeeper they might have done it manually because the last paragraph of the outage says to avoid this class of failures in the future we have updated our zookeeper provisioning checklist and plan on introducing automation to perform zookeeper and kafka cluster maintenance so this shows that it is quite possible that some engineer would have added more nodes into zookeepers or was doing manual rotation of zookeeper cluster in which he or she added a lot of nodes in a very short time which led to this pure speculation not the person or i don't have any insider information to our speculation but in any case with specifically with distributed systems you have to take your time right and whenever you are adding or scaling up your infrastructure add a jitter every single cloud provider gives you that ability if you are doing manual provisioning add a jitter explicitly don't do it like add 50 nodes in one shot right never do that add some jitter add some random delay and then you add nodes slowly and steadily into the cluster right four key important learnings from this particular outage nice so yeah that's it that's it for this one i hope uh i hope i made sense it it is very confusing to be really honest distribute systems are meant to be like this but something that you cannot definitely run away from it's it's the beauty of the distributed systems in any case nice so yeah that's it that's it for this one if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sup i post three in-depth engineering videos every week and i'll see in the next one thanks [Music] you