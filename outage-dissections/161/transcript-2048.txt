thanks, so before we go to the incident report, let's spend some time understanding the importance of zookeeper for a uh, for a kafka cluster.
and kafka is not a single node cluster.
there would be multiple nodes that are running kafka process and each node is called as a broker.
over here, each partition of a particular topic has a leader and has a replica.
a broker can have a leader of one partition and a replica of another partition right, and this is how across such cluster, across this cluster, it is able to sustain a high read load and a high right load.
zookeeper fulfills for his four key responsibilities for apache kafka, first of all is controller election, basically the leader election.
so here, for every partition, for every topic, there is a node which is acting as a leader, while other node is acting as a follower.
who does this leader election zookeeper?
so zookeeper is acting as a very nice brain that is doing this leader election for every topic, for every partition in the cluster.
so now that there are multiple machines on which your kafka is running, you need to know which machine is in, which machine is out, whenever machine joins a cluster, whenever machine leaves the cluster, abrupt failures and what not.
stored as part of topic configuration in apache, kafka right.
then acls and quota, like who is allowed to read from a partition, who is allowed and how much, is something which is managed by, uh, zookeeper.
so now, here you see how important is zookeeper for managing, uh, apache, kafka, right, so it is acting as a brain, as a ring master, to hold and manage the entire cluster.
for us, the just disclaimer: kafka from version 2.8 does not have a true dependency on zookeeper.
now they managed, or they have built their own internal thing to manage the cluster, so they have removed the dependence on zookeeper, but, uh, it still holds true.
so what incident report says: while reprovisioning zookeeper nodes as part of routine upgrades, right, so every cluster needs to go throughout them.
so, while provisioning zookeeper nodes as part of routine upgrades, new hosts were introduced too quickly, which resulted in the election of a second leader.
let's spend some time understanding what exactly happened.
so zookeeper nodes were getting changed right, and when they were, when this was happening, a lot of new nodes were added.
right, and when nodes were added too quickly, what happened is they they ran another election algorithm.
uh, internally, what happens whenever a node is added to a zookeeper cluster?
right, what would happen is zookeeper is made to be near autonomous, right.
so when a node is added, when a node is added to a zookeeper cluster, what it would try to do it would try to understand what the cluster looks like.
right, when a lot of new nodes were added into the cluster, they could not find a leader, or they could not discover a leader might be any reason they could not discover a leader and the majority of information that they found was like they seem to have majority of nodes not knowing where the leader is right.
so, when a lot of nodes are added, these nodes did not know where they were not able to quickly discover the leader.
right, and this is where what happened is.
and because there were too many nodes added too quickly, leader election was successful.
they elected a new leader and the new setup and the newer set of replica or the newer set of nodes were became part of a second logical cluster.
they were all part of this one uh cluster, but within that they elected another leader.
so the the end state of this uh, after they added nodes too quickly, was that they had two logical clusters with two leaders and multiple replicas.
and this is, i am talking about- zookeeper cluster.
right, so now we have two logical zookeeper clusters part of the same infrastructure, right, okay, so now what happened?
uh, basically, a second leader problem or a second brain problem is bound to happen.
right, so this happened for zookeeper.
it happened because we added- uh, because the github team added a lot of nodes in a very short time, right, so leader election had to spun up.
so after they added this, effectively introducing a logically distinct second zookeeper cluster where there should have been only one.
so now how they are operating is now we have two logical zookeeper cluster.
right now, depending on where anyone connects to it would be part of that cluster, correct?
while the zookeeper host were in this state, a single kafka broker in the cluster connected to the newly formed second zookeeper cluster and elected itself as a node controller.
there were a lot of brokers who were connected to old uh leader or they had information about old leader and all and old followers of zookeeper.
right, a single broker, a single broker got added or got connected to this new logical zookeeper cluster.
and when it got connected to this zookeeper cluster, now, this broker just spun up and this broker is also autonomous.
the bootstrap code of this broker would be: what, hey, for this topic, is there any leader?
it does not know the existence of other cluster, right?
so this cluster would say, no, there is no leader for this topic.
so when this broker got connected to this newly formed zookeeper, it elected itself as a controller for a particular topic.
so now you would have multiple controllers in your zookeeper class, uh, in your kafka cluster for a same set of topics- problem.
we already had two distinct logical zookeeper clusters.
now what would happen when clients connect to uh, when, basically, clients are connecting to zookeeper to get the information?
so whenever a write needs to happen to kafka- let's say you're writing it to the kafka- you have to talk to zookeeper to get the information on where to write on.
so when they connect to zookeeper, what they're getting is they're getting conflicting information.
so, client, it would require some time for the client to discover that they are getting conflicting state of information as part of response from zookeeper.
when you make a call to zookeeper cluster and get the information, the zookeeper internally fans it out, grabs it, uh, and then basically returns it to the client.
right, in order to get that consistently of the information, it would have to talk to a lot of stuff.
when it does that, what would happen is the client is getting conflicting information.
this did not happen with github, but there might be some rights that would have gone to the new, from the new broker to the new zookeeper cluster.
but here only one broker was added right.
only one broker got added to this new cluster, so there was not data loss.
but if there were multiple brokers and they themselves elected themselves, okay, i'll be the- uh, i'll be the follower, you'll be the leader and all.
if that would have happened, then the rights would have been successful in that right, but for some time duration, until zookeeper hills itself- there would be a state where your right, where your cluster, is sending inconsistent information, and while it is sending that inconsistent information, what would happen is your rights would start to fail, and that's what happened.
at this point, there were two distinct kafka clusters that were serving conflicting cluster state information to the client.
this incorrect state caused right failures to approximately 10 percent of request to our background job service, resulting in backup of jobs.
so here, what we clearly see is there were two distinct kafka clusters that were serving conflicting cluster state information to client.
this is where your client needs to be uh, smart enough to understand that, that i am getting a, that i am getting a conflicting set of information, so i should not be writing to this right.
and how would you know that there is a second logical cluster zookeeper configuration because your rights are failing?
you can see, totally see, the two leaders are getting elected, or two leaders are operating because of which the rights are failing.
you would be deleting those entries from zookeeper right and removing those nodes right?
so the incorrect state, caused right failure to approximately 10 percent of the request to our background job service, resulting in backup jobs as we migrated traffic and worker capacity to our secondary job processing system.
two keywords: ten percent request fail and secondary job processing system.
right, some internal jobs that they were running they stopped processing because rights were failing and what not?
so did they lose like this looks like a pure case of data loss, because the rights was supposed to happen but rights failed.
so whenever you are putting a message in a broker, what you should be always doing is, if writing that message to a broker fails, have a provision of something called as a dead later queue.
it's a very standard nomenclature, so dead letter q is what would hold the messages that got failed.
so if your client is writing to a message to kafka and if this fails due to some reason, it would write the message to a dead litter queue.
but what would you do if your client tries to write to kafka, it does with some retries, three or four retries, and if it still is not successful, it would write it to a dead later queue.
this becomes your secondary job processing system, right, your primary one is very ingested into kafka and there are workers consuming it.
but in case your right to the kafka fails, have a fallback- and this dead letter q is a fallback to do so, right.
so your client was writing to kafka.
it pushed through dead letter q, right, so there is no message loss whatsoever.
right, and there are consumers- these are the primary consumers which are reading from kafka- that did not get any job because rights were failing.
but what would happen is, because this is not as throughput as kafka, a lot of things would pile up, right.
so it would take some delay, but no data loss- very important, right.
delay is okay for internal job processing system, but not data loss.
so have a dead letter queue, right, okay.
right, have a dead letter queue so that you have a secondary job processing system, so that you don't have any data loss, right?
it's very important that your consumer- the way you are writing your consumers for any job, for any message processing system- the consumer that you are writing has to be idempotent in nature.
third, your clients and consumers should have retrieves right.
your client should not just make one call to kafka and expect it to write.
but if that fails after three or four retries, you put it into a real queue right and the fourth one, automatic cluster provisioning with the jitter.
so here, what happened when github tried to do it, or when github was adding node to zookeeper?
so this shows that it is quite possible that some engineer would have added more nodes into zookeepers or was doing manual rotation of zookeeper cluster, in which he or she added a lot of nodes in a very short time, which led to this pure speculation.
you have to take your time right and whenever you are adding or scaling up your infrastructure, add a jitter.
don't do it like: add 50 nodes in one shot, right?
add some jitter, add some random delay and then you add nodes slowly and steadily into the cluster, right?