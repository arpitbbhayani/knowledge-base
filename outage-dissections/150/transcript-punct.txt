so github wanted to optimize their sql query performance and they had to flip a database index. instead of getting a performance boost, they incurred a down time of more than 60 minutes. this outage gives us a super in-depth insight into mysql and its indexing capabilities. it is so fascinating to see the kind of optimizations engineers have to make while operating at scale. so in this video, let's dissect this outage, understand what happened, why github had to flip their index, how things went wrong and affected end users like us, and conclude with three super important key takeaways from this outage. but before we move forward, i'd like to talk to you about a course on system design that i have been running for over a year now. the course is a cohort based course, which means i won't be rambling a solution and it will not be a monologue. instead, a small, focused group of 50- 60 engineers every cohort will be brainstorming systems and designing it together. this way, we build a solid system and learn from each other's experiences. the course to date is enrolled by 600 plus engineers spanning nine cohorts and 10 countries. engineers from companies like google, microsoft, github, slack, facebook, tesla, yelp, flipkart, dream 11 and many, many, many more have taken this course and have some wonderful things to say. the coolest part about the course is the depth we go into and the breadth we cover. we cover topics ranging from real-time text communication for slack to designing our own toy load balancer, to greek buses, live text commentary to doing impressions counting at scale for any advertisement business. in all we would cover roughly 28 questions and the detailed curriculum- uh split week by week- can be found on the course page, which is linked in the description down below. so if you're looking to learn system design from the first principles, you will love this course. i have two offerings for you. the first one is the live cohort discourse which you see on the left side, and the second one is the recorded course which you can see on the right side. the live cover based course happens every two months and it will go on for eight weeks, while the recorded course contains the recordings from one of the past cohorts, as is. if you are in a hurry and want to binge learn system design, i would highly recommend you going for the recorded one. otherwise, the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings. the decision is totally up to you the course details, prerequisites, testimonials can be found on the course page at fifth binary dot me slash master class and i would highly recommend you to check that out. i put the link of uh the course in the description down below. so if you're interested to learn system design, go for it. check out the link in the description down below and i hope to see you in my next cohort. thanks, so this is the incident report that github released for this particular outage. it only consists of two paragraph, but there are some amazing insights about what happened and how they fixed it. so let's jump. this is what github team says is: this incident was caused by a database migration to flip the order of an index, which means they had an index like any database that you use, let's say sql database, in order to improve the read performance, you create index on top of it. that index, where it is stored onto the disk, has a particular order. now this particular order by default for mysql up until version 8 was ascending. so what happens is when you create an index- let's say you create an index on your blogs table, on that blocks table, you have a column called user id, user id column. when you create an index on it, on disk. when that index is stored, it is stored in the ascending order of user id, right? and github team had to run a database migration to flip the order of an index. let's take a look on why did they have to do it? so what? what would be that situation where you had to flip the order of an index? so let's say hypothetical- this is pure speculation just to make you understand on what's happening behind the scenes. okay, so let's say there is a multi-column index. so let's say you have two columns- uh, user id and date- out of in. in a table where you have 10 or 15 columns, the two important columns are user id and date and you have like, whenever you are querying on it, you are always squaring on user id comma, date combination, which means, in order to improve the performance, for that you would create a composite index or you would create an index that involves multiple columns, right? so let's say the most common use case for us is for the commits table is to get commits for a repository in the descending order, for uh in the descending order of time. that's it. so what you will have: you will have an index of the date column which is in the descending order of time, and because you are querying on user id rate combination, you would have an index in which you are storing the combination user id, comma, date order, uh, which is stored in an ascending order, but you are querying or deciding, which is perfectly fine because it would do a reverse index scan. but now what happens? let's say, for a change in product record- hypothetical requirement- what we want is get commits ordered by date in descending order and for each day, order by user in ascending order. so now what we want is we want to have the same capability of querying the data, but we want date in descending order but user id in ascending order. so when we have a use case like this, where you are ordering by something in a particular order and in the reverse, you are making a call for other column, both part of your index, what happens? your database engine, when it wants to compute this, it has no other resort but to do a file sort which is very expensive as compared to index with respect to index operations, right? so if you would have had the right set of index on those particular column, you would have had a very quick response time, or you or your query would have executed much, much, much faster. but now, because indexing the way your index is structured right now, both ascending- there is no way for a database engine to very quickly compute your answer. that's why it would resort to something called as file sort, in which it does kind of like a full table scan. would not want to go into those details, but very expensive, had to do a lot of disk ios, operate on the actual table data and do a lot of like. basically, it involves a lot of discovers like that's. that's the thing that you can take away from it and which, if you do large, uh disk ios, that would, uh, that would degrade the performance of your system, right. so key, take away from this two columns you want, one ordered in ascending order, one ordered in descending order. but by default both indexes or both the columns are ordered in ascending order in mysql. so they had to flip the order so that they could make this query much more efficient, right? otherwise it would have incurred a lot of disk ios onto a database, degrading the entire performance, right? so now to answer this query, we want a date in descending order, user id in ascending order. this is supported by mysql version 8, right, where you can specify that: hey, i want to create an index on the date column in ascendant, in descending order, while user id on ascending order. so the way this index would be structured onto your disk would have date and descending and the user id in ascending, which would make your uh, your required query much, much, much faster too when it is executing. so this is why they had to flip the index, because earlier both of them were ascending. now one of them has to be descending, another one has to be ascending and it is a composite index. so they had to re-index or they had to create this new index and flip the order. so what they said in the outage is: oh, let me quickly open that. okay, what they say in the outage is: reversing the index caused a full table scan. so they tried to reverse an index, which they had to, and it caused a full table scale, since there was a missed dependency on the changed index by a generated active record query. quite a bit of words, and this, this is the gist of the uh of the incident. this is exactly like only this much that they have revealed about this incident. i've tried to deduce it through my speculation, but amazing insights that you will get from this. so what they say? they said that reversing the index caused a full table scan. so they were trying to flip the index and it caused a full table scan. obviously, when you are creating an index like it would require you to do a full table scan in any case. right, so creating because they were creating a new index in a different order, they had to go through all the rows and create a new index so that anywhere requires full tables can. but this is not the only thing that they are talking about here. what i also say is reversing index caused a full table scan since there was a missed dependency on the changed index. this means that index that was being changed there was a dependency by a generated active record query what is active record? so github one of in the tech stack they have ruby on rails and active record is the orm of ruby of rail, of ruby on rails, so similar. if you are a django person, django rm is what you use to query the database. if your java person use hybrid to query on your database, that is orm. similar to that active record is the orm for ruby on rails, right? so instead of writing raw sequel queries, folks write like normal ruby code using active record that translates into sql query and gets fired onto the database engine. right. so now when you change the order of the index. but what could have happened, right? pure speculation. some queries like: obviously you thought that, hey, now my requirement has changed and i want a date in descending order, user id in ascending order. but there, it's not the only query that is fired on the table. there might be some other use case. who wanted both of them in ascending order could be right. and what if that other query is much more frequent than what you anticipated for, like? but the new query that you are trying to optimize for, like, you didn't even know this query existed, or a query existed that required both of them in ascending order. you didn't know about it. then what would have happened? you are trying. so now you have two queries: one required date and user id in ascending order, which is query that you are not, that you don't know yet, and the query that you are trying to optimize for, which requires date in descending and user id in ascending order. you are trying to optimize this response time, but the other query that you did not know existed is now inefficient because you flip the order. and what if this query, that you are unaware of it, was much more frequent? this would cause a lot of load on your database, because this is much more frequent than what you are trying to optimize for. because of a pure blind spot, you didn't know that query existed or that use case even existed right. second case is some queries might not use the new index, like, for example, you flip the order and you created a new index out of it. now someone has to use this index right now. it is very much possible that when you are firing a query onto your database, your database engine would try to do its best to pick the best index in order to execute your query. what if your database is not picking the new index that you created due to some reason, due to you, one of the reason being that, according to your database estimations, it is not efficient to use the new index that you created due to like it did not analyze the table, it did not get time to check the index and what not, but it is not part of your query exec or it is not part of the query execution plan created by your database, then, even though you have created that index, your database is not using it, which means that it would have to do a full table scan, right? these are the two possible reasons, because github never said anything. these are the two possible reasons that i could think of, which would lead to a very inefficient execution of your query, even though you created a new index trying to optimize it, right? so what next? how to do it? obviously it's not just theoretical, like there has to be some practicality to everything that we discuss. so when you say that, hey, database did not pick it, or database did not thought, or database did not think that using this index would optimize it, but you as a developer, you as an engineer, you know the database should have used this index. so how can you tell your database: hey, stupid database, use this particular index. so almost all databases out there provide you with index hints. with mysql, what you can do is, in the select statement that you are finding, you can end it statement with use space index and, in bracket, pass the index name. when you do that, you are telling your database: hey, database, you can use this index to execute this query. it's an. it's not a forceful thing, it's just a suggestion. but there is another hint called force index, which would tell a database you have to use this index right. so by using index hints. so, for example, due to any reason, if your database is not picking the right index to execute your query- which you, as an engineer, are thinking is much better- then you can force your database to use a particular index by using indexings, right. so this is what you should be thinking of. that, hey, why is my query not performing well? is it not using the right set of indexes? never trust your database. they have. obviously. they also have some logic through which they are trying to pick the best execution plan. but for it- but it might be possible- it is not picking the index that you want it to pick. this is how you can drive that. so what happened next? now we know why full tables can could have happened. obviously, pure speculation out of those two lines. this is what i could deduce, right. so what's next? the performance degradation due to the table scan had a cascading effect, obviously. obviously you see cascading effect taking down your infras so often, so often. so they had a cascading effect on query response time, which resulted in timeouts for various dependent services. as expected, because you are doing a foldtable scan, it is becoming very inefficient to fire that query and obviously it would take a lot of time. and as it takes a lot of time, what happens is other services which are dependent on that goes into time out, and then other services that are dependent on that c's drive out and eventually your end user sees timeout. so because one query was doing a full table scan, it takes time. it puts a load onto a db. first of all. it puts the load onto a db or is doing far more work than what then it worked, then it what had to do. so, putting a load onto the db, it would impact not only the response time of that query but every other query that is executing on the db. this would increase the response time of your service which is directly talking to the db. this would lead to timeout to other services which are depending on this service, which would lead to the main user call that is making or that is waiting for the response to go to see timeouts. so here what you'll see: this is a very cascading effect where the database is under load or is doing a lot of work leading to a lot of time service for going into- like your service talking to database going into timeout depending services going into timeout main service going into the timeout api gateway going into a timeout timer, cascading to our end user. so degraded performance or degraded availability for main website, which is githubcom, and that happens. this typically happens when you have a synchronous dependency between services, and which is why, in most cases, people try to make decoupled systems and avoid the synchronous dependency on services. right. so cascading effect: in most cases, you would see cascading effect or domino effect taking down your infrastructure and that happens, that happens a lot, but you can't you, you just, you just cannot get rid of it. what to do? okay, so how did they mitigate it? to mitigate this issue, we are determining- this is pretty interesting- we are determining better tooling to identify index regression. what is index regression? so, if you are changing a particular index, you need to know what else could be affected. similar to how, when you make api changes, you make you run regression test on that in just order to check that, because of the change that you made, no other changes are getting affected. right, similar to that with index regression by changing the index that you have or if you are changing a particular index you would want to test. any of the other changes should not break. so some better tooling to help you identify index regression. that hey, by changing a particular index, creating a new index, creating a new index- is anything else getting changed? second, is they're also creating an inventory of indexes used by generated queries to further ensure that we are compliant on all active record best practices. i'll talk about it in detail. so key takeaway number one: do not blindly trust your orm, orm active record. django, rm, sql, alchemy, hibernate, pick your favorite right. orm makes our life simple. it is meant you write in your native language. it would automatically create a sql query. fire it on the engine, right. but we, our lives have become simpler because of orm. but the queries generated by orm are they efficient enough? we don't know. we blindly trust them. so never blindly trust your vrm. periodically audit the queries that are generated by your rf to see if they are indeed optimal or not. if not, write a ros equal query or a prepared statement and fire it onto a db instead of relying on rm, because at scale you cannot trust rf to generate always optimal queries. it is not meant to do that like they tried their best, but it cannot cover all possible cases, right? first key takeaway. second key takeaway is always check the query execution plan. so what you should be doing is, whenever you are changing your index or changing, making any change in your database, evaluate the query execution plan. so there is a very simple way to do it. there is an explain statement in my sequel, or any database which gives you how the query, how the query would be actually executed on your database. check that. check for any aberrations or deviations in that, like, for example- just a very simple example on that- take a sample of queries, like all the queries that are fired onto a database, run explain on that. you get query execution plan in your output, store it in your dv and then run a simple script to check what changed that. this was the scan that i did on the 1st of jan. this was the scan that i did on 5th of june. what changed in the query execution plan? this would very simply give you the deviations or something that changed. you can just audit what changed and see if it still is an optimal way to execute your query. if not, alter your query and make it optimal right. so i always check your query execution plan. do not blindly trust your orp for that. and third is audit the queries and indexes they use. so what git update? is they prepared an inventory of the queries and the indexes they use? again, from the query execution plan you can get which indexes are these queries using. first case, second case: in the sql query that you are firing, you, if you are giving, if you are providing index hints while saying use index or force index, you can keep a track of this query is supposed to use this index. if you have a good inventory list, if you have a hundred percent coverage on your inventory list on these queries are using these indexes. if you make any alterations in any table that affects that particular index, you can quickly go and test only those queries which might be affected to just quickly test for regressions at scale. this becomes super important where whenever you are making any change, you have to be very sure that your database is never put or does. it is the most brittle component like, although it's meant to be robust and all, but it is the most brittle corporate. the way we have done so many outages. in most cases we have seen. database is the most brittle one. it's the culprit of all outages out there, right? so yeah, these are the three key takeaways from this outage. i, uh, i would highly encourage you to dive deeper into indexes- what we just discussed- ascending, descending order index. play around with it in mysql 8.. indeed, it's very fascinating on how optimal your sql query execution can get with the right set of indexes. nice, so, yeah, that's it. that's it from me for this one. uh, if i hope this, this was one one of the shortest incident report that i've seen from greater- just two paragraph, but there was so much to learn from it. just that two statement gave us so many deep insights about, uh, their architecture, their database, what they had to change and all nice. so, yeah, that's it. that's it from this one. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel a sup. i post three in-depth engineering videos every week and i'll see you in the next one. thanks again.