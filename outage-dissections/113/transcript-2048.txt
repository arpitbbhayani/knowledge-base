uh, the outage happened because their mysql table hit its integer limit.
the id was set to auto increment id and it hit its limit right.
so this is their incident summary that they have posted on the blog, which says that, hey, during the incident, a shared database tables auto increment id.
uh, the auto increment id column exceeded the size that can be represented by mysql integer type.
so they created a table with a simple mysql integer type and it was set to auto increment.
so anytime you insert a new row, if you do not give an id, it would pick the next id and it would insert the row and obviously, like everything has a limit.
my sql integer type has a limit of 2 billion 147 million 863 647..
so the normal integer overflow part 2 raised to 31 minus 1, is where this person or this auto increment id stopped working.
when the column reached this limit, they start.
they attempted to insert large integers in the column, so they were trying to insert data, but the database rejected the value.
obviously, database would not say hey, because you know integer type: fixed, fixed width.
so in uh, because of it, inserts were rejected and which raised an error.
so this looks like they're using ruby to do this and it throws range error when i mimic this thing.
so if we would ever want to do this on our local machine, let's see, actually see what actually happens here.
we just want to know what happens when, when integer hits the limit, like that's the fun part of it, right?
hypothetically, what we are doing is, let's say we create a very simple table with two columns.
we create a table called users, so their table name was might be little different, but let's say we create a table with this two columns.
so id, which is of type integer, auto increment, and a primary key name is a vehicle type at 8, which is not null, right?
like here, you see the type is integer, right?
so let's just confirm once on what's the type, or basically what's the value of the integer, the maximum value of it.
yeah, so this is my sequel 8 documentation which says that, hey, if my integer type, oh, if my data type of a column is integer, the storage, the width of the integer is going to be 4 bytes and, because i have not given anything, the default value is a signed integer, which means it will support negative values and positive values.
so, which means even in our case, we would hit that particular limit.
now let's insert a few rows and see what actually happens.
now let's just add another row into this called b, and we should see id being auto incremented.
let's say i want to insert c, but with id100.
let's see what happens if i insert.
let's say i try to add d into this without specifying the id value.
would it say: hey, last value with my auto increment is set to three, now the next value should be four, so i'll insert it.
no, my auto increment value is set to two.
so let's put and watch what happens if i add the next value?
okay, now that we know that this is there, let's insert the maximum possible integer value into this.
so the maximum maximum possible integer value is two billion four hundred and seventy four, earning two.
now, what would happen if i try to add a new row without specifying the id, which means the auto increment should kick in?
right, the auto increment should kick in and not this one?
okay, let me insert the user f without specifying any id.
but the strange part is mysql throws a duplicate entry error or a duplicate key error, while the integer is out of range.
but in any case it's an exception of the value is not yet inserted into the table, right?
so what we have to do is integer hit the limit.
so what we have to do is integer hit the limit.
so first, uh, i'll just share my other screen, okay, so we'll start going through this particular part on seeing what exactly happens and how we go about it.
like from the databases side, what could have actually happened when, right through a duplicate key error?
so, given that it is stopping there now, from that outer increment we got that same value with the max value, and now when we are trying to insert the row, it would get the same id.
it would try to insert it, but hey, the row exist with the same id, so then insert would fail because of duplicate key error and not because of change data.
okay, but to be really honest, really, really honest, this is a very poor error, right, for a once in a lifetime situation where your integer hitting where you, where your two billion entries are there in the table, and you're throwing duplicate key error.
i mean, like you should have a separate exception for that, to be really honest, right, okay, so what did github do about to fix this issue?
so they wrote that they have an alert set at 70 when an integer, uh, when an auto incrementing integer column hits it 70 percent, they raise an alert, right, so which is something that we should- we should all should- be using doing on the table that are high injection rate may not be on our table, which is low injection rate, like, let's say you are, let's say you are- a listing page of a particular seller or something like that, where you have a very limited set of very limited set of entries, not not a large number.
so then integer limit is never going to hit there.
so you create a db monitoring service that that periodically pings them, let's say every one hour or every one day, to see if limit is reached or not, and then you raise an alert, right, that's very simple.
so the two approaches is approach number one: make id bigger, like: so, alter the table.
or make it a big integer, because as soon as you make it an unsigned integer you get a bigger range, because sine integer starts from negative 2 raised to 31, to positive, 2 raised to 31 minus 1, but an unsigned integer- wait, i'll just pull it now- but the unsigned integer, see, the maximum unsigned value is 4 billion, so you can directly double your space, right.
so it's not just as simple as an alter table command we fire, because there are lots of things involved.
because the table that you have created hit this limit implies that it is a high injection table, right?
so now, when you are trying to alter this, so you are already facing an outage, right?
first thing, you would say that, hey, i would want to run an alternate command, but before that, i want to set my transaction isolation level as in for this transaction.
basically, it says that, hey, i don't want to take any lock during alteration process so everyone else is allowed to read whatever it's there from the table, like basically, whatever could be working should be working, while when, when we are copying it, if there is no isolation level, which means the database can do it very quickly, like at the maximum speed, because we don't want to lose time on doing constraint checks because we are already having an output.
second, we set foreign key checks to zero again to speed up the entire process, like when we are copying the id column or something it would check for the foreign key references.
so no locks on the table, no funky checks, so that our alter table runs as fast as possible.
so in order table, what we do is we change like alter table users change id to id, integer 11, but unsigned.
so idea is to increase the width of the integer so that we can accommodate new stuff, and then we set it to not null, auto increment, then algorithm equal to copy and lock equal to shared right.
so at the end of this command- and this approach only works when your table size is small- if your table size is big, alter table on an id column with two billion rows will go on for days.
but if your table size is small, you can't do this right, okay?
but let's say, if your table- which i think in case of github, this could be true, this could be true is when you're origin, when your table has the data which you might not need- for example, some historical logs or something like that- because of which it hit that, because 2 billion is a big, big, big number.
if it reached 2 billion, there might be a possibility that the the data that you have on the table you might not need it, like it's just raw injection that is happening on the table.
if you have that use case where you can live without your old data for few days, right, what you can definitely do is you can swap the table.
uh, whenever data, whenever a big migration is run, whenever a table with large amount rose, we would typically do this.
so the idea here, the core idea here, is we create another table with the same structure as the old table, right, but with a larger id range.
we create a table like, let's say, users table had an outage, like it's it's limit reached.
so we created another table called users, underscore 2, like users, which means it will have the same schema.
the saves the same schema as your users table, right?
now what we do is we alter the users 2 table and make integer bigger.
and then what we do is we alter the table users 2..
so now users2 has the same schema as user1, right and have, but has a bigger integer.
now what we do is we alter the table users2 and set auto increment to 2 billion 147 million 483 648, not 47 but 48, because 47 was where to it was the limit of signed integer.
so this is like setting the counter, setting the sequence generator like the auto increment sequence to the big value that it was not able to insert before.
so now when a new row is added into this table- uses 2 table, it will start from this particular id, because now it's possible to add it.
right, so it will start from this particular id.
so the new table that we created uses to, we just rename it to users.
with this, what happens is now we'll have an empty users table with a very large integer, width of like.
it's it's a big integer, so 2 raised to 64 is the limit.
an nt, an empty users table with large integer starting from this id, right, so all data is moved into user's old table.
a new table is created, users to copied into like and it's renamed to users.
so all of that will be slowly copied into this user's table.
it will be part of this user's old table and the data will be slowly migrated and this can go on for days to be realized.
there are many ways to handle it, but if you ever hit into an outage where this hits the max value, this is what you should be doing, right?
amazing, it was a really fun thing when i first i didn't know- i saw hitting limit and that's where we moved into a distributed id generator.