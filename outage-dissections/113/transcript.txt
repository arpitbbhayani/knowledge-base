so on 5th of may 2020 github experienced a minor outage it affected their services like actions pages and dependable uh the outage happened because their mysql table hit its integer limit the id was set to auto increment id and it hit its limit right so in this video we'll dissect this outage uh we'll see it will mimic the same thing on local machine and see what actually happens uh then these folks have not really mentioned on what to do in that situation but i have two approaches that we should be doing in when we ever stuck in a situation like this which would come in very handy so let's jump into it so this is their incident summary that they have posted on the blog which says that hey during the incident a shared database tables auto increment id uh the auto increment id column exceeded the size that can be represented by mysql integer type so they created a table with a simple mysql integer type and it was set to auto increment so anytime you insert a new row if you do not give an id it would pick the next id and it would insert the row and obviously like everything has a limit my sql integer type has a limit of 2 billion 147 million 863 647. this is nothing but 2 raised to the power 31 minus 1 nothing fancy so the normal integer overflow part 2 raised to 31 minus 1 is where this person or this auto increment id stopped working so then what happened when the column reached this limit they start they attempted to insert large integers in the column so they were trying to insert data but the database rejected the value obviously database would not say hey because you know integer type fixed fixed width it cannot add more into it so in uh because of it inserts were rejected and which raised an error active model ranger so this looks like they're using ruby to do this and it throws range error when i mimic this thing uh i did not get range error i got something else i actually verified through it and everyone else got something else but this could be because of some version difference or something and while because the inserts were rejected this resulted in phi xx on our api endpoint so which causes the minor hiccup and it lasted for 2 hours and 24 minutes so if we would ever want to do this on our local machine let's see actually see what actually happens here let me go to my i so now here what do i have is i have a my sequel 8 version set up i don't know what version github was uh using back then uh might be 5 might be something i'm mimicking this thing on 8 because error seems error we just want to know what happens when when integer hits the limit like that's the fun part of it right so that's what we'll do uh we'll go through it we'll try to do i have some script ready hypothetically what we are doing is let's say we create a very simple table with two columns the two columns are id and name we create a table called users so their table name was might be little different but let's say we create a table with this two columns so id which is of type integer auto increment and a primary key name is a vehicle type at 8 which is not null right so you always have to get the name by inserting the table so if i run this query the table gets created i see the table content okay now what like here you see the type is integer right so let's just confirm once on what's the type or basically what's the value of the integer the maximum value of it okay back to the window yeah so this is my sequel 8 documentation which says that hey if my integer type oh if my data type of a column is integer the storage the width of the integer is going to be 4 bytes and because i have not given anything the default value is a signed integer which means it will support negative values and positive values so it will start from negative two billion one hundred forty seven million four hundred and eighty three thousand six hundred and forty eight as in two raised to power thirty one is basically minus two raised to power thirty one two this 2 raised to the power 31 minus 1 so that's the limit so one bit for signs out of the 32 bits that it have the most significant bit stays away for the sign as in uh is if it's a positive number or a negative number and every other bit is used for the value representation so that's the limit so which means even in our case we would hit that particular limit well we would be seeing that particular limit only okay so we created a table now let's insert a few rows and see what actually happens so i'll just insert a row i'll insert into user's name a i'll open this thing so we get id 1 name 8 so auto increment id starts from 1 name added as a now let's just add another row into this called b and we should see id being auto incremented so if i go here if i refresh i see another row added right then let's now let's um do something fun instead of not specifying the id let's specify the id this time and a little larger let's say i want to insert c but with id100 now what would happen would it ignore uh would the mysql engine ignore the id that is passed or would it say hey i am one two now the next id is three i'll use three otherwise it will insert it with hundred let's see what happens if i insert what we get is it inserts the row with id100 so 100 comma c now the fun part let's say i try to add d into this without specifying the id value now what would it do would it say hey last value with my auto increment is set to three now the next value should be four so i'll insert it no my auto increment value is set to two next value is three so i should use three but the last id was used is 100 so should it use 101 so let's put and watch what happens if i add the next value the next id value that it picked is 100 which means that if we ever be very explicit on the id that we are entering and if it's the maximum one it would replace it so auto increment the internal auto increment sequence would be using uh would would change its offset and start from the new one okay now that we know that this is there let's insert the maximum possible integer value into this so the maximum maximum possible integer value is two billion four hundred and seventy four earning two two billion one hundred and forty seven million four hundred and eighty three thousand six hundred and forty seven so two raised to power thirty one minus one now if i insert this the row should be inserted and it did if i check the output the row is inserted with this humongous id which is the cap now what would happen if i try to add a new row without specifying the id which means the auto increment should kick in right the auto increment should kick in and not this one ah i just want to insert the f1 okay let me insert the user f without specifying any id what would happen integer limit is hit it's it's already reached when i try to add something what would happen so let's run and see what happens it gave us an error the error is pretty weird the error says duplicate entry it is not saying out of range error it although uh because its error you definitely see that no value is added but the strange part is mysql throws a duplicate entry error or a duplicate key error while the integer is out of range why so right so github in that block say they got range error i'm not sure if it is explicitly done by orm is it version specific i tried it on my sequel a few words mostly icicle 5.7 but i still get the same error uh might be something i might not have configured because i found out there are something called a sequel modes which is strict mode linear mode and there are few other modes they might have some different database configuration but at least on the default one i'm seeing a duplicate entry error but in any case it's an exception of the value is not yet inserted into the table right so what we have to do is integer hit the limit we'll see this exception the developers will see array what's wrong and then the story of fixing things stuck so now let's i'll just quickly walk you through what we did and now we'll talk about the approaches that needs to be taken over here so first uh i'll just share my other screen okay so we'll start going through this particular part on seeing what exactly happens and how we go about it so dissecting github outage the same thing that i just did it's just written over here few key highlights is 2 raised to the power minus 30 minus 31 to 2 31 minus 1 their impact was actions pages and dependable got affected so which look like they're like which looks like that they have some installation token like they said that some installation token was gone so as in people who were trying to install something wanted to get some token and which is what started failing so not a major outage but few critical services forgot okay now comes the main part because we got a duplicate key error what does this actually mean like from the databases side what could have actually happened when right through a duplicate key error so auto increment this means that auto incrementing stops when the max value is reached so we started with one two three four so on and so forth we start other than 100 so on and so forth then as soon as we hit the max value after that your sequence now your sequence generator started spitting out the same value again and again so it did not increase so in c language and in a lot of other languages this typically wraps up so as in when the max value is when you do plus 1 it goes to the negative part of it right so the so the negative minimum and then it moves forward again so that's a cycle that is created by default but for mysql the maximum value plus 1 is equal to the maximum value only so it does not generate a new value so it just stops there so given that it is stopping there now from that outer increment we got that same value with the max value and now when we are trying to insert the row it would get the same id it would try to insert it but hey the row exist with the same id so then insert would fail because of duplicate key error and not because of change data so this is exactly what is happening and i went through a bunch of internal documentation and then they wrote a similar behavior i'm yet to go through the mysql source code to actually see where they have written this logic but at least on the documentation in the source code the all the comments that they have written in which they have actually specified this part where it stops iterating after that okay but to be really honest really really honest this is a very poor error right for a once in a lifetime situation where your integer hitting where you where your two billion entries are there in the table and you're throwing duplicate key error i mean like you should have a separate exception for that to be really honest right okay so what did github do about to fix this issue in the report they have not mentioned they have done anything like they have not been explicit about it obviously they would have fixed it because two years out it was there but they were but they didn't say what they actually did but what they definitely mentioned is the preventive measure like so this is a key thing that we typically avoid is we should be very explicit into like monitoring things right so they wrote that they have an alert set at 70 when an integer uh when an auto incrementing integer column hits it 70 percent they raise an alert right so which is something that we should we should all should be using doing on the table that are high injection rate may not be on our table which is low injection rate like let's say you are let's say you are a listing page of a particular seller or something like that where you have a very limited set of very limited set of entries not not a large number but let's say you're udemy and you want to list courses you'll never hit 2 billion courses i hope they do but to be really honest in real world you'll never hit 2 billion courses so then integer limit is never going to hit there so but on a table that has high injection rate you definitely need to put a monitoring place monitoring in place where you say that hey when my integer reaches 70 percent of its limit or 80 percent of its limit all right so that you have time to fix that right so you create a db monitoring service that that periodically pings them let's say every one hour or every one day to see if limit is reached or not and then you raise an alert right that's very simple but let's see hypothetically you reach this how do you mitigate because obviously github had to do it obviously gita had to mitigate the issue what they could have done like the two operations which i found on one superbly written article i'll link it in the description right the two operators that that book mentioned is something that i have personally heard of i've never used it to be really honest but when i tried it on my local version it actually works okay so the two approaches is approach number one make id bigger like so alter the table your id was an unsigned integer make it oh sorry your id was a signed integer make it an unsigned integer or make it a big integer because as soon as you make it an unsigned integer you get a bigger range because sine integer starts from negative 2 raised to 31 to positive 2 raised to 31 minus 1 but an unsigned integer wait i'll just pull it now but the unsigned integer see the maximum unsigned value is 4 billion so you can directly double your space right otherwise you can go for a big end which is 64 bytes so which is 8 bytes in size which is 2 raised to power 64. you would never hit that right you would never hit well possibly you would never hit that but that's an immense gigantic rate gigantic range so given that we have situation like this why can't we run and like how do we run an alter table command so it's not just as simple as an alter table command we fire because there are lots of things involved because the table that you have created hit this limit implies that it is a high injection table right so now when you are trying to alter this so you are already facing an outage right so when you fire an alter command what you would want to ensure is hey if i'm firing an alter table command it will take some time to run it and because alter table takes locks on the table so you would have to take a lot of precautionary steps before you even think of firing it right so what would happen first thing you would say that hey i would want to run an alternate command but before that i want to set my transaction isolation level as in for this transaction this should be the isolation level in case you folks are not aware of isolation level i will cover it in depth in the future sections for sure but for now just google it what's the isolation level here we are going with the least with the with me with the most lenient isolation level which is read uncommitted so that we don't basically it says that hey i don't want to take any lock during alteration process so everyone else is allowed to read whatever it's there from the table like basically whatever could be working should be working while when when we are copying it if there is no isolation level which means the database can do it very quickly like at the maximum speed because we don't want to lose time on doing constraint checks because we are already having an output second we set foreign key checks to zero again to speed up the entire process like when we are copying the id column or something it would check for the foreign key references and all those five six extra steps five six extra dix uh disk blocks reads is will make this process much slower so remove foreign key checks for this transaction right so no locks on the table no funky checks so that our alter table runs as fast as possible so now comes the alter table part so in order table what we do is we change like alter table users change id to id integer 11 but unsigned so here we are either you give unsigned or you give bigint as the type so idea is to increase the width of the integer so that we can accommodate new stuff and then we set it to not null auto increment then algorithm equal to copy and lock equal to shared right so basically what we are doing is we are copying the table internally internally we are copying the table when ultracommand is part of the id column the entire table would be copied and so internally the the table is copied into a different part and then the internal switch would happen the internal flip would happen from one table to another table and hence we are not doing it because this needs to be if you do it in place a lot of complications would alright especially because we want to fasten the we have to make the entire process very quick that's why we are doing this right so at the end of this command and this approach only works when your table size is small if your table size is big alter table on an id column with two billion rows will go on for days not kidding it will go on four days right so not a very solid idea but if your table size is small you can't do this right okay second approach which is a really interesting approach works like obviously not does not work at all but let's say if your table which i think in case of github this could be true this could be true is when you're origin when your table has the data which you might not need for example some historical logs or something like that because of which it hit that because 2 billion is a big big big number if it reached 2 billion there might be a possibility that the the data that you have on the table you might not need it like it's just raw injection that is happening on the table if you have that use case where you can live without your old data for few days right what you can definitely do is you can swap the table now here this is an interesting approach swapping the table so this sre teams this is like day in and day out thing to be really honest uh whenever data whenever a big migration is run whenever a table with large amount rose we would typically do this only this is our bread and butter so the idea here the core idea here is we create another table with the same structure as the old table right but with a larger id range very simple so the steps are again no locks we set to transition iso 11 to isolation level to read and committed so that it goes at a very fast pace we disable the foreign key checks now comes the next part we create a table like let's say users table had an outage like it's it's limit reached so we created another table called users underscore 2 like users which means it will have the same schema the saves the same schema as your users table right so users 2 and users now have the same schema now what we do is we alter the users 2 table and make integer bigger so similar to how we did in the previous approach we make integer bigger over here integer becomes big end unsigned so 2 raise to 64. 0 to 2 this is 64 humongous value we have no worries right and then what we do is we alter the table users 2. so now users2 has the same schema as user1 right and have but has a bigger integer now what we do is we alter the table users2 and set auto increment to 2 billion 147 million 483 648 not 47 but 48 because 47 was where to it was the limit of signed integer but now that it is a big integer we can just have one next integer so this is like setting the counter setting the sequence generator like the auto increment sequence to the big value that it was not able to insert before so now when a new row is added into this table uses 2 table it will start from this particular id because now it's possible to add it right so it will start from this particular id then what we do now that users 2 table is all set to accept the incoming insertion request now what we can do is we can just rename the table move users to users old and users to to users so the new table that we created uses to we just rename it to users with this what happens is now we'll have an empty users table with a very large integer width of like it's it's a big integer so 2 raised to 64 is the limit an nt an empty users table with large integer starting from this id right so all data is moved into user's old table a new table is created users to copied into like and it's renamed to users so any incoming insert request will now start inserting the rows properly so your outage is mitigated and then you slowly run this insert query that inserts everything into users table from user sort so all entries with smaller ids from 0 to sorry from 1 to 2 billion 147 number so all of that will be slowly copied into this user's table but because it started from uh 48 and not 47 so the new rows will have 48 till 47 it will be part of this user's old table and the data will be slowly migrated and this can go on for days to be realized this can go on for days right so that's where that's how you can quickly mitigate it part depending on what kind of situations you are in but the best way in any case is going to be having an alerting system so that you are prepared for this either you would want to run a migrator you might want to go into micro services architecture where odd might want to have a distributed id generator sort of stuff and just drop the id column over here there are many ways to handle it but if you ever hit into an outage where this hits the max value this is what you should be doing right amazing it was a really fun thing when i first i didn't know i saw hitting limit and that's where we moved into a distributed id generator we use an external id service but this is something that i learned for the father it was quite quite quite a joy to learn this part uh but doing this for id mitigation for normal data migration we used to always do that but this was something very new to me as well it was quite a fun where this this was a pretty solid hack where you are inserting it so i will just love this part nice so i hope you learned obviously like always when we learn more from the dissections then usual work so that's the fun part of it so i hope you found this amusing if you like this video give this channel like if you like this video please if you like this video give this video a thumbs up if you like the channel give this channel a sup and i'll see you in the next one thanks