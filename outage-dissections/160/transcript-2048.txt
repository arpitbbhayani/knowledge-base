the incident report says that as part of the database maintenance activity, the database team rolled out an updated version of proxy sql on monday, june 22nd.
the primary mysql node on one of our main database clusters failed and was replaced automatically by a new host.
so proxy sequel rollout happened.
a week later, master node crashed and after that something.
the first term that we heard new was proxy sequel.
so api server makes a connection to proxy sequel processable internally, makes connection to your mysql cluster.
a small classification here: whenever i use the term database it is always mysql cluster, because github is built on top of mysql and proxy sql is also specific to my sql.
okay, okay, so proxy sequel sits in between your api servers and your database.
any query that you fire goes through proxy sql and processible, forwards it to a database, gets the response and sends it back.
so your application layer doesn't need to know that it is connecting to proxy sql or mysql cluster.
why do we even need to use proxy sequel in production?
that proxy layer establishes connection with your database and there you can ration it.
right, and whatever the request comes in from your server gets gets accumulated over here and they are fired onto your database, one after another.
so your database you never create more number of connections than your database can never handle, because everything goes to proxy sql- the buffering, the weighting of firing of sql queries, and every single thing happens at the proxy sql layer.
so this protects our database very well, right, reason number one.
because every single query that you fire goes through proxy sql.
so, for example, if you have a three node mysql cluster- one master, two replica- one of the replica is created to serve very frequent reads- small frequent reads- while another one is used to serve gigantic analytics query that you are finding on a massive table.
so, for example, you would want to do something like this: that whenever someone fires a query on a specific table, it should go to replica 2.
whenever someone fires a query on any other table, it should go to replica one, while write should go to master.
this can be seamlessly done on proxy sql, so your api server need not know which data node to connect to to get things done.
proxy's equal would forward it to the corresponding database.
and, yeah, and your performance and your architecture is performed because api server does not have to connect to all the three clusters and get things done.
so that is a very, very, very big advantage that we get by using proxy sql.
as soon as you have a proxy layer sitting in between, the first thing that you might have thought of is caching, like because everything is going through proxy sql, so you can cache sql query responses.
for example, select star from users where id is equal to 10 can be very well cached on proxy sql and after, let's say, a minute or so that cache is invalidated.
it comes to proxy sequel.
so places where you might want to cache things, proxy sequel can be one of such places to do it.
so temporary access management is also done at proxy sequel layer.
so this way, by having a proxy list sitting in between, it makes life so seamless and simple for engineers and api and your api servers to connect to your database.
so that's why scale companies use proxy sql, right?
so now that we have our understanding on proxy sql, let's see what exactly failed.
so, as per incident report what it says, that, uh, the primary mysql node on one of the main database cluster failed and was replaced automatically by a new host.
okay, so your existing master crashed, then an automated failover happened.
then the new master that was appointed, that also crashed, right?
so what happened is this looks like a very classic case of you promoting a master and that master, not being able to like, is not capable of handling the existing load, so it crashes.
so this looks like a case of cascading failure, right?
so what it says is: within seconds, the newly promoted master crashed.
orchestrator's anti-flapping mechanism prevented subsequent automatic failovers.
so orchestrator detected failure and quickly promoted the replica to be the new master and it and it, uh, helped us ensure that there are no cascading like something called as anti-flapping pattern.
so first of all, we know that when the first master crashed, another master was like.
another master was appointed, so replica was promoted to be the new master and it was done automatically.
so rather, orchestrator is a tool that is used to do mysql topology management- just fancy terms of managing mysql clusters- and doing: hey, this is the master, this is a replica.
so it's all about topology management as such for uh through a tool called orchestrator, right?
so what it does is uh orchestrators- anti-flapping mechanism- prevented subsequent automatic failures.
so, apart from having your api servers, your mysql cluster, your proxy sql, there is a fourth component called orchestrator.
orchestrator job is to keep an eye on your database topology, as in how many data nodes you have, which one are replica, which one is master, what is the replication lag if master goes down, promote one of the replica automatically.
when a master goes down, in few seconds a replica is promoted to be the new master, right?
see, you're saying, hey, i only have one data node, i only have one database server.
so it is very important to have discovery, to have you being able to see what's happening in your database cluster, like how is it performing, how is the replication lag right, understand the performance and the topology that you have in your record.
but there has to be a visual way to see it directly by connecting to a database cluster, which is where orchestrator comes in.
so whenever there is any failure, not just master, but even if a replica goes down, it takes it out of the topology and puts it another one uh within that.
so it detects when master goes down, it promotes a replica to the master.
orchestrator is continuously keeping an eye on all the databases in your network to just see how everyone's behaving right.
so anti-flapping is a mechanism through which we prevent cascading failure by not doing automated failovers.
so we just saw how orchestrator detected that the master was down and it promoted another replica to be the master.
it could have promoted another replica to do to be the new master.
okay, let me just give you a very quick walk through on how cascading db failures is a very common thing to happen.
that master node went down right and let's say you have three replicas and you have some x amount of load coming your way.
when master went down, you promoted one replica to be the new master.
now these three nodes has to handle all the load, all the right load, coming to this one master.
now that master goes down because of that, because it is serving partial reads and writes or some other replica would go down.
so this is flapping, where one flap shot, it flapped, other it flapped, other it flapped, other right, so that's like: ah, and this is what typically happens in a major outage at any organization.
so that's where what we need to ensure that, or what orchestrator does, and it has a mechanism or a configuration that says anti-flapping.
so what anti-flapping would do is, if it sees a master go down, it promotes a replica to be the new master.
if this new master also goes down, it would not promote any other replica, because it would know that if i might promote another replica, that would also go down because this second one went down.
like, although it's not making things better, your master is still having an outage, but your reader, your reads, can still be served.
that is anti-flapping mechanism, right?
okay, so this is how it prevents us, how it prevents, uh, cascading failures of database.
for until a pull-off period, and this school of period is typically five to six minutes, where after five to six minutes only it would promote another replica to be the new master.
so that's where, because orchestrator did not again promote another replica to be the new master, what it did?
so that's where what github team did is, after they recovered uh, so uh, because subsequent failovers were stopped with anti-flapping mechanism, after we recovered services, after we recovered services manually, the new primary became cpu start and crash.
okay, so because automated failover did not happen, what would need to do some engineer has to do it manually, because until the cool off period, orchestrator would not promote another replica to be the new master and an engineer needs to be involved.
i would just add another, like i would manually promote a replica to be the new master.
they tried to recover the services manually and when they did it, the new replica that was promoted also crashed.
the new replica cry, the new master crashed.
the newly promoted master also crashed.
so what they did is because what they are seeing is, anytime they are promoting or replica as a new master, things are crashing.
they guess, like someone might have recalled, hey, we just upgraded the version of proxy's equal.
so after they tried everything to get the database cluster up, it wasn't happening.
so they reverted the code that required the upgraded version of proxy sql.
and finally, when they promoted a new master, it started accepting rights and things became normal.
the changes happened on june 22 and a week later a node crashed and it was not recovering or any new node was not taking its place properly, right?
so every new master that was spun up was crashing, and it was a week, so there was a gap of one week.
right, okay, just two things to cover, uh, applications.
so they it because here they did not even know why it is crashing or new master coming up.
so, at scale, when you are managing any database, you need to understand how the core dump looks like, what kind of things it has, how to use it to debug such issue.
but i would really recommend you to just google about it and see how to debug things- and it happens at scale- how to debug it using my sequel code.
okay, four things we learned: uh, how companies handle production databases.
we learned about proxy sequel and orchestrator.
how proxy sequel sits in between and does a lot of things.
fancy orchestrator does auto, does automate- a lot of manual things that an engineer required to do so process equal on orchestrator at a scale.
when you have a large mysql cluster, you would typically see proxy sequel in orchestrator in your architecture.
another thing we learned, and the fourth one: when nothing works, do a full revert, right.
so understand what changed, what happened in the architecture or in the code and do a full revert.
i've used proxy sql a lot.
uh, orchestrator i've definitely not used, but some scripts do these sort of things automatically at few of the places that i've worked uh, so yeah, proxy's equal orchestrator are not just theoretical, it is very well used in the industry across and i hope- and you should be exploring that, uh, if you would want to understand how databases are very well managed in production.