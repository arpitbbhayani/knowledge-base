so how are databases managed in production when master goes down we all have heard that some replica is chosen and promoted to be the new master is this something that an engineer does is this something that is done manually or are there tools to do it like how how it actually happens in this video we dissect yet another data about it but apart from understanding what exactly happened and how github mitigated it we would spend a significant amount of time learning how databases are managed in production and the kind of tools that companies like github use to ensure optimal db performance and very high availability but before we move forward i'd like to talk to you about a course on system design that i have been running for over a year now the course is a cohort based course which means i won't be rambling a solution and it will not be a monologue instead a small focused group of 50 60 engineers every cohort will be brainstorming systems and designing it together this way we build a solid system and learn from each other's experiences the course to date is enrolled by 600 plus engineers spanning nine cohorts and ten countries engineers from companies like google microsoft github slack facebook tesla yelp flipkart dream 11 and many many many more have taken this course and have some wonderful things to say the coolest part about the course is the depth we go into and the breath we cover we cover topics ranging from real-time text communication for slack to designing our own toy load balancer to quick buzzes live text commentary to doing impressions counting at scale for any advertisement business in all we would cover roughly 28 questions and the detailed curriculum uh split week by week can be found on the course page which is linked in the description down below so if you're looking to learn system design from the first principles you will love this course i have two offerings for you the first one is the live cohort business which you see on the left side and the second one is the recorded course which you can see on the right side the live cohort based course happens every two months and it will go on for eight weeks while the recorded course contains the recordings from one of the past cohorts as is if you are in a hurry and want to binge learn system design i would highly recommend you going for the recorded one otherwise the live court is where you can participate and discuss things live with me and the entire cohort and amplify your learnings the decision is totally up to you the course details prerequisites testimonials can be found on the course page at binary dot me slash master class and i would highly recommend you to check that out i put the link of the course in the description down below so if you are interested to learn system design go for it check out the link in the description down below and i hope to see you in my next cohort thanks so like always we start with the incident report the incident report says that as part of the database maintenance activity the database team rolled out an updated version of proxy sql on monday june 22nd the incident happened on 29th so a week later the primary mysql node on one of our main database clusters failed and was replaced automatically by a new host a lot of things to digest we'll start one by one so on may 22nd uh sorry on june 22nd they rolled out an updated version of proxy sequel this is the first new thing we heard so proxy sequel rollout happened a week later master node crashed and after that something something happened we are yet to go through it but let's understand the first term that we heard new was proxy sequel so what happened so what exactly is proxy sql at scale companies do not directly connect to the database for example api servers do not directly connect to the database they put something called as a proxy sql or a proxy layer in between the reason they put a proxy layer in between because it makes things very seamless so api server makes a connection to proxy sequel processable internally makes connection to your mysql cluster a small classification here whenever i use the term database it is always mysql cluster because github is built on top of mysql and proxy sql is also specific to my sql okay okay so proxy sequel sits in between your api servers and your database any query that you fire goes through proxy sql and processible forwards it to a database gets the response and sends it back so your application layer doesn't need to know that it is connecting to proxy sql or mysql cluster it would have a mysql endpoint in this case it would be proxy sequels endpoint and the proxy sequel understands the mysql protocol so whatever it gets in the input it would forward it to an appropriate or a proper database node would get the response and send it back so why why do we even need to use proxy sequel in production one main reason to do it is to do better connection handling so let's say if your api servers scale scale as in they you had hundreds and thousands of api servers right each of those api server would make connection to your mysql cluster that would put unnecessary load on your mysql cluster so what do what do you need to do you need to have a proximate setting in between that proxy layer establishes connection with your database and there you can ration it then you can rush in the number of connections because your database cluster has a theoretical limit on the number of connections that it can handle so proxy sequel ensures that that limit is never breached right and whatever the request comes in from your server gets gets accumulated over here and they are fired onto your database one after another so your database you never create more number of connections than your database can never handle because everything goes to proxy sql the buffering the weighting of firing of sql queries and every single thing happens at the proxy sql layer so this protects our database very well right reason number one reason number two is gatekeeper and enhance security so what happens because every single query that you fire goes through proxy sql proxy sql is acting as your is acting as your gatekeeper now what gatekeeper would do it does two things security and routing so for example if you have a three node mysql cluster one master two replica one of the replica is created to serve very frequent reads small frequent reads while another one is used to serve gigantic analytics query that you are finding on a massive table so for example you would want to do something like this that whenever someone fires a query on a specific table it should go to replica 2 whenever someone fires a query on any other table it should go to replica one while write should go to master this can be seamlessly done on proxy sql so your api server need not know which data node to connect to to get things done rather it would seamlessly fire query to proxy sql proxy's equal would forward it to the corresponding database all the rules that you will write that a write should go to master frequently it should go to a replica reads on this main this gigantic table should go to replica 2. this can be handled very well by proxy sql so it makes life of developers and engineers very simple and yeah and your performance and your architecture is performed because api server does not have to connect to all the three clusters and get things done rather it just makes a connection to process equal processor with internally makes connection to the corresponding data nodes so that is a very very very big advantage that we get by using proxy sql third advantage that we get is caching as soon as you have a proxy layer sitting in between the first thing that you might have thought of is caching like because everything is going through proxy sql so you can cache sql query responses for example select star from users where id is equal to 10 can be very well cached on proxy sql and after let's say a minute or so that cache is invalidated but up until then your request would not even have to go to a database it comes to proxy sequel proxisable has the cached response it would just send it right so places where you might want to cache things proxy sequel can be one of such places to do it the fourth one temporary access management this is an important one so what proxy sql allows you to do is it allows you to create temporary credentials for example you don't want to share your database username and password with every single engineer of the organization so what proxy sequel can do is process equal can create a short time leave a short time bound user let's say some random username is given and the credentials would work only for 1r after one hour it would be automatically deleted so temporary access management is also done at proxy sequel layer so this way by having a proxy list sitting in between it makes life so seamless and simple for engineers and api and your api servers to connect to your database everything just goes through one place and yeah you might think hey i'm adding an extra hop but it is minuscule all it does is just understands the basic tcp request that is coming in does some processing and forwards it back but the advantages that you get are humongous so that's why scale companies use proxy sql right okay so now that we have our understanding on proxy sql let's see what exactly failed so as per incident report what it says that uh the primary mysql node on one of the main database cluster failed and was replaced automatically by a new host within seconds the newly promoted primary crashed okay so your existing master crashed then an automated failover happened then the new master that was appointed that also crashed right so what happened is this looks like a very classic case of you promoting a master and that master not being able to like is not capable of handling the existing load so it crashes and then you promote another one then that also crashes so this looks like a case of cascading failure right so here what we also see is a new term new term called orchestrator so what it says is within seconds the newly promoted master crashed orchestrator's anti-flapping mechanism prevented subsequent automatic failovers okay now this is yet another new term that we all should be learning about so orchestrator detected failure and quickly promoted the replica to be the new master and it and it uh helped us ensure that there are no cascading like something called as anti-flapping pattern we will come to that so first of all we know that when the first master crashed another master was like another master was appointed so replica was promoted to be the new master and it was done automatically then there is nothing automatically it's just like lack of humor intervention but there has to be some process that is doing it that is orchestrator so orchestrator is a library so rather orchestrator is a tool that is used to do mysql topology management just fancy terms of managing mysql clusters and doing hey this is the master this is a replica if master goes down promoter replica load should be equally balanced between replica and whatnot so it's all about topology management as such for uh through a tool called orchestrator right so what it does is uh orchestrators anti-flapping mechanism prevented subsequent automatic failures now this is where things become interesting so apart from having your api servers your mysql cluster your proxy sql there is a fourth component called orchestrator orchestrator job is to keep an eye on your database topology as in how many data nodes you have which one are replica which one is master what is the replication lag if master goes down promote one of the replica automatically so this is where uh you don't need humor you don't need human intervention when a master goes down in few seconds a replica is promoted to be the new master right so orchestrator is used to is typically used to solve two problems first is discovery like you need to know like at scale see you're saying hey i only have one data node i only have one database server see at small scale it doesn't matter but at massive scale you might have 10 or 15 rate replicas uh connected to a multi-master setup split across geography so it is very important to have discovery to have you being able to see what's happening in your database cluster like how is it performing how is the replication lag right understand the performance and the topology that you have in your record like we draw diagrams like we draw diagrams with arrows and data based uh icons and what not it makes us understand how the topology looks like visually but there has to be a visual way to see it directly by connecting to a database cluster which is where orchestrator comes in but apart from that second big advantage more engineering savvy is recovery so whenever there is any failure not just master but even if a replica goes down it takes it out of the topology and puts it another one uh within that so recovery is another key aspect of orchestrator so it detects when master goes down it promotes a replica to the master if a replica goes down it takes it out of the topology so anything and everything around topology management is what orchestrator does orchestrator is continuously keeping an eye on all the databases in your network to just see how everyone's behaving right so keep feature that these guys talked about is anti-flapping pattern what exactly is anti-flapping so anti-flapping is a mechanism through which we prevent cascading failure by not doing automated failovers so we just saw how orchestrator detected that the master was down and it promoted another replica to be the master but that master also went on so what orchestrator could have done it could have promoted another replica to do to be the new master so that is where you would see cascaded db failures and this is a very common uh reason why outages happen okay let me just give you a very quick walk through on how cascading db failures is a very common thing to happen you have a master node that master node went down right and let's say you have three replicas and you have some x amount of load coming your way when master went down you promoted one replica to be the new master now you have one master and two replicas the load is same your master or your now this database cluster instead of having four nodes now has three nodes now these three nodes has to handle all the load all the right load coming to this one master now that master goes down because of that because it is serving partial reads and writes or some other replica would go down so when that new master goes down you promote another replica to be the new master and then that would go down then from another replica to be the newest then that would go down so this is flapping where one flap shot it flapped other it flapped other it flapped other right so that's like ah and this is what typically happens in a major outage at any organization so that's where what we need to ensure that or what orchestrator does and it has a mechanism or a configuration that says anti-flapping so what anti-flapping would do is if it sees a master go down it promotes a replica to be the new master if this new master also goes down it would not promote any other replica because it would know that if i might promote another replica that would also go down because this second one went down right so this is how it tries to prevent like although it's not making things better your master is still having an outage but your reader your reads can still be served right your writes are failing but the reads are served but at least it would prevent you from having a lot of nodes going down only a couple of nodes went down and you stopped there that is anti-flapping mechanism right okay so this is how it prevents us how it prevents uh cascading failures of database right but for how long for until a pull-off period and this school of period is typically five to six minutes where after five to six minutes only it would promote another replica to be the new master but for that time you cannot have a downtime right so that's where because orchestrator did not again promote another replica to be the new master what it did it helped us prevent cascading failures and this requested for a human intervention so that's where what github team did is after they recovered uh so uh because subsequent failovers were stopped with anti-flapping mechanism after we recovered services after we recovered services manually the new primary became cpu start and crash okay so because automated failover did not happen what would need to do some engineer has to do it manually because until the cool off period orchestrator would not promote another replica to be the new master and an engineer needs to be involved so engineer what engineer would do i would just add another like i would manually promote a replica to be the new master so that's what the team did they tried to recover the services manually and when they did it the new replica that was promoted also crashed so they tried automated failover the new replica cry the new master crashed then automated did not happen because that was the configuration of anti-flopping mechanism so they did it manually the newly promoted master also crashed so how did they even recover out of this so what they did is because what they are seeing is anytime they are promoting or replica as a new master things are crashing so after that they don't have any other option they rolled back to the previous version of proxy sequel and disabled a change in our application so that is the final result like they tried to keep things record because they did not like it happened a week later and that is the highlight it did not happen immediately so they didn't know what was the root cause for this they guess like someone might have recalled hey we just upgraded the version of proxy's equal could that be the reason so after they tried everything to get the database cluster up it wasn't happening so that's why what they did is they did a full revert so they reverted the code that required the upgraded version of proxy sql they downgraded the version of proxy sequel that they recently upgraded so they rewarded everything they did a full reward and that solved the problem and finally when they promoted a new master it started accepting rights and things became normal okay right so that is where you would all you would a lot of times see this and this is like it's like area they should have reverted it earlier they could not like the changes happened on june 22 and a week later a node crashed and it was not recovering or any new node was not taking its place properly right so every new master that was spun up was crashing and it was a week so there was a gap of one week how would anyone recall that what happened and what did not right so that is where it was very tricky some good engineer might have recalled hey we upgraded the proxy sql version and they did a full revert of they downgraded the proxy sql version uh two and basically reverted back to the old version and altered the code that required the new version that was there after doing this everything recovered and the storm was done okay so what did we learn from this we learned four very critical things oh but before we talk about that there is one very interesting thing a wait wait wait wait wait okay so what is it is uh we are analyzing application logs mysql codems and internal telemetry as part of continued investigation into cpu starvation issue right okay just two things to cover uh applications so they it because here they did not even know why it is crashing or new master coming up this crash again again and again so for that in order to debug better you need application locks and mysql core dumps this is very important so at scale when you are managing any database you need to understand how the core dump looks like what kind of things it has how to use it to debug such issue so mysql code is something that you should definitely google and check out it's a little heavy on the engineering side but i might cover it in future but i would really recommend you to just google about it and see how to debug things and it happens at scale how to debug it using my sequel code so uh uh to be honest a very fascinating thing and one more thing that they could have done uh over here is to have a list of everything that is deployed or changed at a very handy place so that it would not take longer time for anyone to discover hey what did we push last asking different teams to do it having a single place of all the change logs that happened in your architecture should be there so that someone could just refer it and see hey what might have caused this issue right okay that is another take away from this okay four things we learned uh how companies handle production databases we learned about proxy sequel and orchestrator how proxy sequel sits in between and does a lot of things fancy orchestrator does auto does automate a lot of manual things that an engineer required to do so process equal on orchestrator at a scale when you have a large mysql cluster you would typically see proxy sequel in orchestrator in your architecture third is importance of anti-flapping policy like anti-flapping mechanism and or anti-flapping policy we saw how it was a brilliant idea that after one failover you would not do another failover another automated failure because it might bring down your entire person there would be a lot of nodes to recover another thing we learned and the fourth one when nothing works do a full revert right so understand what changed what happened in the architecture or in the code and do a full revert in most cases that would work right and that is typically the last resort it's like restarts right that it's like when a computer is not working we just do a reset and everything seems to work right similarly in during any production outage full revert is always an option so a lot of companies typically when they are unable to mitigate it quickly they typically do this part and if you see this outage went uh it out it lasted for two and a half hours that's an insane amount of time uh for your database cluster to not accept rights right okay that's it that's it for this one i hope you learned how databases are married production and to be honest is not theoretical i've used proxy sql a lot uh orchestrator i've definitely not used but some scripts do these sort of things automatically at few of the places that i've worked uh so yeah proxy's equal orchestrator are not just theoretical it is very well used in the industry across and i hope and you should be exploring that uh if you would want to understand how databases are very well managed in production nice so yeah that's it that's it that's it for this video if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sup i post three in-depth engineering videos every week and i'll see in the next one thanks again [Music] you