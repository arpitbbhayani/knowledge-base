thanks.
so this is the incident report that we would dissect.
what it says is that our service monitors detected a high error rate on creating check suits for workflow runs which affected action service.
this incident resulted in failures or delays of some cute jobs.
that's okay, but now we'll drill down into what exactly happened.
so what happens is whenever you push something onto a repository, let's say you create a pull request, and then you add commits to it.
for every pull request or for every commit you add, there are some fixed set of checks that github has to execute in order to ensure that it is okay to be merged.
for example, it could be simple linting checks.
it could be simply, it could be simple: secret detection checks.
you can even write your or you can even add your own checks as part of a commit or as part of your pull request, right?
so they let's say what happens is you have multiple repositories.
they all talk to this action service to run these sort of actions and checks on the particular report.
before any action is run, a checksuit is created, right, and this checksuit contains a bunch of checks that needs to be executed.
once all of those checks are done, then you are okay to merge that particular repository- uh, sorry, to merge that particular branch- into the parent branch, right?
now here, one peculiar thing to observe is if it is something, or if a workflow is something that is running on every commit that happens.
this is a very high frequency event, which means that there is a chance that the id might exhaust, and this is exactly what happened.
so whenever a check is created or whenever a check is to be executed, every execution of it is given a unique identifier.
this identifier, obviously, is a row identifier in your relational database, so github very heavily uses mysql, i am assuming, for this data, for this use case also, they are using mysql itself.
for every execution of this check there is an id in the database.
so basically, there is an entry in the table, right?
that, given that this checks run upon every single commit, that happens, there is a lot of entries that happen on the database or in the table, which would make the table explode, right?
so yeah, when i say explode, it's not really going down, but it's more about having large number of entries in it.
so what happens here is your id is of type integer, which is an auto incrementing id, and integer by default.
when you create a mysql table, the size is 32 bits.
so when what happens is because it is a 32 bit, it has a limit, let's say 2 billion.
uh, it's a sign integer and the limit is 2 billion.
but the idea is that because it is a, because it is an integer or 32-bit integer, it has a relatively smaller range.
and for a use case which is this frequent, where you are running checks on every single commit and every single execution is having an entry in the database, this becomes very possible that you will run out of your id, right.
this exact thing happened and, uh, in the past github also experienced an outage because their id or their uh or their id column hit its max limit.
you can check that out on this very channel and i would highly encourage you to do that, because then i discussed how to mitigate that issue, like, if you ever come into that situation, how to fix that with two or three approaches.
so go check that out if you haven't already.
like github already had those checks in place which could tell them: hey, this tables id range might exhaust, so do something about it.
so in such situations, what uh engineers do is they alter the table.
they change the column type from end to big.
it begin is a 64-bit integer, so it gives you a massive, massive range, so that possibly would never uh be exhausted, but it's a gigantic range, right.
so github already anticipated this and six months back they already fixed it.
they already ran the migration and they converted the column type from integer to big integer.
what went wrong if github already was prepared for a situation like this?
so this clearly shows that they had all of the necessary checks in place.
they already did the migration and they were very happy that, hey, this would never happen.
what exactly went wrong?
so what went wrong is very interesting.
let's quickly go through the docu, the incident report.
so, upon further investigation of this incident, we identified this issue was caused by check suit ids exceeding max in 32.
we had anticipated the checksuit id and checkering id would cross the limit and migrated all the database columns.
to begin six months back clearly shows a very good preparedness from the github team in order to ensure that outage because of id exhaustion never happens.
we failed to identify a graphql library we depend on that uses integer 32 when unmarshalling json.
they prepared their database.
they migrated their columns from integer to big integer.
they thought it's hunky dory and uh, their code basis never had an explicit check for uh in 32, which means even a 64 integer, like ruby, go c sharp, all of them supports like big integers.
so that's not a like big integers, not infinite length of integers.
so they were happy that that backend does not need to be changed and the database was prepared.
we failed to identify a graphql library we depend on using in 32 and unmarching json.
so i'll give you a very visual way to see what exactly happened.
github uses some graphql library that, suppose in 32 when marshalling, or that expects in 32 and unmarkedly json.
so when some job needs to be run on a repository, they create a checksuit that we just saw the creation of checksuit worked just fine because the database was already on on integer 64 or a big integer.
so their checksuit worked just fine.
so creation of entry, that, hey, this job needs to be executed, it worked fine.
but what failed is they made an entry into the database that hey, this needs to run, but someone has to run it right.
so what typically happens in such cases is: entry in the database was successful.
there is a service or a process that pulls this rose out of the database and puts it into a queue so that executor can execute it.
so while we create an entry into the database, just a marker that i want to execute this right, but someone has to take it out from the database and actually execute it.
so a typical flow that happens is: from db, a process pulls it, puts it into a queue- there are multiple executors who read from this queue- and then executes it job.
what failed is from puller to enqueuing it into a queue.
this failed because this is where the library, the graphical library that they were talking about, comes into action.
don't know the internals of it, how it comes in, but this is where that graphical library comes in, that it pulls or whatever is pulled, or it tries to pull from the database and puts it into the queue.
this service or this library or this dependency failed, and because it failed, the jobs were not getting queued into the queue for execution.
so that's where what happened is: because jobs failed to be queued, check suits were left in a pending state, they were not getting completed, like they were just waiting, waiting, waiting, waiting, and this was the outage.
they immediately pushed the code fix so as to ensure that the tasks that are cured- uh, so the tasks are queued for execution and then they are picked up by executors to finally execute.
right, so they push the code fix to do that.
that would not have used that particular dependency, but some other dependency that supports in 64..
the idea was to put a code fix that ensures that the tasks are queued for execution, because executors didn't have any problem.
they were working just fine.
database was working fine.
the only thing that failed was puller trying to pull from the database and putting it onto the queue.
this, this particular thing, fit, so mostly it was the outage of the puller that pulls it from the database and puts it into this, like somehow the graphql dependency came into play over here, right?
when actions identify the job needs to be done on a repo, we first create a task, a checksuit.
those individual checksuits were created successfully because database could handle values greater than interdup right database.
they were all prepared for database, but processing those response failed due to external library.
we were using expecting and in 32 and that's where unmatched link failed.
jobs failed to be queued.
we deployed a code fix to mitigate after validating it.
something on the company, right, that's fine, we know what they did to fix it.
another impact of this was that there uh, because the search uh service also used this particular id.
obviously you pull the data out from the queue and uh basically whenever the task is getting created.
so the search service also had an impact because it was uh returning incomplete results, like the workflows or the- yeah, the workflows that had ids greater than integer 32.
hypothetically, let's say they are using elasticsearch further indexing, right, because the job that had to put it into elasticsearch dependent on that dependency.
right, that failed.
right, because it was intended to do the search service itself, like the data was not getting indexed in elasticsearch, so someone has to take the data and put it into elasticsearch.
it was also the responsibility of that same library that they depended on, which failed, and hence the documents having ids greater than integer 32's limit were not because they were not getting unmarshaled, they were not getting pushed into elasticsearch.
so search results for that particular workflow were incomplete.
so they were expect, like users might expect, that these workflows should be reachable from search, but search result never showed that particular workflows because the documents were not even indexed in elasticsearch.
so this was another peripheral impact of this particular outage, right?
then we might have blind spot in a system which we might have overlooked and which may cause an outage like this and what they said.
what we learned from this outage is to help avoid this class of failures, like not just this failure, but this class of failures.
github team audited and updated usage of all external libraries, so they audited every single external library that they are using and they ensure that they don't run into the same class of failures, for example, failure due to integer 32 exhaustion.
this is what happens when you blindly trust any open source repo, any repository, not just opens, but even proprietary.
you need to know on what's happening behind the scenes so that you avoid this sort of outages.
right?
right?
right?
pretty interesting, pretty amusing outage.
i loved it because it taught us that, even if you are prepared, it's if things are supposed to go wrong.
there are always blind spots in the system, especially when you hit a particular scale.
nice, so yeah, this is all about this outage.
if you guys like this video, give this video a thumbs up.
if you guys like the channel, give this channel a sup.
thanks, saturn.