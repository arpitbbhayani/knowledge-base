so could something go wrong when you're trying to run a multi-table command on a massive mysql table? on november 27, 2020, when github had a major outage across their services- like github actions, api requests, code spaces, get operations, pull requests and many more- and it all began with running a simple alter table command, but on a gigantic mysql table. there is so much to learn from this outage. so in this video, let's dissect this outage and do an intense deep dive to extract five amazing insights about the database architecture. see their mitigation strategy and a potential long-term fix. but before we move forward, i'd want to talk to you about a code-based coastal system design that i have been running since march 2021, and if you are looking to learn system design from the first principles, this course is for you. yeah, because this is a cohort based course. it will not just be me rambling a semi-optimized solution, thinking it's the most amazing solution out there. instead, it will be a collaborative environment where every single person who is part of the cohort will can pitch in his or her ideas and we will evolve our system around that right. every single problem statement comes with a brainstorming session where we all together brainstorm and evolve our system. that's why everyone understands the kind of trade-offs we made while making that decision. instead of just saying, hey, we'll use a particular queue, we'll have the justification why we use only that queue, why we use that particular database, why sequel, why not no sql? right? how are we leveraging throughput? how are we ensuring that our system skills? that's the highlight of this course. this course is taken by more than 500 engineers to date, spanning nine countries and seven cohorts. right, people from all top companies have taken this course and the outline is very intriguing. it's very exciting. so we start with week one around. we start with the core foundation of the course where we design online offline indicator. then we try to design our own medium. then we go into database where we go in depth of database locking and take and see few very amazing examples of data log or database locking in in action and how do we ensure that our system scales through that. then the third week is all about going distributed, where we design our load balancer. i'll walk you through the actual code of of a toy load balancer and understand how pcb connections are managed and how simple it is to build load balancer. then week four is about all about social networks. week five is all about building your own storage. indians, like we'll build that intuition on, if you were to ever design your storage agent, how would you do that? right then? week six is about building high throughput system. seven is about building uh ir system- basically information retrieval systems and add-on designs, where we design our own message brokers like sqs, where we design distributed task scheduler. and we conclude the course with week 8, where we talk about the super clever algorithms that has powered or that has made those systems possible. right, i have also attached a video, verbatim as is from my first quote, where we designed and scaled instagram notifications. i will highly encourage you to check this video out. right and now, back to the video. so, in order to dissect this outage, we will refer to the official blog post made by github on this particular incident. i'll link this in the description down below. so what happened? in november? we experienced an outage, one incident resulting in significant impact on degraded state of availability for core github services. negative actions, api requests, code spaces, get operations, issues, packages, pages, pull requests- almost every single thing that we use day in and day out was impacted by this. so what happened? we encountered a novel failure mode when processing a schema migration on a large mysql table. schema migrations are a common task at github- often takes weeks to complete, weeks to complete. what is schema migration? schema variation is basically auto command that you fire in order to add a new column, delete a new column, update the data type of a particular column, create new indexes- all of that. this is this is typically what schema migration is all about, and it is so big that it often takes weeks to complete. why? let's let's dive into this first insight and understand why something as simple as an alter table command would take weeks to complete, right, so alter table command is too slow for large tables. and why is it slow? because with alter table, specifically specifically when you are adding a new column, what would happen is if the entire row needs to be rewritten. either you are using copy or you are using input. so there are two ways to run alter command. you can either specify the algorithm- which basically mysql calls it algorithm, as copy or in place. in either cases, the row needs to be rewritten. when the row read needs to be rewritten, which means that a row is read, a new column is added and that row is rewritten, and then the pointers are arranged in the b plus free loader, because of which, if a table is large, it would have to go through all of those rows, add a new column and then write it. i'm not sure what migrations github did, but it could be something as big as this. so well, it could be as intense as adding a new column. in most cases, because that's what takes the longest time- right and beat copy, beat in place. the table is locked, or, if you are doing it in place, the row is locked for some time and then it is rewritten with the new schema right. so if your table is large, it will take larger time for you to run and complete the migration and for those entire duration, the table or the rows, depending on what your uh, what your database tuning is- they would be locked right. so that impacts the performance, uh, that impacts the overall throughput of your database. but most importantly, here it what it would definitely lead to is it would lead to a long time for your migration to complete, right. in most cases, you would see people preferring in place over copy. but even there is a third strategy to run that migration that will take a look at in some time. so then what happened? the final step in a migration is to perform a rename to move the updated table into the correct place. what? what are they talking about? what is why migration? you just fired an alter query. why would you have to rename something, right? so let's take a look on how big migrations are run. so, although small migrations, you typically did an alter table on that same table and you get those things done. but, as i said that, whenever you are running all the table in gigantic or even a small table, it has to acquire locks on the rows, locks on the table, in order to make the schema changes right. if they are not acquiring logs, then it would result into some weirdish behavior for your end user. so that's where large migrations after a particular scale is something that your developer or your sra team would do, and it's not like just simplifying alter table command. what people typically create is: we create a new ghost table. let's say you have a table called repositories, right? so in order to apply the changes onto this table, if i apply, if i run altered- uh, if i had an alter table query on this table, it would acquire locks: locks on rows, locks on tables, locks on db pages and what not. so what sre team or what database administrators do over here is: whenever a gigantic migration is supposed to run or a migration is supposed to run on a huge table, we create a ghost table. for example, repositories, say. we create repositories one table. this repository is one table. maybe we copy everything from the repositories table to repositories one day. sorry, we first create a ghost repositories one table with no data right. then we alter the schema on this blank table. so it happens in a flash. once the schema is changes, then we start copying the data from repositories table to repositories one table and once the data copy is complete and both the tables are in sync, we rename the table. this is where, basically, this is exactly what, uh, uh, the folks at github are talking about. the final step of migration is renaming the table so that now your repositories one would become repositories and you would be deleting a table. so with this, without acquiring a lock on the rows or without, uh, or basically without affecting the throughput of your database, you were able to successfully apply a migration or a schema alteration on to onto your table. right, and this is exactly what mysql does internally, but because it is doing it on the same table, it would have to acquire logs. but now, when we do it explicitly, the steps that mysql internally runs on alteration of a table, we do it explicitly so that we don't have to acquire locks on that particular table and this does not degrade the performance of your database or acquire logs or affect throughput. in any case, right so, because that is the final step, this particular step of renaming table repositories- one, two repositories- is that step where, there, where the read replicas got stuck right. let's see what happened next during the final step of migration, a significant portion- this is an interesting one- during the final step of migration, which means when they were trying to rename, during this final step of migration, a significant portion of our mysql read replicas entered a semaphore: deadlock, read replicas entering a deadlock. pretty weird, right, because wouldn't like. deadlock happens when you acquire lock, like a waiting on bb, waiting on cc, waiting on d, but you would acquire locks when you are writing something. isn't it strange to think that read replicas entering into a deadlock state but like it's all redirectly because it's serving read request, then where are these rights coming from? who's writing to replica the replication job? right so, while read replicas are serving read requests from the external users. there is this replication job that is continuously replicating data from the master onto the read replica. and when this replication is configured, what replication does? replication reads the bin lock file from the master, which means all the queries that were executed on the master, all the update queries that were executed on the master. they are sent to the read replica for them to execute it as part of the replication job, so that when a new row is inserted in the master, that row is replicated on the replica right. and that is exactly how replication happens. so now rights are happening through the replication job and if a write required to take a lock over here, that same lock will be taken on replica as well, one when the replication is being done. so when this happens, your replica can go in a deadlock state where your right acquired some locks or, sorry, when, when a replication job that you have written, when it is when it is firing that update query, it might take log, exclusive log, shared log, most probably exclusive lock. that's where uh the chances of uh deadlock comes in. exclusive lock it would take on rows which is read by someone else, which is generated by someone else, so then the deadlock would happen. so rights, although they are not user facing rights, or your main production traffic is not right. on, replica, with the replication job is the one who is writing to the replica, which is the one which is causing the deadlock on. to replica right, and this typically happens when you have large amount of data that you would need to handle and migrate and whatnot. right, okay, so on mysql, on mysql cluster. sorry, our mysql clusters consist of a primary node for right traffic. multiple read replicas for production traffic- oh, this is an interesting one. our mysql clusters consist of a primary node for right traffic. this is typically your master node. multiple read replicas for production traffic- interesting, everyone has that. and several replicas that serve internal read traffic for backup and analytics purpose. they are having two fleets of read replica. what, what is this architecture all about? like one, few read replicas are okay, but why so many white multiple fleets of raid replica? so here, what we learn from this is that let's say this master has six replicas: replica one, two, three, four, five, six. what you would typically want to do is you would want to typically have read replicas that are dedicated to serve production traffic, for example, like when we- uh, when we create an issue on github or when we create a pr on github and when we try to read them, that read goes to read replica right. so you should. you would not want to impact the user performance or the user experience, so that's why you would have a dedicated set of read replicas on which the production traffic comes in, the production read traffic comes in right, and then obviously, your main database that you have. you also want to run some analytics or backup or any internal systems, any internal services. that requires that same database. would you want to share the same read replicas that you did for production? no, you will create a different set of replicas and these replicas are the one to which internal services use, like analytics, backup and any other internal service. this way, the load on this three replicas- the top three replicas are the one that is only handling the production grid traffic, while the bottom three replicas they handle the internal traffic. so internal services backup, restore whatever they want to do. so this way there is this total segregation of responsibilities and high production usage will not impact the other database. internal database will not affect the, the main, the, the main production grid replica. this way, your analytics team can fire very gigantic queries onto this replica and the production, uh performance will not be impacted, right? so in most companies you'll find this sort of similar or like this sort of architecture where you would always have a few set of replicas dedicated for the internal teams, while while the remaining set of rate replicas for their actual production traffic that comes in. right, okay, what next? what does this say? next? uh, the read replicas that hit the deadlock. remember the reader, please hit the deadlock right. and then it entered a crash recovery state, causing an increased load on healthy read replicas. due to cascading nature of this scenario, there were not enough active read replicas to handle production request. so again, let's- there are a lot of heavy words here. read replicas that hit the deadlock entered a crash recovery state. what is a crash recovery state? whenever your database crashes, it would the process would reboot, it would try to recover itself, right, and then, because of heavy load, it would again crash. it would again recover, then again crash. so that is a crash recovery state where you are crashing and then recovering, but due to again load, again crashing and then again recovering. so that is because the load is not decreasing your the requests are continuously coming on to your infrastructure. there is no way for you to stop them. so that's why your reader applicants are constantly going into this crash recovery state and because of this- and one very important line that is written over here- is that, due to the cascading nature of this scenario, there were not enough active read replicas to handle production request. what happens over here? what is the cascading failure that they are talking about? so, cascading failure, may. what happens is, let's say you have three read replicas that were handling production traffic. one of the read replica went down- right. so, let's say, you had hundred percent of read traffic. the three read replicas shared them equally. so thirty three percent, thirty three percent, thirty three percent, right. so each read replica was supposed to handle only thirty three percent of the traffic and it was provisioned to handle 33 percent of load. so if it required, uh, let's say, 32 gb ram, it was given 32 gb ram to execute. right, there was no over provisioning done. now what happens is, let's say, one of the read replicas go down. the load of this replica that went down is shared across the other two read replicas. so the read replica who had 32 gb ram was supposed to handle, let's say, 1000 requests per second, is now suddenly getting 1500 requests per second. so let's say the total number of requests that were that we- hypothetically that we were handling- were 3000 requests per second on three read replicas. right, so each read replica handles 1000 requests per second. the third read replica went down. so the 1000 requests per second that went to read this, this read the third read replica. it will now go to the first and the second right, and when that happens, the read replica that was configured to handle only one thousand requests per second is now bearing 1500 requests per second, 1.5 x offload. so what would happen is that node would crash because it's not meant to handle 1500 requests per second. it is there configured to handle 1000 liquids per second, so that would go down and then it would put all the load on to the first. so now, on the only replica you have needs to have 3000 requests per second. that's insanely high. so this is what the cascading nature of failure comes in, so typically in every uh database architecture that you see all of your database- not really exaggerating- all of your databases are prone to cascading failures because if one of the replicas go down, it would immediately transfer all the load, or your other replicas would have to bear all the load that was going to that replica, along with the load that they were already wearing. so this is what is about database failure: cascading. every database is prone to this, right? so then, what? how? how can we fix that? what can we do next to fix that particular problem? so there were not enough reactive read replicas to handle production request, which impacted the availability. obviously, you saw the cascading nature of it. let's say, two or three replicas went down. how are you handling read requests? not possible, right? so, which is why the outage happened, there were not enough read replicas to search. the read request rights were all working fine. master was not affected. we are only talking about read replicas over here. read replicas were not there. cascading failure happened. the active replica also started crashing and they all went into this crash recovery loop, right? so this was the outage. this was why it happened. now, what did they do to mitigate it during the incident? mitigation in an effort to increase capacity. because now, what you think now, in order to mitigate this outage, the natural tendency would be add more read replicas. how hard it can be right? so in order to effort. uh, in order to put an effort to increase the capacity, we promoted all available internal replicas that were in healthy state into production path. now, this is a very smart move. so, what happens? in order to mitigate. so when an outage happens, you as an engineer, you would always be on your toes key, and let me first quickly get up, like, get my site up and running. you don't care about anything else at that time. so when that happens, we know that we had two fleets of read replica, one handling the production traffic, one handling the internal traffic. creating a read replica takes a long time, right, if, like, there are ways to to basically speed up the entire process, but it would still take a long time. when the mitigation is happening, you don't have enough time to let like, like some of the engineers would have created, uh, or basically, would have initiated the creation of read replicas, not denying that. but in order to mitigate, you would find the the quickest hack to get your site up and running. so you wanted to increase the capacity on your production fleet, because on your production fleet you had three read replicas, out of which two are done, and only one was handling what, like, one was barely handling the request right. so now what you wanted? you wanted to increase the capacity, which means you wanted to add more read replicas. but creating new replicas might take time. but you anyway initiated the creation of red replica but, let's say, might take 10 minutes, 15 minutes for it to spawn up. so what can you do to quickly mitigate this? what their engineers did is because there were, like other, fleet of reed replicas replicating from the same master node, but they were serving internal traffic. they literally promoted, they literally promoted the node from their internal fleet into the production plate. so now that replica that was earlier serving internal traffic is now serving products in traffic. so they very quickly added, moved all of their internal replicas, because let the internal team suffer, that is okay. external traffic should not, or, at the main githubcom website should be up and running as soon as possible. so they basically moved all of their, they basically promoted all of their uh, internal read replicas into external read replicas. so basically, just changed. now they basically added it to this plate so that they can now serve the, the traffic, the production rate traffic, and that's how you increase, that's how they quickly increase their production read replica capacity. but it did not work. it did not work because of the heavy load. so now what happened when they added or when they promoted this existing read replicas into the production fleet? what happened? because of the heavy load, the new replicas also started crashing because the external load is not not stopping, because external load is not stopping. what's happening? because the rights have been the. the rights are being served, right, you, just like all of your read replicas, are affected, but your rights are being served. the read loads is still coming to your production read replica fleet, the newly added replicas, which were promoted from the internal fleet to your external production facing fleet. they also started crashing because of heavy load, because as soon as the load load happened, your, your, your replica started crashing, right the replicas that were crashed. they recovered, they again crashed. this was the crash recovery loop, which all of the replicas. they were stuck in right, so the shift was not sufficient for the full recovery. we also observed that the read replica serving production traffic would temporarily recover from the crash recovery state, only to crash again due to load. so the load was so high that, even if they added the machines from your internal fleet to this and added more replica, it started crashing and crashing and crashing and crashing- right, so there was no. like there was no way for this to be fixed. like it was getting affected so much because the load was so high that even this promotion, this quick hack, was not working. so, based on this crash recovery loop, we choose to prioritize. this is one of the most brilliant point i ever read. do do understand how organizations think. right, like a great organization like github, what they said: based on the crash recovery loop, because your replicas were were crashed, they recovered and they crashed again. because of this crash recovery loop, we chose to prioritize data integrity over site availability by proactively removing production traffic from broken replicas until they were able to successfully process the table rename. this is one of the most beautiful thing that an organization can do. like this shows you how, how thoughtful they are when that outage happens, like how important is data for everyone. this shows you that what they did is they prioritize data integrity over availability. so when your replicas are continuously recovering and crashing and recovering and crashing and recovering and crashing, it is anyway have affected your availability, right so? which means that you have. you have accepted the fact that your website is down and your core services are down. but what you would want to prevent is you want to prevent data integrity, which means that no matter how many times the corruption happens- or, sorry, no matter how many times the crash happens, your data should not be corrupted. right? so you choose data integrity because if your data becomes inconsistent or if your data is corrupted, it would take far longer of an effort to fix that as compared to anything else, because how do you even know what's the source and what's the truth and what's not right? but if you notice, aren't these the read replicas that were going down? what? why are they talking about data integrity? like because data integrity would be at lost when your data was in an inconsistent state or while writing the data discovered. who is writing on read replica replication job? right? so when your read replicas were constantly recovering from the crash and again going down, what might have happened? because your read replica came up, it started replicating from the masses, it started pulling in latest changes from the master and then it crashed. it again started, it recovered, it started pulling changes from the master and then it crashed while handling all the load that is coming in. so what is happening is the right here is happening from, from the replication job, the reads, the. then you are getting constantly traffics happening on the replica. so the data on the replica could corrupt because during this replication job- let's say the replication job- ask the replica to fire an update on a particular row and while, while, while updating the database- well, like, like, let's say, there was a bulk update on 100 rows- while just updating it for 50 rows- your database crashed and due to any reason, any reason, any bug in the existing software, anything your data could corrupt, right? so you do not want your replicas to corrupt, because it would take far larger effort for you to solve for that versus just just getting your site up and running. so, in order to protect your replica from not going corrupt or from not getting corrupted, they chose to let it crashed and they removed it out of their production fleet. and now, when they removed it out of the production fluid, what happened is that now they stopped getting read request on that read replica and it had time to complete the schema migration that everything started with right. because why your replication started failing? because there was a deadlock. why there was a deadlock? because there was a schema migration that was happening right. so now what did they do? is they let the replica crash? they let the crash replica not handle any traffic, which means they removed the replica out of the production fleet. then they gave replica the breathing space it needed for it to complete its schema migration with ease. that the replica completed that schema migration. once it knew that it's fully recovered, not corrupted, then it added back that replica into the production fleet. this way, the breathing space that the replica needed, which it was not getting because of high read load coming in from the production traffic, it got that space so it can complete the migration with ease. and once that is done, they added the read replica onto the production plate. i'll say: but the load is there, it would still fail. but think about it. the failure happened because of deadlocks. if deadlocks would not be there, your read replicas would very easily handle the load right. because of the locks that were taken. because of the deadlocks that were taken, your the load, or the cpu load or the processing load, was increased on the system because it was all waiting. your i o wait time increased by a massive margin there. so once that all was done, then you slowly started adding replicas and then slowly the entire capacity was restored. the entire outage happened for 2 hours and 50 minutes and, to be really honest, whenever there is an outrage like this, in most cases a circuit breaker would have really helped. you typically see that a lot of like. with help of circuit record, you would have stopped the traffic coming in onto a read replica. let the read suffer, let your replica get time for it to apply the migration and then basically be healthy and start serving the production request. okay, just on the conclusion part. once the replicas recovered, we were able to move them back into production and restore enough capacity to return to normal operations, and it all took 2 hours and 50 minutes to do it. throughout the incident, right operations remain healthy. master had no idea. master literally had no idea what was happening, what kind of chaos there that that happened behind the scene. it was all happy and handling all the right requests coming in, but reads were the one that were affected by a huge margin. so when the rights operation remain healthy and they, when, when an outage like this happens, when there is lot of crashes and recovery is happening, you always have to check for data corruption, that is, your mysql table corrupted. so there are commands to check if your table is corrupted or not. it typically depends on the database. these folks use mysql. so basically, folks that get abused mysql, so mysql they applied, they run that corresponding command to check for the corruption of the table. if it is corrupted, then there are ways to recover it. that's a little bit of my sequel in terms you can google about it. you'll learn a lot about like you can learn a few details about uh, about a table, corruption and recovery from that. right now. what did these folks do in order to do that long-term fix? now, what is the because? obviously, whenever there is an outage, you have to do that long-term fix, right. so what's the long-term fix over here? so the long-term fix that they're talking about is that talking about something that to address this class of failures and reduce time to recover. see, they are not only focus on uh eliminating the error, because you have to be wary of the fact that periods will happen, outages will happen. so to address this class of failures and reduce the time to recover. the main problem of this outage was not that it failed schema migrations. things would go wrong. you cannot predict the production traffic or the production load that is coming out to a replica, but you should not be taking 2 hours and 50 minutes to recover from that. so then, what do we do to solve it? they said that they would continue to prioritize their efforts in functional partitioning efforts. they would prioritize their thing about functional partitioning. we'll talk about functional particles. what exactly is that? so what is functional partitioning? so what github is doing there? so this is that one place where you see vertical partitioning in action. so what happened is they have this one gigantic database in which they have multiple tables, like, let's say, repositories table, pull request table and issues table and actions and web books. everyone has their own, like. there are, there are tables for every use case that you could think of, right, but because of this, this one, this database become huge because everything is stored in this one database, right? so now what would happen is, when you are running a migration, then all of the services- because they all rely on the same database- they all get affected. so what they are doing is they're doing functional partition. so, which means that a pull request function needs its own database and issues function needs its old database. uh, let's say, let's say, a comments function needs its own database, and all right. so this is what functional partitioning is all about. so you typically create smaller databases with the exact same table. so this is not data sharding. this is not data part. this is what this is not horizontal partitioning. this is vertical partitioning. we are literally taking all the data off a table and putting it into a separate database, so basically creating multiple databases out of kind of like microservices- not exactly, but kind of like microservices, right. so they are just vertically partitioning the database so that this big, gigantic database gets split into smaller databases, each having a few set of table which are very high, cohesive, which means that all the tables required for you to function or pull request will go to this one database. all the tables required for you to power issues on github will go to this database, right? so functional partitioning is what they are talking about over here. this is this one place where you will do this is basically the idea of vertical passenger vertical partitioning as well, right? what advantages would we get by this with this? because you don't have one gigantic table, you have smaller tables. your migrations can run on a small database server. before, on production right now, when they applied the migration, they applied it on the main production db, right, but was there a way for it to test it before? like they would have tested on a smaller set of data but not the complete data, right? so that would have been extremely difficult given the size of the database. so that's why, by doing this vertical partitioning or this functional partitioning, they can run a canary. what is a canary canary is a small database or a small api server, depending on what you're doing. in this case, it's a small database server in which you are running your migration, just to test if everything is working fine, because this database would be much smaller as compared to this one gigantic database. what you'll get is you can actually test your migration and actual data before hitting the actual production. big advantage, right. and second, if your database goes down due to any reason because of unforeseen circumstances, it will not affect others, which means that if you are, if you wanted to run a schema migration for your pull request table, it would not affect other databases or other tables which are present in other databases. right, so you are reducing your blast radius, so your mean time to recover drops down drastically. so you'll have like, even if some outage happens, your other services are working fine, you don't have to think, because here the migration- let's say the migration- happened on pull request table, but because, uh, in the current architecture the pull request table issues web books, everything lied in that one database. the entire database got affected, which affected all the services. but if it would have been functionally partitioned, then the migration that you have applied on the pull request table would have only affected the database holding the pull request table every like. you are not even firing other databases, right? so that is a big advantage that you will get out of doing this. functional partitioning, which is what the team at github is also prioritizing their efforts on. partitioning the cluster, adds resiliency, given migrations can then be run in a canary mode on a single shard, reducing the potential impact of this failure. additionally, we are actively updating our internal procedures to increase the amount of each cluster is over provisioned. this is an important one. the main, like the second biggest reason why this outreach happened was that the when one replica went down, the load came on to the other two. that other two were not provisioned enough to handle 1.5 x load. so that's what these folks are talking about, where they would need to update an internal procedure to increase the amount of each cluster is over provisioned, which means that if it required them 32 gb of ram to process 1000 requests per second, instead of giving 32 gb, let's give it 64 gb, so that even if one of the replica goes down and the load moves to other read replica, they are provisioned to handle that much of load. right? this is an excellent thing, where you will find most of the organizations are typically over provisioning their databases just to handle scenarios like this. if you are like, like if you are a senior engineer, go audit your infrastructure for this particular use case. have your database a little bit over provision just to handle uh loads due to failures of other, or due to low load because of failures of other peripheral components. right, so, as the next steps, we are continuing to investigate the specific failure scenarios and we have paused schema migrations until we know uh, until we know that no more uh, like. we have safeguarded against this issue, so they've stopped data with your migrations until they're pretty sure that if they run a migration again, they would not see this error again. right, so they have safeguarded against this particular issue. then only they'll start applying the schema migrations again. as we continue to test our migration tooling. we classify opportunities to improve it during such scenarios. that's an interesting one. you get to learn so much from these outages like- and this is exactly how real life fire fighting, uh, during production outage- is all about. you have to be very quick, very vt, in order to mitigate this issue. so nice, i hope. basically that's it for this video. i hope you guys like this one. if you guys liked it, give this video a massive thumbs up. if you guys like this channel, give this channel a sup. i post three in-depth engineering videos every week and i'll see you in the next one. thanks.