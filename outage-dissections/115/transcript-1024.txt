so in april 2022, atlassian experienced a major, major, major outage where the permanently deleted the data of 400 of their paying cloud customers, and it will take them weeks to recover it.
we will talk about six key engineering designs and insights on how similar systems are built and, most importantly, understand why is it taking so long for them to recover the data?
on monday, 4th of april, approximately 400 atlassian cloud customers experienced the full outage across our classroom products like jira, confluence, status page and whatnot, and they are in process of restoring websites- uh, restoring the sites, 45 percent of impacted users and the full recovery of for all the impacted customers will happen in next two weeks.
but one key insight before we move forward is, additionally, the majority of restored customers have no data loss, while some have reported a data loss for up to five minutes prior to the incident.
because when the incident happened, they deleted- they permanently deleted- the the data of the customers, right.
where what do they do is whenever, let's say, someone, like any right hub, that happens on the database is then backed up at a frequency of five minutes into their backup db, right?
so five minutes prior to the incident, so when the incident happened, basically when the script ran and it actually permanently deleted the, the data that would have been the data that would have only been present in their main database and was not backed up, was not fully backed up, for example.
so, which is why, when they accidentally deleted the, the data, it was not available in the backup db, right?
so majority of restored customer have no data loss, while some have reported, some have reported it's still not confirmed, but in general, if you have a backup or a restore, or if you have a backup strategy like an incremental backup strategy, in which you run at a particular frequency, then this can have an impact around permanent data loss in case of a permanent delete happening on your primary db, right?
right, and because of this, we needed to deactivate the standalone legacy app on the customer sites that had it installed.
right, but before we move into what exactly happened, let's talk about this deactivating the standalone legacy app.
then what you do is your three customers like c1, c3, c4, might be using the legacy app while you roll out your new version to a subset of users, let's say c2 and c5, and then, once you are sure on that, you would want to roll it out to more users.
what does this mean is, when atlassian tried to do it, they had to deactivate the legacy up, basically remove the connections of legacy app from this customer sites, which requires them to physically alter the data or transform the data or delete it up for some reason.
it's up to them, right, but what it look like is deactivating legacy version required them to delete some data, or basically mark for deletion or something around that.
being marked for deactivation, the team provided the ids of the entire cloud site, which means they, instead of giving them the app id, they give them the customer id.
so high availability implies that if your database goes down, there has to be a copy of your data, which is a point in time, like, literally, if you crash a particular database, you would have zero data loss and you would be able to recover or restore the entire site in a gif.
and so this is how, in general, if you want to ensure very high availability of your critical systems- like, let's say, payments- extremely critical systems, because it is very expensive to do- you would want to have a synchronous standby replica for that right, and this is a very, very, very common practice for any mission critical subsystems of your uh, of your architecture.
they have the strategy where you do a synchronous replication of rights into the synchronous, into your standby replica and from there periodically, let's say after every day, you take the dump of last days worth of data and put it into a cheap storage like s3, like here.
and if they have accidentally deleted, the entire data is restored right.
so now, purely talking from the data's perspective, let's say each customer has its own set of database and then each one of this database has its own the backup policies that we just saw- standby replica and then moving it on to s3 for that immutable backups.
so what most companies do- and purely my speculation, what most companies do- is they have few large databases on which multiple customers share the data, right?
so it becomes very cost effective for companies to do this, and most companies actually multiplex multiple customers data onto the same database, rather into the same table itself, right.
right and now what is happening is: why is it taking so much time for them to recover the data?
let's say the script they got id for customer c2, c4, c7 and it permanently deleted the data for c2, c4, c7.
so like, for example, if this data store one is backed up and they would want to restore it back at a particular time, instant, what would happen is they can just go to that backup and then they would load that back up into a new db and start pointing it to this one.
but here what happened is because, from this entire data store, only a subset of rows were deleted of those particular customers.
so what would have happened is the row 125, 1096 and 2709 would have been deleted, right, but when you are restoring the data from the backup, right.
because they have to physically load the data, store into a database, extract the rows like, understand which rows needs to be restored first, which needs to be restored second, and then apply it onto their main db, and which is why their recovery process is taking weeks for them to happen.