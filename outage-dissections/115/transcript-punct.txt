so in april 2022, atlassian experienced a major, major, major outage where the permanently deleted the data of 400 of their paying cloud customers, and it will take them weeks to recover it. in this video, we will do an engineering deep dive of this outage, trying to extract key insights about their systems and practices. we will talk about six key engineering designs and insights on how similar systems are built and, most importantly, understand why is it taking so long for them to recover the data? instead of speculating, we will use their officially released outage report and dissect it line by line. i will link the report in the description down below and, just before we proceed, a small disclaimer: i do not have any insider information and this is pure speculation based on this outage report. but before we move forward, i'd want to talk to you about a code based course of system design that i have been running since march 2021, and if you're looking to learn system design from the first principles, this course is for you. yeah, because this is a cohort based course. it will not just be me rambling a semi-optimized solution, thinking it's the most amazing solution out there. instead, it will be a collaborative environment where every single person who is part of the cohort will can pitch in his or her ideas and we will evolve our system around that right. every single problem statement comes with a brainstorming session where we all together brainstorm and evolve our system. that's why everyone understands the kind of trade-offs we made while making that decision. instead of just saying hey, we'll use a particular queue, we'll have the justification why we use only that queue, why we use that particular database, why sequel, why not nosql? right, how are we leveraging throughput? how are we ensuring that our system scales? that's the highlight of this course. this course is taken by more than 500 engineers to date, spanning nine countries and seven cohorts. right, people from all top companies have taken this course and the outline is very intriguing. it's very exciting. so we start with week one around. we start with the core foundation of the course where we design online offline indicator. then we try to design our own medium. then we go into database, where we go in depth of database logging and take and see few very amazing examples of data log or database logging in in action and how do we ensure that our system scales through that? then the third week is all about going distributed, where we design our load balancer. i'll have oculus of the actual code of a toy load balancer and understand how tcp connections are managed and how simple it is to build load balancer. then week 4 is about all about social networks. week 5 is all about building your own storage indents, like we'll build that intuition on. if you were to ever design your storage agent, how would you do that right? then week six is about building high throughput system. seven is about building uh ir system- basically information retrieval systems- and adopt designs where we design our own message brokers like sqs, where we design distributed task scheduler. and we conclude the course with week eight, where we talk about the super clever algorithms that has powered or that has made those systems possible. right, i have also attached a video, verbatim, as is, from my first code where we designed and scale instagram notifications. i will highly encourage you to check this video out. right, and now back to the video. so, going through this outage report, we see what exactly happened. on monday, 4th of april, approximately 400 atlassian cloud customers experienced the full outage across our classroom products like jira, confluence, status page and whatnot, and they are in process of restoring websites- uh, restoring the sites, 45 percent of impacted users and the full recovery of for all the impacted customers will happen in next two weeks. so it's a very long outage, taking them very long to recover the the data. so, to be clear, this incident was not a cyber attack, nor was it a failure on our systems to scale. this was because this happened because of a miscommunication, which we will talk about later. but one key insight before we move forward is, additionally, the majority of restored customers have no data loss, while some have reported a data loss for up to five minutes prior to the incident. why so reported a data loss for up to five minutes prior to the incident? because when the incident happened, they deleted- they permanently deleted- the the data of the customers, right. but then why are they like? why the report says that few customers reported a data loss for up to five minutes prior to the incident? this shows that what a classion has is an incremental backup policy, right? where what do they do is whenever, let's say, someone, like any right hub, that happens on the database is then backed up at a frequency of five minutes into their backup db, right? so this has to be an incremental backup that is happening on their system. now, how, in real world, how does this backup actually happen. a keyword that i would want to throw at you is cdc. it is cdc stands for change data capture, in which you take the updates from one db and seamlessly replay those updates. you take the changes happening on the database and put it somewhere else. now this could be another database or a flat file or anything, right? if you look out for cdc, you will find some amazing insights out of it. so cdc is a very, very, very standard procedure that everyone follows. so five minutes prior to the incident, so when the incident happened, basically when the script ran and it actually permanently deleted the, the data that would have been the data that would have only been present in their main database and was not backed up, was not fully backed up, for example. right. so, which is why, when they accidentally deleted the, the data, it was not available in the backup db, right? what does this mean? this means that if this db is deleted, there is no copy of that data ever. so then could be. then it could lead to a permanent data loss, which is what happened in case of some customers, and one thing to note, that this only happened for a few of their customers. so majority of restored customer have no data loss, while some have reported, some have reported it's still not confirmed, but in general, if you have a backup or a restore, or if you have a backup strategy like an incremental backup strategy, in which you run at a particular frequency, then this can have an impact around permanent data loss in case of a permanent delete happening on your primary db, right? okay, let's move to the second part of it. let me start by having saying the idea that this is the, but what happened? this is where the most interesting part is: why did this happen? but how did the permanently delete the data of 400 of the cloud customers? so one of the standalone apps, jira service management and jira software, called insights asset management, was fully integrated into our product as native functionality. so they were building a new version of their app and which had to be integrated into their main suit of products. right, and because of this, we needed to deactivate the standalone legacy app on the customer sites that had it installed. our engineering team plan to use an existing script to deactivate the instance of this standalone application. right, but before we move into what exactly happened, let's talk about this deactivating the standalone legacy app. so this shows us that what atlassian actually has is a strategy around progressive rollout. so what happens is they might like any. in any case, whenever a company is releasing something, they don't release a new shiny thing to all the customers at once. they typically do a progressive roll out of it, right? so one such thing where you might have a legacy app, you might have a new version of this app, you might have five customers. then what you do is your three customers like c1, c3, c4, might be using the legacy app while you roll out your new version to a subset of users, let's say c2 and c5, and then, once you are sure on that, you would want to roll it out to more users. so then seamlessly transition or seamlessly roll the features out, the roll the new version out to the people who are on the legacy version right now. what does this mean is, when atlassian tried to do it, they had to deactivate the legacy up, basically remove the connections of legacy app from this customer sites, which requires them to physically alter the data or transform the data or delete it up for some reason. like you, it is very subjective to the system at hand, but overall, this is how the flow looks like when you do a progressive rollout of an app: you roll the new version out to few users and then you increase it to all the users, so making them a very seamless transition and you being able to very quickly identify if there are any issues or bugs or what not in the system. right so, and to do this, to move from one version to another, the user existing scripts to deactivate the legacy version. now, how do they deactivate legacy version? it's up to them, right, but what it look like is deactivating legacy version required them to delete some data, or basically mark for deletion or something around that. they it required them to update or delete some of the data. right, okay, then next, what happened next? why did this happen? communication gap first, there was a communication gap between the team that requested the deactivation and the team that ran the deactivation, so they have two teams: one who requested for the deactivation key- now this new feature needs to be rolled out to this set of users- and the team who ran the deactivation. now, this is the interesting part. instead of providing the ids of the intended app key, i only want to deactivate this app. being marked for deactivation, the team provided the ids of the entire cloud site, which means they, instead of giving them the app id, they give them the customer id. so when they gave the customer id, they deleted the entire customers data instead of deleting the apps data. so this was a like. this seems very weird, right? how? how this could have happened like this is such a simple rookie mistake. where you were, where people were not explicit enough into stating the requirements. so one thing as engineers, we should always do is be very explicit. give prescriptive information, exactly what needs to be done, rather than saying this needs to be rolled out to this thing. so these are the id's. roll it out. be explicit on what exactly needs to be done. so what happened? when's the t provided i uh ids of their cloud site where the apps to be deactivated. so the team that ran the deactivation got the ids of the sites. so they ran the deactivation of the site. second reason is the faulty script. so now this gives us insight about soft delete versus hardware. so what happened when the team that ran the deactivation ran the script? so the script that we used provided both mark for deletion, which is the capability that they use in the day-to-day operations, and permanently delete capability that is required to permanently remove the data. now here we see two insights out of this. first is soft deletes versus hard deletes. so what happens? the script had functionalities to do both: mark for deletion, which is soft delete, and permanently delete, which is hard delete. so what happens? so mark for deletion versus permanently delete. so whenever you delete a post on any website, or basically whenever you're doing any deletion, it is always preferred to do a soft delete of it. so, engineers, instead of writing delete from this table, they would write, they would update the row and set is deleted to true. this helps them to recover the data in case someone wants to recover it. like, for example, if you wrote a blog on any blogging website, you deleted it. it would go into an archive state and then from which you can recover the- uh, you can recover it. so how is this recovery possible? because when you deleted it, instead of permanently deleting the row from the table, it actually did a soft delete of it. so this is marked for deletion. this is still not permanently delete. then the other mode. so this is typically called a soft delete. and then there is another thing called as permanent deletion, which is hard delete, in which you actually fire delete from this table, which is heart rate. so now when you physically delete the row, it is not possible to recover it, right? but you would say, if the soft delete is working fine, then why do we need hard delete? we need heart delete because compliance. i'll give an example: gdpr. gdpr gives us, or gives users, as in we, as an end consumer, the power like or the right to be forgotten, which means you can reach out to any gdpr compliant website and say: i want to delete all of my data from your website and i have the right to be forgotten, so delete the existence of mine from your system. and that company, if it is gdpr compliant, would have to permanently delete. they cannot do soft delete because if you do soft delete they will still be traces about you in their system. they would have to permanently delete, uh, your data from their system, be it any subsystem, main database, transactional database, analytics database, whatever. they would have to delete anything that could trace that information to you, right? and which is why on this report you say: you see that permanently delayed capability that is required to permanently remove the data, which is required for compliance reasons. and these are the key compliance reasons why people would have to physically hard delete the, the data and the script was executed with the wrong execution mode and the wrong id list, which means that, instead of doing soft delete, they did permanent delete. on what? instead of doing it on the individual app, they did it on the customer website, so they permanently deleted the entire customer site, which led to this outage. right now, what next? the next part is: what are we doing to recover it? this is where we get insights about their replication strategy and their backup strategy, right? so what do they say? they say: to ensure higher ability, we provision and maintain a synchronous standby replica. now, what is in synchronous standby replica? so what happens is- this is the fourth inside that we get- like, how do companies in general give you high availability, right? so high availability implies that if your database goes down, there has to be a copy of your data, which is a point in time, like, literally, if you crash a particular database, you would have zero data loss and you would be able to recover or restore the entire site in a gif. so how do they happen? so a very popular strategy for this is called a synchronous standby replication. so you typically have, like you, whenever user talks to an api server, api server writes to the database, the write would be successful in a singleness standby replication. what do we have? for every database out there there is a synchronous standby replica. so whenever a write happens from the user on to the db, the db first applies the write on its own copy, then it sends out the write to the standby db instance. the standby replica is not serving any traffic. its job is to purely accept the rights from the main db applied there. once the write is applied onto the standby db, send by db replies back to the main database. hey, i have completed the right. once the main database gets an update from the standby key, the right is completed there. it would then acknowledge the right to the user. so the user's right is not complete unless it is written in the main database and the standby replica database, right? so this way you will always have two copies of your data: one in the main db, one in the standby replica db, right? so if your main db goes down due to some reason, the data would always be there in your standby replica db with zero replication lag, which is why it is called as synchronous standby replica and it is standby, it is not serving read requests at all. the only job of this data is to be on standby and have the latest data that happened, or the latest site that happened onto the db, having it on its own. and so this is how, in general, if you want to ensure very high availability of your critical systems- like, let's say, payments- extremely critical systems, because it is very expensive to do- you would want to have a synchronous standby replica for that right, and this is a very, very, very common practice for any mission critical subsystems of your uh, of your architecture. right? what next? so they have synchron standby replica and multiple aws availability zone. the easy failover is automated and typically takes 61.. this is more on the infra side of things, but you basically get the gist on. how are they ensuring the availability of data? right? so if the database goes down, there is still no data loss because there is a permanent uh, sorry, because there is a synchronous standby replica which has zero replication lag and can take the place of the main database. okay, now, what next? we also maintain immutable backups that are designed to be resilient against data corruption events which enable recovery to a previous point in time. backups are rated for 30 days and atlassian continuously, tests and audits, storage backups for restoration. now what do we learn from here? we learn a key highlight is immutable backups. so what are these? so obviously, skipping standby instances might be very costly. right, because you are literally running two instances of the database and doing it at scale for every single database you have is literally doubling your database cost. so what do companies do to to reduce the cost? is they do immutable, like? they have the strategy where you do a synchronous replication of rights into the synchronous, into your standby replica and from there periodically, let's say after every day, you take the dump of last days worth of data and put it into a cheap storage like s3, like here. what they might do like, assuming they like we know that they are on a device, assuming that they might, they are using s3 for that. so from the standby replica periodically, like, let's say, your it's zero replication lag, from main db to replica. now, every day, a job might run that would copy the data of the previous day and put it in a compressed mode and store it onto s3. and this is the. this is that immutable backups that they are talking about, right, and this immutable backups that are stored on s3. this would keep the standby replica size to a bare minimum and they can run a very small instance rather than running an equally scaled up version of the database. right, they can run a small instance and save the cost. their disk cost, cpu cost, everything gets gets reduced, so it's very cost efficient. and how do how? anyone would guarantee- uh, like they said, that they have protection against data corruption. so how do they, how they would have guaranteed that is using error correcting codes, like standard procedure error correcting codes, while they are serializing data in some format to store it onto s3, because you cannot just take the db, dump and store it on your rows and dump and store it. there has to be a, a particular format that they might be using to store that data onto s3 and they might be using some sort of error correcting codes to drive that. what next? using these backups, we regularly roll back individual customers or small set of customers who accidentally delete their own data. now this is where the fun part begins: how their recovery process looks like. so they use these backups- the the backups we just talked about- to roll back individual customers data or a small set of customers who accidentally deleted their own data, because the data is there in the backup. they can just take that dump and restore it into a database and, done so, the data is very easily restored right. and if they have accidentally deleted, the entire data is restored right. and what we have not yet automated is restoring a large subset of customers into our existing environment. now this is where we get insights about that multi-tenant architecture. so a multi-tenant architecture says that every single customer that you have needs to have its own set of infrastructure. that's a pure multi-tenant architecture. fine, a pure multi-run target. so here what we talk about is we talk about multi-tenant architecture from the data stores perspective. so assume that you have eight customers, c one, two, c, eight, right. so what you would want to have in a truly multi-tenant architecture, every customer is isolated and has its own setup of their api, servers, load balancers, databases- everything runs in its own isolated space. but if you see you're having so many database- like if atlassian has 60 or 70 000 customers or even more than that, they would want to have 70 000 databases- it's it's not feasible to have and operate, operated, scale and gain profitability out of it, because it becomes extremely expensive, extremely expensive. so that's where most companies, who most companies? uh, when they say they have multi-rent architecture, they kind of have a hybrid model, right for key. for some of the key customers they would want to have an explicit setup, but while they would be clubbing a few of their smaller customers together, right? so now, purely talking from the data's perspective, let's say each customer has its own set of database and then each one of this database has its own the backup policies that we just saw- standby replica and then moving it on to s3 for that immutable backups. but if we do it for everyone, that's a very it's, it's extremely costly. so what most companies do- and purely my speculation, what most companies do- is they have few large databases on which multiple customers share the data, right? so, for example, in one database they might have custom the data for customers c1, c2 and c3. in second database they have data for customer c, for c8, and in some they might have c5, c6, e7. this way they get the benefit of of load isolation while reducing the cost. so it becomes very cost effective for companies to do this, and most companies actually multiplex multiple customers data onto the same database, rather into the same table itself, right. so this makes their architecture cost efficient for sure. right and now what is happening is: why is it taking so much time for them to recover the data? okay, let me just do this part. within our cloud environment, each data store contains data from multiple customers. this is where that insight is. each data store contains data of multiple customers, because the data deleted in this incident was only a portion of the data store that are continuous, uh, that are continuing to be used by other customers. we have to manually extract and restore the individual pieces. each customer site. recovery is lengthy and complex process. now we'll take a look at how the recovery is happening. this is where the fun part is: why is it taking so much time for them to recover the data? so what is happening? we know that multiple customers data is present on one data store. so let's say, hypothetically, we have three data stores over here: data store one data store to data store three. data store one has data for customer c1, c2, c3. data store 2 has c4, c8. data storage 3s, c5, c6, c7. now what did the script do? let's say the script they got id for customer c2, c4, c7 and it permanently deleted the data for c2, c4, c7. so from this data store, the data for c2 is deleted from this data store, data of c4 is deleted from this data store. the data for c7 is deleted. now what happens? because when we talked about archiving, when we talked about backup, the entire db is backed up, as is right. so when the entire db is backed up, as is when a db is backed up, let's say, data store 1 is backed up. when it is backed up, the data for customer c1, c2, c3, multiplexing on the same data store, is backed up, right so, and similarly for data store 2 and data store 3.. so now what is happening is when they would want to restore the data they can restore, they can have a point in time recovery of that entire data in one shot. so like, for example, if this data store one is backed up and they would want to restore it back at a particular time, instant, what would happen is they can just go to that backup and then they would load that back up into a new db and start pointing it to this one. so the data or the entire data store moves back in time to that particular instant right. but here what happened is because, from this entire data store, only a subset of rows were deleted of those particular customers. restoring them becomes a very painful process because now, hypothetically, let's say, we have a table- this is the table id- having rows and each row having some sort of customer id and what we know, that data from the data store. 1, the date of c2- is deleted. so what would have happened is the row 125, 1096 and 2709 would have been deleted, right, but when you are restoring the data from the backup, right. what would happen is it would restore the data for c2. very well, right, c2, this row, this row would be restored. but because c1 and c3 were not impacted, these customers kept on receiving the rights and because their system was functional, they kept on receiving the rights. so the new data for this customers kept kept on adding to the database. so now, when atlassian would try to restore the data from the backup, they would. they can, number one, restore the entire table as is. but what would this mean? this would mean that the unaffected customer would see a data loss because, let's say, when the c2's data was deleted, after which c1 and c3 received some updates. if you restore the data to a point in time in before the new updates that happened are lost. for unaffected customers. this is very catastrophic because c1, c3 were not affected, but they are experiencing data loss. so now what restoration policy that they would want to employ here is they would be restoring the backup of these users onto a different database and they would have to manually iterate through the rows for their customers and then put it back into the main table. and this is extremely lengthy process because the the backups that we're talking about is in is in few hundred of gbs, because the amount of data atlassian has for customers it's it's insanely high. so for every backup that they have, they would have to restore it, iterate through those rows manually. when they are iterating through those rows manually, they would have to extract the rows for the customers that have been deleted, take that, ingest it into their primary database and then that site is restored. so because this is an extremely lengthy process where it's not just a matter of you restoring that backup and spinning off a new db, they have to literally go through at plus, given that they would have hundreds of tables with so many foreign keys and whatnot as a problem like they cannot insert the data unless the foreign key exists, so they would have to declutter the, the relationships between the table to understand which table to restore first, which table to restore second, and this process. they would have to write scripts to do it, because doing that manually it's obviously not possible. they would have to still go through those scripts and that scripts would take hours and weeks to run, and which is why the recovery is taking so long to happen. because they have to physically load the data, store into a database, extract the rows like, understand which rows needs to be restored first, which needs to be restored second, and then apply it onto their main db, and which is why their recovery process is taking weeks for them to happen. for smaller set of users. it was for for smaller customers, it was simple and they mentioned that they have actually done it in the past. right, but when this at this scale, this happened for the large subset of users- like 400 customers, massive amount of data and some of them might be very large customers as well- it would take time for them to recover the data because they are doing that explicitly, going through row by row and trying to apply that right. so this is why attraction is taking so much time for to do the recovery, because the recovery is a very lengthy and complex process requiring internal validation and final customer verification. when the site is restored again, which table to load first is, and while restoring, there is no data loss. there is no like. existing customers are not affected. so many things to take care of, right? so this is what exactly happened, like my speculation of what, what could have happened during this outage and some key insights about it. bucky, with respect to the status report, they are just saying that how they are restoring the data and they- uh, they are shifting to more automated process: reenabling metadata of deleted sites into a centerless orchestration system. restore customer data extracted from backups, including users, permissions, etc. re-enable existing system app billing information. basically, it's all about that key. they would be much more prepared about and about a similar outage in the future, but i really hope that that that doesn't happen in this. all the communication first versus anything else. so, yeah, that's about status report. i'll link again. i'll link the status report in the description down below and i really hope you understood something interesting and amazing out of this and there is always something new to learn from outages like you get to experience so many components, they come together, so many things that come together and build a system. one thing goes down and so many things are affected. so nice, challah, that's it for this video. if you guys like this video, give this video a thumbs up. if you guys like the channel, give this channel a sub. i post three solid, in-depth engineering videos every week and i'll see in the next one. thanks, saturn.