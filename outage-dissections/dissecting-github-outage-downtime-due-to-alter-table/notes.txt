GitHub Outage Downtime due to ALTER TABLE

Dissecting GitHub Outage Handling large schema migrations GitHub actions , API requests , November 2021 . GitHub experienced major that code spaces , Git operations , outage affected their core services Issues , Packages , Celeb Hooks , The outage was due large Schema Migration Pull requests Insight schema take weeks to complete 1 : migration can Imagine . ALTER TABLE command taking weeks to complete why schema migration takes so long ? ALTER TABLE is too slow for large tables Data to be locked Both COPY and IN PLACE and copied with new schema * The key concern is table locking during alteration lock and rewrite the table with the new schema . larger the table , larger time it will take for migration

Insight 2 : last step of schema migration is RENAME How migration happes : 1- New ghost table is created 2- schema is altered 3. data is copied 4. table is renamed repositories repositories -1 RENAME TABLE repositories -1 To repositories ; ↑ During this step MySQL read replicas entered deadlock Insight 3 : Deadlock on replicas It is really strange to think that replicas can be in deadlock ! Deadlock locks write But who's ? → → , writing on replica Replica I Master Replica 2 Replication Job is writing on Replica

Insight 4 : separate fleet of replicas for internal traffic Replicas for external El El El EI Master El El El Replicas for use cases internal Analytics Backup Other Internal Services Insights : Database failures cascade it replica goes down El El El one . the load other gets on El replicas and they crash Master El El El replicas cannot bear the load if other . i. failures cascades

such ? How to mitigate an outage production Because there are not El El El enough replicas to bear €1 the load . to mitigate Master El El El the issue we have to add more replicas internal . But because takes time What spinning up new replica . can here ? be hack a quick Promote the replica of internal traffic to production But it did not work . . . . load Because of heavy , - the newly added replicas also crashed recovered & crashed again - the crash replicas ' - CRASH - RECOVERY LOOP

Prioritizing Data Integrity Over Availability Data integrity was at stake purely because the crash could corrupt the data But aren't these read replicas ? ? . . Whose writing - - - . . . - - Data on the replica could READS =µ WRITES €1 corrupt because the < > DB Master Replica Could crash while replicating > the data from the master Replication * Replicas are in crash recovery - loop ? so , how to handle this - let the replica crash - let the crashed replica not handle any traffic - let the replica complete its schema migration recovered to handle completely add it production - once , entire and slowly , let the capacity restore

master was * A key detail to note here , unaffected this entire outage the WRITES During . > ⑦ ! were all healthy and fine Master Replica long Term Fix continue to prioritize Functional Partition - Tables ✗ all tables in one DB create multiple DBS each holding a few tables [ vertical partitioning ] Advantages : be - migrations can run on a Canary [ small Database server ] before prod down it will NOT affect others it DB - one goes