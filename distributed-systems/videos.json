[
  {
    "id": 194,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "sZVCpjuVUL8",
    "title": "Two Phase Commit to power Distributed Transactions in a Distributed System",
    "description": "Distributed Transactions are the heart and soul of Distributed Systems and getting all the participating nodes to agree to commit or abort is not an easy job.\n\nIn this video, we talk about the Two-Phase Commit protocol that takes baby steps to ensure that we can get all the nodes to commit or abort a distributed transaction and never let the data go into an inconsistent state.\n\nOutline:\n\n00:00 Agenda\n02:30 Two Phase Commit\n08:37 Failure Scenarios in 2PC",
    "img": "https://i.ytimg.com/vi/sZVCpjuVUL8/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/tkIOnksp2Q7MRIdU6k/giphy.gif",
    "duration": "16:47",
    "view_count": 1224,
    "like_count": 45,
    "comment_count": 2,
    "released_at": "2022-09-16",
    "gist": "Say, we have a distributed database with three nodes, and we want our \"commit\" to succeed when it is successful in all three nodes otherwise we \"abort\".\n\nThis is a classic Distributed Transaction.\n\nAssumptions:\n\n- no messages are lost\n- processes can fail in the middle of the transaction\n- every node knows about every other node in the network\n\n## Two-Phase Commit\n\nSay have the transaction, and N processes are participating. One of the `N` processes becomes the coordinator and it coordinates until the end of the protocol.\n\n### Phase 1\n\nAll the nodes send if they can `commit` or `abort` to the coordinator process. If a process does not send any information, the coordinator will mark it as `abort`.\n\nAt the end of phase 1, the coordinator will have the local decisions of all the nodes. The coordinator decides `commit` if all the nodes can `commit`, otherwise the decision is `abort`.\n\n### Phase 2\n\nProcess A broadcasts its decision to all the nodes in the network, and thus the entire network either commits or aborts; thus completing the transaction.\n\n## Failure Scenarios\n\nIf the coordinator fails before the start of phase 1, then since the consensus did not even start, all the nodes can safely abort.\n\nIf the coordinator fails after initiating phase 1, some of the nodes might have sent their local decision and would be waiting to hear back the final decision. These nodes would remain blocked.\n\nIf a participant crashes before sending its local decision to the coordinator, then the coordinator keeps on waiting for the local decision.\n\nIf the coordinator and one participant crash in phase 2 without other participants knowing anything about the decision then the new coordinator that comes up would have no idea about the decision.\n\nNo one could proceed, because the new coordinator does not know if the crashed node was committed or aborted.\n\nThe Two-Phase Commit looks super-simple but it has a major flaw in the failure scenarios, and hence distributed systems take even finer steps to remediate and reach a consensus more robustly.",
    "notes_gd": "https://drive.google.com/file/d/1FJa0DQPOxVJP2kXdyZDSttdMDI4Q2fUx/view?usp=sharing",
    "slug": "two-phase-commit-to-power-distributed-transactions-in-a-distributed-system"
  },
  {
    "id": 193,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "pi3YA3m1ffw",
    "title": "Exponential Information Gathering (EIG) Algorithm for Byzantine Agreement",
    "description": "Byzantine Agreement is an important problem to address in a Distributed Network. It is all about being tolerant of the nodes that are malicious and trying to ruin the sanity, integrity, and correctness of the network.\n\nIn this video, we talk about an algorithm that gathers an exponential amount of information to build a robust understanding of different values proposed, before reaching a consensus even when a few nodes are corrupt and are trying to ruin everything.\n\nOutline:\n\n00:00 Agenda\n02:42 Introduction to Byzantine Agreement\n03:53 EIG Algorithm and the flow\n09:06 Decision for Byzantine Agreement",
    "img": "https://i.ytimg.com/vi/pi3YA3m1ffw/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l4pTpPHr2do40b4yc/giphy.gif",
    "duration": "13:38",
    "view_count": 734,
    "like_count": 21,
    "comment_count": 6,
    "released_at": "2022-09-14",
    "gist": "Reaching consensus is extremely important in any distributed network. For example, we cannot have two data nodes in a cluster where one thinks that `price` is `$1000` while the other thinks it is `$2000`.\n\nSo, we need the nodes to talk to each other and reach a consensus and converge on the true value of `price`. Reaching consensus is easy when there are no failures - no network failures, or process failures.\n\n## Byzantine Agreement\n\nThe byzantine agreement is a problem of reaching a consensus even when one or many nodes or processes are malicious/corrupt.\n\n## EIG Algorithm for Byzantine Agreement\n\nThe core idea of the Exponential Information Gathering Algorithm is to gather a large amount of information from the network and then apply some decision rules to reach a consensus.\n\n## EIG Data Structure\n\nEIG algorithms require an EIG Tree that is like a Trie and is constructed level by level. If there are `n` nodes in the network and the level of the tree is `k` then the leaf of the tree contains all k-length permutations of `n` nodes.\n\nIf a node `a` received a message from a path `b, c, d` then the incoming message (value) is stored along the path `b`, `c`, `d` at the node `a`.\n\nThus, at each level of the tree, the number of children of each node reduces by 1. The root of the tree is labeled EMPTY, and it has `n` children, say A, B, and C.\n\nEach node in the next level will have 2 children. A will contain `AB` and `AC` while `B` will contain `BA` and `BC`, and `C` will contain `CA` and `CB`. Thus we see at level 2 the leaves contain all 6 2-length permutations of A, B, and C.\n\n## The algorithm\n\nThe construction of the EIG Tree remains the same as it was in the Distributed Consensus and we assume that each node has constructed the EIG tree independently.\n\nThe algorithm is tolerant to `f` faulty process. When a process sends an ill-formed value the nodes participating in the consensus discard the value.\n\nEx: expected int, got string, discard.\n\n### Decision Making\n\nThe processes propagate their values for `f + 1` rounds and each node builds its own EIG Tree. Once the EIG Tree is constructed, to make the decision, a node traverses the entire tree level by level starting from the leaves towards the root.\n\nWhile traversing, the parent's value is the majority of its children's values. If there is no clear majority, then the parent sets to the default value.\n\nThe final consensus is the value that the root node of the EIG Tree decides. If any of the fault nodes send ill-formed values, the value will get absorbed along the tree because most of the nodes are honest and correct values outnumber the fault ones.\n\nHence, gathering exponential information is the key to resolving Byzantine agreements.",
    "notes_gd": "https://drive.google.com/file/d/1nJmizLh_HnHLiSNZH-k0JVFeMlypBbkj/view?usp=sharing",
    "slug": "exponential-information-gathering-eig-algorithm-for-byzantine-agreement"
  },
  {
    "id": 192,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "1g0L6ISxQPE",
    "title": "Exponential Information Gathering (EIG) Algorithm - Distributed Consensus even when processes crash",
    "description": "Exponential Algorithms have to be the worst possible way to solve Distributed Consensus; but are they really that bad?\n\nIn this video, we talk about an exponential, expensive yet important algorithm to achieve distributed consensus named EIG algorithm that gathers an exponential amount of information and then reaches a consensus. Although a little expensive, but this algorithm is critical in laying the foundation for Blockchain to make them resilient to malicious users.\n\nOutline:\n\n00:00 Agenda\n02:35 Introduction to Distributed Consensus\n03:43 EIG Data Structure\n08:09 EIG Algorithm\n18:46 Alternative Decision Strategy",
    "img": "https://i.ytimg.com/vi/1g0L6ISxQPE/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/153ey3xO2C323uQZ0V/giphy.gif",
    "duration": "20:46",
    "view_count": 485,
    "like_count": 12,
    "comment_count": 0,
    "released_at": "2022-09-12",
    "gist": "Reaching consensus is extremely important in any distributed network. For example, we cannot have two data nodes in a cluster where one thinks that `price` is `$1000` while the other thinks it is `$2000`.\n\nSo, we need the nodes to talk to each other and reach a consensus and converge on the true value of `price`. Reaching consensus is easy when there are no failures - no network failures, or process failures.\n\nReaching a consensus\n\n- is impossible when the network is unreliable\n- is simple when there are no network/process failures\n- is tricky when we have to deal with process failures\n\n## EIG Algorithm\n\nThe core idea of the Exponential Information Gathering Algorithm is to gather a large amount of information from the network and then apply some decision rules to reach a consensus.\n\n## EIG Data Structure\n\nEIG algorithms require an EIG Tree that is like a Trie and is constructed level by level. If there are `n` nodes in the network and the level of the tree is `k` then the leaf of the tree contains all k-length permutations of `n` nodes.\n\nIf a node `a` received a message from a path `b, c, d` then the incoming message (value) is stored along the path `b`, `c`, `d` at the node `a`.\n\nThus, at each level of the tree, the number of children of each node reduces by 1. The root of the tree is labeled EMPTY, and it has `n` children, say A, B, and C.\n\nEach node in the next level will have 2 children. A will contain `AB` and `AC` while `B` will contain `BA` and `BC`, and `C` will contain `CA` and `CB`. Thus we see at level 2 the leaves contain all 6 2-length permutations of A, B, and C.\n\n## Algorithm\n\nWe assume at max `f` nodes would fail and hence the algorithm runs for `f + 1` rounds. In each round, a new level of EIG Tree is constructed. Each node independently constructs the level but every single node will have formed the exact same EIG Tree.\n\n### Round 1\n\nEvery process `i`, sends its value to the entire network including itself. Upon receiving the value `v` from `j`, the nodes update their own trees and add nodes `tree[j] = v`.\n\nAt the end of round 1, every node will have an EIG tree of depth 1.\n\n### Round > 2\n\nEvery process `i`, sends all pairs `(x, v)` from the `k - 1` level in the network where `i` is not in `x`. This would lead to the construction of the permutation tree.\n\nAfter the `f + 1` rounds, each node will have the exact same copy of the EIG Tree of depth `f + 2`.\n\n### Decision Rule\n\nThe algorithm stops after the `f + 1` rounds, and each node simply traverses through the entire EIG Tree to find all distinct values it holds.\n\nIf the number of values is 1, the node decides that as the final value. If the distinct values are many, then the node may choose the default value.\n\nThis decision-making is use-case specific, so defaulting to the old value can mean that the transaction is aborted and the node is reverting to the old value.\n\n### Alternative Decision Strategy\n\nDepending on the usecase, we may choose another decision strategy like picking the smallest value or the largest value or the oldest value, or the newest value.\n\nSo long as we have a total ordering of the values, we can define our own decision strategy.",
    "notes_gd": "https://drive.google.com/file/d/1hrBtQP4hDpjcVPEWBsExPzYDAeAW5jW1/view?usp=sharing",
    "slug": "exponential-information-gathering-eig-algorithm-distributed-consensus-even-when-processes-crash"
  },
  {
    "id": 191,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "uS19mAa_tFA",
    "title": "FloodSet Algorithm - Distributed Consensus even when processes crash",
    "description": "Reaching a consensus is extremely critical in a Distributed System as we would have situations day-in and day-out where we need nodes to agree upon a common value. The tricky part here is to achieve agreement even when the nodes participating in the consensus crash.\n\nIn this video, we talk about the simplest algorithm called the FloodSet algorithm that helps us achieve fault-tolerant Distributed Consensus, look at how it fits into the real world, and talk about the complexity it incurs.\n\nOutline:\n\n00:00 Agenda\n02:32 Distributed Consensus\n04:09 Problem Statement\n06:58 FloodSet Algorithm\n12:22 Alternate Decision Strategy\n14:34 Complexity Analysis",
    "img": "https://i.ytimg.com/vi/uS19mAa_tFA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/d3F3FQa5NgjCg/giphy.gif",
    "duration": "16:14",
    "view_count": 847,
    "like_count": 22,
    "comment_count": 2,
    "released_at": "2022-09-09",
    "gist": "Reaching consensus is extremely important in any distributed network. For example, we cannot have two data nodes in a cluster where one thinks that `price` is `$1000` while the other thinks it is `$2000`.\n\nSo, we need the nodes to talk to each other and reach a consensus and converge on the true value of `price`. Reaching consensus is easy when there are no failures - no network failures, or process failures.\n\nReaching a consensus\n\n- is impossible when the network is unreliable\n- is simple when there are no network/process failures\n- is tricky when we have to deal with process failures\n\n## FloodSet Algorithm\n\nThe core idea of the FloodSet algorithm is to keep track of all the values seen so far and use some decision rule such that all nodes choose the same value.\n\nEvery node maintains a set `W` to hold all the values seen so far. If we assume at max `f` nodes would fail then the FloodSet algorithm runs for `f + 1` rounds, giving chances for processes to fail.\n\nEvery node starts with `W = {v}`, its own value. In each round, every node broadcasts `W` in the network. When a node receives the `W` from other nodes it updates its `W` by doing a set union.\n\nAfter the `f + 1` rounds, every node will have the same `W` that holds all possible values of the network participating in the transaction.\n\n### Decision Making\n\nThe decision-making is decentralized. If `W` contains just one value then the node converges to that value. If it contains more than one value, the node defaults to the last value.\n\nThis decision-making is use-case specific, so defaulting to the old value can mean that the transaction is aborted and the node is reverting to the old value.\n\n### Alternative Decision Strategy\n\nDepending on the usecase, we may choose another decision strategy like picking the smallest value or the largest value or the oldest value, or the newest value.\n\nSo long as we have a total ordering of the values, we can define our own decision strategy.",
    "notes_gd": "https://drive.google.com/file/d/1RnSNfalo0RiwFqeYYd5Dy03of8_AXvI7/view?usp=sharing",
    "slug": "floodset-algorithm-distributed-consensus-even-when-processes-crash"
  },
  {
    "id": 190,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "DnOlqghKJMY",
    "title": "Unsolvable Distributed Consensus and The Two Generals' Problem",
    "description": "Distributed Consensus is extremely important to build a robust distributed system; because it is very common for a bunch of nodes to have a need to agree upon a common value - like Leader Node, some secret value, some meta information, or a value of a key.\n\nIn this video, we understand why it is impossible to achieve consensus in a distributed system where the communication channel is unreliable through a real-world analogy called the Two Generals Problem.\n\nOutline:\n\n00:00 Agenda\n02:35 Distributed Consensus\n03:39 Two Generals' Problem\n05:14 When networks are reliable\n06:59 Distributed Consensus in Real World\n08:13 When networks are unreliable\n10:37 How should the generals decide?\n12:25 How systems are built then?",
    "img": "https://i.ytimg.com/vi/DnOlqghKJMY/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l49K2rm0Hjg7PmbUA/giphy.gif",
    "duration": "15:29",
    "view_count": 621,
    "like_count": 24,
    "comment_count": 2,
    "released_at": "2022-09-07",
    "gist": "Reaching consensus is extremely important in any distributed network. For example, we cannot have two data nodes in a cluster where one thinks that `price` is `$1000` while the other thinks it is `$2000`.\n\nSo, we need the nodes to talk to each other and reach a consensus and converge on the true value of `price`. Reaching consensus is easy when there are no failures - no network failures, or process failures.\n\nBut, reaching a consensus becomes impossible when we cannot guarantee message delivery.\n\n## Two Generals' Problem\n\nSay, there are two generals - A and B - and they want to attack the enemy from two different directions. The only way to conquer the enemy is when both generals attack simultaneously. If only one attacks, then the enemy wins.\n\nThe generals communicate via foot soldiers. These foot soldiers can be captured by the enemy and hence the message that generals wanted to send to each other can be lost. So, how would generals coordinate the attack?\n\n### When no messages are lost?\n\nIf the communication channel is reliable, then the generals all send each other messages to agree to attack and everyone responds/ack to everyone else, thus coordinating the attack.\n\n\n## Real World Analogy\n\nCommitting to a distributed database. The commit should succeed when all the nodes of the database agree to commit. If anyone cannot commit then the commit cannot go through.\n\n## When messages are lost\n\nWhen general A sent a message to general B, what if B's response got lost? then general A would not know if it should attack or not.\n\nAlso, since B did not receive an ack from A, then it cannot decide if it should attack or not either.\n\nThis is where we see both generals will keep on waiting for an acknowledgment of an acknowledgment, purely because the communication channel is unreliable.\n\nThis is the class Two Generals' Problem where it is impossible to reach a consensus when the underlying communication channel is unreliable.\n\n## How should the generals decide?\n\nGenerals, instead of sending just a foot soldier, can send multiple foot soldiers increasing the probability that at least one of them would go through.\n\nThis is like we are flooding an unreliable network to get our message delivered.\n\n## But how do we do this in the Real World?\n\nIn the real world, hence we do not assume a completely unreliable network. Instead, we assume a certain fraction of messages will be lost - eg: 1 in 2. Hence to overcompensate we send 2 messages instead of one.",
    "notes_gd": "https://drive.google.com/file/d/1IZorhAB-Dzq_CsLcqlKwHddY1iP-s9Nw/view?usp=sharing",
    "slug": "unsolvable-distributed-consensus-and-the-two-generals-problem"
  },
  {
    "id": 189,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "c5t1rP6CvNs",
    "title": "Minimum Spanning Tree in Distributed Systems - GHS Algorithm - and its Applications",
    "description": "Nodes in a distributed system need to exchange a lot of messages; a naive way to do this is to flood the network with the message across all the connection lines across all the nodes.\n\nThis is highly inefficient and would unnecessarily congest the network. Hence we need our distributed network to maintain a Minimum Spanning Tree so that we can do an easy broadcast of the message to all nodes in the most efficient way.\n\nHence, in this video, we take a look into the famous GHS algorithm to construct the Minimum Spanning Tree in a distributed setup, go through the algorithm step by step, and see how a spanning tree makes leader election a piece of cake.\n\nOutline:\n\n00:00 Agenda\n02:45 Need for Minimum Spanning Tree\n06:49 GHS Algorithm to build Minimum Spanning Tree in Distributed System\n22:20 Termination of GHS Algorithm\n24:36 Complexity Analysis\n26:48 Leader Election using MST",
    "img": "https://i.ytimg.com/vi/c5t1rP6CvNs/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/PkiDwrlAoyZyw/giphy.gif",
    "duration": "33:13",
    "view_count": 1763,
    "like_count": 48,
    "comment_count": 13,
    "released_at": "2022-09-05",
    "gist": "Minimum Spanning Trees are super-critical in distributed systems, and they form the crux of building efficient broadcasting systems.\n\n## Spanning Tree\n\nA tree that covers all the nodes through selective edges. MST is the Spanning Tree such that the summation of the weights of the covered edges is minimum.\n\nThe weights on the edges can quantify communication latency, congestion, cost of the communication, or even distance.\n\n## Distributed Setup\n\nIn a distributed setup, every node does not have information about the entire topology, instead, it holds the information about just itself, the nodes connected to it, and the incident edges.\n\n## GHS Algorithm\n\nThe algorithm operates on a \"level\" and each level is a collection of a bunch of spanning trees. The core idea of this algorithm is to continue grouping spanning trees into a bigger ones until we are left with one huge spanning tree.\n\nLevel 0 consists of all the nodes and no edges and if we have `n` nodes, there will be `n` spanning trees (components) in the forest.\n\nEvery spanning tree would know\n\n- total number of nodes n\n- incident edges to it\n- its UID of the leader of its component\n\nEach node within the component sends a search message to the component to get MWOE (Minimum Weight Outgoing Edge).\n\nEach node upon receiving the message searches for an outgoing edge that is connecting to the node that is not part of the component. Of all such edges, the node picks the edge with the min weight and sends it across to the leader of the component.\n\n### How to test if another node is in another component?\n\nEvery node knows its leader UID and hence a node sends a `test` message to its immediate neighbors and they revert back with their leader UIDs. Comparing this UID with its own, the node knows if the other node is part of the same component or not.\n\n### Merging the components\n\nNow that the leader has the all minimum outgoing edges from the peripheral nodes, it can find a global minimum outgoing edge that is going out of its component.\n\nThe leader of the component now talks to the nodes connected over MWOE and tells them to mark itself in under this component. The new leader of the merged component is one of the two nodes with a higher UID connected over the MWOE.\n\n### Level by Level\n\nThe spanning tree is constructed level by level. It starts with all nodes being their own Spanning Tree. Then Two Level 0 nodes merge to form Level 1 nodes, and then Level 2 and so on.\n\n### Termination of the algorithm\n\nThe algorithm will terminate when the leader of the component is trying to find MWOE but none of the nodes sends it any information given they are all part of the same tree.",
    "notes_gd": "https://drive.google.com/file/d/1s_N8lpPSDdORbAfH818joLsxA3rlR-Om/view?usp=sharing",
    "slug": "minimum-spanning-tree-in-distributed-systems-ghs-algorithm-and-its-applications"
  },
  {
    "id": 188,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "tV3EQNgpZKI",
    "title": "Distributed Shortest-Path Bellman Ford Algorithm in Distributed Systems",
    "description": "To keep our distributed system efficient and performant, we have to ensure that the messages that are sent within the network take up the most efficient path. We may think ... network ... graph ... efficient path ... shortest path ... so can we not use our graph algorithms?\n\nWe cannot use them directly because this is distributed system and there is no single node that holds the information about the entire topology. Every node simply knows about the incident edges, and hence we have to take baby steps while devising a solution.\n\nIn this video, we take a look into a variant of the famous Bellman-Ford shortest path algorithm and see how it operates in a distributed setting.\n\nOutline:\n\n00:00 Agenda\n03:07 Need for Shortest Path in Distributed Systems\n06:59 Bellman-Ford Shortest Path in Distributed Systems\n14:35 Complexity Analysis",
    "img": "https://i.ytimg.com/vi/tV3EQNgpZKI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/1k5k3J5K3BywQOrpNA/giphy.gif",
    "duration": "17:5",
    "view_count": 791,
    "like_count": 26,
    "comment_count": 5,
    "released_at": "2022-09-02",
    "gist": "Determining the shortest path in a distributed system is an important problem to address and it finds its application across multiple use cases like\n\n- delivering messages to a node efficiently\n- efficient routing of messages\n\nA key point to consider here is the fact that \"shortest\" is not only about the distance, but it can also be about the congestion, time, cost of communication lines, cable infra, and much more.\n\n## Problem statement\n\nIn a distributed network, where nodes are connected via paths/edges having some weight assigned, find the shortest path from a specific source to all the nodes\n\n## Bellman-Ford Algorithm in Distributed System\n\nIn this gist, we discuss a synchronous approach which means every node moves forward in the algorithm in sync. There are ways to achieve this, but the implementation of synchronous behavior is out of the scope of this gist.\n\nBecause it is a distributed network no node knows the entire topology and weights. They just know\n\n- total number of nodes\n- their immediate neighbors, and\n- the weights of the edges incident on it.\n\nEvery node keeps track of `dist` which holds the shortest distance to it from the source `i0`. Initially, `dist` at `i0` will be `0` and `dist` at all other nodes will be `inf`.\n\nAt every round, all the nodes will send their `dist` across all of their outgoing edges to their neighboring nodes. Every node `i` upon receiving an incoming `dist` from its immediate neighbor `j` compares\n\n- its own `dist`\n- incoming `dist` + `weight(i, j)`\n\nafter comparing, if the incoming distance plus the weight of the connecting edge is smaller than its own `dist` it means that the distance from `i0` to the current node could be shorter and hence, the node updates the `parent` suggesting that the shortest path from `i0` to `i` goes through `j`.\n\nAfter `n - 1` rounds, the `dist` at every node will contain the shortest distance to it from source `i0`, and the `parent` will contain one of its immediate neighbors that lies in the shortest path.\n\n## Complexity Analysis\n\nWe require `n - 1` rounds to complete the algorithm, the time complexity of Bellman-Ford Shortest Path in Distributed System is `O(n)`. At every round, every node sends `dist` message across all of its edges to its immediate neighbors, the communication complexity becomes `O(n x |E|)`.",
    "notes_gd": "https://drive.google.com/file/d/1IG1MdH-DALxcbx3rtZ25KHxUwSbrwTfo/view?usp=sharing",
    "slug": "distributed-shortest-path-bellman-ford-algorithm-in-distributed-systems"
  },
  {
    "id": 187,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "PTlYBBqAYXA",
    "title": "Synchronous Breadth First Search Algorithm to power broadcast in Distributed Systems",
    "description": "In a distributed system, what if one of the nodes wants to efficiently broadcast a message in the network?\n\nThe situation is not as simple as it sounds, because there is no single node that holds the information about the entire topology; they just know about their immediate neighbors.\n\nIn this video, we take a look into an algorithm that powers synchronous Breadth-First Search traversal in a distributed setup, understand its time and communication complexity, and talk about its applications.\n\nOutline:\n\n00:00 Agenda\n02:36 Breadth-First Search and Distributed Systems\n06:26 BFS Distributed Algorithm\n13:58 Termination of the Algorithm\n18:57 Applications of BFS in Distributed Systems",
    "img": "https://i.ytimg.com/vi/PTlYBBqAYXA/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26n6WywJyh39n1pBu/giphy.gif",
    "duration": "21:46",
    "view_count": 1872,
    "like_count": 66,
    "comment_count": 8,
    "released_at": "2022-08-31",
    "gist": "Breadth First Search is a critical algorithm in Distributed systems as it powers key features like\n\n- Broadcast in minimum time\n- Building topological understanding\n- Topological stat like Diameter\n\nIn this gist, we discuss a synchronous approach which means every node moves forward in the algorithm in sync. There are ways to achieve this, but the implementation of synchronous behavior is out of the scope of this gist.\n\n## Output of BFS\n\nThe output of this traversal is a Breadth-First Directed Spanning tree that covers all the nodes but a subset of edges. This output is important because we can use this spanning tree as a foundation for other applications and algorithms.\n\n## The algorithm\n\nThe node `i0` initiates the BFS and it sends the `search` message to its neighbors. The nodes can either be marked or unmarked. If marked, they are part of the spanning tree already.\n\nWhen an unmarked node receives the `search` message,\n\n- it marks itself\n- updates its parent to the node it received the `search` message from\n\nIn the next round, the nodes that received the message in the previous round participate, and send `search` messages to their neighbors, and the nodes receiving the messages do the needful.\n\nEventually, every node will be receiving the `search` message from some or the other node and will be part of the spanning tree.\n\n## Complexity Analysis\n\nTime taken to complete the BFS is proportional to the diameter of the network and the number of messages exchanged will be equal to the number of outgoing edges in the network.\n\n## Conveying the children\n\nWith the current algorithm, every node knows its parent in the spanning tree but every node also needs to know which of its neighbors are its children in the spanning tree.\n\nTo achieve this, each node has to respond to the `search` message with a `parent/non-parent` message that tells the node if it was chosen to be the parent or not. This way, every node will know its parent and children in the spanning tree.\n\n## Termination of BFS\n\nThe most important part of any distributed algorithm is its termination. How would the node know that BFS is done?\n\nThe approach we use is called Convergecast.\n\nThe idea is to respond to the search message only when it received responses from all its children. This ensures that the node initiating the BFS would receive the messages from its children only after all the nodes respond to their corresponding parents.\n\n## Applications\n\nAfter constructing the BFS Spanning Tree, we can use this constructed path to\n\n- do an efficient broadcast on the network\n- do distributed computation in the network",
    "notes_gd": "https://drive.google.com/file/d/1s39L6dNbDCN0xk_ZpyVKRf2a2OgZsCdr/view?usp=sharing",
    "slug": "synchronous-breadth-first-search-algorithm-to-power-broadcast-in-distributed-systems"
  },
  {
    "id": 186,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "4aeFQpuww4E",
    "title": "FloodMax algorithm for Leader Election in Distributed Systems",
    "description": "Leader Election is necessary to make Distributed Systems auto recover and remain autonomous.\n\nIn this video, we take a detailed look into a Leader Election algorithm called FloodMax that works on any network with any topology. We understand the runtime complexity of it and look at a couple of optimizations that make it efficient.\n\nOutline:\n\n00:00 Agenda\n02:25 FloodMax Algorithm\n08:45 Complexity analysis of FloodMax Algorithm\n09:31 Optimizations in FloodMax Algorithm",
    "img": "https://i.ytimg.com/vi/4aeFQpuww4E/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6nV90vn3oe6bUuIM/giphy.gif",
    "duration": "12:41",
    "view_count": 1813,
    "like_count": 38,
    "comment_count": 4,
    "released_at": "2022-08-28",
    "gist": "Leader Election is a critical component in any distributed system. It enables the system to auto-recover from leader failures. When a leader node goes down, the Leader Election algorithm is triggered to elect the new leader.\n\nLeader Election should work with any topology and hence we take a look into an algorithm called FloodMax.\n\n## FloodMax Algorithm\n\nThe Flood Max Algorithm Flood Max works with a network that is arbitrarily connected. It assumes that every node is given a comparable UID that may be randomly allotted and every node knows the network's diameter.\n\n### The algorithm\n\nThe Flood Max algorithm is designed to elect the node with the maximum UID as the new leader and the core idea is to flood the network with the Max UID until the value converges.\n\nThe election process happens synchronously, which means every node moves forward in the algorithm in sync. There are ways to achieve this, but the implementation of synchronous behavior is out of the scope of this gist.\n\nThe algorithm stops after completing rounds equal to the diameter of the network. In each round, every node\n\n- sends the max UID it has seen to the connected nodes\n- updates the max UID it has seen so far after receiving messages from its neighbors\n\nAfter completing the `diameter` number of rounds, each node will have the max UID it has seen so far which will also be the global maximum; thus every node will know who is the leader of the network.\n\n### Complexity Analysis\n\nIt takes O(diameter) number of rounds to elect the leader and in each round the number of messages exchanged is equal to the number of edges, hence O(|E|); hence communication complexity is O(diameter x |E|).\n\n## Reducing Communication Complexity\n\nTo decrease the number of messages exchanged during the election, nodes can send the Max UID only when it changes. This would significantly reduce the messages exchanged across the network during leader elections.\n\nAnother optimization to reduce the communication is to NOT send the Max UID in the direction of the neighbor from which it was received.",
    "notes_gd": "https://drive.google.com/file/d/1yUZjPZoVKiKls7iof9Y-HF-9YsAoQpNN/view?usp=sharing",
    "slug": "floodmax-algorithm-for-leader-election-in-distributed-systems"
  },
  {
    "id": 185,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "mcKLQVmCsG4",
    "title": "TimeSlice algorithm for Leader Election in Distributed Systems",
    "description": "Leader Election helps our Distributed Systems auto recover without any human intervention and makes the system autonomous.\n\nIn this video, we take a detailed look into a Leader Election algorithm called TimeSlice that is extremely impractical but it still provides us great insight into a seemingly weird implementation.\n\nOutline:\n\n00:00 Agenda\n02:27 Introduction to the TimeSlice Algorithm\n05:21 The TimeSlice Algorithm\n11:03 Complexity Analysis of TimeSlice Algorithm",
    "img": "https://i.ytimg.com/vi/mcKLQVmCsG4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/26tnqkPaDXV7EzPAQ/giphy.gif",
    "duration": "14:3",
    "view_count": 683,
    "like_count": 27,
    "comment_count": 3,
    "released_at": "2022-08-26",
    "gist": "Leader Election is a critical component in any distributed system. It enables the system to auto-recover from leader failures. When a leader node goes down, the Leader Election algorithm is triggered to elect the new leader.\n\n## TimeSlice Algorithm\n\nTimeSlice algorithm for leader election is highly impractical, unbounded, yet an interesting algorithm to understand.\n\nThe algorithm assumes\n\n- each node has a UID that is a positive integer\n- the nodes are arranged in a virtual ring\n- each node knows its immediate neighbor to the right\n- each node knows the total number of nodes `n` in the network\n\n### The flow\n\nThe election proceeds in phases 1, 2, 3, and so on. Each phase consists of `n` rounds. Because the algorithm is synchronous, each node knows when the algorithm is proceeding with rounds and phases.\n\nIn phase `i`, nodes can only forward the message having the candidature of UID `i`. Hence, in phase `3`, a node will forward the message to the next node, only when it is having the candidature of UID `3`.\n\n## The flow\n\nIn phase 1, the node with UID 1 will send the message with its candidature across to the next node in the ring. If no such node exists, then no message is sent.\n\nThus, for `n` rounds within phase 1 there is void silence in the network. Then beings the phase 2.\n\nHence, we see that the messages will be sent across the ring only when the phase `i` beings where `i` is the smallest UID in the network. For all `(i - i) * n` rounds, there will be void silence in the network.\n\nWhen phase `i` begins the node with UID `i` will know that it is the new leader and it initiates the message and sends it to the next neighbor. For `n` successive rounds, the message is sent across the network and thus all `n` nodes know about the new leader `i`.\n\n## Complexity Analysis\n\nThe algorithm thus elects the node with the minimum UID as the new leader in just `O(n)` messages but takes time proportional to the `O(n*i)`.\n\nIf the minimum UID is a large integer, then the algorithm will take a longer time to elect the leader, and hence it is unbounded on the number of nodes in the network.",
    "notes_gd": "https://drive.google.com/file/d/1eZ2xCcikcZJ4krKj6pWuQtVvkFhCGAhL/view?usp=sharing",
    "slug": "timeslice-algorithm-for-leader-election-in-distributed-systems"
  },
  {
    "id": 184,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "inzQQm-kXCo",
    "title": "HS algorithm for Leader Election in Distributed Systems",
    "description": "Leader Election is important in every single distributed system out there as it enables us to auto recover from the failures.\n\nIn this video, we take a detailed look at a synchronous ring-based algorithm called the HS algorithm and see how it works on a bidirectional network in O(N logN) complexity.\n\nOutline:\n\n00:00 Agenda\n02:22 Introduction to HS Algorithm\n05:07 The HS Algorithm\n14:37 Halting the HS algorithm\n16:18 Key implementation detail of HS algorithm",
    "img": "https://i.ytimg.com/vi/inzQQm-kXCo/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/fveKCnZRN7aOrtWNfj/giphy.gif",
    "duration": "18:55",
    "view_count": 2026,
    "like_count": 55,
    "comment_count": 7,
    "released_at": "2022-08-24",
    "gist": "Leader Election is a critical component in any distributed system. It enables the system to auto-recover from leader failures. When a leader node goes down, the Leader Election algorithm is triggered to elect the new leader.\n\n## HS Algorithm\n\nHS algorithm is synchronous which means every node proceeds with the algorithm in sync. A key highlight of this algorithm is that it has communication complexity i.e. number of messages exchanged, of O(n long).\n\nThe algorithm works with the following assumptions\n\n- every node has a comparable unique ID\n- the communication between nodes is bidirectional\n- the nodes are virtually arranged in a circular ring\n- the nodes know their immediate neighbors\n\n### The flow\n\nEvery node participates in the election. To pitch itself, it creates a message with its UID and sends it to its immediate neighbors in both directions.\n\nTo reduce the number of messages required to elect the new leader, the nodes who knows their UID is smaller, they quickly drop out of the election.\n\nThe election proceeds in phases. In each phase `i`, the nodes participating in the election send their candidature to a hop distance of 2^i and wait for it to come back.\n\nEvery node along the path, upon receiving the message,\n\n- if the incoming is greater than its own, it forwards it\n- if the incoming is lesser than its own, it discards\n\nIf the node, receives its UID as an incoming message from both directions then it knows that it has the maximum UID in the 2^i neighborhood. With each phase, the hop distance increases exponentially.\n\nWith each phase, the nodes with smaller UIDs start to drop off from the election while still acting as the transmitter of the messages. After certain phases, there will be only one node remaining and that becomes the new leader.\n\n### Halting\n\nThe leader election in the HS algorithm halts when the node receives its probe message and thus it knows that it is the only one left and hence declares itself as the new leader.\n\nThe new leader creates the announcement message and relays it across the ring announcing itself as the new leader all the nodes locally update it and the election is concluded.\n\n### Key Implementation Detail\n\nThe message sent across during the election contains `<uid, hops, direction>`. It helps the node know\n\n- the number of hops the message has taken\n- stop forwarding when the hop count becomes 0\n- in which direction to reply",
    "notes_gd": "https://drive.google.com/file/d/1CHtYoRO_-LFEkX_R0xtTLPVsWfCO9jQd/view?usp=sharing",
    "slug": "hs-algorithm-for-leader-election-in-distributed-systems"
  },
  {
    "id": 183,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "NDBJr37dBzc",
    "title": "LCR algorithm for Leader Election in Distributed Systems",
    "description": "Leader Election is important for our distributed systems to auto recover from the failures.\n\nIn this video, we take a detailed look into a simple, naive, and intuitive distributed Leader Election algorithm called LCR, and understand how simple yet effective this algorithm can be.\n\nOutline:\n\n00:00 Agenda\n02:19 Introduction to LCR Algorithm\n05:07 The LCR Algorithm\n09:54 Halting the LCR Algorithm\n11:44 Complexity Analysis of LCR Algorithm",
    "img": "https://i.ytimg.com/vi/NDBJr37dBzc/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3orieZaGxNP4493CVy/giphy.gif",
    "duration": "14:20",
    "view_count": 2161,
    "like_count": 101,
    "comment_count": 31,
    "released_at": "2022-08-22",
    "gist": "Leader Election is a critical component in any distributed system. It enables the system to auto-recover from leader failures. When a leader node goes down, the Leader Election algorithm is triggered to elect the new leader.\n\n## LCR Algorithm\n\nLCR Algorithm for leader election is the simplest and the easiest to understand and implement, and its variant can be seen in action in a bunch of distributed systems.\n\nThe LCR algorithm expects the network to have the following properties\n\n### Unique ID\n\nEvery node in the network has a unique identification - UID - that can be compared with other UIDs.\n\n### Virtual Circular Ring\n\nLCR also assumes that the nodes in the network are virtually arranged in a circular ring, and each node knows the node to its right in the ring.\n\nAlthough we want a circular ring, the nodes may be physically connected to other nodes through any topology. The ring mandate is purely virtual and can be maintained only for powering elections.\n\n### The Algorithm\n\nIn the election, every node participates to be the new leader. To participate, it creates a message having its UID and sends it to its neighbor.\n\nWhen a node receives a UID, it compares the incoming UID with its own, and\n\n- if the incoming is greater than its own, it forwards it\n- if the incoming is lesser than its own, it discards\n- if the incoming UID == its own, it declares itself as the new leader\n\nWhen a node receives its UID as an incoming message, it implies that the message survived the entire iteration leading to the assertion that the node has the highest UID in the network; and hence can become the new leader.\n\nThe new leader is then announced through another message passed across the ring.\n\n### Halting the algorithm\n\nHalting is one of the most important aspects of any distributed algorithm, as it can get tricky to know when to stop.\n\nLCR algorithm stops when the new leader initiates the message and announces itself. To announce, it initiates the `HALT` message and sends it across.\n\nThe node receiving the `HALT` message understands that the new leader has been elected and needs to stop participating in the election. The node updates its local state with this information and forwards the message to the next node.\n\n## Complexity\n\nEach node participates in the election and sends messages across the entire ring. Every node could potentially receive the message from every other node; the communication complexity, the number of messages shared, is thus O(n^2).",
    "notes_gd": "https://drive.google.com/file/d/1W-9SA2qEo6jKvxcM4iMi326R63lK7XVm/view?usp=sharing",
    "slug": "lcr-algorithm-for-leader-election-in-distributed-systems"
  },
  {
    "id": 116,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "oMhESvU87jM",
    "title": "Implementing Distributed Transactions using Two Phase Commit Protocol",
    "description": "Previously, we built a theoretical foundation of Distributed Transaction using the Two-Phase Commit protocol. In this video, we implement the Distributed Transaction locally and mimic the food delivery system locally. While implementing we understand how to make the individual operations atomic and the entire distributed transaction atomic. We address resource contention while guaranteeing a consistent user experience.\n\nOutline:\n\n00:00 Revising the Two-Phase Commit\n07:35 Designing Database Schema\n11:40 Defining API Endpoints\n12:24 High-Level Architecture and Request Flow\n19:55 No inconsistent data - Atomicity\n24:14 Code walkthrough",
    "img": "https://i.ytimg.com/vi/oMhESvU87jM/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/RbDKaczqWovIugyJmW/giphy.gif",
    "duration": "39:24",
    "view_count": 6343,
    "like_count": 215,
    "comment_count": 34,
    "released_at": "2022-03-30",
    "gist": "Distributed Transactions are not theoretical; they are very well used in many systems. An example of it is 10-min food/grocery delivery.\n\nPreviously we went through the theoretical foundation for the Two-phase commit protocol; in this one let's spend some time going through the implementation detail and a few things to remember while implementing a distributed transaction.\n\n> The UX we want is: Users should see orders placed only when we have one food item and a delivery agent available to deliver.\n\nA key feature we want from our databases (storage layer) is atomicity. Our storage layer can choose to provide it through atomic operations or full-fledged transactions.\n\nWe will have 3 microservices: Order, Store, and Delivery.\n\nAn important design decision: The store services have food, and every food has packets that can be purchased and assigned. Hence, instead of just playing with the count, we will play with the granular food packets while ordering.\n\n## Phase 1: Reservation\n\nOrder service calls the reservation API exposed on the store and the delivery services. The individual services reserve the food packet (of the ordered food) and a delivery agent atomically (exclusive lock or atomic operation).\n\nUpon reservation, the food packet and the agent become unavailable for any other transaction.\n\n## Phase 2: Assignment\n\nOrder service then calls the store and delivery services to atomically assign the reserved food packet and the delivery agent to the order. Upon success assigning both to the order, the order is marked as successful, and the order service returns a 200 OK to the user.\n\nThe end-user will only see \"Order Placed\" when the food packet is assigned, and the delivery agent is assigned to the order. So, all 4 API calls should succeed for the order to be successfully placed.\n\nNegative cases:\n\n- If any reservation fails, the user will see \"Order Not Placed\"\n- If the reservation is made but assigning fails, the user will see \"Order Not Placed\"\n- If there is any transient issue in any service during the assignment phase, APIs will be retried by the order service to complete the order.\n- To not have a perpetual reservation, every reserved packet and delivery agent will have an expiration timer that will be large enough to cover transient outages.\n\nThus, in any case, an end-user will never experience a moment where we say that the order is placed, but it cannot be fulfilled in the backend.",
    "notes_gd": "https://drive.google.com/file/d/18q2ELr9n6GCemKbJ0aS7q7NyF7wX1kL9/view?usp=sharing",
    "slug": "implementing-distributed-transactions-using-two-phase-commit-protocol"
  },
  {
    "id": 117,
    "topic": {
      "id": 0,
      "uid": "distributed-systems",
      "name": "Distributed Systems",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT1wfRQo2xrrst2SGremT_qd",
      "bgcolor": "#DDF0FF",
      "themecolor": "#1798FF",
      "course_url": null
    },
    "yt_video_id": "7FgU1D4EnpQ",
    "title": "Distributed Transactions: Two-Phase Commit Protocol",
    "description": "Distributed Transactions are tough and intimidating. It is hard to guarantee atomicity across microservices given the network delays, resource contention, and unreliable services.\n\nIn this video, we discuss and take a detailed look into Distributed Transactions, understand why they are needed with a real-world example of Zomato's 10-minute food delivery, and build our understanding of the workings of the Two-Phase Commit protocol.\n\nOutline:\n\n00:00 Why Distributed Transactions\n03:44 Atomicity in Distributed Transactions\n06:47 Two-Phase Commit Protocol for Distributed Transactions\n18:29 Advantages and Disadvantages of Two-Phase Commit",
    "img": "https://i.ytimg.com/vi/7FgU1D4EnpQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o6wO5b4A4Mx965tWE/giphy.gif",
    "duration": "21:21",
    "view_count": 9740,
    "like_count": 354,
    "comment_count": 37,
    "released_at": "2022-03-28",
    "gist": "Distributed Transactions are essential to have strong consistency in a distributed setup.\n\nAn example could be as simple as a 10-min food/grocery delivery- where to guarantee a 10-min delivery, you can only accept orders when there are goods available in the dark store, and a delivery agent is available to deliver the goods.\n\nThis is a classic case of Distributed Transaction where you need a guarantee of atomicity and consistency across two different services. In a distributed setup, we can achieve it using an algorithm called Two-phase Commit.\n\nThe core idea of 2PC is: Split the transaction into two phases: Reservation and Assignment.\n\n## Phase 1: Reservation\n\nThe Order service will first talk to store service to reserve food items and delivery service to reserve a delivery partner. When the food or delivery partner is reserved, they are not notified. By reserving them, we are just making them unavailable for everyone else.\n\nIf the order service fails to reserve any of these, we roll back the reservation and abort the transaction informing the user that the order is not placed. Reservation comes with a timer, which means if we cannot assign a reserved food item to order in \"n\" minutes, we will be releasing the reservation, making them available for other transactions.\n\nWe move forward to the Commit phase only when the order service reserves both- a food item and a delivery agent.\n\n## Phase 2: Commit\n\nIn the Commit phase, the order services reach out to the store service and the delivery service to assign the food and agent to the order. Because the food and the agent were reserved, no other transaction could see it, and hence with a simple assignment, we can get the reserved food and agent assigned to an order.\n\nUpon this assignment, the store and the delivery agent are notified about the order and proceed with their respective duties.\n\nWe retry a few times if any of the assignments fail (which could happen only if the service goes down). If we still cannot get the assignment done, we inform the user that the order cannot be placed.\n\nThe order is placed only after the food item, and the delivery agent is assigned to the order.",
    "notes_gd": "https://drive.google.com/file/d/18WDFAstffIe_vGbtTz_CS117XhvDcBx3/view?usp=sharing",
    "slug": "distributed-transactions-two-phase-commit-protocol"
  }
]