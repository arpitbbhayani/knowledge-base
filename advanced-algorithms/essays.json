[
  {
    "id": 1,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "genetic-knapsack",
    "title": "Genetic Algorithm to solve the Knapsack Problem",
    "description": "The 0/1 Knapsack Problem has a pseudo-polynomial run-time complexity. In this essay, we look at an approximation algorithm inspired by genetics that finds a high-quality solution to it in polynomial time.",
    "gif": "https://media.giphy.com/media/3orieTU2tBje4i5SzS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/156910296-48167efc-dc08-4f26-88b8-7037fdfc6ae3.png",
    "released_at": "2022-03-07",
    "total_views": 441,
    "body": "The Knapsack problem is one of the most famous problems in computer science. The problem has been studied since 1897, and it refers to optimally packing the bag with valuable items constrained on the max weight the bag can carry. The [0/1 Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem) has a pseudo-polynomial run-time complexity. In this essay, we look at an approximation algorithm inspired by genetics that finds a high-quality solution to it in polynomial time.\n\n# The Knapsack Problem\n\nThe Knapsack problem is an optimization problem that deals with filling up a knapsack with a bunch of items such that the value of the Knapsack is maximized. Formally, the problem statement states that, given a set of items, each with a weight and a value, determine the items we pack in the knapsack with a constrained maximum weight that the knapsack can hold, such the the total value of the knapsack is maximum.\n\n## Optimal solution\n\nThe solution to the Knapsack problem uses Recursion with memoization to find the optimal solution. The algorithm covers all possible cases by considering every item picked and not picked. We remember the optimal solution of the subproblem in a hash table, and we reuse the solution instead of recomputing.\n\n```python\nmemo = {}\n\ndef knapsack(W, w, v, n):\n    if n == 0 or W == 0:\n        return 0\n  \n    # if weight of the nth item is more than the weight\n    # available in the knapsack the skip it\n    if (w[n - 1] > W):\n        return knapsack(W, w, v, n - 1)\n    \n    # Check if we already have an answer to the sunproblem\n    if (W, n) in memo:\n        return memo[(W, n)]\n  \n    # find value of the knapsack when the nth item is picked\n    value_picked = v[n - 1] + knapsack(W - w[n - 1], w, v, n - 1)\n\n    # find value of the knapsack when the nth item is not picked\n    value_notpicked = knapsack(W, w, v, n - 1)\n\n    # return the maxmimum of both the cases\n    # when nth item is picked and not picked\n    value_max = max(value_picked, value_notpicked)\n\n    # store the optimal answer of the subproblem\n    memo[(W, n)] = value_max\n\n    return value_max\n```\n\n## Run-time Complexity\n\nThe above solution runs with a complexity of `O(n.W)` where `n` is the total number of items and `W` is the maximum weight the knapsack can carry. Although it looks like a polynomial-time solution, it is indeed a _pseudo-polynomial_ time solution.\n\nGiven that the computation needs to happen by a factor of `W`, the maximum times the function will execute will be proportional to the max value of `W` which will be `2^m` where `m` is the number of bits required to represent the weight `W`.\n\nThis makes the complexity of above solution `O(n.2^m)`. The numeric value of the input `W` is exponential in the input length, which is why a pseudo-polynomial time algorithm does not necessarily run in polynomial time with respect to the input length.\n \nThis raises a need for a polynomial-time solution to the Knapsack problem that need not generate an optimal solution; instead, a good quick, high-quality approximation is also okay. Genetic Algorithm inspired by the Theory of Evolution does exactly this for us.\n\n# Genetic Algorithm\n\n[Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm) is a class of algorithms inspired by the process of natural selection. As per Darwin's Theory of Evolution, the fittest individuals in an environment survive and pass their traits on to the future generations while the weak characteristics die long the journey.\n\nWhile solving a problem with a Genetic Algorithm, we need to model it to undergo evolution through natural operations like Mutation, Crossover, Reproduction, and Selection. Genetic Algorithms help generate high-quality solutions to optimization problems, like [Knapsack](https://en.wikipedia.org/wiki/Knapsack_problem), but do not guarantee an optimal solution. Genetic algorithms find their applications across aircraft design, financial forecasting, and cryptography domains.\n\n## The Genetic Process\n\nThe basic idea behind the Genetic Algorithm is to start with some candidate _Individuals_ (solutions chosen at random) called _Population_. The initial population is the zeroth population, which is responsible for the spinning of the _First Generation_. The First Generation is also a set of candidate solutions that evolved from the zeroth generation and is expected to be better.\n\nTo generate the next generation, the current generation undergoes natural selection through mini-tournaments, and the ones who are fittest reproduce to create offspring. The offspring are either copies of the parent or undergo crossover where they get a fragment from each parent, or they undergo an abrupt mutation. These steps mimic what happens in nature.\n\n![The Genetic Flow - Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156874170-608cd9a4-6241-4882-b123-658d14a64c89.png)\n\nCreating one generation after another continues until we hit a termination condition. Post which the fittest solution is our high-quality solution to the problem. We take the example of the Knapsack problem and try to solve it using a Genetic Algorithm.\n\n# Knapsack using Genetic Algorithm\n\nSay, we have a knapsack that can hold 15kg of weight at max. We have 4 items `A`, `B`, `C`, and `D`; having weights of 7kg, 2kg, 1kg, and 9kg; and value `$5`, `$4`, `$7`, and `$2` respectively. Let's see how we can find a high-quality solution to this Knapsack problem using a Genetic Algorithm and, in the process, understand each step of it.\n\n![Knapsack Problem](https://user-images.githubusercontent.com/4745789/156769708-68ae14b5-4ccd-484b-b5be-7445ef3526cb.png)\n\n> The above example is taken from the Computerphile's [video](https://www.youtube.com/watch?v=MacVqujSXWE) on this same topic.\n\n## Individual Representation\n\nAn _Individual_ in the Genetic Algorithm is a potential solution. We first find a way to represent it such that it allows us to evolve. For our knapsack problem, this representation is pretty simple and straightforward; every individual is an `n`-bit string where each bit correspond to an item from the `n` items we have.\n\n![Individual Representation Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156770627-e6cc63e9-72b7-4afa-a968-60e994963a26.png)\n\nGiven that we have 4 items, every individual will be represented by a 4-bit string and the `i`th position in this string will denote if we picked that item in our knapsack or not, depending on if the bit is `1` or `0` respectively.\n\n## Picking Individuals\n\nNow that we have our _Individual_ representation done, we pick a random set of Individuals that would form our initial _population_. Every single individual is a potential solution to the problem. Hence for the Knapsack problem, from a search space of 2^n, we pick a few individuals randomly.\n\nThe idea here is to evolve the population and make them fitter over time. Given that the search space is exponential, we use evolution to quickly converge to a high-quality (need not be optimal) solution. For our knapsack problem at hand, let us start with the following 6 individuals (potential solutions) as our initial population.\n\n![Initial Population Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156789144-c0d14ee5-2ec9-4c51-aaab-a715728a38af.png)\n\n```python\ndef generate_initial_population(count=6) -> List[Individual]:\n    population = set()\n\n    # generate initial population having `count` individuals\n    while len(population) != count:\n        # pick random bits one for each item and \n        # create an individual \n        bits = [\n            random.choice([0, 1])\n            for _ in items\n        ]\n        population.add(Individual(bits))\n\n    return list(population)\n```\n\n## Fitness Coefficient\n\nNow that we have our initial population randomly chosen, we define a _fitness coefficient_ that would tell us how fit an individual from the population is. The fitness coefficient of an individual totally depends on the problem at hand, and if implemented poorly or incorrectly, it can result in misleading data or inefficiencies. The value of the fitness coefficient typically lies in the range of 0 to 1. but not a mandate.\n\n![Fitness Coefficient Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156890925-13e0f1bf-ec4a-40fe-8d48-60d867cdacae.png)\n\nFor our knapsack problem, we can define the fitness coefficient of an individual (solution) as the summation of the values of the items picked in the knapsack as per the bit string if the total weight of the picked items is less than the weight knapsack can hold. The fitness coefficient of an individual is 0 if the total weight of the item picked is greater than the weight that the knapsack can hold.\n\nFor the bit string `1001` the fitness coefficient will be\n\n```\ntotal_value  = (1 * v(A)) + (0 * v(B)) + (0 * v(C)) + (1 * v(D))\n             = ((1 * 5) + (0 * 4) + (0 * 7) + (1 * 2))\n\t         = 5 + 0 + 0 + 2\n\t         = 7\n\ntotal_weight = (1 * w(A)) + (0 * w(B)) + (0 * w(C)) + (1 * w(D))\n             = ((1 * 7) + (0 * 2) + (0 * 1) + (1 * 9))\n\t         = 7 + 0 + 0 + 9\n\t         = 16\n\nSince, MAX_KNAPSACK_WEIGHT is 15\nthe fitness coefficient of 1001 will be 0\n```\n\nThe higher the individual's fitness, the more are the chances of that individual to move forward as part of the evolution. This is based on a very common evolutionary concept called _Survival of the Fittest_.\n\n```python\ndef fitness(self) -> float:\n    total_value = sum([\n        bit * item.value\n        for item, bit in zip(items, self.bits)\n    ])\n\n    total_weight = sum([\n        bit * item.weight\n        for item, bit in zip(items, self.bits)\n    ])\n\n    if total_weight <= MAX_KNAPSACK_WEIGHT:\n        return total_value\n    \n    return 0\n```\n\n## Selection\n\nNow that we have defined the Fitness Coefficient, it is time to _select_ a few individuals to create the next generation. The selection happens using selection criteria inspired by evolutionary behavior. This is a tunable parameter we pick and experiment with while solving a particular problem.\n\nOne good selection criteria is [Tournament Selection](https://en.wikipedia.org/wiki/Tournament_selection), which randomly picks two individuals and runs a virtual tournament. The one having the higher fitness coefficient wins.\n\n![Selection Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156891433-13a356c7-d219-4a33-b7b3-423cdf10b910.png)\n\nFor our knapsack example, we randomly pick two individuals from the initial population and run a tournament between them. The one who wins becomes the first parent. We repeat the same procedure and get the second parent for the next step. The two parents are then passed onto the next steps of evolution.\n\n```python\ndef selection(population: List[Individual]) -> List[Individual]:\n    parents = []\n    \n    # randomly shuffle the population\n    random.shuffle(population)\n\n    # we use the first 4 individuals\n    # run a tournament between them and\n    # get two fit parents for the next steps of evolution\n\n    # tournament between first and second\n    if population[0].fitness() > population[1].fitness():\n        parents.append(population[0])\n    else:\n        parents.append(population[1])\n    \n    # tournament between third and fourth\n    if population[2].fitness() > population[3].fitness():\n        parents.append(population[2])\n    else:\n        parents.append(population[3])\n\n    return parents\n```\n\n## Crossover\n\nCrossover is an evolutionary operation between two individuals, and it generates children having some parts from each parent. There are different crossover techniques that we can use: single-point crossover, two-point crossover, multi-point crossover, uniform crossover, and arithmetic crossover. Again, this is just a guideline, and we are allowed to choose the crossover function and the number of children to create we desire.\n\nThe crossover does not always happen in nature; hence we define a parameter called the _Crossover Rate,_ which is relatively in the range of 0.4 to 0.6 given that in nature, we see a similar rate of crossover while creating offspring.\n\n![Crossover Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156891548-bfafdc41-0158-4146-b6c6-b9d14d2c536a.png)\n\nFor our knapsack problem, we keep it simple and create two children from two fit parents such that both get one half from each parent and the other half from the other parent. This essentially means that we combine half elements from both the individual and form the two children.\n\n```python\ndef crossover(parents: List[Individual]) -> List[Individual]:\n    N = len(items)\n\n    child1 = parents[0].bits[:N//2] + parents[1].bits[N//2:]\n    child2 = parents[0].bits[N//2:] + parents[1].bits[:N//2]\n\n    return [Individual(child1), Individual(child2)]\n```\n\n\n## Mutation\n\nThe mutation is an evolutionary operation that randomly mutates an individual. This is inspired by the mutation that happens in nature, and the core idea is that sometimes you get some random unexpected changes in an individual.\n\nJust like the crossover operation, the mutation does not always happen. Hence, we define a parameter called the _Mutation Rate,_ which is very low given that mutation rate in nature. The rate typically is in the range of 0.01 to 0.02. The mutation changes an individual, and with this change, it can have a higher or lower fitness coefficient, just how it happens in nature.\n\n![Mutation Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156822218-716ea60d-4d6b-434e-9112-26cba6c93b2c.png)\n\nFor our knapsack problem, we define a mutation rate and choose to flip bits of the children. Our `mutate` function iterates through the bits and sees if it needs to flip as per the mutation rate. If it needs to, then we flip the bit.\n\n```python\ndef mutate(individuals: List[Individual]) -> List[Individual]:\n    for individual in individuals:\n        for i in range(len(individual.bits)):\n            if random.random() < MUTATION_RATE:\n                # Flip the bit\n                individual.bits[i] = ~individual.bits[i]\n```\n\n## Reproduction\n\nReproduction is a process of passing an individual as-is from one generation to another without any mutation or crossover. This is inspired by nature, where evolutionary there is a very high chance that the genes of the fittest individuals are passed as is to the next generation. By passing the fittest individuals to the next generation, we get closer to reaching the overall fittest individual and thus the optimal solution to the problem.\n\nLike what we did with the Crossover and the Mutation, we define a _Reproduction Rate. Upon_ hitting that, the two fittest individuals will not undergo any crossover or mutation; instead, they would be directly passed down to the next generation.\n\n![Reproduction Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156825824-8cac7164-f526-47fd-958b-d24eba11c08a.png)\n\nFor our knapsack problem, we define a _Reproduction Rate_ and, depending on what we decide to pass the fittest individuals to the next generation directly. We keep the reproduction rate to `0.30` implying that 30% of the time, the fittest parents are passed down as is to the next generation.\n\n## Creating Generations\n\nWe repeat the entire process of Selection, Reproduction, Crossover, and Mutation till we get the same number of children as the initial population, and we call it the first generation. We repeat the same process again and create subsequent generations.\n\n![Creating Newer Generations Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156872749-9e93f05d-6ed1-4283-876d-4e7b62a01de9.png)\n\nWhen we continue to create such generations, we will observe that the _Average Fitness Coefficient_ of the population increases and it converges to a steady range. The image above shows how our value converges to 12 over 500 generations for the given problem. The optimal solution to the problem at hand is 16, and we converged to 12.\n\n```python\ndef next_generation(population: List[Individual]) -> List[Individual]:\n    next_gen = []\n    while len(next_gen) < len(population):\n        children = []\n\n        # we run selection and get parents\n        parents = selection(population)\n\n        # reproduction\n        if random.random() < REPRODUCTION_RATE:\n            children = parents\n        else:\n            # crossover\n            if random.random() < CROSSOVER_RATE:\n                children = crossover(parents)\n            \n            # mutation\n            if random.random() < MUTATION_RATE:\n                mutate(children)\n\n        next_gen.extend(children)\n\n    return next_gen[:len(population)]\n```\n\n## Termination Condition\n\nWe can continue to generate generations upon generations in search of the optimal solution, but we cannot go indefinitely, which is where we need a termination condition. A good termination condition is deterministic and capped. For example, we will at max go up to 500 or 1000 generations or until we get the same Average Fitness Coefficient for the last 50 values.\n\nWe can have a similar terminating condition for our knapsack problem where we put a cap at 500 generations. The individual with the max fitness coefficient is our high-quality solution. The overall flow looks something like this.\n\n```python\ndef solve_knapsack() -> Individual:\n    population = generate_initial_population()\n\n    avg_fitnesses = []\n\n    for _ in range(500):\n        avg_fitnesses.append(average_fitness(population))\n        population = next_generation(population)\n\n    population = sorted(population, key=lambda i: i.fitness(), reverse=True)\n    return population[0]\n```\n\n\n# Run-time Complexity\n\nThe run-time complexity of the Genetic Algorithm to generate a high-quality solution for the Knapsack problem is not exponential, but it is polynomial. If we operate with the population size of `P`And iterate till `G` generations, and `F` is the run-time complexity of the fitness function, the overall complexity of the algorithm will be `O(P.G.F)`.\n\nGiven that the parameters are known before starting the execution, you can predict the time taken to reach the solution. Thus, we are finding a non-optimal but high-quality solution to the infamous Knapsack problem in [Polynomial Time](https://en.wikipedia.org/wiki/Time_complexity).\n\n# Multi-dimensional optimization problems\n\nThe problem we discussed was trivial and was enough for us to understand the core idea of the Genetic Algorithm. The true power of the Genetic Algorithm comes into action when we have multiple parameters, dimensions, and constraints. For example, instead of just weight what if we have size and fragility as two other parameters to consider and we have to find a high-quality solution for the same Knapsack problem.\n\n# The efficiency of Genetic Algorithm\n\nWhile discussing the process of Genetic Algorithm, we saw that there are multiple parameters that can increase or decrease the efficiency of the algorithm; a few factors include\n\n## Population\n\nThe size of the initial population is critical in achieving the high efficiency of a Genetic Algorithm. For our Knapsack problem, we started with a simpler problem with 4 items i.e. search space of a mere 16, and an initial population of 6. We took such a small problem set just to wrap our heads around the idea.\n\nIn the real-world, genetic algorithms operate on a much larger search space and typically start with a population size of 500 to 50000. It is observed that if the initial population size does not affect the execution time of the algorithm, it converges faster with a large population than a smaller one.\n\n## Crossover\n\nAs we discussed above there are multiple Crossover functions that we can use, like Single Point Crossover, Two-point Crossover, and Multi-point Crossover. Which crossover function would work better for a problem depends totally on the problem at hand. It is generally observed that the two-point crossover results in the fastest convergence.\n\n> You can find the complete source code to the genetic algorithm discussed above in the repository [github/genetic-knapsack](https://github.com/arpitbbhayani/genetic-knapsack)\n",
    "similar": [
      "fractional-cascading",
      "rule-30",
      "flajolet-martin",
      "1d-terrain"
    ]
  },
  {
    "id": 2,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "lfsr",
    "title": "Pseudorandom Number Generation using LFSR",
    "description": "This essay takes a detailed look into pseudorandom number generation using LFSR, a widely adopted technique to generate random numbers on hardware and on software.",
    "gif": "https://media.giphy.com/media/3orieLKAOlnwdqkTCg/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/154754358-c1bbd5ff-2a0b-4028-b1ec-05cf90300d83.png",
    "released_at": "2022-02-21",
    "total_views": 609,
    "body": "A few essays back, we saw how pseudorandom numbers are generated using [Cellular Automaton - Rule 30](https://arpitbhayani.me/blogs/rule-30). This essay takes a detailed look into random number generation using [LFSR - Linear Feedback Shift Registers](https://en.wikipedia.org/wiki/Linear-feedback_shift_register). LFSR is widely adopted to generate random numbers on microcontrollers because they are very simple, efficient, and easy to adopt both hardware and software.\n\n# Pseudorandom Number Generators\n\nA [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) produces numbers that seem aperiodic (random) but are deterministic. It uses a seed value and generates a sequence of random numbers as a function of the current state and some previous states.\n\nThese are pseudorandom (not truly random) because the following numbers can be determined algorithmically if the seed value is known. True random numbers are either generated using hardware or from natural phenomena like blood volume pulse, atmospheric pressure, thermal noise, quantum phenomenon, etc.\n\n# What is LFSR\n\nA linear feedback shift register is a collection of bits that shifts when triggered, and the next state is a linear function of its previous state. We use [right-shift](https://en.wikipedia.org/wiki/Logical_shift) (`>>`) as the shift operation and [XOR](https://en.wikipedia.org/wiki/Exclusive_or) (`^`) as the linear function to generate the next state of the register.\n\n![8-bit LFSR](https://user-images.githubusercontent.com/4745789/154759305-23a775cd-f4fe-4aa5-9b4a-e68a7365695f.png)\n\nThe LFSR is initiated with a random value called a seed. The next state of the register can be computed deterministically using its previous state and the mentioned operations.\n\n# LFSR in action\n\nA series of latches (bits) connected to the next in line forming a chain is called a register. The diagram below shows an 8-bit shift register that shifts to the right upon an impulse. The rightmost bit that is thrown out during the shift is the output bit.\n\n![8-bit LFSR with a feedback loop](https://user-images.githubusercontent.com/4745789/154759436-7f7d4937-40bb-4a2a-964f-02df14c44d2b.png)\n\nWhen the shift happens, the leftmost latch gets vacant, and it is either\n\n-   filled with the output bit forming a circular shift register\n-   filled with zero, like a pure right shift operation of a programming language\n-   filled with a result of some boolean logic on the latches\n    \n\nLFSR that we used to generate pseudorandom numbers goes with the third approach and applies a boolean XOR on a set of chosen latches, called _taps_, and puts the resultant bit in the leftmost latch, creating a Linear Feedback.\n\n## A simple 4-bit LFSR\n\nAn LFSR has 3 configurable parameters\n\n-   number of bits in the register - `n`\n-   initial n-bit seed value - `seed`\n-   position of taps for XOR - `taps`\n\nWe build a simple `4`-bit LFSR with a seed value of `0b1001` and tap position of `1`. The output bit will be the rightmost bit of the register, and the next state of the LFSR will be computed as the\n\n-   XOR the output bit with the bit in the `1`st position (indexed at 0) from the right\n-   shift the bits of the register by one to the right\n-   set the vacant leftmost bit with the output of the XOR operation\n\n![4-bit LFSR](https://user-images.githubusercontent.com/4745789/154893624-b7a8c040-9e26-4f05-b24d-b0b60bcf88a7.png)\n\nAfter all the above operations are completed, the set of bits in the LFSR becomes the current state and is then used to output the next random bit, thus continuing the cycle.\n\n> The above example is taken from the Computerphile's [video](https://www.youtube.com/watch?v=Ks1pw1X22y4) on this same topic.\n\nGolang-based implementation of the above LFSR is as shown below. We define a struct holding LFSR with the mentioned three parameters - the number of bits in the register, the seed, and the position of the taps. We define the function named `NextBit` on it that returns a pseudorandom bit generated with the logic mentioned above.\n\n```go\ntype LFSR struct {\n\tn    uint32\n\tseed uint32\n\ttaps []uint32\n}\n\nfunc (l *LFSR) NextBit() uint32 {\n\tseed := l.seed\n\t\n\t// output bit is the rightmost bit\n\toutputBit := l.seed & 1\n\t\n\t// XOR all the bits present in the tap positions\n\tfor _, tap := range l.taps {\n\t\tseed = seed ^ (l.seed >> tap)\n\t}\n\t\n\t// the new msb is the output of this XOR\n\tmsb := seed & 1\n\t\n\t// rightsift the entire seed\n\t// and place newly computed msb at\n\t// the leftmost end\n\tl.seed = (l.seed >> 1) | (msb << (l.n - 1))\n\t\n\t// return the output bit as the next random bit\n\treturn outputBit\n}\n```\n\nWhen we execute the above code with seed `0b1001`, tap position `1`, on a `4`-bit LFSR, we get the following random bits as the output, and with a little bit of pretty-printing, we see\n\n```\nlfsr: 1001       output: 1\nlfsr: 1100       output: 0\nlfsr: 0110       output: 0\nlfsr: 1011       output: 1\nlfsr: 0101       output: 1\nlfsr: 1010       output: 0\nlfsr: 1101       output: 1\nlfsr: 1110       output: 0\nlfsr: 1111       output: 1\nlfsr: 0111       output: 1\nlfsr: 0011       output: 1\nlfsr: 0001       output: 1\nlfsr: 1000       output: 0\nlfsr: 0100       output: 0\nlfsr: 0010       output: 0\nlfsr: 1001       output: 1\nlfsr: 1100       output: 0\nlfsr: 0110       output: 0\nlfsr: 1011       output: 1\nlfsr: 0101       output: 1\n```\n\nThe output bits seem random enough, but upon a close inspection, we see that the last 5 output bits are the same as the first 5, and it is because, for the given configuration of seed and tap, the value in the LFSR becomes the same as the seed value `0b1001` after 15 iterations; thus, from the 16th position, we can see the same set of output bits generated.\n\nBy carefully selecting the seed and the taps, we can ensure that the cycle is long enough to never repeat in our process\u2019s lifetime. You can find the detailed source code for this LFSR at [github.com/arpitbbhayani/lfsr](https://github.com/arpitbbhayani/lfsr).\n\n## LFSR Bits to Number\n\nAlthough LFSR generates pseudorandom bits, generating random numbers is fairly simple using it. To generate a `k`-bit random number we need to generate `k` random bits using our routine LFSR and accumulate them in a `k`-bit integer, which becomes our random number.\n\nGolang-based implementation of the above logic can be seen in the function named `NextNumber` defined below.\n\n```go\nfunc (l *LFSR) NextNumber(k int) uint32 {\n\tvar n uint32 = 0\n\t\n\t// generate a random bit using LFSR\n\t// set the random bit in the lsb of a number\n\t// left-shift the number by 1\n\t// repeat the flow `k` times for a k-bit number\n\tfor i := 0; i < k; i++ {\n\t\tn = n << 1\n\t\tn |= l.NextBit()\n\t}\n\treturn n\n}\n```\n\nThe first ten 8-bit random numbers generated with a seed `0b1001` and tap position `1` using the above logic are\n\n```\n154\n241\n53\n226\n107\n196\n215\n137\n175\n19\n```\n\nWe can see that the numbers are fairly random and are within the limits of an 8-bit integer. You can find the detailed source code for this LFSR at [github.com/arpitbbhayani/lfsr](https://github.com/arpitbbhayani/lfsr).\n\n# Applications of LFSR\n\nLFSRs find their application across a wide spectrum of use cases, given how efficient they generate randomness. Their applications include digital counters, generating pseudorandom numbers, pseudo-noise, scramble radio frequencies, and in general, a stream of bytes. We take a detailed look into LFSR for scrambling.\n\n## Scrambling using LFSR\n\nLFSRs are computationally efficient and deterministic for a seed value, i.e., they generate the same set of numbers in the same order for a seed value, and here\u2019s how they find their application in scrambling and unscrambling a stream of bytes.\n\nThe idea of scrambling using LFSR goes like this. We read the raw bytes from the file and pass them to the scrambler function. The function initializes the LFSR with the necessary register length, seed, and tap positions and is kept ready to generate the 8-bit random numbers.\n\nThe stream of bytes, from the input file, is then XORed with the random numbers generated from the LFSR such the `i`th byte of the file is XORed with the `i`th 8-bit number generated from the LFSR. Golang-based implementation of `scramble` function is as shown below.\n\n```go\nfunc scramble(in []byte, n uint32, seed uint32,\n\t\t\t  taps []uint32) []byte {\n  \n  \t// initiatializing LFSR with the provided config\n\tl := lfsr.NewLFSR(n, seed, taps)\n  \n  \t// creating an output byte slice\n\tout := make([]byte, len(in))\n  \n  \t// XOR byte by byte\n\tfor i := range in {\n\t\tout[i] = in[i] ^ byte(l.NextNumber(8))\n\t}\n  \n  \t// return the output slice\n\treturn out\n}\n```\n\nThe unscrambling process exploits the following property of XOR and the fact that the LFSR will generate the same set of random numbers in the same order for the given configuration.\n\n```\na XOR b XOR a = b\n```\n\nSo, if we XOR a byte twice with the same number, we get the same byte in return. We apply the same `scramble` function even to unscramble the scrambled data.\n\n```go\nfunc unscramble(in []byte, n uint32, seed uint32,\n\t\t\t\ttaps []uint32) []byte {\n\n\t// invoking the same scramble function\n\treturn scramble(in, n, seed, taps)\n}\n```\n\n# Concerns with LFSR\n\nAlthough LFSRs are very efficient both on the software and hardware sides, there are some concerns about using them.\n\nThe number of bits in the LFSR is limited (as configured); the register will repeat the same set after generating a certain set of bits. The length of the cycle depends solely on the seed value and the tap configuration. So, while employing LFSR for random number generation, it is essential to pick a good set of tap positions and seeds to ensure a very long cycle.\n\nSuppose we get hold of a few consecutive random numbers generated from the LFSR. In that case, we can put them in a few linear equations and reach the initial configuration, enabling us to predict the future set of random numbers. Given how vulnerable LFSRs can be, they are not used at places that need cryptographic strength.\n",
    "similar": [
      "publish-python-package-on-pypi",
      "efficient-way-to-stop-an-iterating-loop",
      "the-weird-walrus",
      "jaccard-minhash"
    ]
  },
  {
    "id": 23,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fork-bomb",
    "title": "Fork Bomb",
    "description": "In this essay, we explore a simple yet effective DoS attack called Fork Bomb, also called Rabbit Virus. This attack forks out processes infinitely, starving them for any resources.",
    "gif": "https://media.giphy.com/media/l0HeiaW8q9B6tqoHS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/121253160-69db6d80-c8c6-11eb-8b05-202ec958324e.png",
    "released_at": "2021-06-09",
    "total_views": 425,
    "body": "In this essay, we explore a simple yet effective [DoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack) called [Fork Bomb](https://en.wikipedia.org/wiki/Fork_bomb), also called Rabbit Virus. This attack forks out processes infinitely, starving them for any resources.\n\nOnline Coding Platforms and Code Evaluators are susceptible to this attack as they accept raw code from the user and execute it. So, if you are building one, do ensure you are protected against it and infinite loops. We will also discuss how to prevent and defend against Fork Bomb in the final section of this essay.\n\n# How do they work?\n\nFork bombs can be summarized in just three words: *Fork until possible*. When executed, fork bombs continuously fork out non-terminating processes, demanding machine resources like CPU (mostly) and memory.\n\n![Fork Bomb](https://user-images.githubusercontent.com/4745789/121252662-e752ae00-c8c5-11eb-9524-a1c7d4fc24fc.png)\n\nWith so many processes competing for the CPU and other resources (if provisioned), the scheduler and CPU are put under tremendous load. After a specific limit, the entire system stalls.\n\n# Implementing Fork Bombs\n\nBefore we take a look at how to prevent or stop a Fork Bomb, let's look at something more interesting - how to implement a Fork Bomb?\n\nA quick detour, let's see what `fork` does: Upon every invocation, the forked child process is an exact duplicate of the parent process except for a [few details](https://man7.org/linux/man-pages/man2/fork.2.html), but nonetheless what matters to us is that it runs the exact same code as the parent.\n\n## C implementation\n\nA simple C implementation of a Fork Bomb could be, to fork child processes within an infinite for loop, resulting in exponential forking of child processes.\n\n```c\n#include <unistd.h>\nint main(void) {\n    for (;;) {\n        fork();\n    }\n}\n```\n\nWith the `fork` being invoked inside the infinite for loop, every single child process and the parent process will continue to remain stuck in the infinite loop while continuously forking out more and more child processes that execute the same code and stuck in the same loop; and thus resulting in exponential child forks.\n\nThese child processes start consuming the resources and blocking the legitimate programs. This prevents the creation of any new processes. This also freezes the process that responds to Keystrokes, putting the entire system to a standstill.\n\n## Bash implementation\n\nThere is a very famous Fork Bomb implementation in Bash, and the code that does this has no alphabets or numbers in it, just pure symbols.\n\n```bash\n:(){ :|:& };:\n```\n\nAlthough the shell statement looks gibberish, it is effortless to understand. In the statement above, we are defining a function named `:` having body `:|:&` and at the end invoking it using the name `:`, just like any usual shell function.\n\nAs part of the function body, we are again invoking the same function `:` and piping its output to the input of a background process executing another instance of `:`. This way, we are recursively invoking the same function (command) and stalling it by creating a pipe between the two.\n\nA cleaner way to redefine this very implementation of Fork Bomb would be\n\n```bash\nbomb() { \n    bomb| bomb& \n};bomb\n```\n\n# How to prevent them?\n\nTo protect our system against Fork Bombs, we can cap the processes owned by a certain user, thus blocking process creation at that cap.\n\nUsing the *nix utility called `ulimit`, we can set the maximum number of processes that a user can execute in the system, using the flag `-u`. By setting this value to an appropriate (lower) value, we can cap the process creation for a user, ensuring we can never be fork bombed by that user.\n\n# References\n\n- [Fork Bomb](https://en.wikipedia.org/wiki/Fork_bomb)\n- [ulimit - Man Page](https://linuxcommand.org/lc3_man_pages/ulimith.html)\n- [Understanding Bash Fork Bomb](https://www.cyberciti.biz/faq/understanding-bash-fork-bomb/)\n- [Preventing Fork Bombs on Linux](https://resources.cs.rutgers.edu/docs/preventing-fork-bomb-on-linux/)\n- [Fork bomb attack (Rabbit virus) - Imperva](https://www.imperva.com/learn/ddos/fork-bomb/)\n",
    "similar": [
      "bayesian-average",
      "mongodb-cursor-skip-is-slow",
      "inheritance-c",
      "super-long-integers"
    ]
  },
  {
    "id": 27,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fully-persistent-arrays",
    "title": "Fully Persistent Arrays",
    "description": "Persistent Data Structures allow us to hold multiple versions of a data structure at any given instant of time. This enables us to go back in 'time' and access any version that we want. In this essay, we take a detailed look into the implementation of Fully Persistent Arrays.",
    "gif": "https://media.giphy.com/media/26tPplGWjN0xLybiU/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/107943189-55d7cd00-6fb2-11eb-9e03-a1da3870e718.png",
    "released_at": "2021-02-14",
    "total_views": 618,
    "body": "[Persistent Data Structures](https://arpitbhayani.me/blogs/persistent-data-structures-introduction) preserve previous versions of themselves allowing us to revisit and audit any historical version. While we already did an exhaustive introduction in the [previous essay](https://arpitbhayani.me/blogs/persistent-data-structures-introduction), in this essay, we take a detailed look into how we can implement the ***Fully Persistent*** variant of the most common data structure - ***Arrays***.\n\nThe essay is loosely based on the paper titled [Fully Persistent Arrays for Efficient Incremental Updates and Voluminous Reads](https://link.springer.com/chapter/10.1007/3-540-55253-7_7) by Tyng-Ruey Chuang and we implement it using Backer's trick elaborated in the paper [A Persistent Union-Find Data Structure](https://www.lri.fr/~filliatr/ftp/publis/puf-wml07.pdf).\n\n# Fully Persistent Arrays\n\nAn array is an abstract data structure consisting of `n` slots to hold a maximum of `n` elements such that each element is identified by at least one index. A typical array allows the following functions - `create(n)`, `update(index, value)` and `get(index)`.\n\nThe simplest form of an array, that we all are familiar with, is the Linear Array that is designed to hold elements in consecutive memory locations, leveraging spatial locality for faster and more efficient retrievals and scans. Before we jump into the implementation details of Fully Persistent Arrays, let's reiterate what exactly are Fully Persistent Data Structures.\n\nPersistent Data Structures preserve previous versions of themselves allowing us to revisit and audit any historical version. Fully Persistent Data Structures allows access and modification to all the historical versions as well. It does not restrict any modifications whatsoever. This means we can typically revisit any historical version of the data structure, modify it like we are forking out a new branch.\n\n![https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png](https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png)\n\nFully Persistent Arrays are arrays that support Full Persistence which means it supports usual array operations while also allowing us to go back in time and make updates to any of the previous versions. We define the following operations on Fully Persistent Arrays -\n\n- `create(n)` - returns an array of size `n` having all the slots uninitialized\n- `update(array, index, value)` - returns a new array identical to `array` except for the element at the position `index`. The parent array `array` remains unaffected and is still accessible.\n- `get(array, index)` - returns the element present at the index `index` in array `array`\n\n# Implementing Fully Persistent Array\n\nA naive way of implementing these arrays is to do a [Copy-on-Write](https://arpitbhayani.me/blogs/copy-on-write) and keep track of historical versions. This approach very inefficient as it requires `m` times the memory required to hold `n` elements, where `m` is the total number of versions of the array.\n\n![https://user-images.githubusercontent.com/4745789/107803148-3cebd380-6d88-11eb-9889-bb551e83c00a.png](https://user-images.githubusercontent.com/4745789/107803148-3cebd380-6d88-11eb-9889-bb551e83c00a.png)\n\nA better way of implementing these arrays is by using the [Backer's Trick](https://www.lri.fr/~filliatr/ftp/publis/spds-rr.pdf) which enables the required functionality with just one array and a tree of modifications.\n\n# Fully Persistent Arrays using Backer's Trick\n\nA more efficient way of implementing Fully Persistent Arrays is by using a single instance of an in-memory Array and in conjunction use a tree of modifications. Instead of storing all the versions separately, Backer's trick allows us to compute any version of the array by replaying all the changes asked for.\n\n## Tree of modifications\n\nThe tree of modifications is an `n`-ary tree that holds all the versions of the array by storing only the modifications made to the elements. Each version is derived from a parent version and the root points to the in-memory *cache* array which holds the initial version of it.\n\n![https://user-images.githubusercontent.com/4745789/107856766-71c35d80-6e50-11eb-9f59-c3744cdc884f.png](https://user-images.githubusercontent.com/4745789/107856766-71c35d80-6e50-11eb-9f59-c3744cdc884f.png)\n\nEach node of the tree holds three fields - `index`, `value`, and a pointer to the `parent`, making this tree pointing upwards towards to root. Thus each node holds the changed `value`, where did the change happen `index` and on which version the change happened `parent`.\n\nSay we changed the element at the index `1` of the array `9, 6, 3, 5, 1`  to `7` we get array `9, 7, 3, 5, 1`. The tree of modifications has 2 nodes one root node `a0` pointing to the initial array, and another node `a1` denoting the updated version.\n\n![https://user-images.githubusercontent.com/4745789/107858298-996af380-6e59-11eb-99dc-7a68ea25f5b4.png](https://user-images.githubusercontent.com/4745789/107858298-996af380-6e59-11eb-99dc-7a68ea25f5b4.png)\n\nThe node `a1` has 3 fields, `index` set to `1`, `value` set to `7` and `parent` pointing to `a0`. The node implies that it was derived from `a0` by changing the value of the element at the index `1` to `7`. If we try to branch off `a0` with another change say index `4` set to value `9` we would have 3 nodes in the tree. Thus we see how an update translates into just creating a new node and adding it at the right place in the tree.\n\nNow we see with this design how we implement the three functions of an array `create`, `update`, and `get`.\n\n## Implementing `create`\n\nThe `create` function allocates a linear array of size `n` to hold `n` elements. This is a usual array allocation. While doing this we also create the root node of our tree of modifications. The root node, as established earlier, points to the *cache* array.\n\n```python\n# The function creates a new persistent array of size `n`\ndef create(n: int) -> Array:\n    # 1. allocate in-memory cache\n    # 2. initialize the tree of modifications\n    # 3. make the root of the tree point to the cache\n    pass\n```\n\nThe overall complexity of this operation is `O(n)` space and `O(n)` time.\n\n## Implementing `update`\n\nThe `update` operation takes in the `index` that needs to be updated, the `value`, and the version of the array on which update is to be made.\n\n```python\n# The function updates the element at the index `index` with value\n# `value` on array `array` and returns the newly updated array\n# keeping the old one accessible.\ndef update(array: Array, index: int, value: object) -> Array:\n    # 1. create a node in the tree and store index, the value in it\n    # 2. point this new node to the parent array\n    pass\n```\n\nTo do this efficiently, we create a new node in the tree whose parent is set to the array version on which update is performed, index and value are set what was passed during invocation. Thus we see that the `update` operation takes a constant `O(1)` space and `O(1)` time to create and represent a new version of the array.\n\nWith the update operation being made efficient we have to trade-off `get` operation.\n\n## Implementing `get`\n\nThe `get` operation takes in the `index` that needs to be fetched and the version of the array from which the element is to be fetched. The `get` operation seeks no extra space but takes time proportional to the distance between the array version and the root. In the worst case, this distance will be as long as the total number of versions of the array.\n\n```python\n# The function fetches the element from index `index`\n# from the array `array` and returns it.\ndef get(array: Array, index: int) -> object:\n    # 1. Start from the requested array and traverse to the root node.\n    # 2. Allocate a new register to store the requested value\n    # 3. During traversal, if the node.index == `index` update the\n    # register with the value.\n    # 4. return the value of the register\n    pass\n```\n\nThe overall complexity of this operation is `O(1)` space and `O(n)` time.\n\n# Optimizing successive reads on the same version\n\nWe established that the update operation takes constant time and reads are expensive. If our system is write-heavy, then this is pretty handy but if the system has more reads then operation taking `O(n)` time hampers the overall performance of the system. So as to optimize this use case we take a look at the operation called *Rerooting.*\n\n## Rerooting\n\nThe initial array (the first version of the array) has no significance to be the root forever. We can reroot the entire tree such that any child node could become the root and the value it points to - *cache* - represents the true copy of the array. Rerooting is a sequence of rotations to make the desired array version the root.\n\nThe algorithm for rerooting is a classic Backtracking algorithm that requires updates in all the nodes coming in the path from the old node to the new node.\n\n![https://user-images.githubusercontent.com/4745789/107935855-ee1c8480-6fa7-11eb-9870-2ae90d6460a9.png](https://user-images.githubusercontent.com/4745789/107935855-ee1c8480-6fa7-11eb-9870-2ae90d6460a9.png)\n\nThe rerooting operation takes time proportional to the distance between the old and new root ~ `O(n)`. Since the successive reads are happening on the same version the `get` operation becomes `O(1)` as well. Thus depending on the kind of usage of the system we can add rerooting step in either `get` or `update` operation.\n\n# References\n\n- [Persistent Arrays - Wikipedia](https://en.wikipedia.org/wiki/Persistent_array)\n- [Semi-Persistent Data Structures](https://www.lri.fr/~filliatr/ftp/publis/spds-rr.pdf)\n- [Fully Persistent Arrays for Efficient Incremental Updates and Voluminous Reads](https://link.springer.com/chapter/10.1007/3-540-55253-7_7)\n",
    "similar": [
      "phi-accrual",
      "persistent-data-structures-introduction",
      "copy-on-write",
      "lfu"
    ]
  },
  {
    "id": 28,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "persistent-data-structures-introduction",
    "title": "Persistent Data Structures - An Introduction",
    "description": "Persistent Data Structures allow us to hold multiple versions of a data structure at any given instant of time. This enables us to go back in 'time' and access any version that we want.",
    "gif": "https://media.giphy.com/media/1hVi7JFFzplHW/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/107117196-9347a680-689e-11eb-8d3f-75da045b7baf.png",
    "released_at": "2021-02-07",
    "total_views": 1861,
    "body": "Ordinary data structures are ephemeral implying that any update made to it destroys the old version and all we are left with is the updated latest one. Persistent Data Structures change this notion and allow us to hold multiple versions of a data structure at any given instant of time. This enables us to go back in \"time\" and access any version that we want.\n\nIn this essay, we take a detailed look into the world of Persistent Data Structures and see the basics of its implementation along with where exactly we can find them in action. This essay is meant to act as an exhaustive introduction to the topic and in future essays, we will dive deeper into the specifics of each data structure.\n\nThis essay is loosely based on the iconic paper published in 1986 titled [Making Data Structures Persistent](https://www.cs.cmu.edu/~sleator/papers/making-data-structures-persistent.pdf) by James R. Driscoll, Neil Sarnak, Daniel D. Sleator, and Robert E. Tarjan.\n\n# Persistence\n\nPersistent Data Structures preserve previous versions of themselves allowing us to revisit and audit any historical version. Depending on the operations allowed on the previous versions, persistence is classified into three categories\n\n### Partially Persistent\n\nPartially Persistent Data Structures allows access to all the historical versions but allows modification to only the newest one. This typically makes historical versions of the data structure immutable (read-only).\n\n![https://user-images.githubusercontent.com/4745789/107117960-e4a66480-68a3-11eb-97a0-c8c412527471.png](https://user-images.githubusercontent.com/4745789/107117960-e4a66480-68a3-11eb-97a0-c8c412527471.png)\n\n### Fully Persistent\n\nFully Persistent Data Structures allows access and modification to all the historical versions. It does not restrict any modifications whatsoever. This means we can typically revisit any historical version and modify it and thus fork out a new branch.\n\n![https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png](https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png)\n\n### Confluently Persistent\n\nConfluently Persistent Data Structures allow modifications to historical versions while also allowing them to be merged with existing ones to create a new version from the previous two. \n\n![https://user-images.githubusercontent.com/4745789/107117954-da846600-68a3-11eb-9c34-b9489c170710.png](https://user-images.githubusercontent.com/4745789/107117954-da846600-68a3-11eb-9c34-b9489c170710.png)\n\n# Applications of Persistent Data Structures\n\nPersistent Data Structures find their applications spanning the entire spectrum of Computer Science, including but not limited to - Functional Programming Languages, Computational Geometry, Text Editors, and many more.\n\n## Functional Programming Languages\n\nFunctional Programming Languages are ideal candidates for incorporating Persistent Data Structures as they forbid, while some discourage, the mutability of underlying structures. These languages pass around states within functions and expect that they do not update the existing one but return a new state. Programming languages like Haskell, Clojure, Elm, Javascript, Scala have native Persistent implementations of data structures like Lists, Maps, Sets, and Trees.\n\n## Computational Geometry\n\nOne of the fundamental problems in Computational Geometry is the [Point Location Problem](https://en.wikipedia.org/wiki/Point_location) which deals with identifying the region where the query point lies. A simpler version of the problem statement is to find if a point lies within or outside a given polygon. A popular solution to determine a solution to the Point Location problem statement uses [Persistent Red-Black Trees](https://en.wikipedia.org/wiki/Red\u2013black_tree).\n\n## Text and File Editing\n\nThe most common operation required by any Text or File editing tool is *Undo and Redo* and having persisted all historical versions through a persistent data structure makes these most frequent operations very efficient and a breeze.\n\n# Implementing Partial Persistence\n\nThere are a few generic techniques that help in implementing Partial Persistence. Just to reiterate, partial persistence allows access to all the historical versions but allows modification to only the newest one, to create a newer updated copy of data.\n\n## Copy on Write Semantics\n\nA naive way to implement Partial Persistence is by utilizing the Copy-on-Write semantics and naively creating a deep copy upon every update. This technique is inefficient because upon every writes the entire data structure is deep copied.\n\nThere are certain methods built upon certain storage paradigms that copy only what matters making the entire CoW efficient. I have already gone in-depth of [Copy-on-Write semantics](https://arpitbhayani.me/blogs/copy-on-write) and I encourage you to check that out.\n\n## The Fat Node Method\n\nInstead of creating copies of the entire data structure, the Fat Node method suggests that each cell holding the value within the data structure is modified to hold multiple values (one for each version) making it arbitrarily *fat*. Each value node thus holds a value and a version stamp.\n\n```python\nclass Node:\n    def __init__(self, value: object):\n        # references to other Nodes creating a Node topology\n        # that forms the core of the data structure\n        # this could be `next` and `prev` pointers in a linked list\n        # or `left`, `right` in case of a binary search tree.\n        self.refs: List[] = []\n\n        # holding all the values against the version (key)\n        self.values: Dict[int, object] = {}\n```\n\n## The Node-Copying Method\n\nThe Node-Copying method eliminates all the problems with the Fat Node method. It allows each node to hold only a fixed number of values in it. Upon exhaustion of space, a new copy of Node is created and it holds only the newest values in it. The old node also holds pointers to the newly created node allowing browsing.\n\nHaving this structure helps in making update operation slightly efficient by reducing the number of nodes to be copied during writes, considering the in-degrees to each node is bounded.\n\n## Path-Copying Method\n\nThe path-copying method copies all the nodes coming in the path from the root to the node being modified. This way it tries to minimize the copy and promotes reusing some of the unmodified data. This method comes in super handy in Linked Data Structures like Lists and Trees.\n\n![https://user-images.githubusercontent.com/4745789/107144006-33b0d000-695e-11eb-9e13-959eaeba44f4.png](https://user-images.githubusercontent.com/4745789/107144006-33b0d000-695e-11eb-9e13-959eaeba44f4.png)\n\n> Implementation of Full Persistence is a mammoth of a topic on its own and deserves its own essay. Hence skipping it from the scope of this one.\n\n# Reducing the memory footprint\n\nSince persistent data structures thrive on high memory usage, they require some garbage collection system to prevent memory leaks. Algorithms like [Reference Counting](https://en.wikipedia.org/wiki/Reference_counting) or [Mark and Sweep](https://en.wikipedia.org/wiki/Mark_and_sweep) serves the purpose pretty well.\n\nThus when a historical version is not referenced anymore in the program space, the corresponding objects and nodes are freed up.\n\n# References\n\n- [Making Data Structures Persistent](https://www.cs.cmu.edu/~sleator/papers/making-data-structures-persistent.pdf)\n- [Persistent data structure - Wikipedia](https://en.wikipedia.org/wiki/Persistent_data_structure)\n- [Geometric Data Structures](http://www.sccg.sk/~samuelcik/dgs/geom_structures.pdf)\n- [Point Location Problem - Wikipedia](https://en.wikipedia.org/wiki/Point_location)\n- [Text Editor: Data Structures - HackerNews](https://news.ycombinator.com/item?id=15381886)\n",
    "similar": [
      "bitcask",
      "leaderless-replication",
      "master-replica-replication",
      "rum"
    ]
  },
  {
    "id": 32,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "flajolet-martin",
    "title": "Approximate Count-Distinct using Flajolet Martin Algorithm",
    "description": "Measuring distinct elements from a stream of values is one of the most common utilities that finds its application across the spectrum. Most of these use cases do not expect accurate count-distinct rather they expect it to be computed very quickly and efficiently. In this essay, we deep dive into one of the first Count-Distinct approximation algorithm called Flajlet-Martin Algorithm.",
    "gif": "https://media.giphy.com/media/cRMgB2wjHhVN2tDD2z/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/101244544-31977e80-372d-11eb-9ab4-d9e879496d39.png",
    "released_at": "2020-12-06",
    "total_views": 3131,
    "body": "Measuring the number of distinct elements from a stream of values is one of the most common utilities that finds its application in the field of Database Query Optimizations, Network Topology, Internet Routing, Big Data Analytics, and Data Mining.\n\nA deterministic count-distinct algorithm either demands a large auxiliary space or takes some extra time for its computation. But what if, instead of finding the cardinality deterministically and accurately we just approximate, can we do better? This was addressed in one of the first algorithms in approximating count-distinct introduced in the seminal paper titled [Probabilistic Counting Algorithms for Data Base Applications](http://algo.inria.fr/flajolet/Publications/FlMa85.pdf) by Philippe Flajolet and G. Nigel Martin in 1984.\n\nIn this essay, we dive deep into this algorithm and find how wittily it approximates the count-distinct by making a single pass on the stream of elements and using a fraction of auxiliary space.\n\n# Deterministic count-distinct\n\nThe problem statement of determining count-distinct is very simple -\n\n*Given a stream of elements, output the total number of distinct elements as efficiently as possible.*\n\n![https://user-images.githubusercontent.com/4745789/101273043-250c3800-37b8-11eb-9e0c-435e386f3529.png](https://user-images.githubusercontent.com/4745789/101273043-250c3800-37b8-11eb-9e0c-435e386f3529.png)\n\nIn the illustration above the stream has the following elements `4`, `1`, `7`, `4`, `2`, `7`, `6`, `5`, `3`, `2`, `4`, `7` and `1`. The stream has in all `7` unique elements and hence it is the count-distinct of this stream.\n\nDeterministically computing count-distinct is an easy affair, we need a data structure to hold all the unique elements as we iterate the stream. Data structures like [Set](https://en.wikipedia.org/wiki/Set_(abstract_data_type)) and [Hash Table](https://en.wikipedia.org/wiki/Hash_table) suit this use-case particularly well. A simple pythonic implementation of this approach is as programmed below\n\n```python\ndef cardinality(elements: int) -> int:\n    return len(set(elements))\n```\n\nAbove deterministic approach demands an auxiliary space of `O(n)` so as to accurately measure the cardinality. But when we are allowed to approximate the count we can do it with a fraction of auxiliary space using the Flajolet-Martin Algorithm.\n\n# The Flajolet-Martin Algorithm\n\nThe Flajolet-Martin algorithm uses the position of the rightmost set and unset bit to approximate the count-distinct in a given stream. The two seemingly unrelated concepts are intertwined using probability. It uses extra storage of order `O(log m)` where `m` is the number of unique elements in the stream and provides a practical estimate of the cardinalities.\n\n## The intuition\n\nGiven a good uniform distribution of numbers, the probability that the rightmost set bit is at position `0` is `1/2`, probability of rightmost set bit is at position `1` is `1/2 * 1/2 = 1/4`, at position `2` it is `1/8` and so on.\n\n![https://user-images.githubusercontent.com/4745789/101275842-e635ac80-37ce-11eb-9e00-357b966cbac6.png](https://user-images.githubusercontent.com/4745789/101275842-e635ac80-37ce-11eb-9e00-357b966cbac6.png)\n\nIn general, we can say, the probability of the rightmost set bit, in binary presentation, to be at the position `k` in a uniform distribution of numbers is\n\n![https://user-images.githubusercontent.com/4745789/101275886-357bdd00-37cf-11eb-9bc6-346332031eb2.png](https://user-images.githubusercontent.com/4745789/101275886-357bdd00-37cf-11eb-9bc6-346332031eb2.png)\n\nThe probability of the rightmost set bit drops by a factor of `1/2` with every position from the Least Significant Bit to the Most Significant Bit.\n\n![https://user-images.githubusercontent.com/4745789/101276356-1cc0f680-37d2-11eb-858d-3f40061988f0.png](https://user-images.githubusercontent.com/4745789/101276356-1cc0f680-37d2-11eb-858d-3f40061988f0.png)\n\nSo if we keep on recording the position of the rightmost set bit, `\u03c1`, for every element in the stream (assuming uniform distribution) we should expect `\u03c1 = 0` to be `0.5`, `\u03c1 = 1` to be `0.25`, and so on. This probability should become `0` when bit position, `b` is `b > log m` while it should be non-zero when `b <= log m` where `m` is the number of distinct elements in the stream.\n\nHence, if we find the rightmost unset bit position `b` such that the probability is `0`, we can say that the number of unique elements will approximately be `2 ^ b`. This forms the core intuition behind the Flajolet Martin algorithm.\n\n## Ensuring uniform distribution\n\nThe above intuition and approximation are based on the assumption that the distribution of the elements in the stream is uniform, which cannot always be true. The elements can be sparse and dense in patches. To ensure uniformity we hash the elements using a multiplicative hash function\n\n![https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png](https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png)\n\nwhere `a` and `b` are odd numbers and `c` is the capping limit of the hash range. This hash function hashes the elements uniformly into a hash range of size `c`.\n\n## The procedure\n\nThe procedure of the Flajolet-Martin algorithm is as elegant as its intuition. We start with defining a closed hash range, big enough to hold the maximum number of unique values possible - something as big as `2 ^ 64`. Every element of the stream is passed through a hash function that permutes the elements in a uniform distribution.\n\nFor this hash value, we find the position of the rightmost set bit and mark the corresponding position in the bit vector as `1`, suggesting that we have seen the position. Once all the elements are processed, the bit vector will have `1`s at all the positions corresponding to the position of every rightmost set bit for all elements in the stream.\n\nNow we find the position, `b`, of the rightmost `0` in this bit vector. This position `b` corresponds to the rightmost set bit that we have not seen while processing the elements. This corresponds to the probability `0` and hence as per the intuition will help in approximating the cardinality as `2 ^ b`.\n\n```python\n# Size of the bit vector\nL = 64\n\ndef hash_fn(x: int):\n    return (3 * x + 5) % (2 ** L)\n\ndef cardinality_fm(stream) -> int:\n    # we initialize the bit vector\n    vector = 0\n\n    # for every element in the stream\n    for x in skream:\n        \n        # compute the hash value bounded by (2 ** L)\n        # this hash value will ensure uniform distribution\n        # of elements of the stream in range [0, 2 ** L)\n        y = hash_fn(x)\n\n        # find the rightmost set bit\n        k = get_rightmost_set_bit(y)\n\n        # set the corresponding bit in the bit vector\n        vector = set_bit(vector, k)\n\n    # find the rightmost unset bit in the bit vector that\n    # suggests that the probability being 0\n    b = rightmost_unset_bit(vector)\n\n    # return the approximate cardinality\n    return 2 ** b\n```\n\nAlthough the above algorithm does a decent job of approximating count-distinct it has a huge error margin, which can be fixed by averaging the approximations with multiple hash functions. The original Flajolet-Martin algorithm also suggests that the final approximation needs a correction by dividing the approximation by the factor `\u03d5 = 0.77351`.\n\nThe algorithm was [run](https://github.com/arpitbbhayani/flajolet-martin/blob/master/flajolet-martin.ipynb) on a stream size of `1048` with a varying number of distinct elements and we get the following plot.\n\n![https://user-images.githubusercontent.com/4745789/101244923-58ef4b00-372f-11eb-9193-8e9d6dc6a227.png](https://user-images.githubusercontent.com/4745789/101244923-58ef4b00-372f-11eb-9193-8e9d6dc6a227.png)\n\nFrom the illustration above we see that the approximated count-distinct using the Flajolet-Martin algorithm is very close to the actual deterministic value.\n\nA great feature of this algorithm is that the result of this approximation will be the same whether the elements appear a million times or just a few times, as we only consider the rightmost set bit across all elements and do not sample.\n\n### Unique words in Thee Jungle Book\n\nThe algorithm was run on the text dump of [The Jungle Book](https://en.wikipedia.org/wiki/The_Jungle_Book) by Rudyard Kipling. The text was converted into a stream of tokens and it was found that the total number of unique tokens was `7150`. The approximation of the same using the Flajolet-Martin algorithm came out to be `7606` which in fact is pretty close to the actual number.\n\n# References\n\n- [Probabilistic Counting Algorithms for Data Base Applications](http://algo.inria.fr/flajolet/Publications/FlMa85.pdf)\n- [Flajolet-Martin algorithm by Ravi Bhide](http://ravi-bhide.blogspot.com/2011/04/flajolet-martin-algorithm.html)\n- [The Jungle Book by Rudyard Kipling - Project Gutenberg](https://www.gutenberg.org/files/236/236-h/236-h.htm)\n",
    "similar": [
      "slowsort",
      "1d-terrain",
      "isolation-forest",
      "morris-counter"
    ]
  },
  {
    "id": 33,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "2q-cache",
    "title": "2Q Cache Management Algorithm",
    "description": "LRU is one of the most widely used cache eviction algorithms suffers from a bunch of limitations especially when used for managing caches in disk-backed databases. 2Q remediates the limitations and improves upon it by adding multiple parallel buffers.",
    "gif": "https://media.giphy.com/media/cfuL5gqFDreXxkWQ4o/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/100538603-bf1f2e00-3256-11eb-87dc-32594b25ae30.png",
    "released_at": "2020-11-29",
    "total_views": 1705,
    "body": "LRU is one of the most widely used cache eviction algorithms that span its utility across multiple database systems. Although popular, it suffers from a bunch of limitations especially when it is used for managing caches in disk-backed databases like MySQL and Postgres.\n\nIn this essay, we take a detailed look into the sub-optimality of LRU and how one of its variants called 2Q addresses and improves upon it. 2Q algorithm was first introduced in the paper - [2Q: A low overhead high-performance buffer management replacement algorithm](https://www.semanticscholar.org/paper/2Q%3A-A-Low-Overhead-High-Performance-Buffer-Johnson-Shasha/5fa357b43c8351a5d8e7124429e538ad7d687abc) by Theodore Johnson and Dennis Shasha.\n\n# LRU\n\nThe [LRU eviction algorithm](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)) evicts the page from the buffer which has not been accessed for the longest. LRU is typically implemented using a [Doubly Linked List](https://en.wikipedia.org/wiki/Doubly_linked_list) and a [Hash Table](https://en.wikipedia.org/wiki/Hash_table). The intuition of this algorithm is so strong and implementation is so simple that until the early '80s, LRU was the algorithm of choice in nearly all the systems. But as stated above, there are certain situations where LRU performs sub-optimal.\n\n![https://user-images.githubusercontent.com/4745789/100534745-43ae8400-3238-11eb-8855-752a6ef2f3c6.png](https://user-images.githubusercontent.com/4745789/100534745-43ae8400-3238-11eb-8855-752a6ef2f3c6.png)\n\n## Sub-optimality during DB scans\n\nIf the database table is bigger than the LRU cache, the DB process, upon scanning the table will wipe out the entire LRU cache and fill it with the pages from just one scanned table. If these pages are not referenced again, this is a total loss and the performance of the database takes a massive hit. The performance will pickup once these pages are evicted from the cache and other pages make an entry.\n\n## Sub-optimality in evictions\n\nLRU algorithm works with a single dimension - recency - as it removes the pages from the buffer on the basis of recent accesses. Since it does not really consider any other factor, it can actually evict a warmer page and replace it with a colder one - a page that could and would be accessed just once.\n\n# 2Q Algorithm\n\n2Q addresses the above-illustrated issues by introducing parallel buffers and supporting queues. Instead of considering just recency as a factor, 2Q also considers access frequency while making the decision to ensure the page that is really warm gets a place in the LRU cache. It admits only hot pages to the main buffer and tests every page for a second reference.\n\nThe golden rule that 2Q is based on is - *Just because a page is accessed once does not entitle it to stay in the buffer. Instead, it should be decided if it is accessed again then only keep it in the buffer.*\n\nBelow we take a detailed look into two versions of the 2Q algorithm - simplified and improved.\n\n## Simplified 2Q\n\nSimplified 2Q algorithm works with two buffers: the primary LRU buffer - `Am`  and a secondary FIFO buffer - `A1`. New faulted pages first go to the secondary buffer `A1` and then when the page is referenced again, it moves to the primary LRU buffer `Am`. This ensures that the page that moves to the primary LRU buffer is hot and indeed requires to be cached.\n\n![https://user-images.githubusercontent.com/4745789/100536835-41a0f100-3249-11eb-920b-0bcaff905906.png](https://user-images.githubusercontent.com/4745789/100536835-41a0f100-3249-11eb-920b-0bcaff905906.png)\n\nIf the page residing in `A1` is never referenced again, it eventually gets discarded, implying the page was indeed cold and did not deserve to be cached. Thus this simplified 2Q provides protection against the two listed sub-optimality of the simple LRU scheme by adding a secondary buffer and testing pages for a second reference. The pseudocode for the Simplified 2Q algorithm is as follows:\n\n```python\ndef access_page(X: page):\n    # if the page already exists in the LRU cache\n    # in buffer Am\n    if X in Am:\n         Am.move_front(X)\n\n    # if the page exists in secondary storage\n    # and not it gets access.\n    # since the page is accessed again, indicating interest\n    # and long-term need, move it to Am.\n    elif X in A1:\n         A1.remove(X)\n         Am.add_front(X)\n\n    # page X is accessed for the first time\n    else:\n         # if A1 is full then free a slot.\n         if A1.is_full():\n             A1.pop()\n\n         # add X to the front of the FIFO A1 queue\n         A1.add_front(X)\n```\n\nTuning Simplified 2Q buffer is difficult - if the maximum size of `A1` is too small, the test for hotness becomes too strong and if it is too large then due to memory constraint `Am` will get relatively smaller memory making the primary LRU cache smaller, eventually degrading the database performance.\n\nThe full version 2Q algorithm remediates this limitation and eliminates tuning to a massive extent without taking any hit in performance.\n\n## 2Q Full Version\n\nAlthough Simplified 2Q algorithm does a decent job there is still scope of improvement when it comes to handling common database access pattern, that suggests, a page generally receives a lot of references for a short period of time and then no reference for a long time. If a page truly needs to be cached then after it receives a lot (not just one) of references in a short span it continues to receive references and hits on regular intervals.\n\nTo handle this common database access pattern, the 2Q algorithm splits the secondary buffer `A1` into two buffers `A1-In` and `A1-Out`, where the new element always enters `A1-In` and continues to stay in `A1-In` till it gets accesses ensuring that the most recent first accesses happen in the memory.\n\nOnce the page gets old, it gets thrown off the memory but its disk reference is stored in the `A1-Out` buffer. If the page, whose reference is, residing in `A1-Out` is accessed again the page is promoted to `Am` LRU implying it indeed is a hot page that will be accessed again and hence required to be cached.\n\n![https://user-images.githubusercontent.com/4745789/100538168-0bb53a00-3254-11eb-8f69-ddcaf8d33a84.png](https://user-images.githubusercontent.com/4745789/100538168-0bb53a00-3254-11eb-8f69-ddcaf8d33a84.png)\n\nThe `Am` buffer continues to be the usual LRU which means when any page residing in `Am` is accessed it is moved to the head and when a page is needed to be discarded the eviction happens from the tail end.\n\n# 2Q in Postgres\n\nPostgres uses 2Q as its cache management algorithm due to [patent issues](http://www.varlena.com/GeneralBits/96.php) with IBM. Postgres used to have [ARC](https://en.wikipedia.org/wiki/Adaptive_replacement_cache) as its caching algorithm but with IBM getting a patent over it, Postgres moved to 2Q. Postgres also claims that the performance of 2Q is similar to ARC.\n\n# References\n\n- [LRU - Wikipedia](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU))\n- [The Saga of the ARC Algorithm and Patent](http://www.varlena.com/GeneralBits/96.php)\n- [2Q: A low overhead high-performance buffer management replacement algorithm](https://www.semanticscholar.org/paper/2Q%3A-A-Low-Overhead-High-Performance-Buffer-Johnson-Shasha/5fa357b43c8351a5d8e7124429e538ad7d687abc)\n",
    "similar": [
      "mysql-cache",
      "israeli-queues",
      "ts-smoothing",
      "phi-accrual"
    ]
  },
  {
    "id": 34,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "israeli-queues",
    "title": "Israeli Queues",
    "description": "Israeli Queues are fondly named after a peculiar behavior observed in Israel. This behavior was mimicked to solve a very particular problem in Polling Systems. In this essay, we find what makes the Israeli Queue different than the traditional FIFO Queue and how it efficiently addresses the problem at hand.",
    "gif": "https://media.giphy.com/media/3orif9DJNKDfXqXbBS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/99896424-64b22a80-2cb6-11eb-92b2-ed1f44a76ba1.png",
    "released_at": "2020-11-22",
    "total_views": 7250,
    "body": "A queue is a data structure that holds up elements for a brief period of time until a peripheral processing system is ready to process them. The most common implementation of a queue is a FIFO queue - First In First Out - that evicts the element that was inserted the first i.e. it evicts the one that has spent the most time in the queue. There are other variations of Queues one of which is called Priority Queue.\n\nIn Priority Queue, every element is associated with a priority, usually provided by the user during enqueueing; This associated priority is used during eviction where the element with the highest priority is evicted first during dequeuing.\n\nIn this essay, we take a detailed look into a variation of Priority Queue, fondly called Israeli Queues, where the priority of the element is defined by the affinity of it with one of its \"friends\" in the queue. Israeli Queues were first introduced in the paper [Polling with batch service](https://pure.tue.nl/ws/files/2152975/632939.pdf) by Boxma, O. J., Wal, van der, J., & Yechiali, U in the year 2007.\n\n# Israeli Queues\n\nQueues in Israel are usually unorganized, due to which people tend to find their friends, who are already waiting, and instead of adhering to the usual protocol of joining at the back end, they cut through and directly join their friends. Israeli Queues mimic this behavior and hence get this [punny name](https://www.tandfonline.com/doi/abs/10.1080/15326340802427497).\n\n![https://user-images.githubusercontent.com/4745789/99894937-fddc4380-2cac-11eb-8a73-a4dc5c490d2b.png](https://user-images.githubusercontent.com/4745789/99894937-fddc4380-2cac-11eb-8a73-a4dc5c490d2b.png)\n\nIsraeli Queues are a variation of [Priority Queues](https://en.wikipedia.org/wiki/Priority_queue) where instead of associating priority with the element to be enqueued, the priority is implicitly derived using the \"friend\" element and it joins right at the back end of the group that the friend belongs to. The function signature of the enqueue operation is as shown below, while other operations like `dequeue` and `peek` remains fairly similar.\n\n```c\n// Enqueues the element `e`, a friend of element `f`,\n// into the queue `q`.\nvoid enqueue(israeli_queue * q, element * e, element * f);\n```\n\n## How could this help?\n\nEvery Data Structures is designed to solve a niche use case efficiently and Israeli Queues are no different as they prove to be super-efficient where one could batch and process similar elements or where the *set-up* cost for a task is high.\n\nConsider a system where a queue is used to hold up heterogeneous tasks and there is a single machine taking care of processing. Now if some of these tasks are similar and have a high *set-up or preparation cost*, for example downloading large metafiles, or spinning up a parallel infrastructure, or even setting up persistent connections with device farms, queuing them closer and processing them sequentially or in batch helps in reducing redundant processing and computation by promoting reuse.\n\n## Issue of starvation\n\nBy enqueuing elements in between Israeli Queues reduces redundant processing, but by doing that it makes itself vulnerable to the classical case of starvation. Elements stuck at the rear end of the list could potentially starve for longer durations if elements having \"friends\" in the queue keep coming in at high frequency.\n\nThe original implementation of Israeli Queues suggests batch processing where instead of processing tasks one at a time, it processes a batch (a group of friends) in one go. This proves to be super-handy when the time required to processes a single task is much lower than the set-up cost for it.\n\n## Implementation Guidelines\n\nThe best way to implement Israeli Queues is by using a [Doubly Linked List](https://en.wikipedia.org/wiki/Doubly_linked_list) with a bunch of pointers pointing to the head and tail of groups within it. Insertion to an existing group happens at the tail of it while if the element has no friend element, then it goes at the tail end of the list and forms its own group.\n\nA constraint that could be added during implementation is that the friend element should always be the leader (head) element of the group. Details of the implementation could be tweaked so long the core concept remains unaltered.\n\n# The original use case of Israeli Queues\n\nIsraeli Queues were the outcome of a problem statement dealing with Polling Systems. Polling System usually contains `N` queues `Q1`, `Q2`, ..., `Qn` where the processing unit visits each queue in cyclic order processing one element at a time i.e. `Q1`, `Q2`, ..., `Qn`, `Q1`, `Q2`, ..., `Qn`, etc.\n\nWhen the server attends a queue instead of processing just one element from it, it processes the entire batch present in the queue utilizing the setup-cost efficiently assuming that time to process an element from a queue is much lesser than the set-up cost.\n\n# References\n\n- [Polling with batch service](https://pure.tue.nl/ws/files/2152975/632939.pdf)\n- [The Israeli Queue with priorities](http://www.math.tau.ac.il/~uriy/Papers/IQ-with-Priorities.pdf)\n- [Israeli Queues: Exploring a bizarre data structure](https://rapidapi.com/blog/israeli-queues-exploring-a-bizarre-data-structure/)\n",
    "similar": [
      "2q-cache",
      "phi-accrual",
      "morris-counter",
      "copy-on-write"
    ]
  },
  {
    "id": 35,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "1d-terrain",
    "title": "1D Procedural Terrain Generation",
    "description": "Terrains are at the heart of every Computer Game - be it Counter-Strike, Age of Empires, or even Minecraft. The virtual world that these games generate is the key to a great gaming experience. Generating terrain, manually, requires a ton of effort and hence it makes sense to auto-generate a pseudorandom terrain using some procedure. In this essay, we take a detailed look into generating pseudorandom one-dimensional terrain that is very close to real ones.",
    "gif": "https://media.giphy.com/media/xT5LMTf2CRdBhetWPC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/99261379-a9ffd380-2842-11eb-8f50-36b36ed42a95.png",
    "released_at": "2020-11-16",
    "total_views": 1062,
    "body": "Terrains are at the heart of every Computer Game - be it Counter-Strike, Age of Empires, or even Minecraft. The virtual world that these games generate is the key to a great gaming experience. Generating terrain, manually, requires a ton of effort and hence it makes sense to auto-generate a pseudorandom terrain using some procedure. In this essay, we take a detailed look into generating pseudorandom one-dimensional terrain that is very close to real ones.\n\n# 1D Terrain\n\nA one-dimensional terrain is a bunch of heights defined at every point along the X-axis. An example of this could be seen in games like Flappy Bird, Galaxy, and many more. Such terrain could also be traced as the skylines of mountain ranges. \n\n![https://user-images.githubusercontent.com/4745789/99189082-70b55e00-2785-11eb-87c0-6d95568f709a.png](https://user-images.githubusercontent.com/4745789/99189082-70b55e00-2785-11eb-87c0-6d95568f709a.png)\n\nThe illustration above shows a one-dimensional terrain and is taken as a sketch of a distant mountain range. The procedure we define should generate terrains as close to natural ones as possible.\n\n# Generating terrain using `random` values\n\nA naive solution to generate a random terrain is by using the `random` function for each point along the X-axis. The `random` function yields random values in the interval `[0, 1]`, and we map these values to the required height, for example `[0, 100]`.  The 1D terrain generation algorithm using the default random function is thus defined as\n\n```python\nimport random\n\ndef mapv(v, ol, oh, nl, nh):\n    \"\"\"maps the value `v` from old range [ol, oh] to new range [nl, nh]\n    \"\"\"\n    return nl + (v * ((nh - nl) / (oh - ol)))\n\ndef terrain_naive(count) -> List[float]:\n    \"\"\"returns the list of integers representing height at each point.\n    \"\"\"\n    return [\n        mapv(random.random(), 0, 1, 0, 100)\n        for i in range(count)\n    ]\n```\n\nThe `mapv` function defined above comes in very handy as it helps us to map value from a source range to a target range. The terrain generation function `terrain_naive` takes in `count` as input, suggesting the number of points along the X-axis, and it returns a list of float values representing height at each of the `count` number of points along the terrain.\n\n![https://user-images.githubusercontent.com/4745789/99189731-9db74000-2788-11eb-98d3-0138a7378d77.png](https://user-images.githubusercontent.com/4745789/99189731-9db74000-2788-11eb-98d3-0138a7378d77.png)\n\nThe above illustration shows the plot of the one-dimensional terrain using the above `terrain_naive` function. The terrain generated using this procedure has a lot of spikes and abrupt changes in height and it clearly does not mimic the terrains in the real world. Real-world terrains, although random, does not have a lot of sharp spikes, instead, the changes in height are very gradual and ensure some degree of smoothness. \n\nIn order to bring smoothness to `terrain_naive` generated terrains, we take a look at a famous estimation technique called [interpolation](https://en.wikipedia.org/wiki/Interpolation) which estimates intermediate values given a bunch of known points. \n\n# Interpolation\n\nInterpolation is a method of constructing new data points within a range of a discrete set of known data points. Interpolation methods estimate the intermediate data points ensuring a \"smoothened\" transition from one known point to another. There are several interpolation methods but we restrict our focus to [Linear](https://en.wikipedia.org/wiki/Linear_interpolation) and [Cosine](https://en.wikipedia.org/wiki/Trigonometric_interpolation) interpolation methods.\n\n### Linear Interpolation\n\nLinear interpolation estimates the intermediate points between the known points assuming collinearity. Thus given two known points `a` and `b`, using linear interpolation, we estimate an intermediate point `c` at a relative distance of `mu` from `a` using the function defined below\n\n```python\ndef linp(a, b, mu):\n    \"\"\"returns the intermediate point between `a` and `b`\n    which is `mu` factor away from `a`.\n    \"\"\"\n    return a * (1 - mu) + b * mu\n```\n\nThe value of the parameter `mu` ranges in the interval `[0, 1]` where `0` implies the point being estimated and interpolated is at `a` while `1` implies it is at the second point `b`.\n\n![https://user-images.githubusercontent.com/4745789/99184683-d0ead680-276a-11eb-9b4d-6c78dbf3c197.png](https://user-images.githubusercontent.com/4745789/99184683-d0ead680-276a-11eb-9b4d-6c78dbf3c197.png)\n\n### Cosine Interpolation\n\nLinear interpolation is not always desirable as the plot sees a lot of discontinuous and sharp transitions. When the plot is expected to be smoother, it is where the Cosine Interpolation comes in handy. Instead of assuming intermediate points are collinear with the known ones, Cosine Interpolation, plots them on a cosine curve passing through the known points, providing a much smoother transition. This is could be seen in the illustration above.\n\n```python\nimport math\n\ndef cosp(a, b, mu):\n    \"\"\"returns the intermediate point between `a` and `b`\n    which is `mu` factor away from `a`.\n    \"\"\"\n    mu2 = (1 - math.cos(mu * math.pi)) / 2\n    return a * (1 - mu2) + b * mu2\n```\n\nThe above code snippet computes and estimates intermediate point `c` at a relative distance of `mu` from the first point `a` using Cosine Interpolation. Note there are other interpolation methods, but we can solve all major of our use cases using these two.\n\n# Smoothing via Interpolation\n\nWe can apply interpolation to our naively generated terrain and make transitions smoother leading to fewer spikes. In the real-world, the transitions in the terrain are gradual with peaks being widespread; we can mimic the pattern by sampling `k` points from the naive terrain which could become our desired peaks, and interpolate the rest of the points lying between them.\n\nThis should ideally help us reduce the sudden spikes and make the terrain look much closer to really like. A simple python code that outputs linearly interpolated terrain that samples every `sample` points from the naive one is as follows\n\n```python\ndef terrain_linp(naive_terrain, sample=4) -> List[float]:\n    \"\"\"Using naive terrain `naive_terrain` the function generates\n    Linearly Interpolated terrain on sample data.\n    \"\"\"\n    terrain = []\n\n    # get every `sample point from the naive terrain.\n    sample_points = naive_terrain[::sample]\n\n    # for every point in sample point denoting \n    for i in range(len(sample_points)):\n\n        # add current peak (sample point) to terrain.\n        terrain.append(sample_points[i])\n\n        # fill in `sample - 1` number of intermediary points using\n        # linear interpolation.\n        for j in range(sample - 1):\n            # compute relative distance from the left point\n            mu = (j + 1)/sample\n          \n            # compute interpolated point at relative distance of mu\n            a = sample_points[i]\n            b = sample_points[(i + 1) % len(sample_points)]\n            v = linp(a, b, mu)\n\n            # add an interpolated point to the terrain terrain.append(v)\n\n    # return the terrain\n    return terrain\n```\n\nThe above code snippet generates intermediate points using Linear Interpolation but we can very easily change the interpolation function to Cosine Interpolation and see the effect in action. The sampling and interpolating on a naively generated terrain for different values of sample is as shown below\n\n![https://user-images.githubusercontent.com/4745789/99227920-e7983880-2811-11eb-9c79-bed5cad2eed3.png](https://user-images.githubusercontent.com/4745789/99227920-e7983880-2811-11eb-9c79-bed5cad2eed3.png)\n\nFor each interpolation, the 5 plots shown above are sampled for every `1`, `2`, `3`, `4`, and `5` points respectively. We can clearly see the plots of Cosine Interpolation are much smoother than Linear Interpolated ones. The technique does a good job over naive implementation but it still does not mimic what we see in the real world. To make things as close to the real-world as possible we use the concept of [Superposition](https://en.wikipedia.org/wiki/Superposition_principle).\n\n# Superposition Sampled Terrains\n\nSampling and Interpolation are effective in reducing the spikes and making transitions gradual. The concern with this approach is that sudden changes are not really gone. In order to address this situation, we use the principle of Superposition upon multiple such sampled terrains.\n\nThe approach we take here is to generate `k` such terrains with different sampling frequencies and then perform a normalized weighted sum. This way we get the best of both worlds i.e smoothness from the terrain with the least sampled points and aberrations from the one with the most sampled points.\n\nNow the only piece that remains is choosing the weights. The weights and sampling frequency varies by the power of `2`. The number of terrains to be sampled depends on the kind of terrain needed and is to be left for experimentation, but we can assume it to be 6 for most use cases.\n\nThe terrain that samples all the points should be given the least weight as it aims to contribute sudden spikes; and as we increase the sampling frequency by the power of `2` we increase the weight by the power of `2` as well. Hence for the first terrain, the scale is `0.03125` where were sample all the points `256`, the next terrain is sampled with `128` points and has a scale of `0.0625`, and so on till we reach sampled points to be `8` with scale as `1`, giving it the highest weight.\n\nOnce these terrains are generated we perform a normalized weighted sum and generate the final terrain as shown in the illustration below.\n\n![https://user-images.githubusercontent.com/4745789/99264852-0107a780-2847-11eb-9bd1-2e2994c56028.png](https://user-images.githubusercontent.com/4745789/99264852-0107a780-2847-11eb-9bd1-2e2994c56028.png)\n\nThe illustration above shows 6 scaled sampled terrains with different sampling frequencies along with the final super-positioned terrain generated from the procedure. It is very clear that the terrain generated using this procedure is much more closer to the real world terrain than any other we have seen before. Python code that generated the above terrain is as below\n\n```python\ndef terrain_superpos_linp(naive_terrain, iterations=8) -> List[float]:\n    \"\"\"Using naive terrain `naive_terrain` the function generates\n    Linearly Interpolated Superpositioned terrain that looks real world like.\n    \"\"\"\n    terrains = []\n\n    # holds the sum of weights for normalization\n    weight_sum = 0\n\n    # for every iteration\n    for z in range(iterations, 0, -1):\n        terrain = []\n\n        # compute the scaling factor (weight)\n        weight = 1 / (2 ** (z - 1))\n\n        # compute sampling frequency suggesting every `sample`th\n        # point to be picked from the naive terrain.\n        sample = 1 << (iterations - z)\n\n        # get the sample points\n        sample_points = naive_terrain[::sample]\n        \n        weight_sum += weight\n\n        for i in range(len(sample_points)):\n\n            # append the current sample point (scaled) to the terrain\n            terrain.append(weight * sample_points[i])\n\n            # perform interpolation and add all interpolated values to\n            # to the terrain.\n            for j in range(sample - 1):\n                # compute relative distance from the left point\n                mu = (j + 1) / sample\n\n                # compute interpolated point at relative distance of mu\n                a = sample_points[i]\n                b = sample_points[(i + 1) % len(sample_points)]\n                v = linp(a, b, mu)\n\n                # add interpolated point (scaled) to the terrain\n                terrain.append(weight * v)\n\n        # append this terrain to list of terrains preparing\n        # it to be superpositioned.\n        terrains.append(terrain)\n\n    # perform super position and normalization of terrains to\n    # get the final terrain\n    return [sum(x)/weight_sum for x in zip(*terrains)]\n```\n\nIf the terrain to be generated is needed to be smoother then instead of using Linear Interpolation switch to Cosine Interpolation and the resultant terrain will be much smoother and curvier as seen in the illustration below.\n\n![https://user-images.githubusercontent.com/4745789/99264869-06fd8880-2847-11eb-83d7-80d0ab5509da.png](https://user-images.githubusercontent.com/4745789/99264869-06fd8880-2847-11eb-83d7-80d0ab5509da.png)\n\n> This approach is very similar to [Perlin Noise](https://en.wikipedia.org/wiki/Perlin_noise) that is used for generating multi-dimensional terrains. [Ken Perlin](https://en.wikipedia.org/wiki/Ken_Perlin) was awarded an Academy Award for Technical Achievement for creating the algorithm.\n\n# References\n\n- [Superposition](https://en.wikipedia.org/wiki/Superposition_principle)\n- [Interpolation Methods](http://paulbourke.net/miscellaneous/interpolation/)\n- [One Lone Coder - Perlin-like Noise](https://github.com/OneLoneCoder/videos/blob/master/OneLoneCoder_PerlinNoise.cpp)\n- [IPython notebook with the source code](https://github.com/arpitbbhayani/1d-terrain/)",
    "similar": [
      "slowsort",
      "flajolet-martin",
      "jaccard-minhash",
      "idf"
    ]
  },
  {
    "id": 36,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "jaccard-minhash",
    "title": "Set Similarity using Jaccard Similarity Coefficient and MinHash",
    "description": "Set similarity measure finds its application spanning the Computer Science spectrum; some applications being - user segmentation, finding near-duplicate webpages/documents, clustering, recommendation generation, sequence alignment, and many more. In this essay, we take a detailed look into a set-similarity measure called - Jaccard's Similarity Coefficient and how its computation can be optimized using a neat technique called MinHash.",
    "gif": "https://media.giphy.com/media/yvXWADxQxRMkQ4eEID/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/98465225-c4580280-21ed-11eb-9f1f-6508ed229925.png",
    "released_at": "2020-11-08",
    "total_views": 1481,
    "body": "Set similarity measure finds its application spanning the Computer Science spectrum; some applications being - user segmentation, finding near-duplicate webpages/documents, clustering, recommendation generation, sequence alignment, and many more. In this essay, we take a detailed look into a set-similarity measure called - Jaccard's Similarity Coefficient and how its computation can be optimized using a neat technique called MinHash.\n\n# Jaccard Similarity Coefficient\n\nJaccard Similarity Coefficient quantifies how similar two *finite* sets really are and is defined as the size of their intersection divided by the size of their union. This similarity measure is very intuitive and we can clearly see that it is a real-valued measure bounded in the interval `[0, 1]`.\n\n![https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png](https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png)\n\nThe coefficient is `0` when the two sets are mutually exclusive (disjoint) and it is `1` when the sets are equal. Below we see the one-line python function that computes this similarity measure.\n\n```python\ndef similarity_jaccard(a: set, b: set) -> float:\n    return len(a.intersection(b)) / len(a.union(b))\n```\n\n## Jaccard Similarity Coefficient as Probability\n\nJaccard Coefficient can also be interpreted as the probability that an element picked at random from the universal set `U` is present in both sets `A` and `B`. \n\n![https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png](https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png)\n\nAnother analogy for this probability is the chances of throwing a dart and it hitting the intersection. Thus we see how we can transform the Jaccard Similarity Coefficient into a simple probability statement. This will come in very handy when we try to optimize the computation at scale.\n\n## Problem at Scale\n\nComputing Jaccard Similarity Coefficient is very simple, all we require is a union operation and an intersection operation on the participating sets. But these computations go haywire when things run at scale.\n\nComputing set similarity is usually a subproblem fitting in a bigger picture, for example, near-duplicate detection which finds near-duplicate articles across millions of documents. When we tokenize the documents and apply raw Jaccard Similarity Coefficient for every two combinations of documents we find that the computation will take [years](https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/).\n\nInstead of finding the true value for this coefficient, we can rely on an approximation if we can get a considerable speedup and this is where a technique called MinHash fits well.\n\n# MinHash\n\nMinHash algorithm gives us a fast approximation to the Jaccard Similarity Coefficient between any two finite sets. Instead of computing the unions and the intersections every single time, this method once creates *MinHash Signature* for each set and use it to approximate the coefficient.\n\n## Computing single MinHash\n\nMinHash `h` of the set `S` is the index of the first element, from a permuted Universal Set, that is present in the set `S`. But since permutation is a computation heavy operation especially for large sets we use a hashing/mapping function that typically reorders the elements using simple math operation. One such hashing function is\n\n![https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png](https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png)\n\nIf `u` is the total number of elements in the Universal Set `U` then `a` and `b` are the random integers less than `u` and `c` is the prime number slightly higher than `u`.  A sample permute function could be\n\n```python\ndef permute_fn(x: int) -> int:\n    return (23 * x + 67) % 199\n```\n\nNow that we have defined permutation as a simple mathematical operation that spits out the new row index, we can find MinHash of a set as the element that has the minimum new row number. Hence we can define the MinHash function as \n\n```python\ndef minhash(s: set) -> int:\n    return min([permute_fn(e) for e in s])\n```\n\n## A surprising property of MinHash\n\nMinHash has a surprising property, according to which, the probability that the MinHash of random permutation produces the same value for the two sets equals the Jaccard Similarity Coefficient of those sets.\n\n![https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png](https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png)\n\nThe above equality holds true because the probability of MinHash of two sets to be the same is the number of elements present in both the sets divided by the total number of elements in both the sets combined; which in fact is the definition of Jaccard Similarity Coefficient.\n\nHence to approximate Similarity Coefficient using MinHash all we have to do is find the Probability of MinHash of two sets to be the same, and this is where the MinHash Signature comes in to play.\n\n## MinHash Signature\n\nMinHash Signature of a set `S` is a collection of `k` MinHash values corresponding to `k` different MinHash functions. The size `k` depends on the error tolerance, keeping it higher leads to more accurate approximations.\n\n```python\ndef minhash_signature(s: set):\n    return [minhash(s) for minhash in minhash_fns]\n```\n\n> MinHash functions usually differ in the permutation parameters i.e. coefficients `a`, `b` and `c`.\n\nNow in order to compute `Pr[h(A) = h(B)]` we have to compare the MinHash Signature of the participating sets `A` and `B` and find how many values in their signatures match; dividing this number by the number of hash functions `k` will give the required probability and in turn an approximation of Jaccard Similarity Coefficient.\n\n```python\ndef similarity_minhash(a: set, b: set) -> float:\n    sign_a = minhash_signature(a)\n    sign_b = minhash_signature(b)\n    return sum([1 for a, b in zip(sign_a, sign_b) if a == b]) / len(sign_a)\n```\n\n> MinHash Signature could well be computed just once per set.\n\nThus to compute set similarity, we need not perform heavy computation like Union and Intersection and that too across millions of sets at scale, rather we can simply compare `k` items of in their signatures and get a fairly good estimate of it.\n\n# How good is the estimate?\n\nIn order to find how close the estimate is we compute the Jaccard Similarity Coefficient and its approximate using MinHash on two disjoint sets having equal cardinality. One of the sets will undergo a transition where one element of it will be replaced with one element of the other set. So with time, the sets will go from disjoint to being equal.\n\n![https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png](https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png)\n\nThe illustration above shows the two plots and we can clearly see that the MinHash technique provides a fairly good estimate of Jaccard Similarity Coefficient with much fewer computations.\n\n# References\n\n- [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index)\n- [MinHash Wikipedia](https://en.wikipedia.org/wiki/MinHash)\n- [Using of Jaccard Coefficient for Keywords Similarity](https://www.researchgate.net/profile/Ekkachai_Naenudorn/publication/317248581_Using_of_Jaccard_Coefficient_for_Keywords_Similarity/links/592e560ba6fdcc89e759c6d0/Using-of-Jaccard-Coefficient-for-Keywords-Similarity.pdf)\n- [MinHash Tutorial with Python Code](https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/)\n",
    "similar": [
      "idf",
      "constant-folding-python",
      "publish-python-package-on-pypi",
      "rule-30"
    ]
  },
  {
    "id": 37,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "ts-smoothing",
    "title": "Time Series Smoothing - Making Aberrations Stand Out",
    "description": "Time Series smoothing algorithms removes short-term irregularities from the plot while preserving long-term trends. But as an observer, it is important that such smoothing techniques or irregularities do not mask anomalies that need attention. In this essay, we take a look at a smoothing algorithm that smooths out a time series plot while making aberrations and anomalies standout.",
    "gif": "https://media.giphy.com/media/3orieVe5VYqTdT16qk/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/97785060-cbb55580-1bc8-11eb-9d6d-91ff16cc2ddc.png",
    "released_at": "2020-11-01",
    "total_views": 499,
    "body": "Time series is a collection of numerical data points (often measurements), gathered in discrete time intervals, and indexed in order of the time. Common examples of time series data are CPU utilization metrics, Temperature of some geolocation, New User Signups of a product, etc.\n\nObserving time-series of critical metrics helps in spotting trends, aberrations, and anomalies. Time series forecasting helps in predicting future demand and thus aids in altering and adjusting the supply to match that. Software companies continuously monitor hundreds of time series plots for anomalies that, if unattended, could result in downtime or a loss in revenue.\n\nUnfortunately, time series data have a lot of short-term irregularities, often making it harder for the observer to spot the sudden spikes and true anomalies; and which is where the need for *smoothing* arises. By smoothing the plot we get rid of the irregularities, to some extent, while enabling the observer to clearly see the patterns, trends, and anomalies.\n\nIn this essay, we take a detailed look into how we can optimally smooth the time series data to prioritize the user's attention i.e. making it easier for the observer to spot the aberrations. The approach we discuss was introduced in the paper [ASAP: Automatic Smoothing for Attention Prioritization in Streaming Time Series Visualization](https://arxiv.org/abs/1703.00983) by Kexin Rong, Peter Bailis.\n\n# Time Series and need of Smoothing\n\nTime Series, more often than not, is very irregular in nature. Below is the plot of India's Average Temperature - Monthly since 1870. We can clearly see the plot being very irregular making it harder for us to deduce any information out of it whatsoever. Probably the only fact we can point out is that the Monthly Average temperature in India is always between 15 - 30 degrees celsius, which everyone can agree, is not that informative enough.\n\n![https://user-images.githubusercontent.com/4745789/94363195-3cef7d80-00de-11eb-9280-cf0ab83f2230.png](https://user-images.githubusercontent.com/4745789/94363195-3cef7d80-00de-11eb-9280-cf0ab83f2230.png)\n\nIn order to make sense of such an irregular plot and find a pattern or a trend out of it, we have to get rid of **short-term irregularities** without substantial information loss; and this process is called \"smoothing\". Aggregation doesn't work well here because it will not only hide the anomaly but will also reduce the data density making the resultant plot sparse; hence in order to spot anomalies and see long-term trends smoothing is preferred.\n\nIf we smooth the above raw plot using one of the simplest techniques out there, we get the following plot which, everyone would agree, not only looks cleaner but it also clearly shows us the long-term trend while being rich in information.\n\n![https://user-images.githubusercontent.com/4745789/94363189-32cd7f00-00de-11eb-9012-773b42105020.png](https://user-images.githubusercontent.com/4745789/94363189-32cd7f00-00de-11eb-9012-773b42105020.png)\n\n# Time Series Smoothing using Moving Average\n\nThe technique we used to smooth the temperature plot is known as [Simple Moving Average (SMA)](https://en.wikipedia.org/wiki/Moving_average) and it is the simplest, most effective, and one of the most popular smoothing techniques for time series data. Moving Average, very instinctively, smooths out short-term irregularities and highlights longer-term trends and patterns. Computing it is also very simple - each point in the smoothened plot is just an unweighted mean of the data points lying in the sliding window of length `n`. Because of the Sliding Window, SMA ensures that there is no substantial loss of data resolution in the smoothened plot.\n\n![https://user-images.githubusercontent.com/4745789/94834298-d3e56e00-042d-11eb-8c1d-1b339478a7c9.png](https://user-images.githubusercontent.com/4745789/94834298-d3e56e00-042d-11eb-8c1d-1b339478a7c9.png)\n\nWe apply SMA, with window length `11`, to another time series plot and we clearly find the smoothened plot to be visually cleaner with fewer short-term irregularities.\n\n![https://user-images.githubusercontent.com/4745789/94832462-8f58d300-042b-11eb-8d39-f9a12e441519.png](https://user-images.githubusercontent.com/4745789/94832462-8f58d300-042b-11eb-8d39-f9a12e441519.png)\n\n# Making Aberrations Stand Out\n\nWhen an observer is looking at the plot, the primary motive is to spot any aberrations and anomalies. If the plot has irregularities (i.e. it is not smooth enough), spotting anomalies or aberrations becomes tough, and hence smoothing plays a vital role here.\n\nSimple Moving Average is a very effective smoothing technique but choosing the optimal window size is a challenge. Picking a smaller window size will not help in getting rid of irregularities while picking the window size that is too large will mask all the anomalies.\n\n![https://user-images.githubusercontent.com/4745789/94897527-76910180-04ad-11eb-92ab-d38574428dbe.png](https://user-images.githubusercontent.com/4745789/94897527-76910180-04ad-11eb-92ab-d38574428dbe.png)\n\nFrom the over-smoothened plot illustrated above it is clear that having a large window size leads to a heavy information loss and in most cases hides the anomalies and aberrations. Hence we reduce our problem statement to *find the optimal window size for a given plot such that we make anomalies and aberrations standout*.\n\n## Aberrations and Anomalies\n\nIn any data distribution, the anomalies and aberrations form in the long tail which means they are some extreme values that are far away from the mean. Being part of the long tail makes these anomalies - outliers i.e. data points that do not really fit the distribution.\n\nHence in order to find out optimal window size that gets rid of short-term irregularities but makes anomalies stand out, we have to make the resultant distribution \"tail heavy\" implying the presence of anomalies. This is exactly where [Kurtosis](https://en.wikipedia.org/wiki/Kurtosis) - a famous concept from Statistics comes into the picture.\n\n## Kurtosis\n\n[Kurtosis](https://en.wikipedia.org/wiki/Kurtosis) is the measure of \"tailedness\" of the probability distribution (data distribution) and it helps in describing the shape of the plot. Kurtosis is the fourth standardized moment and is defined as\n\n![https://user-images.githubusercontent.com/4745789/94909588-0a1ffd80-04c1-11eb-9b7d-c89bf9dbfb39.png](https://user-images.githubusercontent.com/4745789/94909588-0a1ffd80-04c1-11eb-9b7d-c89bf9dbfb39.png)\n\nThe high value of kurtosis implies that the distribution is heavy on either tail and this is evident when we compute Kurtosis of various distributions with and without any tail noise - mimicking anomalies.\n\n![https://user-images.githubusercontent.com/4745789/94403183-ac22ab80-018a-11eb-9bca-72f6b2e5f98e.png](https://user-images.githubusercontent.com/4745789/94403183-ac22ab80-018a-11eb-9bca-72f6b2e5f98e.png)\n\nIn the illustration above, a small variation (anomaly) is added to the tail of the individual distribution and is encircled in red; and we can clearly see that even a tiny tailedness (anomaly and aberration) that makes the distribution deviate from the mean has a heavy impact on the Kurtosis, making it go much higher.\n\n## Finding the Optimal Window Size\n\nAs established earlier, anomalies and aberrations are extreme values that largely deviate from the mean and hence occupy a position on either tail of the distribution. Hence in order to find the optimal window size that neither under-smooths nor over-smooths the plot while ensuring that it makes anomalies and aberrations stand out, we need to **find the window size that maximizes the Kurtosis**.\n\n```python\nfrom scipy.stats import kurtosis\n\noptimal_window, max_kurt = 1, kurtosis(raw_plot)\n\nfor window_size in range(2, len(raw_plot), 1):\n    # we get the smoothened plot from the `raw_plot` by applying\n    # Simple Moving Average for a window of length `window_size`\n    smoothened_plot = moving_average_plot(raw_plot, window=window_size)\n\n    # measure the kurtosis of the smoothened_plot\n    kurt = kurtosis(smoothened_plot)\n\n    # if kurtosis of the current smoothened plot is greater than the\n    # max we have seen, then we update the optimal window the max_kurt\n    if kurt > max_kurt:\n        max_kurt, optimal_window = kurt, window_size\n```\n\nThe pseudocode above computes the optimal window size that maximizes the Kurtosis and in turn ensuring that the smoothened plot has a heavy tail, making anomalies and aberrations stand out.\n\nFinding the global optimal window size, that maximizes Kurtosis, is not always a good idea, because doing so can totally distort the plot leading to heavy information loss. A better way is to find local optimum within pre-defined limits; for example, an optimal point for window size between 10 and 40. These limits totally depend on the data at hand. Doing this not only leads to a smooth plot that highlights anomalies but also converges the computation to a local optimum much quicker.\n\n# References\n\n- [Kurtosis - Wikipedia](https://en.wikipedia.org/wiki/Kurtosis)\n- [Moving Average - Wikipedia](https://en.wikipedia.org/wiki/Moving_average)\n- [ASAP: Automatic Smoothing for Attention Prioritization in Streaming Time Series Visualization](https://arxiv.org/abs/1703.00983)\n- [Climate Change Earth Surface Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "similar": [
      "better-programmer",
      "mysql-cache",
      "2q-cache",
      "atomicity"
    ]
  },
  {
    "id": 38,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "lfu",
    "title": "Constant Time LFU",
    "description": "The most popular implementation of the LFU Cache Eviction Scheme, using a min-heap, implements all three operations with running time complexity of O(log n) and this makes LFU sub-optimal. In this essay, we take a detailed look at a clever algorithm that implements LFU such that all the operations happen with O(1) running time complexity.",
    "gif": "https://media.giphy.com/media/fWqGY1AC4HVIN3lRyB/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/89711582-cf11ba00-d9a8-11ea-9173-c7069b1537b5.png",
    "released_at": "2020-08-23",
    "total_views": 6035,
    "body": "A common strategy to make any system super-performant is *[Caching](https://en.wikipedia.org/wiki/Cache_(computing)).* Almost all software products, operating at scale, have multiple layers of caches in their architectures. Caching, when done right, does wonder to the response time and is one of the main reasons why products work so well at a massive scale. Cache engines are limited by the amount of memory available and hence once it gets full the engine has to decide which item should be evicted and that is where an eviction algorithm, like [LFU](https://en.wikipedia.org/wiki/Least_frequently_used) and [LRU](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)). kicks in.\n\n[LRU](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)) (Least Recently Used) cache eviction strategy is one of the most popular strategies out there. LFU (Least Frequently Used) strategy fares well in some use cases but a concern with its most popular implementation, where it uses a min-heap, is that it provides a running time complexity of `O(log n)` for all the three operations - insert, update and delete; because of which, more often than not, it is replaced with a sub-optimal yet extremely fast `O(1)` LRU eviction scheme.\n\nIn this essay, we take a look at Constant Time LFU implementation based on the paper [An O(1) algorithm for implementing the LFU cache eviction scheme](http://dhruvbird.com/lfu.pdf) by Prof. Ketan Shah, Anirban Mitra and Dhruv Matani, where instead of using a min-heap, it uses a combination of [doubly-linked lists](https://en.wikipedia.org/wiki/Doubly_linked_list) and [hash table](https://en.wikipedia.org/wiki/Hash_table) to gain a running time complexity of `O(1)` for all the three core operations.\n\n# The LFU cache eviction strategy\n\nLFU, very commonly, is implemented using a [min-heap](https://en.wikipedia.org/wiki/Min-max_heap) which is organized as per the frequency of access of each element. Each element of this heap holds a pair - cached value and the access frequency; and is structured in order of this frequency such that the cached value with the minimum access frequency sits at the top, making it quick to identify the element to be evicted.\n\n![min-heap LFU](https://user-images.githubusercontent.com/4745789/89717235-0fd1f900-d9d2-11ea-968d-9ed67f52a2db.png)\n\nAlthough the identification of the element to be evicted is quick, but in order for the heap to maintain its property - element with lowest access frequency be at the top - it demands a rebalance, and this rebalancing process has a running complexity of `O(log n)`. To make things worse, rebalancing is required every single time the frequency of an item is changed; which means that in the cache that implements LFU, every time an item is either inserted, accessed or evicted, a rebalance is required - making all the three core operations to have the time complexity of `O(log n)`.\n\n# Constant time implementation\n\nThe LFU cache can be implemented with `O(1)` complexity for all the three operations by using one [Hash Table](https://en.wikipedia.org/wiki/Hash_table) and a bunch of [Doubly Linked Lists](https://en.wikipedia.org/wiki/Doubly_linked_list). As stated by the [RUM Conjecture](https://arpitbhayani.me/blogs/rum), in order to get a constant time reads and updates operations, we have to make a compromise with memory utilization. This is exactly what we observe in this implementation.\n\n## The Hash Table\n\nThe Hash Table stores the mapping of the cached key to the Value Node holding the cached value. The value against the key is usually a pointer to the actual Value Node. Given that the lookup complexity of the hash table is `O(1)`, the operation to access the value given the key from this Hash Table could be accomplished in constant time.\n\n![LFU hash table](https://user-images.githubusercontent.com/4745789/90469594-e2561f80-e136-11ea-9ff4-8369a7ea3df3.png)\n\nThe illustration above depicts that the Hash Table holding cache keys `k1`, `k2`, etc are mapped to the nodes holding the values `v1` and `v2` through direct pointers. The nodes are allocated on the heap using dynamic allocation, hence are a little disorganized. The Value Node to which the key maps to, not only hold the cached value, but it also holds a few pointers pointing to different entities in the system, enabling constant-time operations.\n\n## Doubly Linked Lists\n\nThis implementation of LFU requires us to maintain one Doubly Linked List of frequencies, called `freq_list`, holding one node for each unique frequency spanning all the caches values. This list is kept sorted on the frequency the node represents such that, the node representing the lowest frequency is on the one end while the node representing the highest frequency is at the other.\n\nEvery Frequency Node holds the frequency that it represents in the member `freq` and the usual `next` and `prev` pointers pointing to the adjacent Frequency Nodes; it also keeps a `values_ptr` which points to another doubly-linked list holding Value Nodes (referred in the hash table) having the same access frequency `freq`.\n\n![lists LFU](https://user-images.githubusercontent.com/4745789/90469593-e08c5c00-e136-11ea-995b-e4590981dd89.png)\n\nThe overall schematic representation of doubly-linked lists and its arrangement is as shown in the illustration above. The doubly-linked list holding Frequency Nodes is arranged horizontally while the list holding the Value Nodes is arranged vertically, for clearer view and understanding.\n\nSince the cached values `v1` and `v7` both have been accessed `7` times, they both are chained in a doubly-linked list and are hooked with the Frequency Node representing the frequency of `7`. Similarly, the Value Nodes holding values `v5`, `v3`, and `v9` are chained in another doubly-linked list and are hooked with the Frequency Node representing the frequency of `18`.\n\nThe Value Node contains the cached value in member `data`, along with the usual `next` and `prev` pointers pointing to the adjacent Value Nodes in the list. It also holds a `freq_pointer` pointing back to the Frequency Node to which the list if hooked at. Having all of these pointers helps us ensure all the three operations happen in constant time.\n\nNow that we have put all the necessary structures in place, we take a look at the 3 core operations along with their pseudo implementation.\n\n## Adding value to the cache\nAdding a new value to the cache is a relatively simpler operation that requires a bunch of pointer manipulations and does the job with a constant time running complexity. While inserting the value in the cache, we first check the existence of the key in the table, if the key is already present and we try to put it again the function raises an error. Then we ensure the presence of the Frequency Node representing the frequency of `1`, and in the process, we might also need to create a new frequency node also. Then we wrap the value in a Value Node and adds it to the `values_list` of this Frequency Node; and at last, we make an entry in the table acknowledging the completion of the caching process.\n\n```python\ndef add(key: str, value: object):\n    # check if the key already present in the table,\n    # if so then return the error\n    if key in table:\n        raise KeyAlreadyExistsError\n\n    # create the Value Node out of value\n    # holding all the necessary pointers\n    value_node = make_value_node(value)\n\n    first_frequency_node = freq_list.head\n    if first_frequency_node.freq != 1:\n        # since the first node in the freq_list does not represent\n        # frequency of 1, we create a new node\n        first_frequency_node = make_frequency_node(1)\n\n        # update the `freq_list` that holds all the Frequency Nodes\n        # such that the first node represents the frequency 1\n        # and other list stays as is.\n        first_frequency_node.next = freq_list.head\n        freq_list.head.prev = first_frequency_node\n        freq_list.head = first_frequency_node\n\n    # Value Node points back to the Frequency Node to which it belongs\n    value_node.freq_pointer = first_frequency_node\n\n    # add the Value Node in `first_frequency_node`\n    first_frequency_node.values.add(value_node)\n\n    # update the entry in the hash table\n    table[key] = value_node\n```\n\nAs seen in the pseudocode above the entire procedure to add a new value in the cache is a bunch of memory allocation along with some pointer manipulations, hence we observe that the running complexity of `O(1)` for this operation.\n\n## Evicting an item from the cache\n\nEviction, similar to insertion, is a trivial operation where we simply pick the frequency node with lowest access frequency (the first node in the `freq_list`) and remove the first Value Node present in its `values_list`. Since the entire eviction also requires pointer manipulations, it also exhibits a running complexity of `O(1)`.\n\n```python\ndef evict():\n    if freq_list.head and freq_list.head.values:\n        first_value_node = freq_list.head.values.first\n        second_value_node = first_value_node.next\n\n        # make the second element as first\n        freq_list.head = second_value_node\n\n        # ensure second node does not have a dangling prev link\n        second_value_node.prev = None\n\n        # delete the first element\n        delete_value_node(first_value_node)\n\n    if freq_list.head and not freq_list.head.values:\n        # if the Frequency Node after eviction does not hold\n        # any values we get rid of it\n        delete_frequency_node(freq_list.head)\n```\n\n## Getting a value from the cache\n\nAccessing an item from the cache has to be the most common operation of any cache. In the LFU scheme, before returning the cached value, the engine also has to update its access frequency. Ensuring the change in access frequency of one cached value does not require some sort of rebalancing or restructuring to maintain the integrity, is what makes this implementation special.\n\nThe engine first makes a get call to the Hash Table to check that the key exists in the cache. Before returning the cached value from the retrieved Value Node, the engine performs the following operations - it accesses the Frequency Node and its sibling corresponding to the retrieved Value Node. It ensures that the frequency of the sibling is 1 more than that of the Frequency Node; if not it creates the necessary Frequency Node and place it as the new sibling. The Value Node then changes its affinity to this sibling Frequency Node so that it correctly matches the access frequency. In the end, the back pointer from the Value Node to the new Frequency Node is set and the value is returned.\n\n```python\ndef get(key: str) -> object:\n    # get the Value Node from the hash table\n    value_node = table.get(key, None)\n\n    # if the value does not exist in the cache then return an error\n    # stating Key Not Found\n    if not value_node:\n        raise KeyNotFoundError\n\n    # we get the Frequency Node from the Value Node using the\n    # freq_pointer member.\n    frequency_node = value_node.freq_pointer\n\n    # we also get the next Frequency Node to the current\n    # Frequency Node so that we could make a call about\n    # the need to create a new node.\n    next_frequency_node = frequency_node.next\n\n    if next_frequency_node.freq != frequency_node.freq + 1:\n        # create a new Frequency Node\n        new_frequency_node = make_frequency_node(frequency_node.freq + 1)\n        \n        # place the Frequency Node at the correct position in the list\n        frequency_node.next = new_frequency_node\n        new_frequency_node.prev = frequency_node\n\n        next_frequency_node.prev = new_frequency_node\n        new_frequency_node.next = frequency_node\n\n        # going forward we call the new Frequency Node as next\n        # because it represents the the next Frequency Node\n        next_frequency_node = new_frequency_node\n\n    # we add the Value Node in the nex\n    next_frequency_node.values.add(value_node)\n    \n    # we change the parent and adjecent nodes to this Value Nodes\n    value_node.freq_pointer = next_frequency_node\n    value_node.next = None\n    value_node.prev = next_frequency_node.values.last\n    \n    # if the Frequency Node has no elements then deleting the node\n    # so as to avoid the memory leak\n    if len(frequency_node.values) == 0:\n        delete_frequency_node(frequency_node)\n\n    # returning the value\n    return value_node.value\n```\n\nAgain, since this operation also only deals with pointer manipulations through direct pointers, the running time complexity of this operation is also constant time. Thus we see the Constant Time LFU implementation where the necessary time complexity is achieved by using Hash Tables and Doubly-Linked Lists.\n\n# References\n\n- [An O(1) algorithm for implementing the LFU cache eviction scheme](http://dhruvbird.com/lfu.pdf)\n- [When and Why to use a Least Frequently Used (LFU) cache with an implementation in Golang](https://ieftimov.com/post/when-why-least-frequently-used-cache-implementation-golang/)\n",
    "similar": [
      "fully-persistent-arrays",
      "isolation-forest",
      "flajolet-martin",
      "consistent-hashing"
    ]
  },
  {
    "id": 39,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "morris-counter",
    "title": "Morris's Algorithm for Approximate Counting",
    "description": "Morris' Algorithm counts a large number of events using a very small space O(log log n). The algorithm uses probabilistic techniques to increment the counter and in this essay, we take a detailed look at Morris' Algorithm and the math behind it.",
    "gif": "https://media.giphy.com/media/uWXxbGPMbFkxsni3tc/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/89116441-b9d0f300-d4b1-11ea-99e5-cca7c4cef5fb.png",
    "released_at": "2020-08-02",
    "total_views": 2120,
    "body": "Approximate Counting algorithms are techniques that allow us to count a large number of events using a very small amount of memory. It was invented by [Robert Morris](https://en.wikipedia.org/wiki/Robert_Tappan_Morris) in 1977 and was published through his paper [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf). The algorithm uses probabilistic techniques to increment the counter, although it does not guarantee the exactness it does provide a fairly good estimate of the true value while inducing a minimal and yet fairly constant relative error. In this essay, we take a detailed look at Morris' Algorithm and the math behind it.\n\nRobert Morris, while working at Bell Laboratories, had a problem at hand; he was supposed to write a piece of code that counts a large number of events and all he had was one 8-bit counter. Since the number of events easily crossed 256, counting them was infeasible using ordinary methods and this constraint led him to build this approximate counter that instead of providing the exact count, does a pretty good job providing an approximate one.\n\n# Counting and coin flips\n\nA very simple solution, to build an approximate counter, is to count every alternate event. In order to make the decision of whether to count an event or not - we use a coin flip, which means, every time we see a new event, we flip a coin and if it lands *heads* we increase the count otherwise we don't. This way the value preset in our counter register will, on average, represent half of the total events. When we multiply the count (in the register) by 2, we get a good approximation of the actual number of events.\n\nThis coin flip based counting technique is a Binomial Distribution with parameters `(n, p)` where `n` is the total number of events seen and `p` is the success probability i.e. probability of getting *heads* during a coin flip. The expected value `v` in the counter register corresponding to the number of events `n` is given by\n\n![value in coin-flip counter](https://user-images.githubusercontent.com/4745789/89116526-a6725780-d4b2-11ea-9143-a562d6c3ca93.png)\n\nThe standard deviation of this binomial distribution will help us find the error in our measurement i.e. value retrieved using our counter vs the actual number of the events seen. For a binomial distribution twice the standard deviation on either side of the mean covers **95%** of the distribution; and we use this to find the relative and absolute error in our counter value as illustrated below.\n\n![coin-flip binomial distribution](https://user-images.githubusercontent.com/4745789/89117327-82b30f80-d4ba-11ea-8346-a39ae6cb639a.png)\n\nAs illustrated in the image we can say that if the counter holds the value `200`, the closest approximate to the actual number of events that we can make is `2 * 200` = `400`. Even though `400` might not be the actual number of events seen, we can say, with **95%** confidence, that the true value lies in the range `[180, 220]`.\n\nWith this coin flip based counter, we have actually doubled our capacity to count and have also ensured that our memory requirements stay constant. On an 8-bit register, ordinarily, we would have counted till 256, but with this counter in place, we can approximately count till 512.\n\nThis approach can be extended to count even larger numbers by changing the value of `p`. The absolute error observed here is small but the relative error is very high for smaller counts and hence this creates a need for a technique that has a near-constant relative error, something that is independent of `n`.\n\n# The Morris' Algorithm\n\nInstead of keeping track of the total number of events `n` or some constant multiple of `n`, Morris' algorithm suggests that the value we store in the register is\n\n![value in counter Morris' algorithm](https://user-images.githubusercontent.com/4745789/89117993-edb31500-d4bf-11ea-9879-1f0032950ff4.png)\n\nHere we try to exploit the core property of *logarithm* - *the growth of logarithmic function is inverse of the exponential* - which means the value `v` will grow faster for the smaller values of `n` - providing better approximations. This ensures that the relative error is near-constant i.e. independent of `n` and it does not matter if the number of events is fewer or larger.\n\nNow that we have found a function that suits the needs of a good approximate counter, it is time we define what exactly would happen to the counter when we see a new event.\n\n## Incrementing the value\n\nSince we are building a counter, all we know is the value of the counter `v` and have no knowledge of the actual number of events seen. So when the new event comes in we have to decide if `v` needs to change or not. Given the above equation, our best estimate of `n` given `v` can be computed as\n\n![estimate n](https://user-images.githubusercontent.com/4745789/89120289-c7e33b80-d4d2-11ea-92b8-d307b0aa9032.png)\n\nNow that we have seen a new event we want to find the new value `v` for the counter. This value should always, on an average, converge to `n + 1` and we find it as\n\n![next value](https://user-images.githubusercontent.com/4745789/89120467-2eb52480-d4d4-11ea-89ac-d8cacd14d952.png)\n\nSince the value computed using the above method is generally not an integer, performing either round-up or round-down every time will induce a serious error in counting.\n\nFor us to determine if we should increment the value of `v` or not, we need to find the cost (inaccuracy) that we might incur if we made an incorrect call. We establish a heuristic that if the change in the value of `n` by change in `v` is huge, we should have a lower probability of making an increment to `v` and vice versa. We this define `d` to be reciprocal of this jump i.e. difference between `n` corresponding to `v + 1` and `v`.\n\n![defining d](https://user-images.githubusercontent.com/4745789/89120957-38d92200-d4d8-11ea-975f-323b36da325c.png)\n\nThe value of `d` will always be in the interval `(0, 1)` . Smaller the jump between two `n`s larger will be the value of `d` and larger the jump, smaller will be the value of `d`. This also implies that as `n` grows the value of `d` will become smaller and smaller making it harder for us to make the change in `v`.\n\nSo we pick a random number `r` uniformly generated in the interval `[0, 1)` and using this random number `r` and previously defined `d` we state that if this `r` is less than `d` increase the counter `v` otherwise, we keep it as is. As `n` increases, `d` decreases making it tougher for the odds of pick `r` in the range `[0, d)`.\n\n![defiing d 2](https://user-images.githubusercontent.com/4745789/89120929-f9aad100-d4d7-11ea-8f0e-066fc059c066.png)\n\nThe proof that the expected value of `n`, post this probabilistic decision, is `n + 1` can be found in the paper - [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf). After tweaking some parameters and making process stabler the function that Morris came up was\n\n![morris function v](https://user-images.githubusercontent.com/4745789/89121058-3fb46480-d4d9-11ea-9d93-5af712ac08e7.png)\n\nWhen we plot values produced by Morris' Algorithm vs the actual number of events we find that Morris' algorithm indeed generates better approximate values to smaller values of `n` but as `n` increases the absolute error grows but the relative error remains fairly constant. The illustrations shown below describe these facts.\n\n![Morris comparison](https://user-images.githubusercontent.com/4745789/89123322-13eeaa00-d4ec-11ea-9539-ada7f5de9af1.png)\n\n\n> _Python based implementation of Morris' algorithm can be found at [github.com/arpitbbhayani/morris-counter](https://github.com/arpitbbhayani/morris-counter/blob/master/morris-counter.ipynb)_\n\n\n# Space Complexity\n\nIn order to count till `n` the Morris' algorithm requires the counter to go up to `log(n)` and hence the number of bits required to count from `0 to log(n)` ordinarily is `log(log(n))` and hence we say that the space complexity of this technique is `O(log log n)`. Morris' algorithm thus provides a very efficient way to manage cardinalities where we can afford to have approximations.\n\n# References\n\n- [Approximate Counting Algorithm - Wikipedia](https://en.wikipedia.org/wiki/Approximate_counting_algorithm)\n- [Approximate Counting with Morris's Algorithm](http://gregorygundersen.com/blog/2019/11/11/morris-algorithm/)\n- [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf)\n- [Probabilistic Counting and Morris' Algorithm - Texas A&M University](http://cesg.tamu.edu/wp-content/uploads/2014/09/ECEN689-lec11.pdf)\n- [Python-based implementation of Morris' algorithm](https://github.com/arpitbbhayani/morris-counter/blob/master/morris-counter.ipynb)\n",
    "similar": [
      "israeli-queues",
      "slowsort",
      "fast-and-efficient-pagination-in-mongodb",
      "phi-accrual"
    ]
  },
  {
    "id": 40,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "slowsort",
    "title": "Slowsort - A Pessimal Sorting Algorithm",
    "description": "Slowsort is a pessimal sorting algorithm based on the Multiply and Surrender paradigm. The algorithm is designed to be deterministically sub-optimal and it could easily be the worst way anyone could sort an array.",
    "gif": "https://media.giphy.com/media/3NtY188QaxDdC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/88473025-fabb8b80-cf36-11ea-8390-a807fcd57d93.png",
    "released_at": "2020-07-26",
    "total_views": 6485,
    "body": "Slowsort is a sorting algorithm that is designed to be deterministically sub-optimal. The algorithm was published in 1986 by Andrei Broder and Jorge Stolfi in their paper [Pessimal Algorithms and Simplexity Analysis](https://www.researchgate.net/publication/2805500_Pessimal_Algorithms_and_Simplexity_Analysis) where they expressed a bunch of very in-efficient algorithms. These techniques and algorithms are special because they never make a wrong move while solving a problem, instead, they find ways to delay the success. In this essay, we put our focus on the Slowsort algorithm based on the *Multiply and Surrender* paradigm.\n\n# Multiply and Surrender\n\nThe *Multiply and Surrender (MnS)* paradigm, is a humorous take on the popular *Divide and Conquer (DnC)* technique. MnS splits the problem into subproblems, slightly simpler than the original, and continues doing so recursively for as long as possible. The subproblems are multiplied and the evaluation is delayed till the state that the solving could not be further postponed and then it surrenders.\n\nMnS paradigm is very similar to DnC; but where the DnC actually splits the problems into subproblems to reach the solution quicker, MnS does it to procrastinate, making the entire process very inefficient but yet convergent.\n\nAlthough Slowsort is a classic example, recursive Fibonacci with no memoization also fares under the MnS paradigm. The recursive code to find `n`th Fibonacci number is as illustrated below\n\n```python\ndef fib(n):\n    \"\"\"the function returns the `n`th fibonacci number\n    \"\"\"\n\n    # the subproblem could not be postponed anymore\n    # hence we surrender and return the answer\n    if n < 2:\n        return 1\n\n    # we split the problem into subproblems and invoke\n    # the function recursively\n    return fib(n - 1) + fib(n - 2)\n```\n\nWhile computing the Fibonacci numbers, we split the problem into subproblems and do this recursively till we are left with elemental states i.e. `0` or `1` and which is when we return the initial values which are `1` and `1`. This approach is not DnC because we are not splitting the problem into subproblems to achieve optimality, instead are doing a lot of repetitive work and taking a non-polynomial time to generate the Fibonacci numbers.\n\n# Slowsort Algorithm\n\nSlowsort algorithm draws a lot of similarities to the very popular Mergesort, but while Mergesort operates in `O(n . log(n))` the complexity of Slowsort is non-polynomial `O(n ^ log(n))` and its best case performs worse than the worst case of bubble sort.\n\nSlowsort algorithm recursively breaks the array sorting problem into two subarray sorting problems and a few extra processing steps. Once the two subarrays are sorted, the algorithm swaps the rightmost elements such that the greatest among the two becomes the rightmost element of the array i.e. the greatest among the two is placed at the correct position relative to each other, and then it invokes the sorting for all elements except this fixed maximum.\n\nThe algorithm could this be expressed as following broad steps\n\n- sort the first half recursively\n- sort the second half recursively\n- find the maximum of the whole array by comparing the last elements of both the sorted halves and place it at the end of the array\n- recursively sort the entire array except for the maximum one\n\nThe in-place implementation of the Slowsort algorithm is as illustrated below\n\n```python\ndef _slowsort(a, i, j):\n    \"\"\"in-place sorts the integers in the array\n    spanning indexes [i, j].\n    \"\"\"\n    # base condition; then no need of sorting if\n    #  - there is one element to sort\n    #  - when start and end of the array flipped positions\n    if i >= j:\n        return\n\n    # find the mid index of the array so that the\n    # problem could be divided intto sub-problems of\n    # smaller spans\n    m = (i + j) // 2\n\n    # invoke the slowsort on both the subarrays\n    _slowsort(a, i, m)\n    _slowsort(a, m + 1, j)\n  \n    # once both the subproblems are solved, check if\n    # last elements of both subarrays and move the\n    # higher among the both to end of the right subarray\n    # ensuring that the highest element is placed at the\n    # correct relative position\n    if a[m] > a[j]:\n        a[m], a[j] = a[j], a[m]\n  \n    # now that the rightmost element of the array is at\n    # the relatively correct position, we invoke Slowsort on all\n    # the elements except the last one.\n    _slowsort(a, i, j - 1)\n\n\ndef slowsort(a):\n    \"\"\"in-place sorts the array `a` using Slowsort.\n    \"\"\"\n    _slowsort(a, 0, len(a) - 1)\n```\n\nThe Slowsort algorithm typically replaces the `merge` function of the Mergesort with a simple swap that correctly places the largest element (local maxima) and then invokes the sort function on all but this element. So on every invocation, we keep correctly placing the largest element but in a recursive manner.\n\n*A visualization of this algorithm could be found in this [youtube video](https://www.youtube.com/watch?v=QbRoyhGdjnA).*\n\n# Asymptotic Analysis\n\nThe runtime of Slowsort could be computed by the following recurrence relation\n\n![slowsort recurrence relation](https://user-images.githubusercontent.com/4745789/88473102-cb594e80-cf37-11ea-9015-217c3eda50d6.png)\n\nWhen the above recurrence relation is solved and we find that the asymptotic lower bound for `T(n)` is given by\n\n![lower bound of slowsort](https://user-images.githubusercontent.com/4745789/88473128-14a99e00-cf38-11ea-905b-f3f473a0d74c.png)\n\nThe above expression suggests that lower bound of Slowsort is non-polynomial in nature and for a sufficiently large `n` this would be more than `n ^ 2` implying that even the best case of Slowsort is worse than the worst case of Bubble sort. The illustration below compares the time taken by Slowsort and the recursive implementation of Bubblesort.\n\n![slowsort vs recursive bubblesort](https://user-images.githubusercontent.com/4745789/88475549-8e4c8680-cf4e-11ea-8ee4-9f7ed345ff5d.png)\n\n> The iterative implementation of Bubblesort stood no chance in terms of time taken for smaller sets of integers, hence the chart was plotted against the recursive implementation of it. The iterative bubble sort for smaller arrays is nearly flat.\n\n# Slowsort vs Bogosort\n\n[Bogosort](https://en.wikipedia.org/wiki/Bogosort) is a sorting algorithm that has an average case time complexity of `O(n!)` and an unbounded time in the worst case. The algorithm keeps on permuting (shuffling) the array till it is sorted which introduces an unboundedness in its implementation and hence the algorithm is considered to be the worst sorting algorithm ever.\n\n```python\nimport random\n\ndef is_sorted(a):\n    return all(a[i] <= a[i + 1] for i in range(len(a) - 1))\n\ndef bogosort(a):\n    while not is_sorted(a):\n        random.shuffle(a)\n```\n\nThe reason that we should rate Slowsort highly is the fact the Bogosort could sort the list in its first shuffle while Slowsort is deterministic and will take `O(n ^ log(n))` time in best case scenario.\n\nA rule that every algorithm follows is that every step that it takes actually makes some progress towards the final goal. Bogosort does not guarantee progress, and since it randomly shuffles the array, at one iteration we could end up at a nearly sorted array while the next shuffle takes us afar.\n\nSlowsort is deterministic and convergent. The number of swaps made (inversions) during the execution is non-increasing which means once two items are swapped they are in the correct order relative to each other. In other words, we say that Slowsort never makes a wrong move.\n\n# References\n\n- [Slowsort](https://wiki.c2.com/?SlowSort)\n- [Slowsort - Wikipedia](https://en.wikipedia.org/wiki/Slowsort)\n- [Multiply and Surrender](https://wiki.c2.com/?MultiplyAndSurrender)\n- [Pessimal Algorithms and Simplexity Analysis](https://www.researchgate.net/publication/2805500_Pessimal_Algorithms_and_Simplexity_Analysis)\n",
    "similar": [
      "flajolet-martin",
      "1d-terrain",
      "morris-counter",
      "isolation-forest"
    ]
  },
  {
    "id": 42,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "phi-accrual",
    "title": "Phi \u03c6 Accrual Failure Detection",
    "description": "Phi \u03c6 Accrual Failure Detection algorithm, unlike conventional algorithms, is an adaptive failure detection algorithm that instead of providing output as a boolean (system being up or down), outputs the suspicion information (level) on a continuous scale.",
    "gif": "https://media.giphy.com/media/gLoMzjGQB2tQlQtB9P/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/87240958-c5de0d80-c43b-11ea-8e1b-2a7c70586b9b.png",
    "released_at": "2020-07-12",
    "total_views": 478,
    "body": "One of the most important virtues of any distributed system is its ability to detect failures in any of its subsystems before things go havoc. Early detection of failures helps in taking preventive actions and ensuring that the system stays fault-tolerant. The conventional way of failure detection is by using a bunch of heartbeat messages with a fixed timeout, indicating if a subsystem is down or not.\n\nIn this essay, we take a look into an adaptive failure detection algorithm called *Phi Accrual Failure Detection*, which was introduced in a [paper](https://pdfs.semanticscholar.org/11ae/4c0c0d0c36dc177c1fff5eb84fa49aa3e1a8.pdf) by Naohiro Hayashibara, Xavier D\u00e9fago, Rami Yared, and Takuya Katayama. The algorithm uses historical heartbeat information to make the threshold adaptive. Instead of generating a binary value, like conventional methods, it generates continuous values suggesting the confidence level it has in stating if the system crashed or not.\n\n# Conventional Failure Detection\n\nAccurately detecting failures is an impossible problem to solve as we cannot ever say if a system crashed or is just very slow in responding. Conventional Failure Detection algorithms output a boolean value stating if the system is down or not; there is no middle ground.\n\n## Heartbeats with constants timeouts\n\nThe conventional Failure Detection algorithms use *heartbeat* messages with a fixed timeout in order to determine if a system is alive or not. The monitored system periodically sends a heartbeat message to the monitoring system, informing that it is still alive. The monitoring system will suspect that the process crashed if it fails to receive any heartbeat message within a configured timeout period.\n\nHere the value of timeout is very crucial as keeping it short means we detect failures quickly but with a lot of false positives; and while keeping it long means we reduce the false positives but the detection time takes a toll.\n\n# Phi Accrual Failure Detection\n\nPhi Accrual Failure Detection is an adaptive Failure Detection algorithm that provides a building block to implementing failure detectors in any distributed system. A generic Accrual Failure Detector, instead of providing output as a boolean (system being up or down), outputs the suspicion information (level) on a continuous scale such that higher the suspicion value, the higher are the chances that the system is down.\n\n## Detailing \u03c6\n\nWe define \u03c6 as the suspicion level output by this failure detector and as the algorithm is adaptive, the value will be dynamic and will reflect the current network conditions and system behavior. As we established earlier - lower are the chances of receiving the heartbeat, higher are the chances that the system crashed hence higher should be the value of \u03c6; the details around expressing \u03c6 mathematically are as illustrated below.\n\n![Phi Accrual Failure Detection](https://user-images.githubusercontent.com/4745789/87240784-469c0a00-c43a-11ea-8689-9dc41eb1ccf1.png)\n\nThe illustration above mathematically expresses our establishments and shows how we can use `-log10(x)` function applied to the probability to get a gradual negative slope indicating a decline in the value of \u03c6. We observe how, as the probability of receiving heartbeat increases, the value of \u03c6 decreases and approaches `0`, and when the probability of receiving heartbeat decreases and approaches `0`, the value of \u03c6 tends to infinity \u221e.\n\nThe \u03c6 value computed using `-log10(x)` also suggests our likeliness of making mistakes decreases exponentially as the value of \u03c6 increases. So if we say a system is down if \u03c6 crosses a certain threshold `X` where `X` is `1`, it implies that our decision will be contradicted in the future by the reception of a late heartbeat is about `10%`. For `X = 2`, the likelihood of the mistake will be `1%`, for `X = 3` it will be `0.1%`, and so on.\n\n## Estimating the probability of receiving another heartbeat\n\nNow that we have defined what \u03c6 is, we need a way to compute the probability of receiving another heartbeat given we have seen some heartbeats before. This probability is proportional to the probability that the heartbeat will arrive more than `t` units after the previous one i.e. longer the wait lesser are the chances of receiving the heartbeat.\n\nIn order to implement this, we keep a sampled Sliding Window holding arrival times of past heartbeats. Whenever a new heartbeat arrives, its arrival time is stored into the window, and the data regarding the oldest heartbeat is deleted.\n\nWe observe that the arrival intervals follow a [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) indicating most of the heartbeats arrive within a specific range while there are a few that arrive late due to various network or system conditions. From the information stored in the window, we can easily compute the arrival intervals, mean, and variance which we require to estimate the probability.\n\nSince arrival intervals follow a Normal Distribution, we can integrate the [Probability Density Function](https://en.wikipedia.org/wiki/Probability_density) over the interval `(t, \u221e)` to get the probability of receiving heartbeat after `t` units of time. Thus the expression for deriving this can be illustrated below.\n\n![Estimating probability of receiving another heartbeat](https://user-images.githubusercontent.com/4745789/87231591-fbe8a680-c3d5-11ea-9427-d4cd66e8e717.png)\n\nWe observe that if the process actually crashes, the value is guaranteed to accrue (accumulate) over time and will tend to infinity \u221e. Since the accrual failure detectors output value in a continuous range, we need to explicitly define thresholds crossing which we say that the system crashed.\n\n# Benefits of using Accrual Failure Detectors\n\nWe can define multiple thresholds, crossing which we can take precautionary measures defined for it. As the threshold becomes steeper the action could become more drastic. Another major benefit of using this system is that it favors a nearly complete decoupling between application requirements and monitoring as it leaves the applications to define threshold according to their QoS requirements.\n\n# References\n\n- [The \u03c6 Accrual Failure Detector](https://pdfs.semanticscholar.org/11ae/4c0c0d0c36dc177c1fff5eb84fa49aa3e1a8.pdf)\n- [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "similar": [
      "israeli-queues",
      "copy-on-write",
      "2q-cache",
      "morris-counter"
    ]
  },
  {
    "id": 48,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "consistent-hashing",
    "title": "Consistent Hashing",
    "description": "Consistent Hashing is one of the most sought after techniques when it comes to designing highly scalable distributed systems. In this article, we dive deep into the need for Consistent Hashing, the internals of it, and more importantly along the way implement it using arrays and binary search.",
    "gif": "https://media.giphy.com/media/3ofSBqzxwsiN0npCak/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/82760647-92efa400-9e12-11ea-9533-5003bc3c46df.png",
    "released_at": "2020-05-24",
    "total_views": 1262,
    "body": "Consistent hashing is a hashing technique that performs really well when operated in a dynamic environment where the distributed system scales up and scales down frequently. The core concept of Consistent Hashing was introduced in the paper [Consistent Hashing and RandomTrees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web](https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf) but it gained popularity after the famous paper introducing DynamoDB - [Dynamo: Amazon\u2019s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf). Since then the consistent hashing gained traction and found a ton of use cases in designing and scaling distributed systems efficiently. The two famous examples that exhaustively use this technique are Bit Torrent, for their peer-to-peer networks and Akamai, for their web caches. In this article we dive deep into the need of Consistent Hashing, the internals of it, and more importantly along the way implement it using arrays and [Binary Search](https://en.wikipedia.org/wiki/Binary_search_algorithm).\n\n# Hash Functions\nBefore we jump into the core Consistent Hashing technique we first get a few things cleared up, one of which is Hash Functions. Hash Functions are any functions that map value from an arbitrarily sized domain to another fixed-sized domain, usually called the Hash Space. For example, mapping URLs to 32-bit integers or web pages' HTML content to a 256-byte string. The values generated as an output of these hash functions are typically used as keys to enable efficient lookups of the original entity.\n\nAn example of a simple hash function is a function that maps a 32-bit integer into an 8-bit integer hash space. The function could be implemented using the arithmetic operator `modulo` and we can achieve this by taking a `modulo 256` which yields numbers in the range `[0, 255]` taking up 8-bits for its representation. A hash function, that maps keys to such integer domain, more often than not applies the `modulo N` so as to restrict the values, or the hash space, to a range `[0, N-1]`.\n\nA good hash function has the following properties\n - The function is computationally efficient and the values generated are easy for lookups\n - The function, for most general use cases, behaves like a pseudorandom generator that spreads data out evenly without any noticeable correlation\n\nNow that we have seen what a hash function is, we take a look into how we could use them and build a somewhat scalable distributed system.\n\n# Building a distributed storage system\nSay we are building a distributed storage system in which users can upload files and access them on demand. The service exposes the following APIs to the users\n\n - `upload` to upload the file\n - `fetch` to fetch the file and return its content\n\nBehind the scenes the system has Storage Nodes on which the files are stored and accessed. These nodes expose the functions `put_file` and `fetch_file` that puts and gets the file content to/from the disk and sends the response to the main API server which in turn sends it back to the user.\n\nTo sustain the initial load, the system has 5 Stogare Nodes which stores the uploaded files in a distributed manner. Having multiple nodes ensures that the system, as a whole, is not overwhelmed, and the storage is distributed almost evenly across.\n\nWhen the user invokes `upload` function with the path of the file, the system first needs to identify the storage node that will be responsible for holding the file and we do this by applying a hash function to the path and in turn getting the storage node index. Once we get the storage node, we read the content of the file and put that file on the node by invoking the `put_file` function of the node.\n\n```py\n# storage_nodes holding instances of actual storage node objects\nstorage_nodes = [\n    StorageNode(name='A', host='10.131.213.12'),\n    StorageNode(name='B', host='10.131.217.11'),\n    StorageNode(name='C', host='10.131.142.46'),\n    StorageNode(name='D', host='10.131.114.17'),\n    StorageNode(name='E', host='10.131.189.18'),\n]\n\n\ndef hash_fn(key):\n    \"\"\"The function sums the bytes present in the `key` and then\n    take a mod with 5. This hash function thus generates output\n    in the range [0, 4].\n    \"\"\"\n    return sum(bytearray(key.encode('utf-8'))) % 5\n\n\ndef upload(path):\n    # we use the hash function to get the index of the storage node\n    # that would hold the file\n    index = hash_fn(path)\n\n    # we get the StorageNode instance\n    node = storage_nodes[index]\n\n    # we put the file on the node and return\n    return node.put_file(path)\n\n\ndef fetch(path):\n    # we use the hash function to get the index of the storage node\n    # that would hold the file\n    index = hash_fn(path)\n\n    # we get the StorageNode instance\n    node = storage_nodes[index]\n\n    # we fetch the file from the node and return\n    return node.fetch_file(path)\n```\n\nThe hash function used over here simply sums the bytes and takes the modulo by `5` (since there are 5 storage nodes in the system) and thus generating the output in the hash space `[0, 4]`. This output value now represents the index of the storage engine that will be responsible for holding the file.\n\nSay we have 5 files 'f1.txt', 'f2.txt', 'f3.txt', 'f4.txt', 'f5.txt' if we apply the hash function to these we find that they are stored on storage nodes E, A, B, C, and D respectively.\n\nThings become interesting when the system gains some traction and it needs to be scaled to 7 nodes, which means now the hash function should do `mod 7` instead of a `mod 5`. Changing the hash function implies changing the mapping and association of files with storage nodes. We first need to administer the new associations and see which files required to be moved from one node to another.\n\nWith the new hash function the same 5 files 'f1.txt', 'f2.txt', 'f3.txt', 'f4.txt', 'f5.txt' will now be associated with storage nodes D, E, F, G, A. Here we see that changing the hash function requires us to move every single one of the 5 files to a different node.\n\n![File association changed](https://user-images.githubusercontent.com/4745789/82746677-16c47480-9db0-11ea-8dea-7b5a3cb73e91.png)\n\nIf we have to change the hash function every time we scale up or down and if this requires us to move not all but even half of the data, the process becomes super expensive and in longer run infeasible. So we need a way to minimize the data movement required during scale-ups or scale-downs, and this is where Consistent Hashing fits in and minimizes the required data transfer.\n\n# Consistent Hashing\nThe major pain point of the above system is that it is prone to events like scale-ups and scale-downs as it requires a lot of alterations in associations. These associations are purely driven by the underlying Hash Function and hence if we could somehow make this hash function independent of the number of the storage nodes in the system, we address this flaw.\n\nConsistent Hashing addresses this situation by keeping the Hash Space huge and constant, somewhere in the order of `[0, 2^128 - 1]` and the storage node and objects both map to one of the slots in this huge Hash Space. Unlike in the traditional system where the file was associated with storage node at index where it got hashed to, in this system the chances of a collision between a file and a storage node are infinitesimally small and hence we need a different way to define this association.\n\nInstead of using a collision-based approach we define the association as - the file will be associated with the storage node which is present to the immediate right of its hashed location. Defining association in this way helps us\n\n - keep the hash function independent of the number of storage nodes\n - keep associations relative and not driven by absolute collisions\n\n![Associations in Consistent Hashing](https://user-images.githubusercontent.com/4745789/82748149-4d54bc00-9dbd-11ea-8f06-6710a5c98f20.png)\n\n> Consistent Hashing on an average requires only k/n units of data to be migrated during scale up and down; where k is the total number of keys and n is the number of nodes in the system.\n\nA very naive way to implement this is by allocating an array of size equal to the Hash Space and putting files and storage node literally in the array on the hashed location. In order to get association we iterate from the item's hashed location towards the right and find the first Storage Node. If we reach the end of the array and do not find any Storage Node we circle back to index 0 and continue the search. The approach is very easy to implement but suffers from the following limitations\n\n - requires huge memory to hold such a large array\n - finding association by iterating every time to the right is `O(hash_space)`\n\nA better way of implementing this is by using two arrays: one to hold the Storage Nodes, called `nodes` and another one to hold the positions of the Storage Nodes in the hash space, called `keys`. There is a one-to-one correspondence between the two arrays - the Storage Node `nodes[i]` is present at position `keys[i]` in the hash space. Both the arrays are kept sorted as per the `keys` array.\n\n## Hash Function in Consistent Hashing\nWe define `total_slots` as the size of this entire hash space, typically of the order `2^256` and the hash function could be implemented by taking [SHA-256](https://en.wikipedia.org/wiki/SHA-2) followed by a `mod total_slots`. Since the `total_slots` is huge and a constant the following hash function implementation is independent of the actual number of Storage Nodes present in the system and hence remains unaffected by events like scale-ups and scale-downs.\n\n```py\ndef hash_fn(key: str, total_slots: int) -> int:\n    \"\"\"hash_fn creates an integer equivalent of a SHA256 hash and\n    takes a modulo with the total number of slots in hash space.\n    \"\"\"\n    hsh = hashlib.sha256()\n\n    # converting data into bytes and passing it to hash function\n    hsh.update(bytes(key.encode('utf-8')))\n\n    # converting the HEX digest into equivalent integer value\n    return int(hsh.hexdigest(), 16) % total_slots\n```\n\n## Adding a new node in the system\nWhen there is a need to scale up and add a new node in the system, in our case a new Storage Node, we\n\n - find the position of the node where it resides in the Hash Space\n - populate the new node with data it is supposed to serve\n - add the node in the Hash Space\n\nWhen a new node is added in the system it only affects the files that hash at the location to the left and associated with the node to the right, of the position the new node will fit in. All other files and associations remain unaffected, thus minimizing the amount of data to be migrated and mapping required to be changed.\n\n![Adding a new node in the system - Consistent Hashing](https://user-images.githubusercontent.com/4745789/82751279-c959fe80-9dd3-11ea-86de-62d162519262.png)\n\nFrom the illustration above, we see when a new node K is added between nodes B and E, we change the associations of files present in the segment B-K and assign them to node K. The data belonging to the segment B-K could be found at node E to which they were previously associated with. Thus the only files affected and that needs migration are in the segment B-K; and their association changes from node E to node K.\n\nIn order to implement this at a low-level using `nodes` and `keys` array, we first get the position of the new node in the Hash Space using the hash function. We then find the index of the smallest key greater than the position in the sorted `keys` array using binary search. This index will be where the key and the new Storage node will be placed in `keys` and `nodes` array respectively.\n\n```py\ndef add_node(self, node: StorageNode) -> int:\n    \"\"\"add_node function adds a new node in the system and returns the key\n    from the hash space where it was placed\n    \"\"\"\n\n    # handling error when hash space is full.\n    if len(self._keys) == self.total_slots:\n        raise Exception(\"hash space is full\")\n\n    key = hash_fn(node.host, self.total_slots)\n\n    # find the index where the key should be inserted in the keys array\n    # this will be the index where the Storage Node will be added in the\n    # nodes array.\n    index = bisect(self._keys, key)\n\n    # if we have already seen the key i.e. node already is present\n    # for the same key, we raise Collision Exception\n    if index > 0 and self._keys[index - 1] == key:\n        raise Exception(\"collision occurred\")\n\n    # Perform data migration\n\n    # insert the node_id and the key at the same `index` location.\n    # this insertion will keep nodes and keys sorted w.r.t keys.\n    self.nodes.insert(index, node)\n    self._keys.insert(index, key)\n\n    return key\n```\n\n## Removing a new node from the system\nWhen there is a need to scale down and remove an existing node from the system, we\n\n - find the position of the node to be removed from the Hash Space\n - populate the node to the right with data that was associated with the node to be removed\n - remove the node from the Hash Space\n\nWhen a node is removed from the system it only affects the files associated with the node itself. All other files and associations remain unaffected, thus minimizing the amount of data to be migrated and mapping required to be changed.\n\n![Removing a new node from the system - Consistent Hashing](https://user-images.githubusercontent.com/4745789/82751261-b0e9e400-9dd3-11ea-81ee-3fd3f0187857.png)\n\nFrom the illustration above, we see when node K is removed from the system, we change the associations of files associated with node K to the node that lies to its immediate right i.e. node E. Thus the only files affected and needs migration are the ones associated with node K.\n\nIn order to implement this at a low-level using `nodes` and `keys` array, we get the index where the node K lies in the `keys` array using binary search. Once we have the index we remove the key from the `keys` array and Storage Node from the `nodes` array present on that index.\n\n```py\ndef remove_node(self, node: StorageNode) -> int:\n    \"\"\"remove_node removes the node and returns the key\n    from the hash space on which the node was placed.\n    \"\"\"\n\n    # handling error when space is empty\n    if len(self._keys) == 0:\n        raise Exception(\"hash space is empty\")\n\n    key = hash_fn(node.host, self.total_slots)\n\n    # we find the index where the key would reside in the keys\n    index = bisect_left(self._keys, key)\n\n    # if key does not exist in the array we raise Exception\n    if index >= len(self._keys) or self._keys[index] != key:\n        raise Exception(\"node does not exist\")\n\n    # Perform data migration\n\n    # now that all sanity checks are done we popping the\n    # keys and nodes at the index and thus removing the presence of the node.\n    self._keys.pop(index)\n    self.nodes.pop(index)\n\n    return key\n```\n\n## Associating an item to a node\nNow that we have seen how consistent hashing helps in keeping data migration, during scale-ups and scale-downs, to a bare minimum; it is time we see how to efficiently we can find the \"node to the right\" for a given item. The operation to find the association has to be super fast and efficient as it is something that will be invoked for every single read and write that happens on the system.\n\nTo implement this at low-level we again take leverage of binary search and perform this operation in `O(log(n))`. We first pass the item to the hash function and fetch the position where the item is hashed in the hash space. This position is then binary searched in the `keys` array to obtain the index of the first key which is greater than the position (obtained from the hash function). if there are no keys greater than the position, in the `keys` array, we circle back and return the 0th index. The index thus obtained will be the index of the storage node in the `nodes` array associated with the item.\n\n```py\ndef assign(self, item: str) -> str:\n    \"\"\"Given an item, the function returns the node it is associated with.\n    \"\"\"\n    key = hash_fn(item, self.total_slots)\n\n    # we find the first node to the right of this key\n    # if bisect_right returns index which is out of bounds then\n    # we circle back to the first in the array in a circular fashion.\n    index = bisect_right(self._keys, key) % len(self._keys)\n\n    # return the node present at the index\n    return self.nodes[index]\n```\n\nThe source code with the implementation of Consistent Hashing in Python could be found at [github.com/arpitbbhayani/consistent-hashing](https://github.com/arpitbbhayani/consistent-hashing/blob/master/consistent-hashing.ipynb).\n\n# Conclusion\nConsistent Hashing is one of the most important algorithms to help us horizontally scale and manage any distributed system. The algorithm does not only work in sharded systems but also finds its application in load balancing, data partitioning, managing server-based sticky sessions, routing algorithms, and many more. A lot of databases owe their scale, performance, and ability to handle the humongous load to Consistent Hashing.\n\n# References\n - [Hash Functions - Wikipedia](https://en.wikipedia.org/wiki/Hash_function)\n - [Consistent Hashing - Wikipedia](https://en.wikipedia.org/wiki/Consistent_hashing)\n - [Consistent Hashing - Stanford](https://web.stanford.edu/class/cs168/l/l1.pdf)\n - [Consistent Hashing and RandomTrees](https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf)\n - [Dynamo: Amazon\u2019s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)\n",
    "similar": [
      "fully-persistent-arrays",
      "lfu",
      "leaderless-replication",
      "persistent-data-structures-introduction"
    ]
  },
  {
    "id": 50,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fractional-cascading",
    "title": "Fractional Cascading - Speeding up Binary Searches",
    "description": "The performance of binary search when applied on k lists independently can be improved using bridges and the technique is called Fractional Cascading. Fractional Cascading also sees its application in Geometric Data Structures, Segment Trees, and Databases.",
    "gif": "https://media.giphy.com/media/1k5k3J5K3BywQOrpNA/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/82142043-352deb80-9857-11ea-9801-878d771248da.png",
    "released_at": "2020-05-10",
    "total_views": 486,
    "body": "Binary Search is an algorithm that finds the position of a target value in a sorted list. The algorithm exploits the fact that the list is sorted, and is devised such that is does not have to even look at all the `n` elements, to decide if a value is present or not. In the worst case, the algorithm checks the `log(n)` number of elements to make the decision.\n\nBinary Search could be tweaked to output the position of the target value, or return the position of the smallest number greater than the target value i.e. position where the target value should have been present in the list.\n\nThings become more interesting when we have to perform an iterative binary search on `k` lists in which we find the target value in each of the `k` lists independently. The problem statement could be formally defined as\n\n> Given `k` lists of `n` sorted integers each, and a target value `x`, return the position of the smallest value greater than or equal to `x` in each of the `k` lists. Preprocessing of the list is allowed before answering the queries.\n\n# The naive approach - k binary searches\nThe expected output of this iterative search is the position of the smallest value greater than or equal to `x` in each of the `k` lists. This is a classical Binary Search problem and hence in this approach, we fire `k` binary searches on `k` lists for the target value `x` and collect the positions.\n\n![k-binary searches](https://user-images.githubusercontent.com/4745789/81492614-dbf21500-92b6-11ea-9f75-29eb3522186f.png)\n\nPython has an in-built module called `bisect` which has the function `bisect_left` which outputs the smallest value greater than or equal to `x` in a list which is exactly what we need to output and hence python-based solution using this k-binary searches approach could be \n\n```py\nimport bisect\n\narr = [\n    [21, 54, 64, 79, 93],\n    [27, 35, 46, 47, 72],\n    [11, 44, 62, 66, 94],\n    [10, 35, 46, 79, 83],\n]\n\ndef get_positions_k_bin_search(x): \n    return [bisect.bisect_left(l, x) for l in arr]\n\n>>> get_positions_k_bin_search(60)\n[2, 4, 2, 3]\n```\n\n## Time and Space Complexity\nEach of the `k` lists have size `n` and we know the time complexity of performing a binary search in one list of `n` elements is `O(log(n))`. Hence we deduce that the time complexity of this k-binary searches approach is `O(klog(n))`. \n\nThis approach does not really require any additional space and hence the space complexity is `O(1)`.\n\nThe k-binary searches approach is thus super-efficient on space but not so much on time. Hence by trading some space, we could reap some benefits on time, and on this exact principle, the unified binary search approach is based.\n\n# Unified binary search\nThis approach uses some extra space, preprocessing and computations to reduce search time. The preprocessing actually involves precomputing the positions of all elements in all the `k` lists. This precomputation enables us to perform just one binary search and get the required precalculated positions in one go.\n\n## Preprocess\nThe preprocessing is done in two phases; in the first phase, we compute a position tuple for each element and associate it with the same. In phase two of preprocessing, we create an auxiliary list containing all the elements of all the lists, on which we then perform a binary search for the given target value.\n\n### Computing position tuple for each element\nPosition tuple is a `k` item tuple where every `i`th item denotes the position of the associated element in the `i`th list. We compute this tuple by performing a binary search on all the `k` lists treating the element as the target value.\n\nFrom the example above, the position tuple of 4th element in the 4th list i.e 79 will be `[3, 5, 4, 3]` which denotes its position in all 4 lists. In list 1, 79 is at index `3`, in list 2, 79 is actually out of bounds but would be inserted at index `5` hence the output `5`, we could also have returned a value marking out of bounds, like `-2`, in list 3, 79 is not present but the smallest number greater than 79 is 94 and which is at index `4` and in list 4, 79 is present at index `3`. This makes the position tuple for 79 to be `[3, 5, 4, 3]`.\n\nGiven a 2-dimensional array `arr` we compute the position tuple for an element `(i, j)` by performing a binary search on all `k` lists as shown in python code below\n\n```py\nfor i, l in enumerate(arr):\n    for j, e in enumerate(l):\n        for k, m in enumerate(arr):\n            positions[i][j][k] = int(bisect.bisect_left(m, e))\n```\n\n### Creating a huge list\nOnce we have all the position tuples and they are well associated with the corresponding elements, we create an auxiliary list of size `k * n` that holds all the elements from all the `k` lists. This auxiliary list is again kept sorted so that we could perform a binary search on it.\n\n## Working\nGiven a target value, we perform a binary search in the above auxiliary list and get the smallest element greater than or equal to this target value. Once we get the element, we now get the associated position tuple. This position tuple is precisely the position of the target element in all the `k` lists. Thus by performing one binary search in this huge list, we are able to get the required positions.\n\n![unified binary search](https://user-images.githubusercontent.com/4745789/81492609-ca107200-92b6-11ea-8fdf-999852f4d9b1.png)\n\n## Complexity\nWe are performing binary search just once on the list of size `k * n` hence, the time complexity of this approach is `O(log(kn))` which is a huge improvement over the k-binary searches approach where it was `O(klog(n))`.\n\nThis approach, unlike k-binary searches, requires an additional space of `O(k.kn)` since each element holds `k` item position tuple and there are in all `k * n` elements.\n\nFractional cascading is something that gives us the best of both worlds by creating bridges between the lists and narrowing the scope of binary searches on subsequent iterations. Let's find out how.\n\n# Fractional Cascading\nFractional cascading is a technique through which we speed up the iterative binary searches by creating bridges between the lists. The main idea behind this approach is to dampen the need to perform binary searches on subsequent lists after performing the search on one.\n\nIn the k-binary searches approach, we solved the problem by performing `k` binary searches on `k` lists. If, after the binary search on the first list, we would have known a range within which the target value was present in the 2nd list, we would have limited our search within that subset which helps us save a bunch of computation time. The bridges, defined above, provides us with a shortcut to reach the subset of the other list where that target value would be present.\n\n![Fractional Cascading the Idea](https://user-images.githubusercontent.com/4745789/81495324-241c3200-92cd-11ea-9d7d-9c9b0911071b.png)\n\nFractional cascading is just an idea through which we could speed up binary searches, implementations vary with respect to the underlying data. The bridges could be implemented using pointers, graphs, or array indexes.\n\n## Preprocess\nPreprocessing is a super-critical step in fractional cascading because it is responsible for speeding up the iterative binary searches. Preprocessing actually sets up all the bridges from all the elements from one list to the range of items in the lower list where the element could be found. These bridges then cascade to all the lists on the lower levels.\n\n### Create Auxiliary Lists\nThe first step in pre-processing is to create `k` auxiliary lists from `k` original lists. These lists are created bottom-up which means lists on the lower levels are created first - `M(i+1)` is created before `M(i)`. An auxiliary list `M(i)` is created as a sorted list of elements of the original list `L(i)` and half of the previously created auxiliary list `M(i+1)`. The half elements of auxiliary lists are chosen by picking every other element from it.\n\n![Create Auxiliary Lists](https://user-images.githubusercontent.com/4745789/81494077-8112ea80-92c3-11ea-9416-bb2422334744.png)\n\nBy picking every other element from lower-level lists, we fill the gaps in value ranges in the original list `L(i)`, giving us a uniform spread of values across all auxiliary lists. Another advantage of picking every other element is that we eradicate the need for performing binary searches on subsequent lists altogether. Now we only need to perform a binary search for list `M(0)` and for every other list, we only need to check the element we reach via the bridge and an element before that - a constant time comparison.\n\n### Position tuples\nA position tuple for Fractional Cascading is a 2 item tuple, associated with each element of the auxiliary list, where the first item is the position of the element in the original list on the same level - serving as the required position - and the second element is the position of the element in the auxiliary list on the lower level - serving as the bridge from one level to another.\n\n![Create position pointerss](https://user-images.githubusercontent.com/4745789/89712282-83adda80-d9ad-11ea-888f-2b20d839252f.png)\n\nThe position tuple for each element in the auxiliary array can be created by doing a binary search on the original list and the auxiliary list on the lower level. Given a 2-dimensional array `arr` and auxiliary lists `m_arr` we compute the position tuples for element `(i, j)` by performing a binary search on all `k` original and auxiliary lists as shown in python code below\n\n```py\nfor i, l in enumerate(m_arr):\n    for j, m in enumerate(m_arr[i]):\n        pointers[i][j] = [\n            bisect.bisect_left(arr[i], m_arr[i][j]),\n            bisect.bisect_left(m_arr[i+1], m_arr[i][j]),\n        ]\n```\n\n## Fractional Cascading in action\nWe start by performing a binary search on the first auxiliary list `M(0)` from which we get the element corresponding to the target value. The position tuple for this element contains the position corresponding to the original list `L(0)` and bridge that will take us to the list `M(1)`. Now when we move to the list `M(1)` through the bridge and have reached the index `b`.\n\nSince auxiliary lists have uniform range spread, because of every other element being promoted, we are sure that the target value should be checked again at the index `b` and `b - 1`; because if the value was any lower it would have been promoted and bridged to other value and hence the trail we trace would be different from what we are tracing now.\n\nOnce we know which of the `b` and `b-1` index to pick (depending on the values at the index and the target value) we add the first item of the position tuple to the solution set and move the auxiliary list on the lower level and the entire process continues.\n\nOnce we reach the last auxiliary list and process the position tuple there and pick the element, our solution set contains the required positions and we can stop the iteration.\n\n```py\ndef get_locations_fractional_cascading(x): \n    locations = []\n\n    # the first and only binary search on the auxiliary list M[0]\n    index = bisect.bisect_left(m_arr[0], x)\n\n    # loc always holds the required location from the original list on same level\n    # next_loc holds the bridge index on the lower level\n    loc, next_loc = pointers[0][index]\n\n    # adding loc to the solution\n    locations.append(loc)\n\n    for i in range(1, len(m_arr)):\n        # we check for the element we reach through the bridge\n        # and the one before it and make the decision to go with one\n        # depending on the target value.\n        if x <= m_arr[i][next_loc-1]:\n            loc, next_loc = pointers[i][next_loc-1]\n        else:\n            loc, next_loc = pointers[i][next_loc]\n\n        # adding loc to the solution\n        locations.append(loc)\n\n    # returning the required locations\n    return locations\n```\n\nThe entire working code could be found here [github.com/arpitbbhayani/fractional-cascading](https://github.com/arpitbbhayani/fractional-cascading/blob/master/fractional-cascading.ipynb)\n\n## Time and space complexity\nIn Fractional Cascading, we perform binary search once on the auxiliary list `M(0)` and then make `k` constant comparisons for each of the subsequent levels; hence the time complexity is `O(k + log(n))`.\n\nThe auxiliary lists could at most contain all the elements from the original list plus `1/2 |L(n)| + 1/4 |L(n-1)| + 1/8 |L(n-2)| + ...` which is less than all elements of the original list combined. Thus the total size of the auxiliary list cannot exceed twice the original lists. The position tuple for each of the elements is also a constant 2 item tuple thus the space complexity of Fractional Cascading is `O(kn)`.\n\nThus Fractional Cascading has time complexity very close to the k-binary searches approach with a very low space complexity as compared to the unified binary searches approach; thus giving us the best of both worlds.\n\n## Fractional Cascading in real world\nFractional Cascading is used in [FD-Trees](http://pages.cs.wisc.edu/~yinan/fdtree.html) which are used in databases to address the asymmetry of read-write speeds in tree indexing on the flash disk. Fractional cascading is typically used in [range search](https://en.wikipedia.org/wiki/Range_searching) data structures like [Segment Trees](https://en.wikipedia.org/wiki/Segment_tree) to speed up lookups and filters.\n\n# References\n - [Fractional Cascading - Wikipedia](https://en.wikipedia.org/wiki/Fractional_cascading)\n - [Fractional Cascading - Original Paper by Bernard Chazelle and Leonidas Guibas](https://www.cs.princeton.edu/~chazelle/pubs/FractionalCascading1.pdf)\n - [Fractional Cascading Revisited](http://www.cse.iitd.ernet.in/~ssen/journals/frac.pdf)\n - [Fractional Cascading - Brown University](http://cs.brown.edu/courses/cs252/misc/resources/lectures/pdf/notes08.pdf)\n",
    "similar": [
      "genetic-knapsack",
      "flajolet-martin",
      "slowsort",
      "1d-terrain"
    ]
  },
  {
    "id": 53,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fsm",
    "title": "Building Finite State Machines with Python Coroutines",
    "description": "The most intuitive way of building and implementing Finite State Machines is by using Python Coroutines and in this article, we find how and why.",
    "gif": "https://media.giphy.com/media/KhdQ2Ia3FJuKs/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/79684359-3ebf3600-824e-11ea-97cc-0f06c2919aeb.png",
    "released_at": "2020-04-19",
    "total_views": 7625,
    "body": "Finite State Machine is a mathematical model of computation that models a sequential logic. FSM consists of a finite number of states, transition functions, input alphabets, a start state and end state(s). In the field of computer science, the FSMs are used in designing Compilers, Linguistics Processing, Step workflows, Game Design, Protocols Procedures (like TCP/IP), Event-driven programming, Conversational AI and many more.\n\nTo understand what a finite machine is, we take a look at Traffic Signal. Finite State Machine for a Traffic Signal is designed and rendered below. `Green` is the start/initial state, which upon receiving a trigger moves to `Yellow`, which, in turn, upon receiving a trigger, transitions to `Red`. The `Red` then circles back to `Green` and the loop continues.\n\n![traffic signal fsm](https://user-images.githubusercontent.com/4745789/79678813-d572ff00-821c-11ea-8437-b4a3b7fd1a60.png)\n\nAn FSM must be in exactly one of the finite states at any given point in time and then in response to an input, it receives, the machine transitions to another state. In the example above, the traffic signal is exactly in one of the 3 states - `Green`, `Yellow` or `Red`. The transition rules are defined for each state which defines what sequential logic will be played out upon input.\n\nImplementing an FSM is crucial to solving some of the most interesting problems in Computer Science and in this article, we dive deep into modeling a Finite State Machine using Python coroutines.\n\n# Python Coroutines\nBefore diving into the implementation we take a detour and look at what Generators and Coroutines are, how they keep implementation intuitive and fits into the scheme of things.\n\n## Generators\nGenerators are **resumable functions** that yield values as long as someone, by calling `next` function, keeps asking it. If there are no more values to yield, the generator raises a `StopIteration` exception.\n\n```py\ndef fib():\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a+b\n```\n\nThe `yield` statement is where the magic happens. Upon reaching the `yield` statement, the generator function execution is paused and the yielded value is returned to the caller and the caller continues its execution. The flow returns back to the generator when the caller function asks from the next value. Once the next value is requested by calling `next` (explicitly or implicitly), the generator function resumes from where it left off i.e. `yield` statement.\n\n```py\n>>> fgen = fib()\n>>> [next(fgen) for _ in range(10)]\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n```\n\nUsing a Fibonacci generator is memory-efficient as now we need not compute a lot of Fibonacci numbers and hold them in memory, in a list, rather the requesting process could ask for as many values as it needs and the generator would keep on yielding values one by one.\n\n## Coroutines\nCoroutines, just like generators, are resumable functions but instead of generating values, they consume values on the fly. The working of it is very similar to the generator and again the `yield` statement is where the magic happens. When a coroutine is paused at the `yield` statement, we could send the value it using `send` function and the value could be used using the assignment operator `=` on `yield` as shown below\n\n```py\ndef grep(substr):\n    while True:\n        line = yield\n        if substr in line:\n            print(f\"found {substr}\")\n```\n\nIn the example above, we wrote a simple `grep` utility that checks for a substring in a given stream of text. When the coroutine `grep` is paused at the `yield` statement, using the `send` function, we send the text to it, and it will be referenced by the variable `line`. The coroutine then continues its execution to check if `substr` is in `line` or not. Once the flow reaches the `yield` statement again, the coroutine pauses and waits for the caller to `send` it a new value.\n\nNote that, this is not a thread that keeps on running and hogging the CPU. It is just a function whose execution is paused at the `yield` statement waiting for the value; the state is persisted and the control is passed back to the caller. When resumed the coroutine starts from the same state where it left off.\n\n> Before sending the value to a coroutine we need to \"prime\" it so that the flow reaches the yield statement and the execution is paused while waiting for the value to be sent.\n\n```py\n>>> g = grep(\"users/created\")\n>>> next(g)  # priming the generator\n>>>\n>>> g.send(\"users/get api took 1 ms.\")\n>>> g.send(\"users/created api took 3 ms.\")\nfound users/created\n>>> g.send(\"users/get api took 1 ms.\")\n>>> g.send(\"users/created api took 4 ms.\")\nfound users/created\n>>> g.send(\"users/get api took 1 ms.\")\n```\n\nIn the function invocations above we see how we could keep on sending the text to the coroutine and it continues to spit out if it found the given substring `users/created` in the text. This ability of coroutine to pause the execution and accept input on the fly helps us model FSM in a very intuitive way.\n\n# Building a Finite State Machine\nWhile building FSMs, the most important thing is how we decide to model and implement states and transition functions. States could be modeled as Python Coroutines that run an infinite loop within which they accept the input, decides the transition and updates the current state of the FSM. The transition function could be as simple as a bunch of `if` and `elif` statements and in a more complex system it could be a decision function.\n\nTo dive into low-level details, we build an FSM for the regular expression `ab*c`, which means if the given string matches the regex then the machine should end at the end state, only then we say that the string matches the regex.\n\n![fsm for ab*c](https://user-images.githubusercontent.com/4745789/79634655-84fe9180-8189-11ea-9b94-f9ee563394bf.png)\n\n## State\nFrom the FSM above we model the state `q2` as\n\n```py\ndef _create_q2():\n    while True:\n        # Wait till the input is received.\n        # once received store the input in `char`\n        char = yield\n\n        # depending on what we received as the input\n        # change the current state of the fsm\n        if char == 'b':\n            # on receiving `b` the state moves to `q2`\n            current_state = q2\n        elif char == 'c':\n            # on receiving `c` the state moves to `q3`\n            current_state = q3\n        else:\n            # on receiving any other input, break the loop\n            # so that next time when someone sends any input to\n            # the coroutine it raises StopIteration\n            break\n```\n\nThe coroutine runs as an infinite loop in which it waits for the input token at the `yield` statement. Upon receiving the input, say `b` it changes the current state of FSM to `q2` and on receiving `c` changes the state to `q3` and this precisely what we see in the FSM diagram.\n\n## FSM Class\nTo keep things encapsulated we will define a class for FSM which holds all the states and maintains the current state of the machine. It will also have a method called `send` which reroutes the received input to the current state. The current state upon receiving this input makes a decision and updates the `current_state` of the FSM as shown above.\n\nDepending on the use-case the FSM could also have a function that answers the core problem statement, for example, does the given line matches the regular expression? or is the number divisible by 3?\n\nThe FSM class for the regular expression `ab*c` could be modeled as\n\n```py\nclass FSM:\n    def __init__(self):\n        # initializing states\n        self.start = self._create_start()\n        self.q1 = self._create_q1()\n        self.q2 = self._create_q2()\n        self.q3 = self._create_q3()\n        \n        # setting current state of the system\n        self.current_state = self.start\n\n        # stopped flag to denote that iteration is stopped due to bad\n        # input against which transition was not defined.\n        self.stopped = False\n\n    def send(self, char):\n        \"\"\"The function sends the curretn input to the current state\n        It captures the StopIteration exception and marks the stopped flag.\n        \"\"\"\n        try:\n            self.current_state.send(char)\n        except StopIteration:\n            self.stopped = True\n        \n    def does_match(self):\n        \"\"\"The function at any point in time returns if till the current input\n        the string matches the given regular expression.\n\n        It does so by comparing the current state with the end state `q3`.\n        It also checks for `stopped` flag which sees that due to bad input the iteration of FSM had to be stopped.\n        \"\"\"\n        if self.stopped:\n            return False\n        return self.current_state == self.q3\n\n    ...\n    \n    @prime\n    def _create_q2(self):\n        while True:\n            # Wait till the input is received.\n            # once received store the input in `char`\n            char = yield\n\n            # depending on what we received as the input\n            # change the current state of the fsm\n            if char == 'b':\n                # on receiving `b` the state moves to `q2`\n                self.current_state = self.q2\n            elif char == 'c':\n                # on receiving `c` the state moves to `q3`\n                self.current_state = self.q3\n            else:\n                # on receiving any other input, break the loop\n                # so that next time when someone sends any input to\n                # the coroutine it raises StopIteration\n                break\n    ...\n\n```\n\nSimilar to how we have defined the function `_create_q2` we could define functions for the other three states `start`, `q1` and `q3`. You can find the complete FSM modeled at [arpitbbhayani/fsm/regex-1](https://github.com/arpitbbhayani/fsm/blob/master/regex-1.ipynb)\n\n## Driver function\nThe motive of this problem statement is to define a function called `grep_regex` which tests a given `text` against the regex `ab*c`. The function will internally create an instance of `FSM` and will pass the stream of characters to it. Once all the characters are exhausted, we invoke `does_match` function on the FSM which suggests if the given `text` matches the regex `ab*c` or not.\n\n```py\ndef grep_regex(text):\n    evaluator = FSM()\n    for ch in text:\n        evaluator.send(ch)\n    return evaluator.does_match()\n\n>>> grep_regex(\"abc\")\nTrue\n\n>>> grep_regex(\"aba\")\nFalse\n```\n\n> The entire execution is purely running sequential - and that's because of Coroutines. All states seem to run in parallel but they that are all executing in one thread concurrently. The coroutine of the current state is executing while all others are suspended on their corresponding `yield` statements. When a new input is sent to the coroutine it is unblocked completes its execution, changes the current state of FSM and pauses itself on its `yield` statement again.\n\n# More FSMs\nWe have seen how intuitive it is to build Regular expression FSMs using Python Coroutines, but if our hypothesis is true things should equally intuitive when we are implementing FSMs for other use cases and here we take a look at two examples and see how a state is implemented in each\n\n## Divisibility by 3\nHere we build an FSM that tells if a given stream of digits of a number is divisible by 3 or not. The state machine is as shown below.\n\n![div3](https://user-images.githubusercontent.com/4745789/79641628-564ae000-81b6-11ea-9c84-147cae3a30a6.png)\n\nWe can implement the state `q1` as a coroutine as\n\n```py\ndef _create_q1(self):\n    while True:\n        digit = yield\n        if  digit in [0, 3, 6, 9]:\n            self.current_state = self.q1\n        elif  digit in [1, 4, 7]:\n            self.current_state = self.q2\n        elif  digit in [2, 5, 8]:\n            self.current_state = self.q0\n```\n\nWe can see the similarity between the coroutine implementation and the transition function for a state. The entire implementation of this FSM can be found at [arpitbbhayani/fsm/divisibility-by-3](https://github.com/arpitbbhayani/fsm/blob/master/divisibility-by-3.ipynb).\n\n## SQL Query Validator\nHere we build an FSM for a SQL Query Validator, which for a given a SQL query tells if it is a valid SQL query or not. The FSM for the validator that covers all the SQL queries will be massive, hence we just deal with the subset of it where we support the following SQL queries\n\n```\nSELECT * from TABLE_NAME;\nSELECT column, [...columns] from TABLE_NAME;\n```\n\n![fsm for sql query validator](https://user-images.githubusercontent.com/4745789/79635523-1c1a1800-818f-11ea-8afe-fe8065b55791.png)\n\nWe can implement the state `explicit_cols` as a coroutine as\n\n```py\ndef _create_explicit_cols(self):\n    while True:\n        token = yield\n        if token == 'from':\n            self.current_state = self.from_clause\n        elif token == ',':\n            self.current_state = self.more_cols\n        else:\n            break\n```\n\nAgain the coroutine through which the state is implemented is very similar to the transition function of the state keeping things intuitive. The entire implementation of this FSM can be found at [arpitbbhayani/fsm/sql-query-validator](https://github.com/arpitbbhayani/fsm/blob/master/sql-query-validator.ipynb).\n\n# Conclusion\nEven though this may not be the most efficient way to implement and build FSM but it is the most intuitive way indeed. The edges and state transitions, translate well into `if` and `elif` statements or the decision functions, while each state is being modeled as an independent coroutine and we still do things in a sequential manner. The entire execution is like a relay race where the baton of execution is being passed from one coroutine to another.\n\n# References and Readings\n\n - [Finite State Machines - Wikipedia](https://en.wikipedia.org/wiki/Finite-state_machine)\n - [Finite State Machines - Brilliant.org](https://brilliant.org/wiki/finite-state-machines/)\n - [FSM Applications](https://web.cs.ucdavis.edu/~rogaway/classes/120/spring13/eric-applications.pdf)\n - [What Are Python Coroutines?](https://realpython.com/lessons/what-are-python-coroutines/)\n - [How to Use Generators and yield in Python](https://realpython.com/introduction-to-python-generators/)\n",
    "similar": [
      "recursion-visualizer",
      "constant-folding-python",
      "python-prompts",
      "python-iterable-integers"
    ]
  },
  {
    "id": 54,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "bayesian-average",
    "title": "Better Ranking using Bayesian Average",
    "description": "Ranking a list of movies, products, books or even restaurants is tricky and in this article, we find what works for such a rating system and the math behind it.",
    "gif": "https://media.giphy.com/media/dJ4vNQ7r72pb4nDhN5/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/78990379-ebfdc400-7b53-11ea-88b7-cd64e39eabf6.png",
    "released_at": "2020-04-12",
    "total_views": 526,
    "body": "Almost every single website, app or platform on the internet has some sort of rating system in place. Whenever you purchase a product or use a service, you are asked to rate it on a scale, say 1 to 5. The platform then uses this data to generate a score and build a ranking system around it. The score is the measure of quality for each product or service. By surfacing the most quality content on top of the list, the platform tries to up their sales and ensure better engagement with their users.\n\nComing up with an aggregated score is not an easy thing - we need to crunch a million ratings and then see that the score is, in fact, the true measure of quality. If it isn't then it would directly affect the business. Today we discuss how we should define this score in a rating based system; spoiler alert! the measure is called [Bayesian Average](https://en.wikipedia.org/wiki/Bayesian_average).\n\nTo keep things simple we define the problem statement as\n\n> Given the ratings, on a scale of 1 to 5, that users give to a movie, we generate a score that is a measure of how good a movie is which then helps us get the top 10 movies of all time.\n\nWe will use the [MovieLens Dataset](https://grouplens.org/datasets/movielens/) to explore various scoring functions in this article. In the dataset, we get user ratings for each movie and the ratings are made on a scale of 1 to 5.\n\n# Generating the score\nThe score we generate for each item should be proportional to the quality quotient which means higher the score, superior is the item. Hence we say that the score of an item is the function of all the `m` ratings that it received.\n\n![score function](https://user-images.githubusercontent.com/4745789/79067003-cf8b9400-7cd9-11ea-9b16-c1875933725a.png)\n\n## Arithmetic Mean\nThe simplest and the most common strategy to compute this aggregated score for an item is by taking an [Arithmetic Mean (average)](https://en.wikipedia.org/wiki/Arithmetic_mean) of all the ratings it received. Hence for each item we sum all the ratings that it received and divide it by its cardinality, giving us the average value.\n\n![arithmetic mean](https://user-images.githubusercontent.com/4745789/79049349-b387e400-7c40-11ea-9adf-b40aa377778f.png)\n\n### Issues with arithmetic mean\nThe arithmetic mean falls apart pretty quickly. Let's say there is an item with just one rating of 5 on 5, the item would soar high on the leaderboard ranking. But does it deserve that place? probably not. Because of low cardinality (number of ratings), the score (and hence the rank) of the item will fluctuate more and will not give a true measure of quality.\n\nWith the movie dataset, we are analyzing here are the top 10 movies ranked using Arithmetic Mean.\n\n![top 10 movies arithmetic mean](https://user-images.githubusercontent.com/4745789/79049814-58a3bc00-7c43-11ea-980e-a12ae10379f7.png)\n\nThrough this measure, all of the top 10 movies have a score of 5 (out of 5) and all of them have just 1 rating. Are these really the top 10 movies of all time? Probably not. Looks like we need to do a lot better than the Arithmetic Mean.\n\n## Cumulative Rating\nTo remedy the issue with Arithmetic Mean, we come up with an approach of using Cumulative Rating as the scoring function hence instead of taking the average we only consider the sum of all the ratings as the final score.\n\n![cumulative rating as scoring function](https://user-images.githubusercontent.com/4745789/79050470-e1245b80-7c47-11ea-824b-ecd5cbb40912.png)\n\nCumulative Rating actually does a pretty decent job, it makes popular items with a large number of ratings bubble up to the top of the leaderboard. When we rank the movies in our dataset using Cumulative Ratings we get the following as the top 10.\n\n![top 10 movies through cunulative rating](https://user-images.githubusercontent.com/4745789/79050520-2d6f9b80-7c48-11ea-8e48-1c12fbbc0a88.png)\n\nThe top 10 movies now feature Shawshank Redemption, Forrest Gump, Pulp Fiction, etc. which are in fact considered as the top movies of all times. But is Cumulative Rating fool-proof?\n\n### Issues with cumulative rating\nCumulative Rating favors high cardinality. Let's say there is an extremely poor yet popular item `A` that got 10000 ratings of 1 on 5, and there is another item `B` which is very good but it got 1000 rating of 5 on 5 Cumulative Rating thus gives a score of 10000 * 1 = 10000 to item `A` and 1000 * 5 = 5000 to item `B`, but `B` clearly is far superior of an item than `A`.\n\nAnother issue with Cumulative Rating is the fact that it generates an unbounded score. Ideally, any ranking system expects a normalized bounded score so that the system becomes predictable and consistent.\n\nWe established that Cumulative Rating is better than Arithmetic Mean but it is not fool-proof and that's where the Bayesian Average comes to the rescue.\n\n# The Bayesian Average\nBayesian Average computes the mean of a population by not only using the data residing in the population but also considering some outside information, like a pre-existing belief - a derived property from the dataset, for example, prior mean.\n\n## The intuition\nThe major problem with Arithmetic Mean as the scoring function was how unreliable it was when we had a low number of data points (cardinality) to compute the score. Bayesian Average plays a part here by introducing pre-belief into the scheme of things.\n\nWe start by defining the requirements of our scoring function\n - for an item with a fewer than average number of ratings - the score should be around the system's arithmetic mean\n - for an item with a substantial number of ratings - the score should be the item's arithmetic mean\n - as the number of ratings that an item receives increases, the score should gradually move from system's mean to item's mean\n\nBy ensuring the above we neither prematurely promote nor demote an item in the leaderboard. An item is given a fair number of chances before its score falls to its own Arithmetic mean. This way we use the prior-belief - System's Arithmetic mean, to make the scoring function more robust and fair to all items.\n\n## The formula\nGiven the intuition and scoring rules, we come up with the following formula\n\n![bayesian average formula for rating system](https://user-images.githubusercontent.com/4745789/79066315-ab798400-7cd4-11ea-804b-e5e8479824b2.png)\n\nIn the above formula, `w` indicates the weight that needs to be given the item's Arithmetic Mean `A` while `S` represents the System's Arithmetic Mean. If `A` and `S` are bounded then the final score `s` will also be bounded in the same range, thus solving the problem with Cumulative Rating.\n\nSuppose the number of ratings that an item `i` receives is denoted by `m` and the average number of ratings that any item in the system receives is denoted by `m_avg`, we define the requirements of weight `w` as follows\n\n - `w` is bounded in the range [0, 1]\n - `w` should be monotonically increasing\n - `w` should be close to 0 when `m` is close to 0\n - `w` should reach 0.5 when number `m` reaches `m_avg`\n - `w` tries to get closer to 1 as `m` increases\n\nFrom the above requirements, it is clear that `w` is acting like a knob which decides in what proportions we should consider an item's mean versus the system's mean. As `w` increases we tilt more towards item's mean. We define the `w` as\n\n![weight function for bayesian average](https://user-images.githubusercontent.com/4745789/79066802-4162de00-7cd8-11ea-8068-467ce3305810.png)\n\nWhen we combine all of the above we get the final scoring function as\n\n![scoring function for bayesian average rating system](https://user-images.githubusercontent.com/4745789/79066769-111b3f80-7cd8-11ea-979e-6437334ccbba.png)\n\nOne of the most important properties of Bayesian Average is the fact that the pre-existing belief acts as support which oversees that the score does not fluctuate too abruptly and it smoothens with more number of ratings.\n\n## Applying Bayesian Average to movies dataset\nAfter applying the above mentioned Bayesian Average scoring function to our Movie dataset, we get the following movies as top 10\n\n![top 10 movies by Basysian Average](https://user-images.githubusercontent.com/4745789/79066961-686ddf80-7cd9-11ea-87d7-7e7e582ab9ac.png)\n\nPretty impressive list! The list contains almost all the famous movies that we all think make the cut. Bayesian average thus provides a bounded score that is a measure of the quality of the item, by using prior-belief i.e. system's mean.\n\n## Analyzing how Bayesian Average changes the rank\nNow that we have seen that the Bayesian Average is, in fact, an excellent way to rank items in a rating system, we find how the rank of an item changes as it receives more ratings. Below we plot the change in the percentile rank of the movies: [Kingsman](https://en.wikipedia.org/wiki/Kingsman:_The_Secret_Service), [Logan](https://en.wikipedia.org/wiki/Logan_(film)) and [The Scorpion King](https://en.wikipedia.org/wiki/The_Scorpion_King).\n\n![Kingsman position with ratings](https://user-images.githubusercontent.com/4745789/79068414-53e31480-7ce4-11ea-884a-90e7aee326d8.png)\n\n![Logan rankings](https://user-images.githubusercontent.com/4745789/79068443-7f65ff00-7ce4-11ea-9623-6f03451235de.png)\n\n![Scorpion King](https://user-images.githubusercontent.com/4745789/79068524-35c9e400-7ce5-11ea-8726-d1836a6b9c23.png)\n\nWe observe that the fluctuations in percentile rank are more in the case of Arithmetic Mean. Sometimes even after receiving a good number of reviews, the rank fluctuates sharply. In the case of Bayesian Average after an initial set of aberrations, the rank smoothens and converges.\n\n# A note on Bayesian Average\nBayesian Average is not a fixed formula that we have seen above, but it is a concept where we make the scoring function \"smoother\" by using a pre-existing belief as support. Hence we can tweak the formula as per our needs, or use multiple prior beliefs and still it would classify as a Bayesian Average.\n\n# References\n\n - [Bayesian Average](https://en.wikipedia.org/wiki/Bayesian_average)\n - [How not to sort by Average Rating](https://evanmiller.org/how-not-to-sort-by-average-rating.html)\n - [How to Rank (Restaurants)](http://www.ebc.cat/2015/01/05/how-to-rank-restaurants/)\n - [Of Bayesian average and star ratings](https://fulmicoton.com/posts/bayesian_rating/)\n - [Code to compute Bayesian Average](https://github.com/arpitbbhayani/ranking-on-ratings/blob/master/movie-lens.ipynb)\n",
    "similar": [
      "fork-bomb",
      "fast-and-efficient-pagination-in-mongodb",
      "efficient-way-to-stop-an-iterating-loop",
      "the-weird-walrus"
    ]
  },
  {
    "id": 56,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "idf",
    "title": "Inverse Document Frequency",
    "description": "TF-IDF is extensively used in search engines and in various document classification and clustering techniques. Instead of taking the formula by the word, we take a detour and dive deep into the better half of it and find its connection with Probability, the role it plays in document relevance and the intuition behind it.",
    "gif": "https://media.giphy.com/media/3ornjWIRSzXEw61KH6/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/76207579-10e4db80-6224-11ea-91ba-b67359125156.png",
    "released_at": "2020-03-06",
    "total_views": 417,
    "body": "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is one of the most popular measures that quantify document relevance for a given term. It is extensively used in [Information Retrieval](https://en.wikipedia.org/wiki/Information_retrieval) (ex: Search Engines), Text Mining and even for text-heavy Machine Learning use cases like Document Classification and Clustering. Today we explore the better half of TF-IDF and see its connection with Probability, the role it plays in TF-IDF and even the intuition behind it.\n\nInverse Document Frequency (IDF) is a measure of term rarity which means it quantifies how rare the term, in the corpus, really is (document collection); higher the IDF, rarer the term. A rare term helps in discriminating, distinguishing and ranking documents and it contributes more information to the corpus than what a more frequent term (like `a`, `and` and `the`) does.\n\nThe IDF was heuristically proposed in the paper \"[A statistical interpretation of term specificity and its application in retrieval](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.8343&rep=rep1&type=pdf)\" (Sp\u00e4rck Jones, 1972) and was originally called Term Specificity.\n\n# The intuition behind IDF\nIn order to quantify the term rarity, the heuristic says we need to give higher weight to the term that occurs in fewer documents and lesser weights to the frequent ones. Thus this measure (weight) `w` of the term is __inversely proportional__ to the number of documents in which it is present (called Document Frequency) - and hence the measure is called Inverse Document Frequency.\n\n![IDF Inversely proportional to Document Frequency](https://user-images.githubusercontent.com/4745789/76211536-85237d00-622c-11ea-82f5-c0b655634839.png)\n\nAny function that adheres to the requirement of being inversely proportional to the document frequency i.e. a decreasing function, would do the job; it may not yield optimality but could be used as an IDF for some use cases. Some decreasing functions that could be used as an IDF for some use cases are shown below\n\n![Decreasing functions](https://user-images.githubusercontent.com/4745789/76213296-63c49000-6230-11ea-9d24-94ce048732bc.png)\n\nThe more frequent words, like `a`, `and` and `the` will lie on the far right of the plot and will have a smaller value of IDF.\n\n# The most common IDF\nA widely adapted IDF measure that performs better in most use cases is defined below\n\n![common idf function](https://user-images.githubusercontent.com/4745789/76239930-633fef80-6258-11ea-823a-2011c04a1e97.png)\n\nwhere\n\n - `N` is the number of documents in the corpus\n - `df(t)` is the number of documents that has an occurrence of the term `t`\n\nIf we plot the above IDF function against the document frequency we get a nice smooth decreasing function as shown below. For lower values of X i.e. Document Frequency we see the IDF is very high as it suggests a good discriminator and as the Document Frequency increases the plot smoothly descends and reaches 0 for `df(t) = N`.\n\n![IDF Graph](https://user-images.githubusercontent.com/4745789/76215908-ae94d680-6235-11ea-8e50-498aae029ea2.png)\n\n# IDF and Probability\nWhat would be the probability that a random document picked from a corpus of `N` documents contains the term `t`? The answer to this question is the fraction of documents, out of N, that contains the term `t` and, as seen above, this is its Document Frequency.\n\n![Probability](https://user-images.githubusercontent.com/4745789/76229411-29ff8380-6248-11ea-9518-6cbc4c6947da.png)\n\nThe fraction inside the logarithm in the IDF function is oddly similar to the above probability, in fact, it is the inverse of probability defined above. Hence we could redefine IDF using this probability as \n\n![IDF as probability](https://user-images.githubusercontent.com/4745789/76229704-a09c8100-6248-11ea-9960-0cfd5f45dcce.png)\n\nBy defining IDF as a probability, we could now estimate the true IDF of a term by observing a random sample instead and computing IDF on this sampled data.\n\n# IDF of conjunction\nComputing IDF for a single term is fine but what happens when we have multiple terms? How would that fare out? This is a very common use case in Information Retrieval where we need to rank documents for a given search query, and the search query more often than not contains multiple terms.\n\nFor finding IDF of multiple terms in conjunction we make an assumption - the occurrences of terms are statistically independent and because of this the equation below holds true\n\n![Probability of conjunction](https://user-images.githubusercontent.com/4745789/76239792-2d9b0680-6258-11ea-8da2-56899540cab0.png)\n\nGiven this, we could derive the IDF of two terms in conjunction as follows\n\n![IDF derivation](https://user-images.githubusercontent.com/4745789/76232475-c2980280-624c-11ea-8a3a-37d17704a221.png)\n\nFrom the derivation above we see that the IDF of conjunction is just the summation of IDF of individual terms. Extending this to search engines we could say that the score of a document for a given search query is the summation of scores that document gets for individual terms of the query.\n\n> Note: IDF on conjunction could be made much more complex by not assuming statistical independence.\n\n# Other measures of IDF\nThe decreasing functions we see in the first section of this article were just some examples of possible IDF functions. But there are IDF functions that are not just examples but are also used in some specific use cases and some of them are:\n\n![Other IDF Measures](https://user-images.githubusercontent.com/4745789/76232678-0db21580-624d-11ea-864c-1094559e0790.png)\n\nMost of the IDF functions only differ in the bounds they produce for a given range of document frequency. The plots of 3 IDF functions namely - Common IDF, Smooth IDF, and Probabilistic IDF, are shown below:\n\n![Plot IDF Functions](https://user-images.githubusercontent.com/4745789/76232756-2de1d480-624d-11ea-81cb-8d29109bd594.png)\n\nBy observing the plots of 3 different IDF functions it becomes clear that we should use Probabilistic IDF function when we want to penalize a term that occurs in more than 50% of document by giving it a negative weight; and use a Smooth IDF when we do not want a bounded IDF value and not `undefined` (for `DF(t) = 0`) and `0` (for `DF(t) = N`) as such values ruins a function where IDF is multiplied with some other scalar (like Term Frequency).\n\nSimilarly, we could define our own IDF function by deciding when and how the penalty to be applied and defining the parameters accordingly.\n\n# Role of IDF in TF-IDF\nTF-IDF suggests how important a word is to a document in a collection (corpus). It helps search engines identify what it is that makes a given document special for a given query. It is defined as the product of Term Frequency (number of occurrences of the term in the document) and Inverse Document Frequency.\n\nFor the document to have a high TF-IDF score (high relevance) it needs to have high term frequency and a high inverse document frequency (i.e. low document frequency) of the term. Thus IDF primarily downscales the frequent occurring of common words and boosts the infrequent words with high term frequency.\n\n# References\nThis article is mostly based on the wonderful paper [Understanding Inverse Document Frequency: On theoretical arguments for IDF](https://pdfs.semanticscholar.org/8397/ab573dd6c97a39ff4feb9c2d9b3c1e16c705.pdf) by Stephen Robertson.\n\nOther references:\n\n - [TF-IDF - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n - [Inverse Document Frequency and the Importance of Uniqueness](https://moz.com/blog/inverse-document-frequency-and-the-importance-of-uniqueness)\n\nImages used in other measures of IDF are taken from [Wikipedia page of TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "similar": [
      "jaccard-minhash",
      "inheritance-c",
      "recursion-visualizer",
      "constant-folding-python"
    ]
  },
  {
    "id": 59,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "rule-30",
    "title": "Pseudorandom Number Generation using Cellular Automata - Rule 30",
    "description": "Generating pseudorandom numbers is an interesting problem in Computer Science. In this article, we dive deep into an algorithm for generating pseudorandom numbers using Rule 30 of Cellular Automaton.",
    "gif": "https://media.giphy.com/media/26uflDxU6cEhrhmUg/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/74463952-b07aac80-4eb8-11ea-8d8e-6f286767ec7e.png",
    "released_at": "2020-02-14",
    "total_views": 1011,
    "body": "A pseudorandom number generator produces numbers deterministically but they seem aperiodic (random) most of the time for most use-cases. The generator accepts a seed value (ideally a true random number) and starts producing the sequence as a function of this seed and/or a previous number of the sequence. These are Pseudorandom (not truly random) because if seed value is known they can be determined algorithmically. True random numbers are hardware generated or generated from blood volume pulse, atmospheric pressure, thermal noise, quantum phenomenon, etc.\n\nThere are lots of [techniques](https://en.wikipedia.org/wiki/List_of_random_number_generators#Pseudorandom_number_generators_(PRNGs)) to generate Pseudorandom numbers, namely: [Blum Blum Shub algorithm](https://en.wikipedia.org/wiki/Blum_Blum_Shub), [Middle-square method](https://en.wikipedia.org/wiki/Middle-square_method), [Lagged Fibonacci generator](https://en.wikipedia.org/wiki/Lagged_Fibonacci_generator), etc. Today we dive deep into [Rule 30](https://en.wikipedia.org/wiki/Rule_30) that uses a controversial science called [Cellular Automaton](https://en.wikipedia.org/wiki/Cellular_automaton). This method passes many standard tests for randomness and was used in [Mathematica](https://www.wolfram.com/mathematica/online/) for generating random integers.\n\n# Cellular Automaton\nBefore we dive into Rule 30, we will spend some time understanding [Cellular Automaton](https://en.wikipedia.org/wiki/Cellular_automaton). A Cellular Automaton is a discrete model consisting of a regular grid, of any dimension, with each cell of the grid having a finite number of states and a neighborhood definition. There are rules that determine how these cells interact and transition into the next generation (state). The rules are mostly mathematical/programmable functions that depend on the current state of the cell and its neighborhood.\n\n![Cellular Automata](https://user-images.githubusercontent.com/4745789/74360178-9bcfe300-4dea-11ea-8c87-91005e89c881.png)\n\nIn the above Cellular Automaton, each cell has 2 finite states `0` (shown in red), `1` (shown in black). Each cell transitions into the next generation by XORing the state values of its 8 neighbors. The first generation (initial state) of the grid is allocated at random and the state transitions, of the entire grid, is as below\n\n![Cellular Automata Demo](https://media.giphy.com/media/J27aUn6QIWZFnVWzEB/giphy.gif)\n\nCellular Automata was originally conceptualized in the 1940s by [Stanislaw Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam) and [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann); it finds its application in computer science, mathematics, physics, complexity science, theoretical biology and microstructure modeling. In the 1980s, [Stephen Wolfram](https://en.wikipedia.org/wiki/Stephen_Wolfram) did a systematic study of one-dimensional cellular automata (also called elementary cellular automata) on which Rule 30 is based.\n\n# Rule 30\nRule 30 is an elementary (one-dimensional) cellular automaton where each cell has two possible states `0` (shown in red) and `1` (shown in black). The neighborhood of a cell is its two immediate neighbors, one on its left and other on right. The next state (generation) of the cell depends on its current state and the state of its neighbors; the transition rules are as illustrated below\n\n![Rule 30](https://user-images.githubusercontent.com/4745789/74396927-78805480-4e39-11ea-8349-b6774d05a600.png)\n\nThe above transition rules could be simplified as `left XOR (central OR right)`.\n\nWe visualize Rule 30 in a 2-dimensional grid where each row represents one generation (state). The next generation (state) of the cells is computed and populated in the row below. Each row contains a finite number of cells which \"wraps around\" at the end.\n\n![Rule 30 in action](https://media.giphy.com/media/d9YuURGwsOD8qVt8uE/giphy.gif)\n\nThe above pattern emerges from an initial state (row 0) in a single cell with state 1 (shown as black) surrounded by cells with state 0 (red). The next generation (as seen in row 1) is computed using the rule chart mentioned above. The vertical axis represents time and any horizontal cross-section of the image represents the state of all the cells in the array at a specific point in the pattern's evolution.\n\n![Chaos in Rule 30](https://user-images.githubusercontent.com/4745789/74433188-f1a59900-4e85-11ea-970d-c60af22568ea.png)\n\nAs the pattern evolves, frequent red triangles of varying sizes pop up but the structure as a whole has no recognizable pattern. The above snapshot of the grid was taken at a random point of time and we could observe chaos and aperiodicity. This property is exploited to generate pseudorandom numbers.\n\n## Pseudorandom Number Generation\nAs established earlier, Rule 30 is exhibits aperiodic and chaotic behavior and hence it produces complex, seemingly random patterns from simple, well-defined rules. To generate random numbers from using Rule 30 we use the center column and pick a batch of `n` random bits and form the required `n` bit random number from it. The next random number is built using the next `n` bits from the column.\n\n![Pseudorandom Number Rule 30](https://user-images.githubusercontent.com/4745789/74435575-c2455b00-4e8a-11ea-835b-ca5f722dae9e.png)\n\nIf we always start from the first row, the sequence of the numbers we generate will always be predictable - which is not what we want. To make things pseudorandom, we take a random seed value (ex: current timestamp) and skip that number of bits and then pick batches of `n` and build random numbers.\n\n> The pseudorandom numbers generated using Rule 30 are not cryptographically secure but are suitable for simulation as long as we do not use bad seed like `0`.\n\nOne major advantage of using Rule 30 to generate pseudorandom numbers is that we could generate multiple random numbers in parallel by picking multiple columns to batch `n` bits each at random. A sample 8-bit random integer sequence generated using this method with seed `0` is `220`, `197`, `147`, `174`, `117`, `97`, `149`, `171`, `240`, `241`, etc.\n\nThe seed value could also be used as the initial state (row 0) for Rule 30 and random numbers are then simply the `n` bits batches picked from the center column starting from row 0. This approach is more efficient but is heavily dependent on the quality of seed value, as a bad seed value could make things extremely predictable. A demonstration of this approach could be found on [Wolfram Cloud Demonstration Page](https://demonstrations.wolfram.com/UsingRule30ToGeneratePseudorandomRealNumbers/).\n\n## Rule 30 in the real world\n\nRule 30 is also seen in nature, on the shell of code snail species [Conus textile](https://en.wikipedia.org/wiki/Conus_textile). The [Cambridge North railway station](https://en.wikipedia.org/wiki/Cambridge_North_railway_station#Facilities) is decorated with architectural panels displaying the evolution of Rule 30.\n\n# Conclusion\nIf you found Rule 30 interesting I urge you to write your own simulation of using [p5 library](https://p5js.org/); you could keep it generic enough to so that the program could generate patterns for different rules like 90, 110, 117, etc. The patterns generated using these rules are quite interesting. If you want, you could things to the next level and extend rule to work in 3 dimensions and see how patterns evolve. I believe programming is fun when it is visual.\n\nIt is exciting when two seemingly unrelated fields, Cellular Automata and Cryptography, come together and create something wonderful. Although this algorithm is not widely used anymore, because of more efficient algorithms, it urges us to be creative in using Cellular Automata in more ways than one. This article is first in the series of Cellular Automata, so stay tuned and watch this space for more.\n",
    "similar": [
      "efficient-way-to-stop-an-iterating-loop",
      "publish-python-package-on-pypi",
      "jaccard-minhash",
      "fast-and-efficient-pagination-in-mongodb"
    ]
  },
  {
    "id": 61,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "isolation-forest",
    "title": "Isolation Forest Algorithm for Anomaly Detection",
    "description": "Anomaly detection is an age-old problem and in this article, we dive deep into an unsupervised algorithm, Isolation Forest, that beautifully exploits the characteristics of anomalies. Instead of profiling normal points and labeling others as anomalies, the algorithm is actually is tuned to detect anomalies.",
    "gif": "https://media.giphy.com/media/xGdvlOVSWaDvi/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/73280907-34743980-4215-11ea-89f0-eac4a71df6e5.png",
    "released_at": "2020-01-28",
    "total_views": 1134,
    "body": "Anomaly detection is identifying something that could not be stated as \"normal\"; the definition of \"normal\" depends on the phenomenon that is being observed and the properties it bears. In this article, we dive deep into an unsupervised anomaly detection algorithm called [Isolation Forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf). This algorithm beautifully exploits the characteristics of anomalies, keeping it independent of data distributions making the approach novel.\n\n### Characteristics of anomalies\nSince anomalies deviate from normal, they are few in numbers (minority) and/or have attribute values that are very different from those of normal. The paper nicely puts it as **few and different**. These characteristics of anomalies make them more susceptible to isolation than normal points and form the guiding principle of the Isolation Forest algorithm.\n\n# The usual approach for detecting anomalies\nThe existing models train to see what constitutes \"normal\" and then label everything that does not conform to this definition as anomalies. Almost every single algorithm has its own way of defining a normal point/instance; some do it through statistical methods, some use classification or clustering but in the end, the process remains the same - define normal and filter out everything else.\n\n### The issue with the usual approach\n\nThe usual methods are not optimized to detect anomalies, instead, they are optimized to find normal instances, because of which the result of anomaly detection either contains too many false positives or might detect too few anomalies.\nMany of these methods are computationally complex and hence suit low dimensional and/or small-sized data.\n\nIsolation Forest algorithm addresses both of the above concerns and provides an efficient and accurate way to detect anomalies.\n\n# The algorithm\nNow we take a go through the algorithm, and dissect it stage by stage and in the process understand the math behind it. Fasten your seat belts, it's going to be a bumpy ride.\n\n## The core principle\nThe core of the algorithm is to \"isolate\" anomalies by creating decision trees over random attributes. The random partitioning produces noticeable shorter paths for anomalies since\n\n - fewer instances (of anomalies) result in smaller partitions\n - distinguishable attribute values are more likely to be separated in early partitioning\n\nHence, when a forest of random trees collectively produces shorter path lengths for some particular points, then they are highly likely to be anomalies.\n\n![Decision tree splits for normal points and anomalies](https://user-images.githubusercontent.com/4745789/73243800-804fc000-41ce-11ea-826f-14cbc407af99.png)\n\nThe diagram above shows the number of splits required to isolate a normal point and an anomaly. Splits, represented through blue lines, happens at random on a random attribute and in the process building a decision tree. The number of splits determines the level at which the isolation happened and will be used to generate the anomaly score.\n\nThe process is repeated multiple times and we note the isolation level for each point/instance. Once the iterations are over, we generate an anomaly score for each point/instance, suggesting its likeliness to be an anomaly. The score is a function of the average level at which the point was isolated. The top `m` gathered on the basis of the score, are labeled as anomalies.\n\n## Construction of decision tree\nThe decision tree is constructed by splitting the sub-sample points/instances over a split value of a randomly selected attribute such that the instances whose corresponding attribute value is smaller than the split value goes left and the others go right, and the process is continued recursively until the tree is fully constructed. The split value is selected at random between the minimum and maximum values of the selected attribute.\n\nThere are two types of node in the decision tree\n\n### Internal Node\n\nInternal nodes are non-leaf and contain the split value, split attribute and pointers to two child sub-trees. An internal node is always a parent to two child sub-trees making the entire decision tree a proper binary tree.\n\n### External Node\n\nExternal nodes are leaf nodes that could not be split further and reside at the bottom of the tree. Each external node will hold the size of the un-built subtree which is used to calculate the anomaly score.\n\n![Decision tree with internal and external nodes](https://user-images.githubusercontent.com/4745789/73272711-d5a8c300-4208-11ea-9bb7-80894312f16c.png)\n\n## Why sub-sampling helps\nThe Isolation Forest algorithm works well when the trees are created, not from the entire dataset, but from a sub-sampled data set. This is very different from almost all other techniques where they thrive on data and demands more of it for greater accuracy. Sub-sampling works wonder in this algorithm because normal instances can interfere with the isolation process by being a little closer to the anomalies.\n\n![Importance of sub-sampling in Isolation Forest](https://user-images.githubusercontent.com/4745789/73296518-df91ec80-422f-11ea-8c6b-2a2fcbf8afc8.png)\n\nThe image above shows how sub-sampling actually makes a clear separation between normal points and anomalies. In the original dataset, we see that normal points and very close to anomalies making detection tougher and inaccurate (with a lot of false negatives). Because of sub-sampling, we could see a clear separation of anomalies and normal instances. This makes the entire process of anomaly detection efficient and accurate.\n\n### Optimizing decision tree construction\n\nSince anomalies are susceptible to isolation and have a tendency to reside closer to the root of the decision tree, we construct the decision tree till it reaches a certain height `max_height` and not split points further. This height is the height post which we are (almost) sure that there could not be any anomalies.\n\n```py\ndef construct_tree(X, current_height, max_height):\n  \"\"\"The function constructs a tree/sub-tree on points X.\n\n  current_height: represents the height of the current tree to\n    the root of the decision tree.\n  max_height: the max height of the tree that should be constructed.\n\n  The current_height and max_height only exists to make the algorithm efficient\n  as we assume that no anomalies exist at depth >= max_height.\n  \"\"\"\n  if current_height >= max_height:\n    # here we are sure that no anomalies exist hence we\n    # directly construct the external node.\n    return new_external_node(X)\n\n  # pick any attribute at random.\n  attribute = get_random_attribute(X)\n\n  # for set of inputs X, for the tree we get a random value\n  # for the chosen attribute. preferably around the median.\n  split_value = get_random_value(max_value, min_value)\n\n  # split X instances based on `split_values` into Xl and Xr\n  Xl = filter(X, lambda x: X[attribute] < split_value)\n  Xr = filter(X, lambda x: X[attribute] >= split_value)\n\n  # build an internal node with its left subtree created from Xl\n  # and right subtree created from Xr, recursively.\n  return new_internal_node(\n    left=construct_tree(Xl, current_height + 1, max_height),\n    right=construct_tree(Xr, current_height + 1, max_height),\n    split_attribute=attribute,\n    split_value=split_value,\n  )\n```\n\n## Constructing the forest\nThe process of tree construction is repeated multiple times and each time we pick a random sub-sample and construct the tree. There are no strict rules to determine the number of iterations, but in general, we could say the more the merrier. The sub-sampling count is also a parameter and could change depending on the data set.\n\nThe pseudocode for forest construction is as follows\n\n```py\ndef construct_forest(X, trees_count, subsample_count):\n  \"\"\"The function constructs a forest from given inputs/data points X.\n  \"\"\"\n  forest = []\n  for i in range(0, trees_count):\n    # max_height is in fact the average height of the tree that would be\n    # constructed from given points. This acts as max_height for the\n    # construction because we are only interested in data points that have\n    # shorter-than-average path lengths, as those points are more likely\n    # to be anomalies.\n    max_height = math.ceil(math.log2(subsample_count))\n\n    # create a sample with cardinality of `subsample_count` from X\n    X_sample = get_sample(X, subsample_count)\n\n    # construct the decision tree from the sample\n    tree = construct_tree(X_sample, 0, max_height)\n\n    # add the tree to the forest\n    forest.append(tree)\n\n  return forest\n```\n\nWhile constructing the tree we pass `max_height` as `log2(nodes_count)` as that is the average height of a proper binary tree that could be constructed from `nodes_count` number of nodes. Since anomalies reside closer to the root node it is highly unlikely that any anomaly will isolate after the tree has reached height `max_height`. This helps us save a lot of computation and tree construction making it computationally and memory efficient.\n\n## Scoring the anomalies\nEvery anomaly detection algorithm has to score its data points/instances and quantify the confidence the algorithm has on its potential anomalies. The generated anomaly score has to be bounded and comparable. In Isolation Forest, that fact that anomalies always stay closer to the root, becomes our guiding and defining insight that will help us build a scoring function. The anomaly score will a function of path length which is defined as\n\n> Path Length `h(x)` of a point `x` is the number of edges `x` traverses from the root node.\n\nAs the maximum possible height of the tree grows by order of `n`, the average height grows by `log(n)` - this makes normalizing of the scoring function a little tricky. To remedy this we use the insights from the structure of the decision tree. The decision tree has two types of nodes internal and external such that external has no child while internal is a parent to exactly two nodes - which means the decision tree is a proper binary tree and hence we conclude\n\n> The average path length `h(x)` for external node termination is the same as the average path length of unsuccessful search in BST.\n\nIn a BST, an unsuccessful search always terminates at a `NULL` pointer and if we treat external node of the decision tree as `NULL` of BST, then we could say that average path length of external node termination is same as average path length of unsuccessful search in BST (constructed only from internal nodes of the decision tree), and it is given by\n\n![BST unsuccessful search estimation](https://user-images.githubusercontent.com/4745789/73191802-198ac200-414e-11ea-9500-039483b6e780.png)\n\nwhere `H(i)` is the [harmonic number](https://en.wikipedia.org/wiki/Harmonic_number) and it can be estimated by `ln(i) + 0.5772156649` ([Euler\u2013Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant)). `c(n)` is the average of path length `h(x)` given `n`, we use it to normalize `h(x)`.\n\n_To understand the derivation in detail refer to the references at the end of this article._\n\nThe anomaly score of an instance `x` is defined as\n\n![scoring function](https://user-images.githubusercontent.com/4745789/73192432-075d5380-414f-11ea-86dc-ae6acda7b7d4.png)\n\nwhere `E(h(x))` is the average path length (average of `h(x)`) from a collection of isolation trees. From the scoring function defined above, we could deduce that if\n\n - the score is very close to 1, then they are definitely anomalies\n - the score is much smaller than 0.5, then they are quite safe to be regarded as normal instances, and\n - all the instances return around 0.5, then the entire sample does not really have any distinct anomaly\n\n## Evaluating anomalies\n\nIn the evaluation stage, an anomaly score is derived from the expected path length `E(h(x))` for each test instance. Using `get_path_length` function (pseudocode below), a single path length `h(x)` is calculated by traversing through the decision tree.\n\nIf iteration terminates at an external node where `size > 1` then the return value is `e` (number of edges traversed till current node) plus an adjustment `c(size)`, estimated from the formula above. This adjustment is for the unbuilt decision tree (for efficiency) beyond the max height. When `h(x)` is obtained for each node of each tree, an anomaly score is produced by computing `s(x, sample_size)`. Sorting instances by the score `s` in descending order and getting top `m` will yield us `m` anomalies.\n\n```py\ndef get_path_length(x, T, e):\n  \"\"\"The function returns the path length h(x) of an instance\n  x in tree `T`.\n\n  here e is the number of edges traversed from the root till the current\n  subtree T.\n  \"\"\"\n  if is_external_node(T):\n    # when T is the root of an external node subtree\n    # we estimate path length and return.\n\n    # here c is the function which estimates the average path length\n    # for external node termination.\n    return e + c(len(T))\n\n  # T is the root of an internal node then we\n  if x[T.split_attribute] < T[split_value]:\n    # instance x may lie in left subtree\n    return get_path_length(x, T.left, e + 1)\n  else:\n    # instance x may lie in right subtree\n    return get_path_length(x, T.right, e + 1)\n```\n\n## References for BST unsuccessful search estimation\n - [IIT KGP, Algorithms, Lecture Notes - Page 7](https://cse.iitkgp.ac.in/~pb/algo-1-pb-10.pdf)\n - [What is real big-O of search in BST?](https://www.cs.csustan.edu/~john/classes/previous_semesters/cs3100_datastructures/2000_04_Fall/Examples/Trees/averageSearchInBST.html)\n - [CMU CMSC 420: Lecture 5 - Slide 13](https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/trees.pdf)\n - [CISE UFL: Data Structures, Algorithms, & Applications - 1st Proof](https://www.cise.ufl.edu/~sahni/dsaac/public/exer/c18/e47.htm)\n\n\n# Conclusion\nThe isolation forest algorithm thrives on sub-sampled data and does not need to build the tree from the entire data set; it works well with sub-sampled data. While constructing the tree, we need not build tree taller than `max_height` (very cheap to compute), making it low on memory footprint. Since the algorithm does not depend on computationally expensive operations like distance or density calculation, it executes really fast. The training stage has a linear time complexity with a low constant and hence could be used in a real-time online system.\n\nI hope this article helped you to understand Isolation Forest, an unsupervised anomaly detection algorithm. I stumbled upon this through an engineering [blog](https://lambda.grofers.com/anomaly-detection-using-isolation-forest-80b3a3d1a9d8) of [Grofers](https://grofers.com/). This algorithm was very interesting to me because of its novel approach and hence I dived deep into it. FYI: In 2018, Isolation Forest was extended by [Sahand Hariri, Matias Carrasco Kind, Robert J. Brunner](https://arxiv.org/pdf/1811.02141.pdf). I have not read the Extended Isolation Forest algorithm but have definitely added it to my reading list. I recommend that if you liked this algorithm you should definitely give the extended version a skim.\n",
    "similar": [
      "slowsort",
      "flajolet-martin",
      "phi-accrual",
      "morris-counter"
    ]
  },
  {
    "id": 69,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "how-sleepsort-helped-me-understand-concurrency-in-golang",
    "title": "Sleepsort and Concurrency in Golang",
    "description": "Understanding concurrency in any programming language is tricky let alone Golang; hence to get my hands dirty the first thing I usually implement is sleepsort.",
    "gif": "https://media.giphy.com/media/QmJ3e9So5M9NdNkOGo/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63222328-203b0380-c1c4-11e9-9dd0-34c4bd9d1c6b.png",
    "released_at": "2017-07-16",
    "total_views": 460,
    "body": "For me learning concurrency have always been tricky; Every language has a different way to handle/emulate concurrency, for example, old languages like Java uses [Threads](https://docs.oracle.com/javase/tutorial/essential/concurrency/) and modern languages like NodeJS and Python uses something called as [event loops](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/) for its asynchronous IO which is there to make IO based things concurrent.\n\nRecently I started diving deep into concurrency in [Golang](https://golang.org/) and I wanted to start with a good `\"Hello World\"` program for it. This time I thought of taking an unconventional way to write my first concurrent program. Going through various examples over the Internet I could not find anything that made it fun. I suddenly recalled [Sleepsort](http://www.geeksforgeeks.org/sleep-sort-king-laziness-sorting-sleeping/) and it was the ideal way (fun + new = <3) to learn concurrency.\n\n### The Concept\nFor people who do not know what Sleep Sort is, the basic goes something like this:\nspin `n` threads/co-routine (or whatever concurrent element the language has) for `n` numbers (to sort) and for each number `x` wait for time proportional to `x` (lets say `x` seconds) and then print/collect the number.\n\n### Implementation in Go\nThis is a very basic Implementation of Sleep Sort in Golang using Go Routines and [WaitGroup](https://golang.org/pkg/sync/#WaitGroup).\n\n```go\n// prints a number of sleeping for n seconds\nfunc sleepAndPrint(x int, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\n\t// Sleeping for time proportional to value\n\ttime.Sleep(time.Duration(x) * time.Millisecond)\n\n\t// Printing the value\n\tfmt.Println(x)\n}\n\n// Sorts given integer slice using sleep sort\nfunc Sort(numbers []int) {\n\tvar wg sync.WaitGroup\n\n\t// Creating wait group that waits of len(numbers) of go routines to finish\n\twg.Add(len(numbers))\n\n\tfor _, x := range numbers {\n\t\t// Spinning a Go routine\n\t\tgo sleepAndPrint(x, &wg)\n\t}\n\n\t// Waiting for all go routines to finish\n\twg.Wait()\n}\n```\n\nI have published the code in a [Github Repository](https://github.com/arpitbbhayani/go-sleep-sort). Feel\nfree to fork and play around with it.\n\n### What else can you do with it\nI encourage you to try it out, and trust me it is really fun to learn concurrency through this; Apart from running the basic sleep sort you should also try to do/learn with it. For example,\n\nConcurrency essentials\n - Go Channels for inter go-routine communication\n - Mutex for synchronization making things routine-safe\n\nYou can also try to\n - collect the elements in a slice, in place of printing\n - make Sleep Sort handle negative numbers too\n - sort the numbers in descending order\n\nIf you find any interesting way to learn concurrency or any new use case here, please post a comment below.\nI would love to know them.\n",
    "similar": [
      "setting-up-graphite-grafana-using-nginx-on-ubuntu",
      "udemy-sql-taxonomy",
      "setting-up-graphite-using-nginx-on-ubuntu",
      "the-weird-walrus"
    ]
  }
]