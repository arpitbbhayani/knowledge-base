[
  {
    "id": 1,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "genetic-knapsack",
    "title": "Genetic Algorithm to solve the Knapsack Problem",
    "description": "The 0/1 Knapsack Problem has a pseudo-polynomial run-time complexity. In this essay, we look at an approximation algorithm inspired by genetics that finds a high-quality solution to it in polynomial time.",
    "gif": "https://media.giphy.com/media/3orieTU2tBje4i5SzS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/156910296-48167efc-dc08-4f26-88b8-7037fdfc6ae3.png",
    "released_at": "2022-03-07",
    "total_views": 441,
    "body": "The Knapsack problem is one of the most famous problems in computer science. The problem has been studied since 1897, and it refers to optimally packing the bag with valuable items constrained on the max weight the bag can carry. The [0/1 Knapsack Problem](https://en.wikipedia.org/wiki/Knapsack_problem) has a pseudo-polynomial run-time complexity. In this essay, we look at an approximation algorithm inspired by genetics that finds a high-quality solution to it in polynomial time.\n\n# The Knapsack Problem\n\nThe Knapsack problem is an optimization problem that deals with filling up a knapsack with a bunch of items such that the value of the Knapsack is maximized. Formally, the problem statement states that, given a set of items, each with a weight and a value, determine the items we pack in the knapsack with a constrained maximum weight that the knapsack can hold, such the the total value of the knapsack is maximum.\n\n## Optimal solution\n\nThe solution to the Knapsack problem uses Recursion with memoization to find the optimal solution. The algorithm covers all possible cases by considering every item picked and not picked. We remember the optimal solution of the subproblem in a hash table, and we reuse the solution instead of recomputing.\n\n```python\nmemo = {}\n\ndef knapsack(W, w, v, n):\n    if n == 0 or W == 0:\n        return 0\n  \n    # if weight of the nth item is more than the weight\n    # available in the knapsack the skip it\n    if (w[n - 1] > W):\n        return knapsack(W, w, v, n - 1)\n    \n    # Check if we already have an answer to the sunproblem\n    if (W, n) in memo:\n        return memo[(W, n)]\n  \n    # find value of the knapsack when the nth item is picked\n    value_picked = v[n - 1] + knapsack(W - w[n - 1], w, v, n - 1)\n\n    # find value of the knapsack when the nth item is not picked\n    value_notpicked = knapsack(W, w, v, n - 1)\n\n    # return the maxmimum of both the cases\n    # when nth item is picked and not picked\n    value_max = max(value_picked, value_notpicked)\n\n    # store the optimal answer of the subproblem\n    memo[(W, n)] = value_max\n\n    return value_max\n```\n\n## Run-time Complexity\n\nThe above solution runs with a complexity of `O(n.W)` where `n` is the total number of items and `W` is the maximum weight the knapsack can carry. Although it looks like a polynomial-time solution, it is indeed a _pseudo-polynomial_ time solution.\n\nGiven that the computation needs to happen by a factor of `W`, the maximum times the function will execute will be proportional to the max value of `W` which will be `2^m` where `m` is the number of bits required to represent the weight `W`.\n\nThis makes the complexity of above solution `O(n.2^m)`. The numeric value of the input `W` is exponential in the input length, which is why a pseudo-polynomial time algorithm does not necessarily run in polynomial time with respect to the input length.\n \nThis raises a need for a polynomial-time solution to the Knapsack problem that need not generate an optimal solution; instead, a good quick, high-quality approximation is also okay. Genetic Algorithm inspired by the Theory of Evolution does exactly this for us.\n\n# Genetic Algorithm\n\n[Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm) is a class of algorithms inspired by the process of natural selection. As per Darwin's Theory of Evolution, the fittest individuals in an environment survive and pass their traits on to the future generations while the weak characteristics die long the journey.\n\nWhile solving a problem with a Genetic Algorithm, we need to model it to undergo evolution through natural operations like Mutation, Crossover, Reproduction, and Selection. Genetic Algorithms help generate high-quality solutions to optimization problems, like [Knapsack](https://en.wikipedia.org/wiki/Knapsack_problem), but do not guarantee an optimal solution. Genetic algorithms find their applications across aircraft design, financial forecasting, and cryptography domains.\n\n## The Genetic Process\n\nThe basic idea behind the Genetic Algorithm is to start with some candidate _Individuals_ (solutions chosen at random) called _Population_. The initial population is the zeroth population, which is responsible for the spinning of the _First Generation_. The First Generation is also a set of candidate solutions that evolved from the zeroth generation and is expected to be better.\n\nTo generate the next generation, the current generation undergoes natural selection through mini-tournaments, and the ones who are fittest reproduce to create offspring. The offspring are either copies of the parent or undergo crossover where they get a fragment from each parent, or they undergo an abrupt mutation. These steps mimic what happens in nature.\n\n![The Genetic Flow - Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156874170-608cd9a4-6241-4882-b123-658d14a64c89.png)\n\nCreating one generation after another continues until we hit a termination condition. Post which the fittest solution is our high-quality solution to the problem. We take the example of the Knapsack problem and try to solve it using a Genetic Algorithm.\n\n# Knapsack using Genetic Algorithm\n\nSay, we have a knapsack that can hold 15kg of weight at max. We have 4 items `A`, `B`, `C`, and `D`; having weights of 7kg, 2kg, 1kg, and 9kg; and value `$5`, `$4`, `$7`, and `$2` respectively. Let's see how we can find a high-quality solution to this Knapsack problem using a Genetic Algorithm and, in the process, understand each step of it.\n\n![Knapsack Problem](https://user-images.githubusercontent.com/4745789/156769708-68ae14b5-4ccd-484b-b5be-7445ef3526cb.png)\n\n> The above example is taken from the Computerphile's [video](https://www.youtube.com/watch?v=MacVqujSXWE) on this same topic.\n\n## Individual Representation\n\nAn _Individual_ in the Genetic Algorithm is a potential solution. We first find a way to represent it such that it allows us to evolve. For our knapsack problem, this representation is pretty simple and straightforward; every individual is an `n`-bit string where each bit correspond to an item from the `n` items we have.\n\n![Individual Representation Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156770627-e6cc63e9-72b7-4afa-a968-60e994963a26.png)\n\nGiven that we have 4 items, every individual will be represented by a 4-bit string and the `i`th position in this string will denote if we picked that item in our knapsack or not, depending on if the bit is `1` or `0` respectively.\n\n## Picking Individuals\n\nNow that we have our _Individual_ representation done, we pick a random set of Individuals that would form our initial _population_. Every single individual is a potential solution to the problem. Hence for the Knapsack problem, from a search space of 2^n, we pick a few individuals randomly.\n\nThe idea here is to evolve the population and make them fitter over time. Given that the search space is exponential, we use evolution to quickly converge to a high-quality (need not be optimal) solution. For our knapsack problem at hand, let us start with the following 6 individuals (potential solutions) as our initial population.\n\n![Initial Population Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156789144-c0d14ee5-2ec9-4c51-aaab-a715728a38af.png)\n\n```python\ndef generate_initial_population(count=6) -> List[Individual]:\n    population = set()\n\n    # generate initial population having `count` individuals\n    while len(population) != count:\n        # pick random bits one for each item and \n        # create an individual \n        bits = [\n            random.choice([0, 1])\n            for _ in items\n        ]\n        population.add(Individual(bits))\n\n    return list(population)\n```\n\n## Fitness Coefficient\n\nNow that we have our initial population randomly chosen, we define a _fitness coefficient_ that would tell us how fit an individual from the population is. The fitness coefficient of an individual totally depends on the problem at hand, and if implemented poorly or incorrectly, it can result in misleading data or inefficiencies. The value of the fitness coefficient typically lies in the range of 0 to 1. but not a mandate.\n\n![Fitness Coefficient Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156890925-13e0f1bf-ec4a-40fe-8d48-60d867cdacae.png)\n\nFor our knapsack problem, we can define the fitness coefficient of an individual (solution) as the summation of the values of the items picked in the knapsack as per the bit string if the total weight of the picked items is less than the weight knapsack can hold. The fitness coefficient of an individual is 0 if the total weight of the item picked is greater than the weight that the knapsack can hold.\n\nFor the bit string `1001` the fitness coefficient will be\n\n```\ntotal_value  = (1 * v(A)) + (0 * v(B)) + (0 * v(C)) + (1 * v(D))\n             = ((1 * 5) + (0 * 4) + (0 * 7) + (1 * 2))\n\t         = 5 + 0 + 0 + 2\n\t         = 7\n\ntotal_weight = (1 * w(A)) + (0 * w(B)) + (0 * w(C)) + (1 * w(D))\n             = ((1 * 7) + (0 * 2) + (0 * 1) + (1 * 9))\n\t         = 7 + 0 + 0 + 9\n\t         = 16\n\nSince, MAX_KNAPSACK_WEIGHT is 15\nthe fitness coefficient of 1001 will be 0\n```\n\nThe higher the individual's fitness, the more are the chances of that individual to move forward as part of the evolution. This is based on a very common evolutionary concept called _Survival of the Fittest_.\n\n```python\ndef fitness(self) -> float:\n    total_value = sum([\n        bit * item.value\n        for item, bit in zip(items, self.bits)\n    ])\n\n    total_weight = sum([\n        bit * item.weight\n        for item, bit in zip(items, self.bits)\n    ])\n\n    if total_weight <= MAX_KNAPSACK_WEIGHT:\n        return total_value\n    \n    return 0\n```\n\n## Selection\n\nNow that we have defined the Fitness Coefficient, it is time to _select_ a few individuals to create the next generation. The selection happens using selection criteria inspired by evolutionary behavior. This is a tunable parameter we pick and experiment with while solving a particular problem.\n\nOne good selection criteria is [Tournament Selection](https://en.wikipedia.org/wiki/Tournament_selection), which randomly picks two individuals and runs a virtual tournament. The one having the higher fitness coefficient wins.\n\n![Selection Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156891433-13a356c7-d219-4a33-b7b3-423cdf10b910.png)\n\nFor our knapsack example, we randomly pick two individuals from the initial population and run a tournament between them. The one who wins becomes the first parent. We repeat the same procedure and get the second parent for the next step. The two parents are then passed onto the next steps of evolution.\n\n```python\ndef selection(population: List[Individual]) -> List[Individual]:\n    parents = []\n    \n    # randomly shuffle the population\n    random.shuffle(population)\n\n    # we use the first 4 individuals\n    # run a tournament between them and\n    # get two fit parents for the next steps of evolution\n\n    # tournament between first and second\n    if population[0].fitness() > population[1].fitness():\n        parents.append(population[0])\n    else:\n        parents.append(population[1])\n    \n    # tournament between third and fourth\n    if population[2].fitness() > population[3].fitness():\n        parents.append(population[2])\n    else:\n        parents.append(population[3])\n\n    return parents\n```\n\n## Crossover\n\nCrossover is an evolutionary operation between two individuals, and it generates children having some parts from each parent. There are different crossover techniques that we can use: single-point crossover, two-point crossover, multi-point crossover, uniform crossover, and arithmetic crossover. Again, this is just a guideline, and we are allowed to choose the crossover function and the number of children to create we desire.\n\nThe crossover does not always happen in nature; hence we define a parameter called the _Crossover Rate,_ which is relatively in the range of 0.4 to 0.6 given that in nature, we see a similar rate of crossover while creating offspring.\n\n![Crossover Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156891548-bfafdc41-0158-4146-b6c6-b9d14d2c536a.png)\n\nFor our knapsack problem, we keep it simple and create two children from two fit parents such that both get one half from each parent and the other half from the other parent. This essentially means that we combine half elements from both the individual and form the two children.\n\n```python\ndef crossover(parents: List[Individual]) -> List[Individual]:\n    N = len(items)\n\n    child1 = parents[0].bits[:N//2] + parents[1].bits[N//2:]\n    child2 = parents[0].bits[N//2:] + parents[1].bits[:N//2]\n\n    return [Individual(child1), Individual(child2)]\n```\n\n\n## Mutation\n\nThe mutation is an evolutionary operation that randomly mutates an individual. This is inspired by the mutation that happens in nature, and the core idea is that sometimes you get some random unexpected changes in an individual.\n\nJust like the crossover operation, the mutation does not always happen. Hence, we define a parameter called the _Mutation Rate,_ which is very low given that mutation rate in nature. The rate typically is in the range of 0.01 to 0.02. The mutation changes an individual, and with this change, it can have a higher or lower fitness coefficient, just how it happens in nature.\n\n![Mutation Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156822218-716ea60d-4d6b-434e-9112-26cba6c93b2c.png)\n\nFor our knapsack problem, we define a mutation rate and choose to flip bits of the children. Our `mutate` function iterates through the bits and sees if it needs to flip as per the mutation rate. If it needs to, then we flip the bit.\n\n```python\ndef mutate(individuals: List[Individual]) -> List[Individual]:\n    for individual in individuals:\n        for i in range(len(individual.bits)):\n            if random.random() < MUTATION_RATE:\n                # Flip the bit\n                individual.bits[i] = ~individual.bits[i]\n```\n\n## Reproduction\n\nReproduction is a process of passing an individual as-is from one generation to another without any mutation or crossover. This is inspired by nature, where evolutionary there is a very high chance that the genes of the fittest individuals are passed as is to the next generation. By passing the fittest individuals to the next generation, we get closer to reaching the overall fittest individual and thus the optimal solution to the problem.\n\nLike what we did with the Crossover and the Mutation, we define a _Reproduction Rate. Upon_ hitting that, the two fittest individuals will not undergo any crossover or mutation; instead, they would be directly passed down to the next generation.\n\n![Reproduction Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156825824-8cac7164-f526-47fd-958b-d24eba11c08a.png)\n\nFor our knapsack problem, we define a _Reproduction Rate_ and, depending on what we decide to pass the fittest individuals to the next generation directly. We keep the reproduction rate to `0.30` implying that 30% of the time, the fittest parents are passed down as is to the next generation.\n\n## Creating Generations\n\nWe repeat the entire process of Selection, Reproduction, Crossover, and Mutation till we get the same number of children as the initial population, and we call it the first generation. We repeat the same process again and create subsequent generations.\n\n![Creating Newer Generations Genetic Algorithm for Knapsack](https://user-images.githubusercontent.com/4745789/156872749-9e93f05d-6ed1-4283-876d-4e7b62a01de9.png)\n\nWhen we continue to create such generations, we will observe that the _Average Fitness Coefficient_ of the population increases and it converges to a steady range. The image above shows how our value converges to 12 over 500 generations for the given problem. The optimal solution to the problem at hand is 16, and we converged to 12.\n\n```python\ndef next_generation(population: List[Individual]) -> List[Individual]:\n    next_gen = []\n    while len(next_gen) < len(population):\n        children = []\n\n        # we run selection and get parents\n        parents = selection(population)\n\n        # reproduction\n        if random.random() < REPRODUCTION_RATE:\n            children = parents\n        else:\n            # crossover\n            if random.random() < CROSSOVER_RATE:\n                children = crossover(parents)\n            \n            # mutation\n            if random.random() < MUTATION_RATE:\n                mutate(children)\n\n        next_gen.extend(children)\n\n    return next_gen[:len(population)]\n```\n\n## Termination Condition\n\nWe can continue to generate generations upon generations in search of the optimal solution, but we cannot go indefinitely, which is where we need a termination condition. A good termination condition is deterministic and capped. For example, we will at max go up to 500 or 1000 generations or until we get the same Average Fitness Coefficient for the last 50 values.\n\nWe can have a similar terminating condition for our knapsack problem where we put a cap at 500 generations. The individual with the max fitness coefficient is our high-quality solution. The overall flow looks something like this.\n\n```python\ndef solve_knapsack() -> Individual:\n    population = generate_initial_population()\n\n    avg_fitnesses = []\n\n    for _ in range(500):\n        avg_fitnesses.append(average_fitness(population))\n        population = next_generation(population)\n\n    population = sorted(population, key=lambda i: i.fitness(), reverse=True)\n    return population[0]\n```\n\n\n# Run-time Complexity\n\nThe run-time complexity of the Genetic Algorithm to generate a high-quality solution for the Knapsack problem is not exponential, but it is polynomial. If we operate with the population size of `P`And iterate till `G` generations, and `F` is the run-time complexity of the fitness function, the overall complexity of the algorithm will be `O(P.G.F)`.\n\nGiven that the parameters are known before starting the execution, you can predict the time taken to reach the solution. Thus, we are finding a non-optimal but high-quality solution to the infamous Knapsack problem in [Polynomial Time](https://en.wikipedia.org/wiki/Time_complexity).\n\n# Multi-dimensional optimization problems\n\nThe problem we discussed was trivial and was enough for us to understand the core idea of the Genetic Algorithm. The true power of the Genetic Algorithm comes into action when we have multiple parameters, dimensions, and constraints. For example, instead of just weight what if we have size and fragility as two other parameters to consider and we have to find a high-quality solution for the same Knapsack problem.\n\n# The efficiency of Genetic Algorithm\n\nWhile discussing the process of Genetic Algorithm, we saw that there are multiple parameters that can increase or decrease the efficiency of the algorithm; a few factors include\n\n## Population\n\nThe size of the initial population is critical in achieving the high efficiency of a Genetic Algorithm. For our Knapsack problem, we started with a simpler problem with 4 items i.e. search space of a mere 16, and an initial population of 6. We took such a small problem set just to wrap our heads around the idea.\n\nIn the real-world, genetic algorithms operate on a much larger search space and typically start with a population size of 500 to 50000. It is observed that if the initial population size does not affect the execution time of the algorithm, it converges faster with a large population than a smaller one.\n\n## Crossover\n\nAs we discussed above there are multiple Crossover functions that we can use, like Single Point Crossover, Two-point Crossover, and Multi-point Crossover. Which crossover function would work better for a problem depends totally on the problem at hand. It is generally observed that the two-point crossover results in the fastest convergence.\n\n> You can find the complete source code to the genetic algorithm discussed above in the repository [github/genetic-knapsack](https://github.com/arpitbbhayani/genetic-knapsack)\n",
    "similar": [
      "fractional-cascading",
      "rule-30",
      "flajolet-martin",
      "1d-terrain"
    ]
  },
  {
    "id": 2,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "lfsr",
    "title": "Pseudorandom Number Generation using LFSR",
    "description": "This essay takes a detailed look into pseudorandom number generation using LFSR, a widely adopted technique to generate random numbers on hardware and on software.",
    "gif": "https://media.giphy.com/media/3orieLKAOlnwdqkTCg/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/154754358-c1bbd5ff-2a0b-4028-b1ec-05cf90300d83.png",
    "released_at": "2022-02-21",
    "total_views": 609,
    "body": "A few essays back, we saw how pseudorandom numbers are generated using [Cellular Automaton - Rule 30](https://arpitbhayani.me/blogs/rule-30). This essay takes a detailed look into random number generation using [LFSR - Linear Feedback Shift Registers](https://en.wikipedia.org/wiki/Linear-feedback_shift_register). LFSR is widely adopted to generate random numbers on microcontrollers because they are very simple, efficient, and easy to adopt both hardware and software.\n\n# Pseudorandom Number Generators\n\nA [pseudorandom number generator](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) produces numbers that seem aperiodic (random) but are deterministic. It uses a seed value and generates a sequence of random numbers as a function of the current state and some previous states.\n\nThese are pseudorandom (not truly random) because the following numbers can be determined algorithmically if the seed value is known. True random numbers are either generated using hardware or from natural phenomena like blood volume pulse, atmospheric pressure, thermal noise, quantum phenomenon, etc.\n\n# What is LFSR\n\nA linear feedback shift register is a collection of bits that shifts when triggered, and the next state is a linear function of its previous state. We use [right-shift](https://en.wikipedia.org/wiki/Logical_shift) (`>>`) as the shift operation and [XOR](https://en.wikipedia.org/wiki/Exclusive_or) (`^`) as the linear function to generate the next state of the register.\n\n![8-bit LFSR](https://user-images.githubusercontent.com/4745789/154759305-23a775cd-f4fe-4aa5-9b4a-e68a7365695f.png)\n\nThe LFSR is initiated with a random value called a seed. The next state of the register can be computed deterministically using its previous state and the mentioned operations.\n\n# LFSR in action\n\nA series of latches (bits) connected to the next in line forming a chain is called a register. The diagram below shows an 8-bit shift register that shifts to the right upon an impulse. The rightmost bit that is thrown out during the shift is the output bit.\n\n![8-bit LFSR with a feedback loop](https://user-images.githubusercontent.com/4745789/154759436-7f7d4937-40bb-4a2a-964f-02df14c44d2b.png)\n\nWhen the shift happens, the leftmost latch gets vacant, and it is either\n\n-   filled with the output bit forming a circular shift register\n-   filled with zero, like a pure right shift operation of a programming language\n-   filled with a result of some boolean logic on the latches\n    \n\nLFSR that we used to generate pseudorandom numbers goes with the third approach and applies a boolean XOR on a set of chosen latches, called _taps_, and puts the resultant bit in the leftmost latch, creating a Linear Feedback.\n\n## A simple 4-bit LFSR\n\nAn LFSR has 3 configurable parameters\n\n-   number of bits in the register - `n`\n-   initial n-bit seed value - `seed`\n-   position of taps for XOR - `taps`\n\nWe build a simple `4`-bit LFSR with a seed value of `0b1001` and tap position of `1`. The output bit will be the rightmost bit of the register, and the next state of the LFSR will be computed as the\n\n-   XOR the output bit with the bit in the `1`st position (indexed at 0) from the right\n-   shift the bits of the register by one to the right\n-   set the vacant leftmost bit with the output of the XOR operation\n\n![4-bit LFSR](https://user-images.githubusercontent.com/4745789/154893624-b7a8c040-9e26-4f05-b24d-b0b60bcf88a7.png)\n\nAfter all the above operations are completed, the set of bits in the LFSR becomes the current state and is then used to output the next random bit, thus continuing the cycle.\n\n> The above example is taken from the Computerphile's [video](https://www.youtube.com/watch?v=Ks1pw1X22y4) on this same topic.\n\nGolang-based implementation of the above LFSR is as shown below. We define a struct holding LFSR with the mentioned three parameters - the number of bits in the register, the seed, and the position of the taps. We define the function named `NextBit` on it that returns a pseudorandom bit generated with the logic mentioned above.\n\n```go\ntype LFSR struct {\n\tn    uint32\n\tseed uint32\n\ttaps []uint32\n}\n\nfunc (l *LFSR) NextBit() uint32 {\n\tseed := l.seed\n\t\n\t// output bit is the rightmost bit\n\toutputBit := l.seed & 1\n\t\n\t// XOR all the bits present in the tap positions\n\tfor _, tap := range l.taps {\n\t\tseed = seed ^ (l.seed >> tap)\n\t}\n\t\n\t// the new msb is the output of this XOR\n\tmsb := seed & 1\n\t\n\t// rightsift the entire seed\n\t// and place newly computed msb at\n\t// the leftmost end\n\tl.seed = (l.seed >> 1) | (msb << (l.n - 1))\n\t\n\t// return the output bit as the next random bit\n\treturn outputBit\n}\n```\n\nWhen we execute the above code with seed `0b1001`, tap position `1`, on a `4`-bit LFSR, we get the following random bits as the output, and with a little bit of pretty-printing, we see\n\n```\nlfsr: 1001       output: 1\nlfsr: 1100       output: 0\nlfsr: 0110       output: 0\nlfsr: 1011       output: 1\nlfsr: 0101       output: 1\nlfsr: 1010       output: 0\nlfsr: 1101       output: 1\nlfsr: 1110       output: 0\nlfsr: 1111       output: 1\nlfsr: 0111       output: 1\nlfsr: 0011       output: 1\nlfsr: 0001       output: 1\nlfsr: 1000       output: 0\nlfsr: 0100       output: 0\nlfsr: 0010       output: 0\nlfsr: 1001       output: 1\nlfsr: 1100       output: 0\nlfsr: 0110       output: 0\nlfsr: 1011       output: 1\nlfsr: 0101       output: 1\n```\n\nThe output bits seem random enough, but upon a close inspection, we see that the last 5 output bits are the same as the first 5, and it is because, for the given configuration of seed and tap, the value in the LFSR becomes the same as the seed value `0b1001` after 15 iterations; thus, from the 16th position, we can see the same set of output bits generated.\n\nBy carefully selecting the seed and the taps, we can ensure that the cycle is long enough to never repeat in our process\u2019s lifetime. You can find the detailed source code for this LFSR at [github.com/arpitbbhayani/lfsr](https://github.com/arpitbbhayani/lfsr).\n\n## LFSR Bits to Number\n\nAlthough LFSR generates pseudorandom bits, generating random numbers is fairly simple using it. To generate a `k`-bit random number we need to generate `k` random bits using our routine LFSR and accumulate them in a `k`-bit integer, which becomes our random number.\n\nGolang-based implementation of the above logic can be seen in the function named `NextNumber` defined below.\n\n```go\nfunc (l *LFSR) NextNumber(k int) uint32 {\n\tvar n uint32 = 0\n\t\n\t// generate a random bit using LFSR\n\t// set the random bit in the lsb of a number\n\t// left-shift the number by 1\n\t// repeat the flow `k` times for a k-bit number\n\tfor i := 0; i < k; i++ {\n\t\tn = n << 1\n\t\tn |= l.NextBit()\n\t}\n\treturn n\n}\n```\n\nThe first ten 8-bit random numbers generated with a seed `0b1001` and tap position `1` using the above logic are\n\n```\n154\n241\n53\n226\n107\n196\n215\n137\n175\n19\n```\n\nWe can see that the numbers are fairly random and are within the limits of an 8-bit integer. You can find the detailed source code for this LFSR at [github.com/arpitbbhayani/lfsr](https://github.com/arpitbbhayani/lfsr).\n\n# Applications of LFSR\n\nLFSRs find their application across a wide spectrum of use cases, given how efficient they generate randomness. Their applications include digital counters, generating pseudorandom numbers, pseudo-noise, scramble radio frequencies, and in general, a stream of bytes. We take a detailed look into LFSR for scrambling.\n\n## Scrambling using LFSR\n\nLFSRs are computationally efficient and deterministic for a seed value, i.e., they generate the same set of numbers in the same order for a seed value, and here\u2019s how they find their application in scrambling and unscrambling a stream of bytes.\n\nThe idea of scrambling using LFSR goes like this. We read the raw bytes from the file and pass them to the scrambler function. The function initializes the LFSR with the necessary register length, seed, and tap positions and is kept ready to generate the 8-bit random numbers.\n\nThe stream of bytes, from the input file, is then XORed with the random numbers generated from the LFSR such the `i`th byte of the file is XORed with the `i`th 8-bit number generated from the LFSR. Golang-based implementation of `scramble` function is as shown below.\n\n```go\nfunc scramble(in []byte, n uint32, seed uint32,\n\t\t\t  taps []uint32) []byte {\n  \n  \t// initiatializing LFSR with the provided config\n\tl := lfsr.NewLFSR(n, seed, taps)\n  \n  \t// creating an output byte slice\n\tout := make([]byte, len(in))\n  \n  \t// XOR byte by byte\n\tfor i := range in {\n\t\tout[i] = in[i] ^ byte(l.NextNumber(8))\n\t}\n  \n  \t// return the output slice\n\treturn out\n}\n```\n\nThe unscrambling process exploits the following property of XOR and the fact that the LFSR will generate the same set of random numbers in the same order for the given configuration.\n\n```\na XOR b XOR a = b\n```\n\nSo, if we XOR a byte twice with the same number, we get the same byte in return. We apply the same `scramble` function even to unscramble the scrambled data.\n\n```go\nfunc unscramble(in []byte, n uint32, seed uint32,\n\t\t\t\ttaps []uint32) []byte {\n\n\t// invoking the same scramble function\n\treturn scramble(in, n, seed, taps)\n}\n```\n\n# Concerns with LFSR\n\nAlthough LFSRs are very efficient both on the software and hardware sides, there are some concerns about using them.\n\nThe number of bits in the LFSR is limited (as configured); the register will repeat the same set after generating a certain set of bits. The length of the cycle depends solely on the seed value and the tap configuration. So, while employing LFSR for random number generation, it is essential to pick a good set of tap positions and seeds to ensure a very long cycle.\n\nSuppose we get hold of a few consecutive random numbers generated from the LFSR. In that case, we can put them in a few linear equations and reach the initial configuration, enabling us to predict the future set of random numbers. Given how vulnerable LFSRs can be, they are not used at places that need cryptographic strength.\n",
    "similar": [
      "publish-python-package-on-pypi",
      "efficient-way-to-stop-an-iterating-loop",
      "the-weird-walrus",
      "jaccard-minhash"
    ]
  },
  {
    "id": 3,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "indexing-on-partitioned-data",
    "title": "Indexing on Partitioned Data",
    "description": "In this essay, we will take a detailed look into how we could index the partitioned data, allowing us to query the data on secondary attributes quickly.",
    "gif": "https://media.giphy.com/media/3o6Mb9cGKe3JzhbqPC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/152698544-c62436c9-8735-4490-90be-4d117902736b.png",
    "released_at": "2022-02-07",
    "total_views": 380,
    "body": "The previous essay looked at two popular ways to [horizontally partition](https://arpitbhayani.me/blogs/data-partitioning-strategies) the data - Range-based Partitioning and Hash-based Partitioning. In this essay, we will take a detailed look into how we could [index](https://en.wikipedia.org/wiki/Database_index) the partitioned data, allowing us to query the data on secondary attributes quickly.\n\n# Partitioning and Querying\n\nIn a [partitioned database](https://arpitbhayani.me/blogs/data-partitioning), the data is split horizontally on the partitioned key. Given that each partition is required to handle a fragment of data, the query that is bound to a single partition is answered very quickly vs the query that requires cross partition execution. But what happens when we want to query the data on any attribute other than the partitioned key; that is where things become very interesting.\n\nSay we have a `movies` collection partitioned on `id` (the movie ID), and each record has the following structure.\n\n```json\n{\n\t\"id\": tt0111161,\n\t\"name\": \"The Shawshank Redemption\",\n\t\"genre\": [\"Drama\"],\n\t\"year\": 1994\n}\n```\n\nGiven that the collection is partitioned on `id`, querying a movie by its `id` will be lightning-quick as it would need to hit just one partition to grab the record as determined by the Hash function.\n\n![Pointed Query in Partitioned Database](https://user-images.githubusercontent.com/4745789/152688735-16e15acf-fcee-491c-9b74-965e3590df9c.png)\n\nWhat if we need to get the list of all the movies belonging to a particular genre? Answering this query is very expensive as we would have to go through every record across all the partitions and see which ones match our criteria, accumulate them, and return them as the response. Given that this process is tedious we leverage indexing to compute the answer quickly.\n\n# Indexing\n\nIndexing is a popular technique to make reads super-efficient, and it does so by creating a query-able mapping between indexed attributes and the identity of the document. An index that maps non-primary key attributes to the record id is called a Secondary Index.\n\nSay, we have the following 6 movie documents, partitioned on `id` (the movie ID) and split across 2 partitions as shown below\n\n```json\n{ \"id\": tt0111161, \"name\": \"The Shawshank Redemption\", \"genre\": [\"Drama\"], \"year\": 1994 }\n{ \"id\": tt0068646, \"name\": \"The Godfather\", \"genre\": [\"Crime\", \"Drama\"], \"year\": 1972 }\n{ \"id\": tt0071562, \"name\": \"The Godfather: Part II\", \"genre\": [\"Crime\", \"Drama\"], \"year\": 1974 }\n\n\n{ \"id\": tt0468569, \"name\": \"The Dark Knight\", \"genre\": [\"Action\", \"Crime\", \"Drama\"], \"year\": 2008 }\n{ \"id\": tt0050083, \"name\": \"12 Angry Men\", \"genre\": [\"Crime\", \"Drama\"], \"year\": 1957 }\n{ \"id\": tt0108052, \"name\": \"Schindler's List\", \"genre\": [\"Biography\", \"Drama\", \"History\"], \"year\": 1993 }\n```\n\n![movies Partitioned across 2 partitions](https://user-images.githubusercontent.com/4745789/152688989-d70c541f-92c9-4f3f-ad54-740f660f7bd0.png)\n\nTo query movies by `genre = Crime`, we will have to index the data on `genre` allowing us to find the relevant documents quickly. Indexes are a little tricky in a partitioned database, and there are two ways to implement them: Local Indexing and Global Indexing. [AWS's DynamoDB](https://aws.amazon.com/dynamodb/) is a partitioned KV store that supports secondary indexes on non-partitioned attributes, and it supports both of these indexing techniques.\n\n## Local Secondary Index\n\nLocal Secondary Indexing creates indexes on a non-partitioned attribute on the data belonging to the partition. Thus, each partition has a secondary index that is built on that data owned by that partition and it knows nothing about the data present in other partitions. Hence, on the example that we have at hand, the Local Secondary Index on attribute `genre` would look like this\n\n![Local Secondary Index - Movies](https://user-images.githubusercontent.com/4745789/152689634-c3235f38-5cf2-4446-af87-ecd58807296d.png)\n\nThe key advantage of having a Local Secondary Index is that whenever a write happens on a partition, the index update happens locally without needing any cross partition communication (mostly a network IO). When the data is fetched from a Local Secondary Index, it is fetched from the partition that holds the index data and the entire record; so execution takes a minimal time.\n\nLocal Secondary Indexes come in handy when we want to query the data in conjunction with the partitioned key. For example, if the movies were partitioned by `genre` (instead of `id`) and we create an index on `year` it will help us efficiently answer the queries like movies of a particular `genre` released in a specific `year`.\n\n### When Local Secondary Indexes suffer?\n\nAlthough Local Secondary Indexes are great, they cannot efficiently answer the queries that require cross partition fetch. For example, if we fire the query to get all `Crime` movies through a Local Secondary Index, we will be getting the records that are local to the partition on which the query executes.\n\nBut, answering the query to fetch all the movies from the `Crime` genre requires us to go through all the partitions and individually execute the query, then gather (accumulate) the results and return. This is an extremely expensive process that is also prone to network delays, partitioning, and unreliability.\n\n![Scatter Gather Local Secondary Index](https://user-images.githubusercontent.com/4745789/152691045-f9958236-b532-4fed-a5d3-902962edd1b0.png)\n\nWe face this limitation because the movies with the `crime` genre are distributed across partitions because there is no way to ensure all movies with the `Crime` genre belong to the same partition when the data partitioning is done on `id`.\n\nHence, it is very important to structure data partitioning and indexing depending on the queries we want to support ensuring that the queries can be answered through just one partition. To address this problem of being able to query the data on an indexed attribute, we create Global Secondary Indexes.\n\n## Global Secondary Index\n\nGlobal Secondary Indexes choose not to be local to a partition's data instead, this indexing technique covers the entire dataset. Global Secondary Index is a kind of re-partitioning of data on a different partition key allowing us to have faster reads and a global view on the indexed attribute.\n\nOn the example that we have at hand, Global Secondary Index on `genre` would look like this.\n\n![Global Secondary Index - Movies example](https://user-images.githubusercontent.com/4745789/152696486-33d94f29-6918-48f6-8644-b6ee809a2a81.png)\n\nThe key advantage of having a Global Secondary Index is that it allows us to query the data on the indexed attribute globally and not limit ourselves to a fragment of the data. Since it literally re-partitions the data on a different attribute, firing query on the indexed attribute requires it to hit just one partition for execution and thus saving fanning out to multiple partitions.\n\n### When Global Secondary Indexes suffer?\n\nThe database takes a performance hit when a Global Secondary Index needs to be synchronously updated as soon as the update happened on the main record, and if the updation happens asynchronously then the readers need to be aware of a possible stale data fetch.\n\nSynchronous updation of a Global Secondary Index is an extremely expensive operation given that every write on primary data will be translated to a number of synchronous updation across partitions for index updation wrapped in a long [Distributed Transaction](https://en.wikipedia.org/wiki/Distributed_transaction) to ensure [Data Consistency](https://en.wikipedia.org/wiki/Data_consistency).\n\n![Global Secondary Index updation](https://user-images.githubusercontent.com/4745789/152697444-4b90d88b-fa18-4456-b8ba-bdde15ddbac4.png)\n\nHence, in practice, most Global Secondary Indexes are updated asynchronously involving a rick of Replication Lag and stale data reads. The readers from these indexes should be okay with reading stale data and the system being eventually consistent. The delay in propagation could vary from a second to a few minutes, depending on the underlying hardware's CPU consumption and network capacity.\n",
    "similar": [
      "data-partitioning",
      "conflict-detection",
      "data-partitioning-strategies",
      "architectures-in-distributed-systems"
    ]
  },
  {
    "id": 4,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "data-partitioning-strategies",
    "title": "Data Partitioning Strategies",
    "description": "In this essay, we take a detailed look into the two common approaches to horizontally partition the data - Hash Based and Range Based Partitioning.",
    "gif": "https://media.giphy.com/media/xTiTnrliW65rlwud8I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/151733223-2f1f5611-7b4a-463d-aadf-fd522b535d67.png",
    "released_at": "2022-01-31",
    "total_views": 302,
    "body": "[Partitioning](https://arpitbhayani.me/blogs/partitioning) plays a vital role in scaling a database beyond a certain scale of reads and writes. This essay takes a detailed look into the two common approaches to horizontally partition the data.\n\n# Partitioning\n\nA database is partitioned when we split it, logically or physically, into mutually exclusive segments. Each partition of the database is a subset that can operate as a smaller independent database on its own.\n\n## Our goal with partitioning\n\nOur primary goal with partitioning is to spread the data across multiple nodes, each responsible for only a fraction of the data allowing us to dodge the limitations with vertical scaling. A database is uniformly partitioned across 5 data nodes; each node will be roughly responsible for a fifth of the reads and writes hitting the cluster, allowing us to handle a greater load seamlessly.\n\n## What if partitioning is skewed?\n\nPartitioning does help in handling the scale only when the load spreads uniformly. Partitions are skewed when few (hot) partitions are responsible for bulk data or query load. This happens when the partitioning logic does not respect the data and access pattern of the use-case at hand.\n\n![Skewed Partitioning](https://user-images.githubusercontent.com/4745789/150775353-358b6183-30a2-4fb4-8291-e3642c668747.png)\n\nIf the partitioning is skewed, the entire architecture will be less effective on performance and cost. Hence, the access and storage pattern of the use-case is heavily considered while deciding on the partitioning attribute, algorithm, and logic.\n\n# Ways of Partitioning Data\n\n## Range-based Partitioning\n\nOne of the most popular ways of partitioning data is by assigning a continuous range of data to each partition, making each partition responsible for the assigned fragment. Every partition, thus, knows its boundaries, making it deterministic to find the partition given the partition key.\n\n![Range-based Partitioning](https://user-images.githubusercontent.com/4745789/150777106-4ee22e27-de48-4dda-999e-f1b286a7d5f5.png)\n\nAn example of range-based partitioning is splitting a Key-Value store over 5 partitions with each partition responsible for a fragment, defined as,\n\n```\npartition 1: [a - e]\npartition 2: [f - k]\npartition 3: [l - q]\npartition 4: [r - v]\npartition 5: [w - z]\n```\n\nEach partition is thus responsible for the set of keys starting with a specific character. This allows us to define how our entire key-space will be distributed across all partition nodes.\n\nGiven that we partition the data to evenly distribute the load across partition nodes, we create the range of the keys that uniformly distributes the load and not the keyspace. Hence in range-based partition, it is not uncommon to see an uneven distribution of key-space. The goal is to optimize the load distribution and not the keyspace.\n\n### When Range-based partitioning fails?\n\nA classic use-case where range-based partitioning fails is when we range-partition the time-series data on timestamp. For example, we create per-day partitions of data coming in from thousands of IoT sensors.\n\nSince IoT sensors will continue to send the latest data, there will always be just one partition that will have to bear the entire ingestion while others will just be sitting idle. When the write-volume for time-series data is very high, it may not be wise to partition the data on time.\n\n## Hash-based Partitioning\n\nAnother popular approach for horizontal partitioning is by hashing the partitioned attribute and determining the partition that will own the record. The hashing function used in partitioning is not cryptographically strong but does a good job evenly distributing values across the given range.\n\nEach partition owns a set of hashes. We hash the partitioned attribute when a record needs to be inserted or looked up. A partition that owns the hash will own and store the record. While fetching the record, we first hash the partition key find the owning partition, and then fire the query to get our record from it.\n\n![Hash-based Partitioning](https://user-images.githubusercontent.com/4745789/150777895-b524d8b2-56f3-4a53-bf8b-27f06b824bc6.png)\n\nHash-based partitioning defers the problem of hot partition to statistics and relies on the randomness of hash-based distribution. But, there is still a slim chance of some partition being hot when many records get hashed to the same partition; this issue is addressed to some extent with the famous [Consistent Hashing](https://arpitbhayani.me/blogs/consistent-hashing).\n\n### When Hash-based partitioning fails?\n\nHash-based partitioning is a very common technique of data partitioning and is quite prevalent across databases. Although the method is good, it suffers from a few major problems.\n\nSince the record is partitioned on an attribute through a hash function, it is difficult to perform a range query on the data. Since the data is unordered and scattered across all partitions, we will have to visit all the partitions, making the entire process inefficient to perform a range query on key.\n\nRange queries are doable when the required range lies on one partition. This is something leveraged by [Amazon's DynamoDB](https://aws.amazon.com/dynamodb/) that asks us to specify Partition Key (Hash Key) and Range Key. The data is stored across multiple partitioned and is partitioned by the Hash Key. The records are ordered by Range Key within each partition, allowing us to fire range queries local to one partition.\n",
    "similar": [
      "indexing-on-partitioned-data",
      "data-partitioning",
      "mistaken-beliefs-of-distributed-systems",
      "conflict-detection"
    ]
  },
  {
    "id": 5,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "data-partitioning",
    "title": "Data Partitioning",
    "description": "In this essay, we take a detailed look into Partitioning basics and understand how it can help us scale our Reads and Writes beyond a single machine.",
    "gif": "https://media.giphy.com/media/l0G17UXa1wk36OIAE/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/150692301-6abb0b95-c91f-4a8f-8d10-6e0f49b4d58d.png",
    "released_at": "2022-01-24",
    "total_views": 435,
    "body": "Partitioning plays a vital role in scaling a database beyond a certain scale of reads and writes. In this essay, we take a detailed look into Partitioning basics and understand how it can help us scale our Reads and Writes beyond a single machine.\n\n# What is Partitioning?\n\nA database is partitioned when we split it, logically or physically, into mutually exclusive segments. Each partition of the database is a subset that can operate as a smaller independent database on its own. A database is always deterministically partitioned on a particular attribute like User ID, Time, Location, etc., allowing all records having the same attribute value to reside in the same partition. This will enable us to fire localized queries on a partitioned attribute.\n\n![Partitioning](https://user-images.githubusercontent.com/4745789/149617510-73d710c4-4ff1-4f6c-8ba7-6f8345847248.png)\n\nSay a database has grown to be 100GB big, and we choose to _partition_ it on User ID into 4 partitions. To decide which record goes in which partition, we can use a [Hash Function](https://en.wikipedia.org/wiki/Hash_function) applied on the User ID to map one record to exactly one partition. Hence to trace where a record of a particular user resides, we pass it through the same Hash Function and find the owning partition.\n\n> We will talk about partitioning strategies in detail in future [essays](https://arpitbhayani.me/blogs), so keep an eye.\n\n# Why do we partition?\n\nWe need to partition a database for several reasons, but load distribution and availability are the primary reasons. Let's dive deeper into each and see how partitioning benefits us.\n\n## Load Distribution\n\nA database is partitioned when it needs to handle more reads or writes than one over-scaled database. Our go-to strategy to handle more reads or more writes is to scale the database vertically. Given that vertical scaling has a limit due to hardware constraints, we have to go horizontal and distribute the load across multiple nodes.\n\n### Scaling Reads\n\nBy partitioning a database into multiple segments, we get a significant boost in the performance of localized queries. Say we have a database with 100M rows split into 4 partitions with roughly 25M rows each. Now, instead of one database supporting querying over 100M rows, we split the read load across 4 databases allowing us to quickly execute the query and serve the results to the users.\n\nIf the read query is localized by partitioned attribute, we need only one (of the four) partitions to execute the query and get the results, thus distributing the read load. For example, in a blogging platform, if our database is partitioned by User ID and we want to find the total number of posts made by a user, this query only needs to be executed on one small partition of data.\n\n![Scaling Reads with Partitioning](https://user-images.githubusercontent.com/4745789/149617513-2dd6bd59-7fea-413a-a73d-313fad080661.png)\n\nSuppose the read queries require us to fetch records from multiple partitions, given that each partition is independent. In that case, we can parallelize the execution and then merge the results before sending them out to the users. In either case, we get a massive performance boost in query execution.\n\n![Scaling Reads with Partitioning - Parallel Reads](https://user-images.githubusercontent.com/4745789/149617508-e62d16d1-bc3e-4aec-9b5c-49785699cff8.png)\n\n### Scaling Writes\n\nIn a traditional [Master-Replica setup](https://arpitbhayani.me/blogs/master-replica-replication), there is one Master node that takes in all the write requests, and to scale reads, this Master has a few configured Replicas. To handle more Write operations in such a setup, one approach is to scale the Master node vertically by adding more CPU and RAM. The second approach is to scale it horizontally by adding multiple nodes acting as independent Multiple Master nodes.\n\nGiven that vertical scaling has a limit, scaling writes that adding multiple independent Master nodes becomes a go-to strategy beyond a certain scale, where Partitioning plays a key role. In a partitioned setup, since one record can is present on one partition, the total write operations are evenly distributed across all the Master nodes, allowing us to scale beyond a single machine.\n\n![Scaling writes with Partitioning](https://user-images.githubusercontent.com/4745789/149632842-1497874e-13a3-4af2-86fd-096c1eb2e1d7.png)\n\n## Improving Availability\n\nBy partitioning a database, we also get a massive improvement in data availability. Since our data is divided across multiple data nodes, even if one of the nodes abruptly crashes and becomes unrecoverable, we only lose a fraction of our data and not the whole of it.\n\nWe can further improve the availability of our data by replicating it across multiple secondary data nodes. Thus each partition resides on multiple data nodes, and in case of them crashes, we can fully recover the lost data from the secondary node, giving our fault tolerance a massive boost.\n\n![Partitioning for High Availability](https://user-images.githubusercontent.com/4745789/149632846-d9be03ca-104b-4628-9d5a-b03f9c6ea690.png)\n\nEach record thus belongs to exactly one partition, but the replicated copy of the record can be stored on other data nodes for fault tolerance. These replicated copies are similar to [Read Replicas](https://arpitbhayani.me/blogs/master-replica-replication) that either synchronously or asynchronously follow the primary copy and keep itself updated.\n\n# Types of partitioning\n\nData can be partitioned in two ways - Horizontal and Vertical. In terms of relational databases, Horizontal Partitioning involves putting different rows into different partitions, and Vertical Partitioning involves putting different columns into separate partitions.\n\nHorizontal partitioning is a very common practice in scaling relational and non-relational databases. It allows us to visit just one partition and get our query answered. It also enables us to split our query load across partitions by making one partition responsible for a particular row/record.\n\nVertical partitioning is seen in action in [Data Warehouses](https://en.wikipedia.org/wiki/Data_warehouse), where we have to crunch a lot of numbers and fire complex aggregation queries. Vertical partitioning is particularly useful when we are not querying all the columns of a particular record and refer to querying a fewer set of columns in each query.\n",
    "similar": [
      "multi-master-replication",
      "indexing-on-partitioned-data",
      "handling-outages-master-replica",
      "data-partitioning-strategies"
    ]
  },
  {
    "id": 6,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "leaderless-replication",
    "title": "Leaderless Replication",
    "description": "In this essay, we take a look into a different way of replication, called Leaderless Replication, that comes in handy in a multi-master setup that demands strong consistency.",
    "gif": "https://media.giphy.com/media/HUkOv6BNWc1HO/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/149646378-0d38d51f-af73-483b-8f05-d5251d6e5b4b.png",
    "released_at": "2022-01-16",
    "total_views": 444,
    "body": "Traditional leader-centric replication strategies revolve around the fact that there will be one Master (leader) node that will acknowledge and accept all the writes operations and then replicate the updates across the replicas (read or master). In this essay, we take a look into a different way of replication, called Leaderless Replication, that comes in handy in a multi-master setup that demands strong consistency.\n\n# Leader-centric Replication\n\nIn a leader-centric replication, there is a Master node that accepts the writes. Upon applying the writes on its copy of data, the database engine sends out the updates across read-replicas or master nodes. Given that all the writes flow through the Master node, the order of the writes is deterministic, slimming down the chances of having a write conflict.\n\n![Leader-centric Replication](https://user-images.githubusercontent.com/4745789/148632480-29a90496-0d65-41f0-a31a-5fb83cf208d0.png)\n\nLeader-centric replication is not fault-tolerant by design because we lose the write operation when the Master node is down. Leaderless replication addresses this concern and ensures our system can handle Write operations even when a subset of nodes are having an outage.\n\n# Leaderless Replication\n\nLeaderless Replication eradicates the need of having a leader accepting the writes; instead, it leverages quorum to ensure strong consistency across multiple nodes and good tolerance to failures. Here's how `WRITE` and `READ` operations happen in such a system.\n\n## Write Operation\n\nSay we have a database cluster of `5` nodes (all Masters). In Leaderless Replication, when a client wants to issue a write operation, it fires this operation on all `5` nodes and waits for `ACK` from at least `3` nodes. Once it receives `ACK` from a majority of the nodes, it marks the write as `OK` and returns; otherwise, it marks the operation as `FAIL`.\n\n![Write Operation in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634243-36259bc6-ee6e-4fd2-b399-c475ad7a405d.png)\n\nEvery record in the database has a monotonically increasing version number. Every successful write updates this version number allowing us and the system to identify the latest value of the record upon conflict.\n\nSay, when the write operation was triggered, it reached just `4` nodes because the fifth node was down; so, when this node comes back up, it gossips with the other `4` nodes and identifies the writes that it missed and then pulls the necessary updates.\n\nOnce the write is `ACK` and confirmed, the nodes gossip internally to propagate the writes to other nodes. There could be a significant delay for the writes to propagate and sync across all nodes; hence we need to ensure that our reading strategy is robust enough to handle this delay while guaranteeing strong consistency.\n\n## Read Operation\n\nGiven that there could be a significant delay in the updates to propagate across all `N` nodes, the Read strategy in Leaderless Replication needs to be robust enough.\n\nLike how the client fanned out the write operation to all the nodes, it also fans out the Read operation to all `N` nodes. The client waits to get responses from at least `N/2 + 1` nodes. Upon receiving the responses from a majority of the nodes, it returns the value having the largest version number.\n\n![Read Operation in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634242-adf09717-a063-455f-b5d2-57497e60ca27.png)\n\nGiven that we mark a write as `OK` only when at least `N/2 + 1` of them `ACK` it and we return our read-only when we get responses from at least `N/2 + 1` nodes, we ensure that there is at least one node that is sending the latest value of the record.\n\nIf we send our read operation to just one node chosen at random, there is a high chance that the value it returns in the response is a stale one, defeating our guarantee of having a strong consistency.\n\n# Generic Leaderless Replication\n\nThe Leaderless Replication system we just discussed is specific because it restricts clients to send write to all `N` nodes and wait for `ACK` from at least `N/2 + 1` nodes. This constraint is generalized in real-world with\n\n - `w`: number of nodes that confirm the writes with an `ACK`\n - `r`: number of nodes we query for the read\n - `n`: total number of nodes\n\nto have a strong consistency i.e. we can expect to get the latest value of any record so long as `w + r > n`, because with this there will be at least one node that has the latest value that will return it. Such reads and writes are called Quorum Reads and Writes.\n\n## Approaches for Leaderless Replication\n\nNow that we understand Leaderless Replication, we look at approaches to implementing it. In the flow discussed above, the approach we saw was the client was sending reads and writes to all the replicated data nodes and, depending on the quorum configuration, decides the correctness. This approach is called *client-driven fan-out* and is very popular.\n\n![Client Driven Fan-out in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634241-245bb1bd-c766-44d0-8f0a-ebd0618c6628.png)\n\nAnother popular approach to implement Leaderless Replication is to have a *Node Coordinator*. The client will make the request to any one node, and it then starts to act as the coordinator for that transaction. This node coordinator will then take care of the fan-out to other nodes and complete the transaction. Upon completion, it returns the response to the client.\n\n![Node Coordinator Driven Fan-out in Leaderless Replication](https://user-images.githubusercontent.com/4745789/148634240-0c05e68f-ec35-4ce8-8053-e7334f616dd6.png)\n\nNode coordinator-based replication makes life simpler for clients by offloading all the complications and coordination to the node coordinator. [Apache Cassandra](https://cassandra.apache.org/_/index.html) uses this approach for implementing Leaderless Replication.\n\n# Why Leaderless Replication?\n\nNow that we understand the micro-nuances of Leaderless Replication, let's address the elephant in the room - Why should we even use Leaderless Replication when it is so complex to set up? There are a couple of strong reasons.\n\n## Strong Consistency\n\nLeaderless Replication is strongly consistent by default. In an over-simplified database cluster of `N` nodes, we are fan-out writes to all `N` nodes and wait for the `ACK` from at least `N/2 + 1` nodes; and while reading, we fan-out reads to all `N` nodes and wait for a response from at least `N/2 + 1` nodes, and then return the value that is most recent among all.\n\nGiven that we are playing with the majority here, the set of nodes that handle reads and that handles writes cannot be mutually exclusive and will always have at least `1` overlapping node having the latest value. The assurance of fetching the latest value when Read, no matter how immediate, is how Leaderless Replication ensures a Strong Consistency by design.\n\n> Note: As discussed in previous sections, it is not mandatory to fan-out reads and writes to majority nodes, instead we need a subset of nodes for reads `r`, and another subset for writes `w`, and ensure `r + w > N` for strong consistency.\n\n## Fault tolerance by design\n\nLeaderless Replication is fault-tolerant by design with no [Single Point of Failure](https://en.wikipedia.org/wiki/Single_point_of_failure) in the setup. Given there is no single node acting as a leader, the system allows us to fan-out writes to multiple nodes and wait for an `ACK`; and once we get it from a majority of nodes we are assured that our Write is registered and will never be lost.\n\nSimilarly, the reads do not go to just one node; instead, the reads are also fanned-out to all the nodes, and upon receiving the response from a majority of the nodes, we are bound to get the most recent value in one of those responses given there will be an overlap of at least `1` node where the latest write went and the value was read from.\n",
    "similar": [
      "rum",
      "master-replica-replication",
      "persistent-data-structures-introduction",
      "replication-formats"
    ]
  },
  {
    "id": 7,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "conflict-resolution",
    "title": "Conflict Resolution",
    "description": "In this essay, go in-depth to understand ways to resolve and avoid conflicts in a multi-master setup.",
    "gif": "https://media.giphy.com/media/l2JeiBuUsaFNQIfEk/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/148543014-5b785b5a-fa6c-4ef5-8e8a-656177033e9f.png",
    "released_at": "2022-01-09",
    "total_views": 275,
    "body": "\nEvery multi-master replication setup comes with a side-effect - Conflicts. Conflict happens when two or more database accepts conflicting updates on the same record. We say that the updates are conflicting when we cannot resolve them to one value deterministically. In the previous essay, we took a detailed look into [Conflict Detection](https://arpitbhayani.me/blogs/conflict-detection), and in this one, we go in-depth to understand ways to handle conflicts (resolve and avoid).\n\n# Conflict Resolution\n\nIn the case of a single-master setup, conflicts are avoided by funneling all the writes sequentially. When there are multiple updates on the same field, the last write operation on the record will determine the final value. We can leverage this insight into devising solutions that apply to multi-master setup.\n\nGiven that the writes can hit any Master in a multi-master setup, the challenge is to deterministically find the _order_ of the operations to identify which operation came last. So, the approaches for conflict resolution will all revolve around determining or assigning the order to the operations, somehow.\n\n## Globally unique ID to transaction\n\nOne possible way to determine the order of the write operations spanning multiple masters is to assign globally unique monotonically increasing IDs to each write operation. When conflict is detected, the write operation having the largest ID overwrites everything else.\n\n![Globally Unique ID](https://user-images.githubusercontent.com/4745789/148541564-aafe6b1d-66e8-434e-8879-85180d09be8f.png)\n\nA globally unique, monotonically increasing ID generator has challenges and is an exhausting problem to solve for scale across distributed nodes. Still, it is essential to consider the idea behind the solution and understand the pattern.\n\nThis approach is similar to ordering write on a single master node but without affecting write throughput and concurrent updates. The monotonically increasing globally unique ID gives an implicit ordering to the writes to determine which one came last and hence mimics *Last Write Wins*.\n\n## Precedence of a database\n\nGiven that managing an ID generator at scale could be taxing, another possible solution is to assign the order to the master nodes. Upon conflict, the write from the Master having the highest number wins.\n\n![Database Precedence - Conflict Resolution](https://user-images.githubusercontent.com/4745789/148541568-7f1da590-62ad-4764-9995-a3569fc23e0a.png)\n\nThis approach is very lightweight, given that assigning orders to master nodes is simple and an infrequent activity. This approach will not guarantee the actual ordering of writes, so it is possible that the actual Last Write got overwritten by some write that happened on Master with the higher ID (precedence).\n\n## Track and Resolve\n\nIf, for a use case, it is not possible to resolve the conflicts at the database level, then the best approach in such a scenario is to record the conflict in a data structure designed to preserve all the information. Build a separate job that reads this data structure and resolves the conflict through a custom business logic.\n\n# When to resolve conflict?\n\nThere are two possible places where we can inject our conflict resolution logic (handler); the first one is upon writing, and the second one is upon reading.\n\n## On Write\n\nIn this approach, as soon as a conflict is detected, the custom conflict resolution logic has triggered that resolve the conflict and make the data consistent. This is a more proactive approach to conflict resolution.\n\n## On Reading\n\nThe other approach is to be lazy and resolve conflict when someone tries to read the conflicting data. The custom conflict resolution handler is triggered when the read is triggered on the conflicting data, the database engine realizing it and then invoking the solution handler.\n\nThis lazy approach can be seen in action in scenarios where we have to ensure that the [writes are never rejected](https://arpitbhayani.me/blogs/conflict-detection), no matter what.\n\n# Conflict Avoidance\n\nNow that we have gone through these seemingly complex ways of conflict resolution, it seems better to try to avoid conflicts in the first place. This is indeed the simplest and widely adopted strategy for dealing with conflicts.\n\nA possible way to avoid conflict is by adding stickiness in the system, allowing all writes of a particular record to go to a specific Master node, ensuring sequential operations, and simplifying the core requirement of *Last Write Wins*.\n",
    "similar": [
      "architectures-in-distributed-systems",
      "conflict-detection",
      "mistaken-beliefs-of-distributed-systems",
      "monotonic-reads"
    ]
  },
  {
    "id": 8,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "conflict-detection",
    "title": "Conflict Detection",
    "description": "In this essay, we talk about conflicts and understand what they are, how to detect them in a multi-master setup.",
    "gif": "https://media.giphy.com/media/3o6MbiJPqtMPHykonK/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/143674841-79dc836f-e989-4d25-bfce-ee2939843ad4.png",
    "released_at": "2021-11-28",
    "total_views": 709,
    "body": "Every [multi-master replication](https://arpitbhayani.me/blogs/multi-master-replication) setup comes with a side-effect - Conflicts. Conflict happens when two or more database accepts conflicting updates on the same record. We say that the updates are conflicting when we are unable to resolve them to one value deterministically. In this essay, we talk about conflicts and understand what they are, how to detect them.\n\n# Conflicts\n\nSay we are building an online book store allowing users to purchase books at the click of a button. Like any e-commerce application, even ours has a _Shopping Cart_, which acts as a staging area for everything the user shops for. The user likes a book, adds it to the cart, and proceeds to pay once the shopping is done, proceed to payment. When a user adds a book to the cart, this operation can never be forgotten or rejected - as it will result in loss of revenue and a very poor user experience.\n\nSay the user had books `B1, B2, B3` in the cart already, and this state of the cart is consistent in both the master nodes `Master 1` and `Master 2`. Say the user just added book `B4` to the cart, and this request went to `Master 1` which makes the local state of `Master 1` to be `B1, B2, B3, B4`, while `Master 2` continues to remain at `B1, B2, B3`.\n\nNow, say the user added another book `B5` to the cart, and this request went to `Master 2`. The request reached `Master 2` before the changes from `Master 1` got a chance to be propagated. Since the master node will accept the write the local state of `Master 2` becomes `B1, B2, B3, B5` while state of `Master 1` is `B1, B2, B3, B4`.\n\n![Conflict in real world - shopping cart](https://user-images.githubusercontent.com/4745789/143672208-5be61867-13ba-41dd-bae5-d3f856512d54.png)\n\nThus, we have two versions of the same shopping cart. When the two master nodes syn, they will detect a conflict that needs to be resolved. The resolution to this conflict is not as simple as replacing one with the other because replacement will lead to the loss of information.\n\nThe correct way to address this situation is that we will have to merge the two versions of the cart such similar to the set union. This is a classical case of Conflict Detection and Resolution, and the possible resolution strategy depends on the application and context.\n\nIn the above example, we saw a custom conflict resolution strategy in a real use case, but resolving conflicts is not always possible. For example, when two users book the same seat for the same movie, the requests go to two different Masters, and both successfully acknowledge the user confirming the seat. Thus, in this case, the same seat for the same movie show was allotted to two different users. But, since we already sent the confirmation to the user, there is no way to resolve this conflict without giving one of the users an extremely poor experience.\n\n# Conflict Detection\n\nDetecting conflict is simple when we have a single Master node, given that we can serialize all the writes going through it - the second write waits while the first one executes.\n\nBut when we have a multi-master setup, all the Master nodes can accept the writes and successfully apply them to their copy of data. When the changes [asynchronously](https://arpitbhayani.me/blogs/replication-strategies) propagate to other Master nodes, the conflict is detected. Given that both the writes requests were successfully accepted and applied, there is no way to communicate the conflict to the client.\n\n![Conflict Detection - Async Replication](https://user-images.githubusercontent.com/4745789/143669401-7dbe6429-a802-496a-83ec-aafc58ca2989.png)\n\nGiven that it becomes tough to do something after we detect a conflict when the master-master replication is asynchronous, a possibly easier way out would be to make replication [synchronous](https://arpitbhayani.me/blogs/replication-strategies). In this setup, when one of the Master nodes accepts write, let is successfully apply to its own copy of data and synchronously propagate the write to other Master nodes before responding to the client.\n\n![Conflict Detection - Sync Replication](https://user-images.githubusercontent.com/4745789/143669672-51fcf264-97df-434e-940b-f77e3bfd3f2a.png)\n\nAlthough this approach solves the problem of detecting conflicts and getting a chance to resolve them, it makes the setup lose its main advantage of allowing multiple masters to accept writes in parallel.\n",
    "similar": [
      "architectures-in-distributed-systems",
      "mistaken-beliefs-of-distributed-systems",
      "indexing-on-partitioned-data",
      "monotonic-reads"
    ]
  },
  {
    "id": 9,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "multi-master-replication",
    "title": "Multi-Master Replication",
    "description": "In this essay, we look at what Multi-Master Replication is, the core challenge it addresses, use-cases we can find this replication in action, and the possible concerns of using it.",
    "gif": "https://media.giphy.com/media/3orieW7ikndkDjQuT6/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/140054768-d9bd1eaf-4d26-49bc-9035-024e4a7e010a.png",
    "released_at": "2021-11-03",
    "total_views": 497,
    "body": "\nA not-so-common yet super-useful replication strategy is Multi-Master replication - in which multiple nodes in the cluster accept writes, contrary to what is observed in a typical [Master-Replica replication](https://arpitbhayani.me/blogs/master-replica-replication). In this essay, we look at what Multi-Master Replication is, the core challenge it addresses, use-cases we can find this replication in action, and the possible concerns of using it.\n\n# Multi-Master Replication\n\nA Multi-Master setup has multiple nodes of a database cluster accepting the write requests. It typically depends on the client to pick a Master node, or an abstracted proxy may choose one. As soon as a Master node gets the write request, it accepts it and applies the changes. Once the update is made on one Master, it is [propagated](https://arpitbhayani.me/blogs/replication-strategies) to all other Master nodes of the cluster asynchronously, making the data eventually consistent.\n\nEach Master, thus, also acts as a replica (not read-replica) for the other Masters, reading and applying the updates on its copy of data. Each Master node can optionally form a sub-cluster by adding [read-replicas](https://arpitbhayani.me/blogs/master-replica-replication) and scaling the overall incoming reads.\n\n![Multi-Master Replication](https://user-images.githubusercontent.com/4745789/139211714-fc9266bd-ca22-48c4-9095-c6bff0ae99e6.png)\n\n# Why do we need a Multi-Master setup?\n\nAn exciting exploration is to find why we would ever need a Multi-Master setup and what kind of problem it would solve for us? Here are three key reasons to have a Multi-Master setup.\n\n## Sharing Load\n\nThe most common reason to have a Multi-Master setup is to allow our database cluster to handle more write traffic than a single node. Vertical scaling has theoretical and practical limitations, and the machine can\u2019t go beyond a particular scale.\n\nTo truly scale the database cluster, we have to scale the reads by adding read-replicas and having multiple machines that handle writes. Hence, when the writes on a database cluster become a bottleneck, have multiple Master nodes instead of a single one that can take in incoming writes allowing our cluster to share the load and handle multi-fold of write requests.\n\nTypically, clients choose one of the many Master nodes to send their Write requests. These updates are then propagated [asynchronously](https://arpitbhayani.me/blogs/replication-strategies) to other Masters keeping them in sync with the changes and making the system eventually consistent.\n\n## Maintaining a second copy\n\nThe second common scenario where Multi-Master comes in handy is when we want to keep a second consistent copy of our Master database, which is also required to accept the write requests. This sounds convoluted, but in the real world, such a requirement is widespread. Let\u2019s go through a few scenarios.\n\n### No SPoF Master\n\nJust like any other node in the database cluster, the Master node can also crash. If the only Master node of the cluster takes all the write requests, crashes, it makes the entire ineffective resulting in a massive downtime. This is a classical case of our Master node becoming the [Single Point of Failure](https://en.wikipedia.org/wiki/Single_point_of_failure).\n\nGiven that the failures and crashes are inevitable, it makes sense to have multiple masters running in a cluster and all of them entertaining the write requests. This way, if one of the Master nodes crashes, the other Master can continue to handle the write requests seamlessly, and the cluster will continue to function.\n\n### Lower latencies across geographies\n\nWhen the clients of your database are spread across geographies, the write latencies shoot up since all the writes across all geographies have to go to this one region where the Master resides.\n\nTo keep the write latencies to a minimum across geographies, we set up Multi-Master such that one Master node resides in one region closer to the user. When a client makes the write request, the request can be served from the closest Master giving a great user experience.\n\n### Upgrading the database to a newer version\n\nEvery database needs at least a yearly upgrade, and it is never easy to do it on the fly. Before the version is upgraded, every dependent service typically tests its business logic on a newer version. We need to have two databases running during this exercise - one with the older version handling production and the other with the newer version. Both of these databases require to be kept in sync, and both should accept writes. The writes on the newer database will not be as dense as on the production, considering that the service teams will test their workflows on it.\n\nA typical way to facilitate this parallel setup is to have a Multi-Master replication set up between the two databases - one with an older version serving production traffic, the other with a newer version given to application teams to test their workflows. Apart from testing their workflows, the parallel setup also helps incrementally move traffic from old to new versions keeping the blast radius at a bare minimum in case of failure.\n\nThe other two similar scenarios where Multi-Master replication comes in handy and are very similar to database upgrade are\n\n-   Encrypting the database without taking in a significant downtime\n-   Downscaling an over-provisioned database without a massive downtime\n    \n\n## Need of a split-brain\n\nThe third but particular reason for a Multi-Master setup is where having a split-brain is the necessity and the core of the system. In a general sense, split-brain is considered an erroneous situation that causes mayhem, but it is not a bug but a feature in these scenarios.\n\nA great example of such a system is Collaborative Editing tools like Google Docs, where multiple users on the same document are editing it simultaneously. Each user has its copy of data and edits as if it owns the document wholly. Another example of a split-brain use case has multiple clients using an offline database to work on the same set of values offline and then sync them with a central value store once the internet is back.\n\n# Concerns with Multi-Master\n\nAlthough Multi-Master replication is excellent and solves a wide range of problems in the real world, it comes with its own set of concerns. Before deciding if you want to have a Multi-Master setup, do consider the following concerns.\n\n## Eventual Consistency\n\nWith the replication between Multi-Master being asynchronous, the updates made on one Master will take some time to reflect on the other Masters, making the system eventually consistent. Because of this eventual consistency, a relational database running in Multi-Master mode will lose its ACID guarantees.\n\n## Sluggish Performance\n\nEvery update happening on one Master needs to be sent to every other Master node in the cluster. This data movement adds a considerable load on the network bandwidth and could lead to a sluggish network performance at scale.\n\n## Conflict Resolution\n\nThe main concern while running a database in Multi-Master mode is Conflict. Since all Master nodes accept writes, there may arise situations where the same entity is updated on multiple Master simultaneously, leading to conflicts while syncing. The way these conflicts are handled depends on the application at hand. Some use cases would suggest discarding the entire sequence of writes, while others would mean the last write wins. It becomes the responsibility of the business logic and the use case to define steps to be taken upon a conflict.\n",
    "similar": [
      "handling-outages-master-replica",
      "data-partitioning",
      "replication-formats",
      "data-partitioning-strategies"
    ]
  },
  {
    "id": 10,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "monotonic-reads",
    "title": "Monotonic Reads",
    "description": "Asynchronous replication leads to a fascinating situation where it feels like we are going through a wormhole traveling back and forth in time. In this essay, we understand why this happens and the consequences and devise a quick solution to address it.",
    "gif": "https://media.giphy.com/media/3o6Mb84ODTP8FgQQuI/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/135490742-ecf2f507-2a61-465d-a2de-515a913f5588.png",
    "released_at": "2021-10-03",
    "total_views": 348,
    "body": "Asynchronous replication leads to a fascinating situation where it feels like we are going through a wormhole traveling back and forth in time. In this essay, we understand why this happens and the consequences and devise a quick solution to address it.\n\n# Through the wormhole\n\nAs per Wikipedia, a [wormhole](https://en.wikipedia.org/wiki/Wormhole) can be visualized as a tunnel with two ends at different points in spacetime (i.e., different locations, different points in time, or both), allowing us to traverse back and forth in time again and again. So, where exactly is a wormhole in the context of a distributed datastore?\n\nSay, we have a distributed KV store having one Master and 2 Replica nodes, and we make three updates on a key `X`, the first update `U1` sets `X` as `1`, the second update `U2` sets it to `2`, while the third update `U3` sets `X` to `3`. Like in a typical Master Replica setup, the writes go to the Master, and they are propagated to Replicas through an Asynchronous replication. The reads are typically sent to any one of the Replicas at random.\n\nThe writes are propagated to the Replicas [asynchronously](https://arpitbhayani.me/blogs/replication-strategies), which means both the Replicas will have slight replication lags and say this lag on Replica 1 is of `2 seconds`, and on Replica 2 is `1 second`. As of current time instant, all the three updates `U1`, `U2`, and `U3` have happened on the Master, while only update `U1` has reached Replica 1, and it is lagging behind Replica 2 that saw updates `U1` and `U2`.\n\n![time traveling database - monotonic reads](https://user-images.githubusercontent.com/4745789/135746302-4ff940ba-9ca4-4925-9362-d5fc03f166f6.png)\n\nSay, after making the update `U3` at instant `t`, the User initiates a read that hits Replica 2. Since the update `U3` is yet to reach the Replica 2, it returned `2`, an old value of `X`. This breaks [Read your write consistency](https://arpitbhayani.me/blogs/read-your-write-consistency) and make the user feel that the recent write is lost. Say the user makes another read after this one, which now reaches Replica 1, and since the Replica 1 has just seen the update `U1`, it returns the value `1`, which is even older than the last returned value.\n\nHere we see that after the latest write `U3`, the two successive reads yielded historical values depending on which Replica it hit, giving a feel of traveling back in time. The situation becomes even more interesting when the Replica starts to catch up. Depending on which Replica the read request went to, the User would be oscilating between the old and new values of `X`, giving it a feel of going through the wormhole.\n\n# Monotonic Reads\n\nMonotonic read guarantees users to see value always moving forward in time, no matter how many or how quickly it tries to read the data. It is a weaker guarantee than strong consistency but a stronger one than eventual consistency.\n\n## Achieving Monotonic Reads\n\nThe root cause of this seemingly random fetch lies in allowing the read request to hit Replicas with different Replication Lags. For a particular Replica, the writes are always applied in order, moving forward in time. So, a niche solution for this problem is to make the read request of a user sticky to a replica.\n\n![monotonic reads](https://user-images.githubusercontent.com/4745789/135746307-2c3fc584-7154-4d13-96d9-b1a2b29c7d49.png)\n\nOnce it is ensured that a particular user's request only goes to a specific replica, that User will see updates always moving forward in time as the Replica continues to catch up with the Master.\n\nTo implement stickiness, the server can pick the Replica using the [hash](https://en.wikipedia.org/wiki/Hash_function) of the User ID instead of picking it randomly. This way, the stickiness between a user and a Replica helping us achieve Monotonic Reads.\n",
    "similar": [
      "conflict-detection",
      "architectures-in-distributed-systems",
      "conflict-resolution",
      "indexing-on-partitioned-data"
    ]
  },
  {
    "id": 11,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "read-your-write-consistency",
    "title": "Read-your-write Consistency",
    "description": "Read-Your-Writes consistency states that the system guarantees that, once an item has been updated, any attempt to read the record by the same client will return the updated value.",
    "gif": "https://media.giphy.com/media/3o6MbsTCszAbi6f1aE/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/134203710-5284102a-4d0f-43b1-bcd9-6668040ac550.png",
    "released_at": "2021-09-22",
    "total_views": 1120,
    "body": "The most common way to scale the reads hitting a distributed data store is by adding [Read Replicas](https://arpitbhayani.me/blogs/master-replica-replication). These replicas handle all the reads of the systems freeing up the Master to deal with the writes. Although Replicas do help us scale, it brings a new set of problems; and in this essay, we discuss one such issue, called \"Read-your-write\" consistency, and look at possible solutions.\n\n# The Problem\n\nIn a [Master-Replica](https://arpitbhayani.me/blogs/master-replica-replication) setup, the Writes happening on the Master take some time to reach the Replica. This delay in propagation is called Replication Lag. If a client has made a Write and is immediately trying to read the written item, this read may go to the Read Replica that is yet to sync with the Master.\n\nWhen the client issues the Read on a Replica that has yet to receive the write, it leads to an undesirable behavior wherein the client will see the old value (or null) and think that the write it made was lost.\n\n**Read-Your-Writes consistency** states that the system guarantees that, once an item has been updated, any attempt to read the record by the same client will return the updated value. This consistency makes no promises about other clients getting the updated value immediately after the Write and is meant to reassure the user that their Write is successful.\n\n## Problem in action\n\nTo ensure that the issue we are trying to address is not something made up, let's see what happens across industries when we do not ensure Read-your-write consistency.\n\nImagine you made a post on a Social Platform, and when you refreshed the page, it threw a 404 error saying Post does not exist, or when you fixed a spelling mistake on the post and refreshed the page, you still see the old text with the same spelling mistake. This inconsistency leads to a terrible user experience.\n\n![Read after Write Fails - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198510-78129b65-5c4c-4d88-a10a-39523d1886d7.png)\n\nIn some cases, caching is the root cause; but it is also possible that the Read request for the post was routed to the Read Replica, which was yet to apply the write that happened on the Master, typically due to Replication Lag.\n\nA few more examples of why we need Read-Your-Write consistency:\n - Imagine getting a match on Tinder and disappearing upon refresh\n - Imagine buying an AAPL Stock and seeing no trace of it on the orders page\n - Imagine adding items in your Amazon cart and realizing it empty when placing the order\n\n# Implementing Read-your-write consistency\n\nThe primary root cause of not having Read-your-write consistency is Replication Lag. The longer it takes for the write to propagate to the Replica, the longer our end user will see an inconsistent behavior depending on which Read Replica serves the read. So, every single solution revolves all-around reading from a place where Replication Lag is zero. We start dissecting and devising approaches to address this problem.\n\n## Synchronous Replication\n\nReplication Lag exists because the writes are propagated to Replica [asynchronously](https://arpitbhayani.me/blogs/replication-strategies). If the replication is done synchronously, every Write operation on Master is not termed completed unless it is done replicating it on all the Replicas. This way, the Master and the Replica will always remain in sync with ZERO Replication Lag, and no matter to which Replica the read is forwarded, it will always have the latest copy of the data.\n\n![Synchronous Replication - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/128765459-67347320-5b77-4722-884b-015fc1b0c5fb.png)\n\nSynchronous Replication sounds tempting and foolproof approach, but it comes at a massive cost. Synchronous Replication severely affects the write throughput of the database. More than that, write failing on any one of the Replica will choke the entire system. Gaining such Strong Consistency at the expense of write throughput and availability is not a great choice.\n\n## Pinning User to Master\n\nInstead of serving Read requests from the Replica, what if we also serve them from the Master. Forwarding all the read requests to Master defeats the purpose of creating Read Replica - scaling reads. But since we know that the Master will always have the latest copy of the data, can we devise something around it?\n\nInstead of routing all the reads from all the users to the Master, what if we routed reads of the User who recently performed the Write to the Master? This sounds promising and addresses our concern, and this exactly is Pinning the User to the Master.\n\nWhen a user performs a Write operation, for a specific time window, we pin the User to the Master node, which means every single Read and Write coming from the user will go to the Master, which means the Reads will happen from the data node that always has the latest copy of the data and hence we would achieve Read-your-write consistency.\n\nThe time window for pinning should be big enough to ensure that the Writes happened on the Master would have propagated to all the Replicas; this ensures that once the pinning window is over and reads of the user start hitting the Replica, it would continue access the latest copy of the data.\n\n![User PInning to Master - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198508-4c8bd1e4-2336-4063-8ceb-06e675c24554.png)\n\nAlthough this solves the problem well, it is not optimal when the system is very write-heavy. If in a system most users Write, this would mean the requests most users will go to the Master, most of the time, defeating the purpose of Replica and becoming the bottleneck. \n\n## Fragmented Pinning\n\nPinning a user to the Master would mean queries, both Read and Write, made by the user will hit the Master for a configured time window. But instead of pinning everything, what if we pick only a few critical reads to hit the Master; this is Fragmented Pinning.\n\nFor example, in social media, once the user made or updated a post, pin the user to the Master for 10 minutes such that the request for getting the post goes to the Master; all other reads would continue to hit the Replica.\n\n![Fragmented Pinning - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198504-5b886713-9a16-45ba-9a63-332d19b5894c.png)\n\nBy doing fragmented pinning, we ensure that most critical and most likely Read operations, during the pinned window, go to the Master, ensuring that our Master is not overwhelmed even when the system is write-heavy.\n\n## Master Fallback\n\nThere is one more way of ensuring Read-your-write consistency, but it works well for a system with a lower Replication Lag, and most queries made on the data store are for keys that exist, i.e., fewer 404s, and the approach is using Master as a fallback.\n\n![Master Fallback - Read Your Write Consistency](https://user-images.githubusercontent.com/4745789/134198497-099bce25-bef1-468e-84b2-69b31e1ae3e0.png)\n\nThere is no User Pinning in this approach, and all the Read operations go to the Replica while the Master node only handles Write. The Master and Replica are kept in sync using [asynchronous replication](https://arpitbhayani.me/blogs/replication-strategies). If the Read request that went to the Replica resulted in the 404, i.e., Key Not Found, the application forwards the same query on the Master node and then returns the response.\n\nSince the reads go to the Replica and the Master every time the data is not present in Replica, for this system to be efficient, we need fewer cases where this particular path would be taken, and also the Replication Lag to not inflate much.\n",
    "similar": [
      "new-replica",
      "handling-outages-master-replica",
      "multi-master-replication",
      "replication-strategies"
    ]
  },
  {
    "id": 12,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "handling-outages-master-replica",
    "title": "Handling outages in a Master-Replica setup",
    "description": "This essay talks about the worse - nodes going down - impact,  recovery, and real-world practices.",
    "gif": "https://media.giphy.com/media/xT5LMskx2CiLPpAkk8/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/132286179-14358b33-95cf-4d48-9419-5eaa727ffd56.png",
    "released_at": "2021-09-07",
    "total_views": 375,
    "body": "Master-Replica architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines. In the previous essay, we saw how a [new replica](https://arpitbhayani.me/blogs/new-replica) is set up in a distributed data store and the challenges that come with that. This essay talks about the worse - nodes going down - impact, recovery, and real-world practices.\n\n# Nodes go down, and it is okay\n> Anything that can go wrong will go wrong. - [Murphy's Law](https://en.wikipedia.org/wiki/Murphy%27s_law)\n\nOutages are inevitable; we cannot completely eradicate them, but we can be prepared enough to minimize the impact. One way to make any distributed data store robust is by assuming that the node goes down after executing every operation. Building systems around these aggressive failure scenarios is the easiest way to make any distributed system robust and fault-tolerant.\n\nA few reasons why nodes crash are: system overload, hardware issues, overheating, physical damage, or worse, a natural disaster. Nodes going down is a very common phenomenon, and it happens all the time in massive infrastructures. So, instead of panicking, deal with it by minimizing cascaded failures and speeding up the recovery.\n\nIn a Master-Replica setup, there are two kinds of nodes - Master and Replica. Let's see what happens when the nodes start crashing and recover the system after the crash.\n\n# Replica outage\n\nWhen Replica is down, the reads going to it fail. If the nodes are within an abstracted cluster, the front-facing proxy can redirect the request to another Replica upon discovering the outage.\n\nTalking about writes during the Replica outage, we know that the Replica does not handle any writes directly from the client. Still, it does pull the updates through the [Replication log](https://arpitbhayani.me/blogs/replication-formats) and re-applies the changes on its own copy of data. So, are these writes affected during the outage?\n\nTo keep track of the updates that Replica already applied to its data, it keeps track of the [Sequence Number](https://arpitbhayani.me/blogs/new-replica) of the operation. This **sequence number** is stored on disk and is atomically updated after pulling and applying update operation from the Master.\n\nWhen the Replica crashes and recovers, it reconnects to the Master, and the hindered replication resumes from the last processed sequence number from what was persisted on the disk. Once the replication resumes, the Replica will fetch all the updates that happened on the Master, apply them on its own copy of data, and eventually catch up. \n\n# Master outage\n\nMaster is the most critical component in any distributed data store. When the Master crashes and becomes unavailable, it does not accept any request (read or write) coming to the system. This directly impacts the business, resulting in the loss of new writes coming to the system. \n\nThe 3 typical steps in fixing the Master outage are:\n\n - Discover the crash\n - Set up the new Master\n - Announce the new Master\n\n## Discovering the crashed Master\n\nThe first step of fixing the crashed master is to identify that the Master crashed. This discovery step is not just limited to Master but is also applicable to Replica and other relevant nodes in the datastore. So, although we might use the term \"Master\" while discovering the crash, the process and the flow would be the same for other nodes.\n\nA typical process of detecting a crash is as simple as checking the **heartbeat** of the Master. This means either Master ping the orchestrator that it is alive or the orchestrator checking with the Master if it is healthy. In both cases, the orchestrator would expect that the Master responds within a certain threshold of time, say 30 seconds; and if the Master does not respond, the orchestrator can infer the crash.\n\n![discovering an outage](https://user-images.githubusercontent.com/4745789/132089584-a5177e7a-3104-4f86-8d9d-cbb106b7ce35.png)\n\nIn a typical architecture, the orchestrator is a separate node that keeps an eye on all the nodes and repeatedly checks how they are doing. There are [Failure Detection](https://en.wikipedia.org/wiki/Failure_detector) systems that specialize in detecting failures, and one efficient and interesting algorithm for detecting failure is called [Phi \u03c6 Accrual Failure Detection](https://arpitbhayani.me/blogs/phi-accrual) that instead of expecting a heartbeat at regular intervals estimates the probability and sets a dynamic threshold.\n\n## Setting up the new Master\n\nWhen the Master crashes, there are two common ways of setting up the new Master - manual and automated. Picking one over the other depends on the sophistication and maturity of the organization and the infrastructure. Still, it is typically observed that smaller organizations tend to do it manually, while the larges ones have automation in place.\n\n### The manual way\n\nOnce the orchestrator detects the Master crash, it raises the alarm to the infrastructure team. Depending on the severity of the incident and the nature of the issue, the infrastructure engineer will either.\n\n - reboot the Master node, or\n - restart the Master database process, or\n - promote one of the Replica as the new Master\n\nSetting up the Master after the crash manually is common when the team is lean, and the organization is not operating at a massive infrastructure scale. Master crashing is also once in a blue moon event, and setting up complex automation just for one rare occurrence might not be the best use of the engineer's time.\n\nThe entire process is automated once the infrastructure grows massive, and even an outage of a few minutes becomes unacceptable.\n\n### The automated way\n\nIn a Master-Replica setup, the automation is mainly around promoting an existing replica as the new Master. So, when the orchestrator detects that the Master crashed, it triggers the [Leader Election](https://en.wikipedia.org/wiki/Leader_election) among the Replicas to elect the new Master. The elected Replica is then promoted, the other Replicas are re-configured to follow this new Master.\n\n![electing the new master](https://user-images.githubusercontent.com/4745789/132089586-d52e558f-10e2-4f10-8807-5920f9117ea8.png)\n\nThe new Master is thus ready to accept new incoming writes and reads to the system. This automated way of setting up the new Master is definitely faster, but it requires a lot of sophistication from the infrastructure, the algorithms, and practices before implementation.\n\n## Announcing the new Master\n\nOnce the new Master is set up, either manually or elected among the Replicas, this information must be conveyed to the end clients connecting to the Master. So, as the final step of the process, the Clients are re-configured such that they now start sending writes to this new Master.\n\n![announcing the new master](https://user-images.githubusercontent.com/4745789/132089587-dc7e25c6-f43d-4be5-a2c1-e09253ec2205.png)\n\n# The Big Challenge \n\nWhat if the Master crashed before propagating the changes to the Replica? In such scenarios, if the Replica is promoted as the new Master, this would result in data loss, or conflicts, or split-brain problems if the old Master continues to act as the Master.\n\n## The Passive Master\n\nData loss or inconsistencies is unacceptable in the real world, so as the solution to this problem, the Master always has a passive standby node. Every time the write happens on the Master, it is [synchronously replicated](https://arpitbhayani.me/blogs/replication-strategies) to this stand by passive node, and asynchronously to configured Replicas.\n\nThe write made on the Master is marked as complete only after the updates are synchronously made on this passive Master. This way, the passive Master node is always in sync with the Master. So when the main Master node crashes, we can safely promote this passive Master instead of an existing Replica.\n\n![The Passive Master](https://user-images.githubusercontent.com/4745789/132282367-50feb0be-f952-4bf3-ab62-85ab4f6c86d6.png)\n\nThis approach is far better and accurate than running an election across asynchronously replicated Replicas. Still, it incurs a little extra cost of running a parallel Master that will never serve production traffic. But given that it helps in avoiding data loss, this approach is taken by all managed database services.\n",
    "similar": [
      "multi-master-replication",
      "replication-formats",
      "replication-strategies",
      "data-partitioning"
    ]
  },
  {
    "id": 13,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "new-replica",
    "title": "The New Replica",
    "description": "In this one, we take a look into how these Replicas are set up and understand some quirky nuances about Replication.",
    "gif": "https://media.giphy.com/media/l0HlyVvfbfr1RKR0I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/130425162-e02fcffe-3c61-4e90-98b6-6c7a25cb884e.png",
    "released_at": "2021-08-23",
    "total_views": 293,
    "body": "A distributed data store adds Replica to scale their reads and improve availability. This is the most common and logical way to scale the throughput of a system without massively changing the architecture. In the previous essays, we talked about [Master-Replica Replication](https://arpitbhayani.me/blogs/master-replica-replication), different [Replication Strategies](https://arpitbhayani.me/blogs/replication-strategies), and [Replication Formats](https://arpitbhayani.me/blogs/replication-formats). In this one, we take a look into how these Replicas are set up and understand some quirky nuances.\n\n# Setting up a Replica\n\nIn the [Master-Replica setup](https://arpitbhayani.me/blogs/master-replica-replication), Replica is a node that follows the Master. The updates happening on the Master are communicated to the Replica through the process called Replication. The Master publishes the updates on the Replication Log, which are then pulled by the Replica and applied on its own copy of data.\n\nThe Replica nodes are read-only in the Master-Replica setup, making this architecture pattern suitable for scale reads and improving availability. The most typical steps taken when a new Replica is set up are\n\n1.  Take a point-in-time **snapshot** of the Master data.\n2.  **Spin up** a Replica node with this snapshot.\n3.  **Start** the process on the Replica and configure it to follow the Master.\n4.  The process of **Replication** begins, and the Replica eventually **catches up**.\n\n![The new Replica](https://user-images.githubusercontent.com/4745789/130204028-db759df1-2ea9-4aa5-98f4-6cb3e2b16813.png)\n\nNow that we have talked about the general process of setting up a new Replica, let's dissect the steps and answer really quirky questions about it.\n\n## Replica keeping track of Replication\n\nOnce the Replication is set up between the Replica and the Master, one of the key things to understand is how a Replica keeps track of operations and updates that it has pulled and applied on its own copy of data.\n\nThe idea to achieve this is simple. Every update on the Master is associated with a monotonically increasing **sequence number**. Both the Master and the Replica keep track of this sequence number, and it denotes the sequence number of the last operation executed on their respective copy of the data.\n\nSince the Master generates the sequence number, it holds the latest one. The Replica could be a couple of sequence numbers behind, as it needs to pull the updates from the Master, apply the updates, and then update the sequence number. Thus, by tracking the sequence number, the Replica keeps track of the Replication, order of the updates, and understands the Replication lag.\n\n![Sequence Number: Replica](https://user-images.githubusercontent.com/4745789/130345784-8892f5f4-7ed1-4588-bbac-08ce39b7c752.png)\n\nSince the Replica persists the sequence number on disk, even if the server reboots, it can continue to resume the Replication since the reboot.\n\n## Why do we need a point-in-time snapshot?\n\nNow that we know how a Replica keeps track of the replication, we answer an interesting question; do we really need a point-in-time snapshot of Master to create a Replica?\n\nThe answer to this situation is simple; it is not mandatory to take a point-in-time snapshot of Master and create Replica out of it. We can also do it on a blank data node with no hiccups at all. The only caveat here is that when we set up Replication on a blank data node, it will have to pull in all the update operations on the Master node and apply them to its own copy of the data.\n\nWhen a Replica needs to pull in literally every single update operation and apply, it will take a far longer time to catch up with the Master. The Replica will start with an extremely high Replica lag, but eventually, this lag will reduce. Nonetheless, it will take a lot of time to catch the Master, rendering this approach unsuitable.\n\nWhen the point-in-time snapshot is taken, the sequence number of the Master, at that instant, is also captured. This way, when the Replication is set up on this copy of data, it will have far fewer operations to replicate before it catches up with the Master. Hence, instead of creating Replica from scratch, setting it up from a recent point-in-time snapshot of Master makes the Replica quickly catch up with the Master.\n\n## How does a Replica catch up with the Master?\n\nReplica pulls the replication log from the Master node and applies the changes on its own copy of data. If Replica is already serving live read requests, how it actually catches up with the Master?\n\nThe entire Replication process is run by a separate Replication thread that pulls the data from the Replication Log of the Master and applies the updates on its own copy of the data. For Replication to happen, the thread needs to be scheduled on the CPU. The more CPU this Replication thread gets, the faster the replication would happen. This is how the Replica continues to Replicate the updates from the Master while serving the live traffic.\n\n## Is it possible for a Replica never to catch the Master?\n\nIf the progress of Replication depends on the CPU cycles that the Replication thread gets, does this mean it is possible for a Replica never to catch the Master?\n\nYes. It is very much possible for a replica to never catch up with the Master. Since the Replica typically also serves the live read traffic, if some queries are CPU intensive or take massive locks on the tables, there are chances that the Replication thread might get a minimal CPU to continue to replication.\n\nAnother popular reason a Replica might never catch up with the Master is when the Master is overwhelmed with many write operations. The influx of write is more than what Replica can process, leading to an ever-increasing Replica lag.\n\nHence whenever a Replica sees a big enough Replica lag, the remediation is\n\n-   to kill read queries that are waiting for a long time, or\n-   to not let it serve any live traffic for some time, or\n-   to kill CPU intensive read queries, or\n-   to kill queries that have taken locks on critical data\n\nWe ensure that the Replication thread gets CPU that it deserves to continue the replication by taking some or all of the above actions. Our intention while fighting high replica lag is to reduce somehow the load on the CPU, whatever it takes.\n",
    "similar": [
      "replication-strategies",
      "read-your-write-consistency",
      "handling-outages-master-replica",
      "multi-master-replication"
    ]
  },
  {
    "id": 14,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "replication-formats",
    "title": "Replication Formats",
    "description": "When we are employing a Master-Replica pattern to improve availability, throughput, and fault-tolerance, the big question that pops up is how the writes happening on the Master propagates to the  Replica. In this essay, we will talk about exactly this and find out about Replication Formats.",
    "gif": "https://media.giphy.com/media/l0HlyVvfbfr1RKR0I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/129456630-73db5669-5fb9-47a6-bd13-72fd0c5a2e54.png",
    "released_at": "2021-08-15",
    "total_views": 334,
    "body": "[Master-Replica](https://arpitbhayani.me/blogs/master-replica-replication) architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines.\n\nSo, when we are employing a Master-Replica pattern to improve availability, throughput, and fault-tolerance, the big question that pops up is how the writes happening on the Master propagates to the Replica. In this essay, we will talk about exactly this and find out about Replication Formats.\n\n# Write Propagation\nOnce the write operation is successful on the Master node, the changes must be propagated to all the Replicas. This is done via a log known as Replication Log, Commit Log, or Binary Log. Once the write on the Master is successful, an event is recorded in this Log file; and this file is then **pulled** by the Replicas.\n\nOnce the Replica gets this log file, it goes through the events and starts applying the necessary changes on its copy of the data. This way, it continuously follows the changes happening on the Master node. The time elapsed between the write operation on the Master and the operation taking effect to the Replica is called Replication Lag.\n\nNow that we have a solid understanding of Write Propagation, we focus on the primary agenda of this essay, Replication Format, and talk about how these formats share the entire Replication process.\n\n# Replication Formats\nAny write operation happening on the Master is logged in the Replication log file as an event. The format in which these events are logged in the Log file is called Replication Format. The two Replication formats that are widely used across distributed data stores are Statement-based and Row-based formats.\n\n## Statement-based Format\nIn Statement-based format, the Master node records the operation as an event in its log, and when the Replica reads this log, it executes the same operation on its copy of data. This way, the operation on the Master node is executed on the Replica, which keeps it in sync with the Master.\n\nSay the Client fires an operation on the Master to bulk update all the `5` tasks of a user to be marked as `done`. The operation fired by the Client on the Master node would look something like this.\n\n```sql\n  UPDATE tasks SET is_done = true WHERE user_id = 53;\n```\n\nWhen the write operation is completed on the Master, this exact operation is recorded as an event in the Replication Log file. When the Replica reads this log file, the node executes this operation and updates the same `5` tasks on its own copy of the data.\n\n![statement based replication](https://user-images.githubusercontent.com/4745789/129456634-be745df6-541b-4e75-a1e3-4f4f625cc45e.png)\n\n### Advantages\nThe events recorded in the Replication Log are the actual operations that happen on the Master. Hence, the log files take up the bare minimum storage space required. It will not matter if the operation affects one row or thousand; it will be recorded as one event in the Log file.\n\nAnother great benefit of this format is that it can be used to audit the operations on the database because we are recording the operations verbatim in the Log file.\n\n### Disadvantages\nThe biggest and the most significant disadvantage of the Statement-based format show up when the non-deterministic operations are fired on the Master. The operations such as `UUID()`, `RAND()`, `NOW()`, etc, generate value depending on factors that are not under our control. When these operations fire on the Replica, they might generate values different from the value they yielded on the Master, leading to data corruption.\n\nSince the Replica node, apart from replicating from the Master, is also actively handling requests, some locks might be taken on some of its entities by the executing queries. When a conflicting query is fired from the replication thread, it could result in unpredictable deadlock or stalls.\n\n## Row-based Format\nIn Row-based format, the Master node logs the updates on the individual data item instead of the operation. So the entry made in the Log file would indicate how the data has changed on the Master. Hence, when the Replica reads this log, it updates its copy of the data by applying the changes on its data items. This way, the operation on the Master node happens on the Replica, and the Replica nodes remain in sync with the Master.\n\nSay the Client fires an operation on the Master to bulk update all the `5` tasks of a user to be marked as `done`. The operation fired by the Client on the Master node would look something like this.\n\n```sql\n  UPDATE tasks SET is_done = true WHERE user_id = 53;\n```\n\nIn the row-based format, instead of recording the operation, the Master node records the updates made on the data items. Since the operation in question updated `5` rows, the events recorded in the Replication Log file would contain `5` entries, one for each data item changed and would look something similar to\n\n```sql\ntasks:121 is_done=true\ntasks:142 is_done=true\ntasks:643 is_done=true\ntasks:713 is_done=true\ntasks:862 is_done=true\n```\n\nHence, one operation on the Master is fanned out as series of updates on the data items and is consumed by the Replica. The Replica then reads these events and applies the changes on its copy of the data.\n\n![row based replication](https://user-images.githubusercontent.com/4745789/129456632-ad7b67ae-7ff0-4d35-97b0-0ea6d6a3bd87.png)\n\n### Advantages\nThe major advantage of the Row-based format is that all the changes can be safely and predictably applied on the Replica. This approach is safe even with the non-deterministic operations because what gets written is the computed value.\n\n### Disadvantages\nWhen the Master node completes the operation, it takes the lock on the Replication Log file and then records the events. Since the number of events recorded in the log file will be the number of data items changed, the lock taken by the Master node will be longer, choking the throughput.\n\nAnother obvious disadvantage in this approach is fan-out. If an operation changes 5000 data items, it will result in 5000 events in the Log file, and if such operations are frequent, this will make Logfile take a lot of storage space.\n",
    "similar": [
      "handling-outages-master-replica",
      "multi-master-replication",
      "replication-strategies",
      "rum"
    ]
  },
  {
    "id": 15,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "replication-strategies",
    "title": "Replication Strategies",
    "description": "In this essay, we take a quick yet verbose look into Synchronous, Asynchronous, and Semisynchronous replication strategies and understand their implications.",
    "gif": "https://media.giphy.com/media/l0HlyVvfbfr1RKR0I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/128717208-a0f495c1-ccf6-4039-b936-f1cc29d722f3.png",
    "released_at": "2021-08-10",
    "total_views": 821,
    "body": "In a distributed system, when replication is set up between data nodes, there are typically three replication strategies - Synchronous, Asynchronous, and Semi-synchronous. Depending on the criticality of data, its consistency, and the use-case at hand, the system chooses to apply one over another. In this essay, we take a quick yet verbose look into these strategies and understand their implications.\n\nBefore we jump into the replication strategies, let's first understand the need for them. When the data is replicated, multiple copies of the same data are created and placed on multiple machines (nodes). A system replicates the data to\n\n-   improve availability, or\n-   prepare for disaster recovery, or\n-   improve performance by leveraging parallel reads across replicated data, or\n-   keep the data geographically close to the user, e.g., CDN\n\nThe Replication comes into action when the Client initiates the write on the Master node. Once the Master updates its copy of the data, the replication strategy dictates how the data would be replicated across the Replicas.\n\n## Synchronous Replication\nIn Synchronous Replication, once the Master node updates its own copy of the data, it initiates the write operation on its replicas. Once the Replicas receive the update, they apply the change on their copy of data and then send the confirmation to the Master. Once the Master receives the confirmation from all the Replicas, it responds to the client and completes the operation.\n\n![synchronous replication](https://user-images.githubusercontent.com/4745789/128765459-67347320-5b77-4722-884b-015fc1b0c5fb.png)\n\nIf there is more than one Replica in the setup, the Master node can propagate the write sequentially or parallelly. Still, in either case, it will continue to wait until it gets a confirmation, which will continue to keep the client blocked. Thus having a large number of Replicas means a longer block for the Client, affecting its throughput.\n\nSynchronous Replication ensures that the Replicas are always in sync and consistent with the Master; hence, this setup is fault-tolerant by default. Even if the Master crashes, the entire data is still available on the Replicas, so the system can easily promote any one of the Replicas as the new Master and continue to function as usual.\n\nA major disadvantage of this strategy is that the Client and the Master can remain blocked if a Replica becomes non-responsive due to a crash or network partition. Due to the strong consistency check, the Master will continue to block all the writes until the affected Replica becomes available again, thus bringing the entire system to a halt.\n\n## Asynchronous Replication\nIn Asynchronous Replication, once the Master node updates its own copy of the data, it immediately completes the operation by responding to the Client. It does not wait for the changes to be propagated to the Replicas, thus minimizing the block for the Client and maximizing the throughput.\n\nThe Master, after responding to the client, asynchronously propagates the changes to the Replicas, allowing them to catch up eventually. This replication strategy is most common and is the default configuration of most distributed data stores out there.\n\n![asynchronous replication](https://user-images.githubusercontent.com/4745789/128765466-944bf36e-6817-4cf3-9ea4-0ffa724f0d58.png)\n\nThe key advantage of using a fully Asynchronous Replication is that the client will be blocked only for the duration that the write happens on the Master, post which the Client can continue to function as before, thus elevating the system's throughput.\n\nOne major disadvantage of having a fully Asynchronous Replication is the possibility of data loss. What if the write happened on the Master node, and it crashed before the changes could propagate to any of the Replicas. The changes in data that are not propagated are lost permanently, defeating durability. Although Durability is the most important property of any data store,\n\nAsynchronous Replication is the default strategy for most data stores because it maximizes the throughput. The third type of replication strategy addresses durability without severely affecting throughput, and it is called Semi-synchronous Replication.\n\n## Semi-synchronous Replication\nIn Semi-synchronous Replication, which sits right between the Synchronous and Asynchronous Replication strategies, once the Master node updates its own copy of the data, it synchronously replicates the data to a subset of Replicas and asynchronously to others.\n\n![semi-synchronous replication](https://user-images.githubusercontent.com/4745789/128833772-d0bbae7d-5e00-4771-90e5-996326affb60.png)\n\nThe Semi-synchronous Replication thus addresses the durability of data, in case of Master crash, at the cost of degrading the Client's throughput by a marginal factor.\n\nMost of the distributed data stores available have configurable replication strategies. Depending on the problem at hand and the criticality of the data, we can choose one over the other.\n",
    "similar": [
      "new-replica",
      "handling-outages-master-replica",
      "replication-formats",
      "multi-master-replication"
    ]
  },
  {
    "id": 16,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "master-replica-replication",
    "title": "Master-Replica Replication",
    "description": "In this essay, we talk about everything we should know about Master-Replica replication pattern.",
    "gif": "https://media.giphy.com/media/l1KtVkekpcV4gKjyE/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/128563709-a95f9f55-f87b-458d-b6c4-562ef894805d.png",
    "released_at": "2021-08-07",
    "total_views": 350,
    "body": "Master-Replica architecture is one the most common high-level architectural pattern prevalent in distributed systems. We can find it in use across databases, brokers, and custom-built storage engines. In this essay, we talk about everything we should know about the Master-Replica replication pattern.\n\nA system that adheres to the Master-Replica replication architecture contains multiple nodes and each node, called Replica, holds an identical copy of the entire data. Thus if there are N nodes in the system, there will be N copies of the data.\n\n![Master-Replica Replication](https://user-images.githubusercontent.com/4745789/128564165-92d3413a-a329-4456-b055-177ed83e989a.png)\n\n# Scaling Reads\n\nWith  N nodes capable of serving the data, we easily scale up the reads by a  factor of N. Hence, this pattern is commonly put in place to amplify and scale the reads that the system can handle.\n\n# Handling Writes\n\nWith  N nodes that hold and own the data, the writes become tricky to handle.  In the Master-Replica setup, the writes go to one of the pre-decided nodes that act as the Master. This Master node is responsible for taking up all the writes that happen in the system.\n\nMaster is not any special node; rather, it is just one of the Replicas with this added responsibility. Thus in the system of N nodes, 1 node is the Master that takes in all the writes, while the other N - 1 node caters to the read requests coming from the clients.\n\n# Write Propagation\n\nOnce the write operation is successful on the Master node, the changes are propagated to all the Replicas through Replication Log (Commit Log, Bin  Log, etc.), letting the system eventually catch up.\n\nThe time elapsed between the write operation on the Master and the operation propagating to the Replica is called Replication Lag. This is one of the core metrics that is observed 100% of the time.\n\n# What happens when the Master goes down?\n\nSince the Master node takes in all the write operations, it going down is a  massive event. The write operation that happens when the Master is facing an outage results in an error.\n\nWhen the system detects such an outage, it tries to auto-recover by promoting one active Replica as the new Master by running a Leader Election algorithm. All the healthy Replicas participate in this election and, through a consensus,  decide the new Master.\n\nOnce the new Master is elected, the system starts accepting and processing the writes again.\n\n# Master-Replica in action\n\nThis is a widespread pattern that we can find across almost all the databases and distributed systems. Some of the most common examples are:\n\n - Relational databases like MySQL, PostgreSQL, Oracle, etc.\n - Non-relational databases like MongoDB, Redis, etc.\n - Distributed brokers like Kafka, etc.  \n",
    "similar": [
      "rum",
      "leaderless-replication",
      "replication-formats",
      "multi-master-replication"
    ]
  },
  {
    "id": 17,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "durability",
    "title": "ACID - Durability",
    "description": "Durability seems to be a taken-for-granted requirement, but to be honest, it is the most important one. Let's deep dive and find why it is so important? How do databases achieve durability in the midst of thousands of concurrent transactions? And how to achieve durability in a distributed setting?",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/126130275-10c1c4cf-483a-42d7-bd78-2f0ff4da4947.png",
    "released_at": "2021-07-19",
    "total_views": 335,
    "body": "After discussing the \"[A](https://arpitbhayani.me/blogs/atomicity)\", the \"[C](https://arpitbhayani.me/blogs/consistency)\", and the \"[I](https://arpitbhayani.me/blogs/isolation)\", it is time to take a look at the \"D\" of ACID - [Durability](https://en.wikipedia.org/wiki/Durability_(database_systems)).\n\nDurability seems to be a taken-for-granted requirement, but to be honest, it is the most important one. Let's deep dive and find why it is so important? How do databases achieve durability in the midst of thousands of concurrent transactions? And how to achieve durability in a distributed setting?\n\n# What is Durability?\n\nIn the context of Database, Durability ensures that once the transactions commit, the changes survive any outages, crashes, and failures, which means any writes that have gone through as part of the successful transaction should never abruptly vanish.\n\nThis is exactly why Durability is one of the essential qualities of any database, as it ensures zero data loss of any transactional data under any circumstance.\n\nA typical example of this is your purchase order placed on Amazon, which should continue to exist and remain unaffected even after their database faced an outage. So, to ensure something outlives a crash, it has to be stored in non-volatile storage like a Disk; and this forms the core idea of durability.\n\n# How do databases achieve durability?\n\nThe most fundamental way to achieve durability is by using a fast transactional log. The changes to be made on the actual data are first flushed on a separate transactional log, and then the actual update is made.\n\nThis flushed transactional log enables us to reprocess and replay the transaction during database reboot and reconstruct the system's state to the one that it was in right before the failure occurred - typically the last consistent state of the database. The write to a transaction log is made fast by keeping the file append-only and thus minimizing the disk seeks.\n\n![Durability in ACID](https://user-images.githubusercontent.com/4745789/126114187-0febc1ad-e35f-4d49-991c-8a5d8a0d9221.png)\n\n# Durability in a distributed setting\n\nIf the database is distributed, it supports Distributed Transactions, ensuring durability becomes even more important and trickier to handle. In such a setting, the participating database servers coordinate before the commit using a Two-Phase Commit Protocol.\n\nThe distributed computation is converged into a step-by-step process where the coordinator communicates the commit to all the participants, waits for all acknowledgments, and then further communicates the commit or rollback. This entire process is split into two phases - Prepare and Commit.\n\n# References\n - [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n - [Durability - Wikipedia](https://en.wikipedia.org/wiki/Durability_(database_systems))\n - [Two-phase commit protocol](https://en.wikipedia.org/wiki/Two-phase_commit_protocol)\n - [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n",
    "similar": [
      "isolation",
      "atomicity",
      "bitcask",
      "consistency"
    ]
  },
  {
    "id": 18,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "isolation",
    "title": "ACID - Isolation",
    "description": "Isolation is the ability of the database to concurrently process multiple transactions in a way that changes made in one do not another.",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/124763085-36d1db80-df51-11eb-985e-1d98e9b78cba.png",
    "released_at": "2021-07-05",
    "total_views": 301,
    "body": "After talking about the \"A\" and the \"C\" in ACID, let's talk about the \"I\" in ACID - Isolation. In this one, we do a micro-dive into Isolation in the context of database. We will take a detailed look into Isolation, understand its importance, functioning, and how the database implements it.\n\n# What is Isolation?\n\nIsolation is the ability of the database to concurrently process multiple transactions in a way that changes made in one does not affect the other. A simple analogy is how we have to make our data structures and variables thread-safe in a multi-threaded (concurrent) environment.\n\nAnd similar to how we use Mutex and Semaphores to protect variables, the database uses locks (shared and exclusive) to protect transactions from one another.\n\n![https://user-images.githubusercontent.com/4745789/124764636-caf07280-df52-11eb-8d6b-d9d316d31102.png](https://user-images.githubusercontent.com/4745789/124764636-caf07280-df52-11eb-8d6b-d9d316d31102.png)\n\n# Why is Isolation important?\n\nIsolation is one of the most important properties of any database engine, the absence of which directly impacts the integrity of the data.\n\n## Example 1: Cowin Portal\n\nWhen 500 slots open for a hospital, the system has to ensure that a max of 500 people can book their slots.\n\n## Example 2: Flash Sale\n\nWhen Xiaomi conducts a flash sale with 100k units, the system has to ensure that orders of a max of 100k units are placed.\n\n## Example 3: Flight Booking\n\nIf a flight has a seating capacity of 130, the airlines cannot have a system that allows ticket booking of more than that.\n\n## Example 4: Money transfers\n\nWhen two or more transfers happen on the same account simultaneously, the system has to ensure that the end state is consistent with no mismatch of the amount. Sum of total money across all the parties to remain constant.\n\nThe isolation property of a database engine allows the system to put these checks on the database, which ensures that the data never goes into an inconsistent state even when hundreds of transactions are executing concurrently.\n\n# How is isolation implemented?\n\nA transaction before altering any row takes a lock (shared or exclusive) on that row, disallowing any other transaction to act on it. The other transactions might have to wait until the first one either commits or rollbacks.\n\nThe granularity and the scope of locking depend on the isolation level configured. Every database engine supports multiple Isolation levels, which determines how stringent the locking is. The 4 isolation levels are\n\n- Serializable\n- Repeatable reads\n- Read committed\n- Read uncommitted\n\nWe will discuss Isolation Levels in detail in some other essay.\n\n# References\n\n- [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n- [Isolation - Wikipedia](https://en.wikipedia.org/wiki/Isolation_(database_systems))\n- [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n- [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)\n- [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)\n",
    "similar": [
      "atomicity",
      "durability",
      "consistency",
      "image-steganography"
    ]
  },
  {
    "id": 19,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "consistency",
    "title": "ACID - Consistency",
    "description": "In the context of databases, Consistency is Correctness, which means that under no circumstance will the data lose its correctness.",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/124225698-86657100-db25-11eb-8284-9771f5a0b545.png",
    "released_at": "2021-07-02",
    "total_views": 387,
    "body": "In this short essay, we dive deep and understand the \"C\" in ACID - Consistency.\n\nIn this quick read, we will take a detailed look into [Consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems)), understand its importance, functioning, and how the database implements it.\n\n# What is Consistency?\n\nIn the context of databases, Consistency is Correctness, which means that under no circumstance will the data lose its correctness.\n\nDatabase systems allow us to define rules that the data residing in our database are mandated to adhere to. Few handy rules could be\n\n - balance of an account should never be negative\n - no orphan mapping: there should not be any mapping of a person whose entry from the database is deleted.\n - no orphan comment: there should not be any comment in the database that does not belong to an existing blog.\n\nThese rules can be defined on a database using Constraints, [Cascades](https://en.wikipedia.org/wiki/Foreign_key#CASCADE), and Triggers; for example, [Foreign Key constraints](https://en.wikipedia.org/wiki/Foreign_key), [Check constraints](https://en.wikipedia.org/wiki/Check_constraint), On Delete Cascades, On Update Cascades, etc.\n\n![Consistency ACID Database](https://user-images.githubusercontent.com/4745789/124226533-e7417900-db26-11eb-8e88-1c50a9391c44.png)\n\n### Role of the database engine in ensuring Consistency\nAn ACID-compliant database engine has to ensure that the data residing in the database continues to adhere to all the configured rules. Thus, even while executing thousands of concurrent transactions, the database always moves from one consistent state to another.\n\n### What happens when the database discovers a violation?\nDatabase Engine rollbacks the changes, which ensures that the database is reverted to a previous consistent state.\n\n### What happens when the database does not find any violation?\nDatabase Engine will continue to apply the changes, and once the transaction is marked successful, this state of the database becomes the newer consistent state.\n\n# Why is consistency important?\nThe answer is very relatable. Would you ever want your account to have a negative balance? No. This is thus defined as a rule that the database engine would have to enforce while applying any change to the data.\n\n# How does the database ensure Consistency?\nIntegrity constraints are checked when the changes are being applied to the data.\n\nCascade operations are performed synchronously along with the transaction. This means that the transaction is not complete until the primary set of queries, along with all the eligible cascades, are applied. Most database engines also provide a way to make them asynchronous, allowing us to keep our transactions leaner.\n\n\u2728 Next up is \"I\" in ACID - Isolation. Stay tuned.\n\n# References\n - [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n - [Consistency](https://en.wikipedia.org/wiki/Consistency_(database_systems))\n - [Foreign Key Constraints](https://en.wikipedia.org/wiki/Foreign_key)\n - [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n - [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)\n - [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)\n",
    "similar": [
      "image-steganography",
      "isolation",
      "durability",
      "atomicity"
    ]
  },
  {
    "id": 20,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "atomicity",
    "title": "ACID - Atomicity",
    "description": "A single database transaction often contains multiple statements to be executed on the database. In Relational Databases, these are usually multiple SQL statements, while in the case of non-Relational Databases, these could be multiple database commands.",
    "gif": "https://media.giphy.com/media/iB4PoTVka0Xnul7UaC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/124224260-ef97b500-db22-11eb-8d0f-cc808093eb00.png",
    "released_at": "2021-06-28",
    "total_views": 748,
    "body": "In this short essay, we dive deep and understand the \"A\" of ACID - Atomicity.\n\nIn this quick read, we will take a detailed look into [Atomicity](https://en.wikipedia.org/wiki/Atomicity_(database_systems)), understand its importance, and learn about implementing it at various levels.\n\n# What is atomicity?\n\nA single database transaction often contains multiple statements to be executed on the database. In Relational Databases, these are usually multiple SQL statements, while in the case of non-Relational Databases, these could be multiple database commands.\n\nAtomicity in ACID mandates that each transaction should be treated as a single unit of execution, which means either all the statements/commands of that transaction are executed, or none of them are.\n\nAt the end of the successful transaction or after a failure while applying the transaction, the database should never be in a state where only a subset of statements/commands is applied.\n\nAn atomic system thus guarantees atomicity in every situation, including successful completion of transactions or after power failures, errors, and crashes.\n\n![Atomicity ACID](https://user-images.githubusercontent.com/4745789/124223798-0e497c00-db22-11eb-868d-8faefc44361c.png)\n\nA great example of seeing why it is critical to have atomicity is Money Transfers.\n\nImagine transferring money from bank account A to B. The transaction involves subtracting balance from A and adding balance to B. If any of these changes are partially applied to the database, it will lead to money either not debited or credited, depending on when it failed.\n\n# How is atomicity implemented?\n\n## Atomicity in Databases\nMost databases implement Atomicty using logging; the engine logs all the changes and notes when the transaction started and finished. Depending on the final state of the transactions, the changes are either applied or dropped.\n\nAtomicity can also be implemented by keeping a copy of the data before starting the transaction and using it during rollbacks.\n\n## Atomicity in File Systems\nAt the file system level, atomicity is attained by atomically opening and locking the file using system calls: open and flock. We can choose to lock the file in either Shared or Exclusive mode.\n\n## Atomicity at Hardware Level\nAt the hardware level, atomicity is implemented through instructions such as Test-and-set, Fetch-and-add, Compare-and-swap.\n\n## Atomicity in Business Logic\nThe construct of atomicity can be implemented at a high-level language or business logic by burrowing the concept of atomic instructions; for example, you can use compare and swap to update the value of a variable shared across threads concurrently.\n\nAtomicity is not just restricted to Databases; it is a notion that can be applied to any system out there.\n\n\u2728 Next up is \"C\" in ACID - Consistency. Stay tuned.\n\n# References\n - [ACID - Wikipedia](https://en.wikipedia.org/wiki/ACID)\n - [Atomicity - Wikipedia](https://en.wikipedia.org/wiki/Atomicity_(database_systems))\n - [ACID Explained - BMC](https://www.bmc.com/blogs/acid-atomic-consistent-isolated-durable/)\n - [ACID properties of transactions](https://www.ibm.com/docs/en/cics-ts/5.4?topic=processing-acid-properties-transactions)\n - [ACID Compliance: What It Means and Why You Should Care](https://mariadb.com/resources/blog/acid-compliance-what-it-means-and-why-you-should-care/)\n",
    "similar": [
      "isolation",
      "durability",
      "consistency",
      "image-steganography"
    ]
  },
  {
    "id": 21,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "architectures-in-distributed-systems",
    "title": "Architectures in Distributed Systems",
    "description": "While designing a Distributed System, it is essential to pick the right kind of architecture. Usually, architectures are evolving, but picking the right one at the inception could make your system thrive.",
    "gif": "https://media.giphy.com/media/3orieKSJONEWV51x3q/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/122890572-6d103800-d361-11eb-92e0-c0f226c6821c.png",
    "released_at": "2021-06-22",
    "total_views": 563,
    "body": "While designing a Distributed System, it is essential to pick the right kind of architecture. Usually, architectures are evolving, but picking the right one at the inception could make your system thrive.\n\nSo, here are 4 common architectures in Distributed Systems\n\n## Client-server\n\nTo render some information to the end-user, the client contacts the server (holding the data) with a request; the server sends back the data requested. The client is smart enough to understand how to request the data, post-process it, format it, and then serve it to the end-user.\n\nThis kind of architecture is not common in our day-to-day web applications. Still, it is prevalent when multiple services share a common database (server) and request data directly from the database.\n\n## 3-tier\n\nThis is one of the most widely used topologies out there. Unlike client-server architecture, the clients in a 3-tier architecture are not smart and are stateless. This architecture introduces a middle layer holding the business logic. The client talks to the business layer, and this business layer talk to the server (holding the data).\n\nMost of the web applications are 3-tier applications where your client (browser) talks to the business layer (webserver), which in turn queries the server (database) for the data. The business layer processes and formats the data (optional) and sends it back to the client. The client does not know how the data is being fetched from the server, making it stateless.\n\n## n-tier\n\nAs an extension to the 3-tier architecture, the n-tier application is where your middle layer (business layer) talks to another service to get information. This is typically seen when there is multiple independent business logic in the system.\n\nA classic example of an n-tier architecture is Microservices based architecture. Each service is responsible for its information, and the service communicates with other services to get the required data. Thus, a 3-tier application typically evolves into an n-tier application.\n\n## Peer-to-peer\n\nPeer-to-peer architecture is typically a decentralized system wherein no special machines hold all the responsibilities; instead, the responsibility is split across all the machines equally. The peers act as both clients and servers, and they communicate with each other to serve the request.\n\nA couple of popular examples of P2P architecture are BitTorrent and Bitcoin networks. The n-tier architecture can optionally evolve into a P2P, but this evolution is not that popular. Usually, going P2P is a choice that is made during the inception of the service.\n\n# References\n - [Distributed Systems - Wikipedia](https://en.wikipedia.org/wiki/Distributed_computing)\n",
    "similar": [
      "conflict-detection",
      "mistaken-beliefs-of-distributed-systems",
      "conflict-resolution",
      "monotonic-reads"
    ]
  },
  {
    "id": 22,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "mistaken-beliefs-of-distributed-systems",
    "title": "Mistaken Beliefs of Distributed Systems",
    "description": "In this essay, we learn about a set of false assumptions that programmers new to distributed applications invariably make. Understand these to build robust distributed systems.",
    "gif": "https://media.giphy.com/media/SqmkZ5IdwzTP2/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/122346879-11137100-cf67-11eb-95a1-5e717dd53810.png",
    "released_at": "2021-06-17",
    "total_views": 358,
    "body": "The only way to infinitely scale your system is by making it distributed, which means adding more servers to serve your requests, more nodes to perform computations in parallel, and more nodes to store your partitioned data. But while building such a complex system, we tend to assume a few things to be true, which, in reality, are definitely not true.\n\nThese mistaken beliefs were documented by [L Peter Deutsch](https://en.wikipedia.org/wiki/L._Peter_Deutsch) and others at Sun Microsystems, and it describes a set of false assumptions that programmers new to distributed applications invariably make.\n\n## Myth 1: The network is reliable;\n\nNo. The network is not reliable. There are packet drops, connection interruptions, and data corruptions when they are transferred over the wire. In addition, there are network outages, router restarts, and switch failures to make the matter worse. Such an unreliable network has to be considered while designing a robust Distributed System.\n\n## Myth 2: Latency is zero;\n\nNetwork latency is real, and we should not assume that everything happens instantaneously. For every 10 meters of fiber optic wire, we add 3 nanoseconds to the network latency. Now imagine your data moving across the transatlantic communications cable. This is why we keep components closer wherever possible and have to handle out-of-order messages.\n\n## Myth 3: Bandwidth is infinite;\n\nThe bandwidth is not infinite; neither of your machine, or the server, or the wire over which the communication is happening. Hence we should always measure the number of packets (bytes) of data transferred in and out of your systems. When unregulated, this results in a massive bottleneck, and if untracked, it becomes near impossible to spot them.\n\n## Myth 4: The network is secure;\n\nWe put our system in a terrible shape when we assume that the data flowing across the network is secure. Many malicious users are constantly trying to sniff every packet over the wire and de-code what is being communicated. So, ensure that your data is encrypted when at rest and also in transit.\n\n## Myth 5: Topology doesn't change;\n\nNetwork topology changes due to software or hardware failures. When the topology changes, you might see a sudden deviation in latency and packet transfer times. So, these metrics need to be monitored for any anomalous behavior, and our systems would be ready to embrace this change.\n\n## Myth 6: There is one administrator;\n\nThere is one internet, and everyone is competing for the same resources (optic cables and other communication channels). So, when building a super-critical Distributed system, you need to know which path your packets are following to avoid high-traffic competing and congested areas.\n\n## Myth 7: Transport cost is zero;\n\nThere is a hidden cost of hardware, software, and maintenance that we all bear when using a distributed system. For example, if we use a public cloud-like AWS, then the data transfer cost is real. This cost looks near zero from a bird's eye view, but it becomes significant when operating at scale.\n\n## Myth 8: The network is homogeneous.\n\nThe network is not homogeneous, and your packets travel to all sorts of communication channels like optic cables, 4G bands, 3G bands, and even 2G bands before reaching the user's device. This is also true when the packets move within your VPC through different types of connecting wires and network cards. When there is a lot of heterogeneity in the network, it becomes harder to find the bottleneck; hence having a setup that gives us enough transparency is the key to a good Distributed System design.\n\n# References\n\n- [Fallacies of distributed computing](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)\n- [The Eight Fallacies of Distributed Computing](https://www.youtube.com/watch?v=JG2ESDGwHHY)\n",
    "similar": [
      "conflict-detection",
      "architectures-in-distributed-systems",
      "conflict-resolution",
      "data-partitioning-strategies"
    ]
  },
  {
    "id": 23,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fork-bomb",
    "title": "Fork Bomb",
    "description": "In this essay, we explore a simple yet effective DoS attack called Fork Bomb, also called Rabbit Virus. This attack forks out processes infinitely, starving them for any resources.",
    "gif": "https://media.giphy.com/media/l0HeiaW8q9B6tqoHS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/121253160-69db6d80-c8c6-11eb-8b05-202ec958324e.png",
    "released_at": "2021-06-09",
    "total_views": 425,
    "body": "In this essay, we explore a simple yet effective [DoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack) called [Fork Bomb](https://en.wikipedia.org/wiki/Fork_bomb), also called Rabbit Virus. This attack forks out processes infinitely, starving them for any resources.\n\nOnline Coding Platforms and Code Evaluators are susceptible to this attack as they accept raw code from the user and execute it. So, if you are building one, do ensure you are protected against it and infinite loops. We will also discuss how to prevent and defend against Fork Bomb in the final section of this essay.\n\n# How do they work?\n\nFork bombs can be summarized in just three words: *Fork until possible*. When executed, fork bombs continuously fork out non-terminating processes, demanding machine resources like CPU (mostly) and memory.\n\n![Fork Bomb](https://user-images.githubusercontent.com/4745789/121252662-e752ae00-c8c5-11eb-9524-a1c7d4fc24fc.png)\n\nWith so many processes competing for the CPU and other resources (if provisioned), the scheduler and CPU are put under tremendous load. After a specific limit, the entire system stalls.\n\n# Implementing Fork Bombs\n\nBefore we take a look at how to prevent or stop a Fork Bomb, let's look at something more interesting - how to implement a Fork Bomb?\n\nA quick detour, let's see what `fork` does: Upon every invocation, the forked child process is an exact duplicate of the parent process except for a [few details](https://man7.org/linux/man-pages/man2/fork.2.html), but nonetheless what matters to us is that it runs the exact same code as the parent.\n\n## C implementation\n\nA simple C implementation of a Fork Bomb could be, to fork child processes within an infinite for loop, resulting in exponential forking of child processes.\n\n```c\n#include <unistd.h>\nint main(void) {\n    for (;;) {\n        fork();\n    }\n}\n```\n\nWith the `fork` being invoked inside the infinite for loop, every single child process and the parent process will continue to remain stuck in the infinite loop while continuously forking out more and more child processes that execute the same code and stuck in the same loop; and thus resulting in exponential child forks.\n\nThese child processes start consuming the resources and blocking the legitimate programs. This prevents the creation of any new processes. This also freezes the process that responds to Keystrokes, putting the entire system to a standstill.\n\n## Bash implementation\n\nThere is a very famous Fork Bomb implementation in Bash, and the code that does this has no alphabets or numbers in it, just pure symbols.\n\n```bash\n:(){ :|:& };:\n```\n\nAlthough the shell statement looks gibberish, it is effortless to understand. In the statement above, we are defining a function named `:` having body `:|:&` and at the end invoking it using the name `:`, just like any usual shell function.\n\nAs part of the function body, we are again invoking the same function `:` and piping its output to the input of a background process executing another instance of `:`. This way, we are recursively invoking the same function (command) and stalling it by creating a pipe between the two.\n\nA cleaner way to redefine this very implementation of Fork Bomb would be\n\n```bash\nbomb() { \n    bomb| bomb& \n};bomb\n```\n\n# How to prevent them?\n\nTo protect our system against Fork Bombs, we can cap the processes owned by a certain user, thus blocking process creation at that cap.\n\nUsing the *nix utility called `ulimit`, we can set the maximum number of processes that a user can execute in the system, using the flag `-u`. By setting this value to an appropriate (lower) value, we can cap the process creation for a user, ensuring we can never be fork bombed by that user.\n\n# References\n\n- [Fork Bomb](https://en.wikipedia.org/wiki/Fork_bomb)\n- [ulimit - Man Page](https://linuxcommand.org/lc3_man_pages/ulimith.html)\n- [Understanding Bash Fork Bomb](https://www.cyberciti.biz/faq/understanding-bash-fork-bomb/)\n- [Preventing Fork Bombs on Linux](https://resources.cs.rutgers.edu/docs/preventing-fork-bomb-on-linux/)\n- [Fork bomb attack (Rabbit virus) - Imperva](https://www.imperva.com/learn/ddos/fork-bomb/)\n",
    "similar": [
      "bayesian-average",
      "mongodb-cursor-skip-is-slow",
      "inheritance-c",
      "super-long-integers"
    ]
  },
  {
    "id": 24,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "chained-operators-python",
    "title": "Chained Comparison Operators in Python",
    "description": "In this essay, we find how chained comparison expressions are evaluated, understand how short-circuit evaluations happen internally, and alter Python's expression evaluation strategy.",
    "gif": "https://media.giphy.com/media/LpLd2NGvpaiys/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/116305344-47334080-a7c1-11eb-888a-a60b203ebe9d.png",
    "released_at": "2021-04-27",
    "total_views": 275,
    "body": "Python supports chaining of comparison operators, which means if we wanted to find out if `b` lies between `a` and `c` we can do `a < b < c`, making code super-intuitive. Python evaluates such expressions like how we do in mathematics. which means `a < b < c` is evaluated as `(a < b) and (b < c)`. C language on the other hand, evaluates `a < b < c` as `((a < b) < c)`.\n\nDepending on how we evaluate such expressions, the final evaluated changes. So, in python, if we evaluate `-3 < -2 < -1`, we get `True` and if evaluate `3 > 2 == 1` we get `False`.\n\n```python\n>>> -3 < -2 < -1\nTrue\n>>> 3 > 2 == 1\nFalse\n```\n\nBut on the other hand, if we evaluate this very expression in C language the output is `False`.\n\n```c\n#include <stdio.h>\n\nint main(int argc, char *argv[]) {\n    printf(\"%d\\n\", -3 < -2 < -1);\n    printf(\"%d\\n\", 3 > 2 == 1);\n    return 0;\n}\n\n$ gcc test.cpp\n$ ./a.out\n0\n1\n```\n\nIt does so because `(-3 < -2) = True = 1` and `1 < -1 is False`. Also, to get a better understanding of how such expressions evaluate, try playing around with different values and see if your predicted value matches the actual output.\n\nThis essay is going to be extra special; in this one, we find out\n\n- how Python evaluates chained comparison operators?\n- how Python implements short-circuiting?\n- how could you make Python-like evaluation a C-like evaluation? implying at the end of this essay we alter the CPython source code such that the expression `-3 < -2 < -1` will evaluate to `False`.\n\nI know this sounds tempting, so let's jump right into it.\n\n# Chaining comparison operators\n\nPython has a plethora of comparison operators like `<`, `>`, `<=`, `>=`, `==`, `!=`, `in`, and `is`. The output of the comparison operator is a boolean value - `True` or `False`. Python allows chaining of comparison operators which means, to check if `b` lies between `a` and `c`, we can simply do\n\n```python\n>>> a < b < c\n```\n\nThis is possible because internally Python evaluates this chained expression `a < b < c` as `(a < b) and (b < c)`. To make this efficient, the sub-expression `b` is evaluated only once and the evaluation also follows short-circuit evaluation; which means, if `(a < b)` is evaluated as `False` then Python would not evaluate further sub-expressions - `c` and `(b < c)`. Now that we have set the context, let's find out what happens under the hood.\n\n# Chaining under the hood\n\nDisassembling the expression would give out the set of Python instructions that will be executed on the runtime's execution stack. If we disassemble `a < b < c`, we get the following set of instructions. By the way, this is a great way to jump into the internals of anything in Python.\n\n```python\n>>> import dis\n>>> dis.dis('a < b < c')\n\n  1           0 LOAD_NAME                0 (a)\n              2 LOAD_NAME                1 (b)\n              4 DUP_TOP\n              6 ROT_THREE\n              8 COMPARE_OP               0 (<)\n             10 JUMP_IF_FALSE_OR_POP    18\n             12 LOAD_NAME                2 (c)\n             14 COMPARE_OP               0 (<)\n             16 RETURN_VALUE\n        >>   18 ROT_TWO\n             20 POP_TOP\n             22 RETURN_VALUE\n```\n\nHere is the summary of what each of the above instructions does; having this understanding will help us understand the entire execution process.\n\n- `LOAD_NAME`: Loads the variable on the top of the stack\n- `DUP_TOP`: Duplicates the top of the stack\n- `ROT_THREE`: Rotates the top 3 elements of the stack by 1, such that the second element becomes the top, the third becomes the second while the top becomes the third.\n- `COMPARE_OP`: Pops the top two elements from the stack, compare them (depending on the operator), compute the output, and puts it on the top of the stack.\n- `JUMP_IF_FALSE_OR_POP`: Checks if the top of the stack is `False` if it is false then jumps to provided offset, and if it is `True` it pops the value.\n- `RETURN_VALUE`: Pops the top of the stack and returns it\n- `ROT_TWO`: Rotates the top two elements of the stack such that the top elements become the second, while the second becomes the top.\n- `POP_TOP`: Pops the top element from the stack, kind of discarding it.\n\nYou can find details about these opcodes in the file [ceval.c](https://github.com/python/cpython/blob/master/Python/ceval.c). Now, let's do an instruction by instruction walkthrough for the expression `1 < 2 < 3` to see how it evaluates to `True`.\n\n## Evaluating `1 < 2 < 3`\n\nWhen we run disassembler on `1 < 2 < 3` we get a similar disassembled code. It starts with the loading of two constant values `1` and `2` on the stack. Then it duplicates the top which makes our stack `2, 2, 1`. Now upon `ROT_THREE` the `2` on the top of the stack goes at the third spot while the other moves up one place. at this instruction, our stack looks like `2, 1, 2`.\n\n![https://user-images.githubusercontent.com/4745789/116093465-508ab300-a6c4-11eb-8587-2922c4095eaa.png](https://user-images.githubusercontent.com/4745789/116093465-508ab300-a6c4-11eb-8587-2922c4095eaa.png)\n\nNow, the `COMPARE_OP` operation pops out two elements from the stack and performs the comparison. The first popped value becomes the right operand while the second popped becomes the left operand. Post comparison the evaluated value is put on top of the stack again. Since `1 < 2`, the expression is evaluated as `True` and this `True` is put on to of the stack. So, after the `COMPARE_OP` instruction, the stack would look like `True, 2`.\n\nThen comes the instruction `JUMP_IF_FALSE_OR_POP` which checks the top of the stack. Since the top of the stack is `True` (not `False`), it pops the value, making our stack `2`. Now `3` is loaded onto the stack making our stack `3, 2`.\n\nNow `COMPARE_OP` pops out two elements, compares them, and since `2 < 3` it evaluates to `True` and this `True` is stacked on top. After this operation, the stack has just one element `True`.\n\nThe next instruction is `RETURN_VALUE`, which pops out the top of the stack i.e. `True`, and returns it; and this is how the expression `1 < 2 < 3` is evaluated to `True`.\n\n## Short-circuit Evaluation\n\nA very interesting instruction is sitting right in the middle - `JUMP_IF_FALSE_OR_POP`. This instruction is the one that is doing short-circuiting. Once the runtime encounters this instruction it checks the top of the stack,\n\n- if top == `False` the flow jumps to the last few instructions, bypassing the loading and comparing other sub-expressions.\n- if top == `True` it does not jump, but rather continues its evaluation of the next instructions.\n\nTo get a better understanding, try doing an instruction by instruction walkthrough for the expression `6 > 7 > 8` and you will find out how it bypasses the evaluating next sub-expressions.\n\n![https://user-images.githubusercontent.com/4745789/116093532-60a29280-a6c4-11eb-9ad9-24a73dddb683.png](https://user-images.githubusercontent.com/4745789/116093532-60a29280-a6c4-11eb-9ad9-24a73dddb683.png)\n\nNow we know, why the [official documentation](https://docs.python.org/3/reference/expressions.html#comparisons) says,\n\n> Comparisons can be chained arbitrarily, e.g., x < y <= z is equivalent to x < y and y <= z, **except that y is evaluated only once (but in both cases z is not evaluated at all when x < y is found to be false)**.\n\n## How does it \"and\" sub-expressions?\n\nWe have established and also seen in action how Python evaluates chained comparison operators. We also understand that it evaluates `1 < 2 < 3` as `(1 < 2) and (2 < 3)` but exactly where is this very logic implemented? The magic happens with two instructions `DUP_TOP` and `ROT_THREE`.\n\nSo, if we keenly observe, to evaluate `1 < 2 < 3` as `(1 < 2) and (2 < 3)` we would need to repeat the middle operand and keep it ready as the first operand of the second comparison. Now, to \"repeat\" the middle operand, we call `DUP_TOP`.\n\nOnce the two operands are loaded on the stack we see that the right operand sits on the top and by invoking `DUP_TOP` we are copying the middle operand and putting it on the top of the stack. This copied top (middle operand) needs to be preserved to be used as the first operand in the next comparison, and to do this we call `ROT_THREE` that puts the stack top to the third from the top.\n\nAfter the first comparison is evaluated the stack contains - the copied middle operand and on top of it the evaluated value. The evaluated value is discarded or returned depending on if it is `True` or `False`, keeping the copied middle operand on the stack, making it the first operand of the next comparison.\n\n# Make chain evaluation C-like\n\nNow that we have understood how Chained Operators are evaluated and what how the evaluation is made \"mathematics\" like, let's manipulate the code to make the evaluation C-like; which means we have to evaluate operands left to right and use the evaluated value as the first operand for next comparison. To be honest, if you have understood the importance of `DUP_TOP` and `ROT_THREE` making evaluation C-like is fairly straightforward. \n\nThe code that generates instructions for comparison expressions is in file [Python/compile.c](https://github.com/python/cpython/blob/master/Python/compile.c#L4031). The snippet that interests us is the function `compiler_compare` which can be seen below\n\n```c\nstatic int\ncompiler_compare(struct compiler *c, expr_ty e)\n{\n    ...\n        for (i = 0; i < n; i++) {\n            VISIT(c, expr,\n                (expr_ty)asdl_seq_GET(e->v.Compare.comparators, i));\n            ADDOP(c, DUP_TOP);\n            ADDOP(c, ROT_THREE);\n            ADDOP_COMPARE(c, asdl_seq_GET(e->v.Compare.ops, i));\n            ADDOP_JUMP(c, JUMP_IF_FALSE_OR_POP, cleanup);\n            NEXT_BLOCK(c);\n        }\n    ...\n        ADDOP(c, ROT_TWO);\n        ADDOP(c, POP_TOP);\n    ...\n    }\n    return 1;\n}\n```\n\nTo make the evaluation C-like we\n\n- should not copy the middle operand\n- ensure that the evaluated value (output from the first compare) remains on top - so that it becomes the first operand of the next expression\n\nTo achieve this, all we have to do is comment out `3` lines that do exactly that. Post changes the snippet would look something like this.\n\n```c\nstatic int\ncompiler_compare(struct compiler *c, expr_ty e)\n{\n    ...\n        for (i = 0; i < n; i++) {\n            VISIT(c, expr,\n                (expr_ty)asdl_seq_GET(e->v.Compare.comparators, i));\n\n            // ADDOP(c, DUP_TOP);\n            // ADDOP(c, ROT_THREE);\n\n            ADDOP_COMPARE(c, asdl_seq_GET(e->v.Compare.ops, i));\n\n            // ADDOP_JUMP(c, JUMP_IF_FALSE_OR_POP, cleanup);\n\n            NEXT_BLOCK(c);\n        }\n    ...\n        ADDOP(c, ROT_TWO);\n        ADDOP(c, POP_TOP);\n    ...\n    }\n    return 1;\n}\n```\n\nRecall that the expression `-3 < -2 < -1` on a usual Python interpreter evaluates to `True` because `-2` is between `-3` and `-1`. But post these changes, if we build the binary and start the interpreter we would see the output of expression `-3 < -2 < -1` as `False`, just like C; as it evaluated the expression from left to right and kept reusing the output of the previous comparison as the first operand of the next one. \n\nHere is the disassembled code and an instruction by instruction execution post our changes.\n\n![https://user-images.githubusercontent.com/4745789/116272065-37eecb80-a79e-11eb-8345-cae342a8441c.png](https://user-images.githubusercontent.com/4745789/116272065-37eecb80-a79e-11eb-8345-cae342a8441c.png)\n\nThe engine first evaluated `-3 < -2`, and put the result `True` on top of the stack and then loaded `-1` to perform the comparison `True < -1`. Since `True == 1` the expression `True < -1` is evaluated as `False` and hence the output of the entire statement is `False`, just like in C. This also means that the expression `3 > 2 == 1` should evaluate to `True` and it actually does.\n\n![https://user-images.githubusercontent.com/4745789/116272093-3d4c1600-a79e-11eb-9981-655744c86348.png](https://user-images.githubusercontent.com/4745789/116272093-3d4c1600-a79e-11eb-9981-655744c86348.png)\n\n# References\n\n- [Under the Hood: Python Comparison Breakdown](https://pybit.es/guest-python-comparison-breakdown.html)\n- [How do chained comparisons in Python actually work?](https://stackoverflow.com/questions/28754726/how-do-chained-comparisons-in-python-actually-work)\n- [Comparisons: Python Language Reference](https://docs.python.org/3/reference/expressions.html#comparisons)\n",
    "similar": [
      "the-weird-walrus",
      "setting-up-graphite-grafana-using-nginx-on-ubuntu",
      "making-http-requests-using-netcat",
      "how-sleepsort-helped-me-understand-concurrency-in-golang"
    ]
  },
  {
    "id": 25,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "udemy-sql-taxonomy",
    "title": "Designing Taxonomy on a Relational DB",
    "description": "In this essay, design taxonomy on a SQL-based Relational database by taking Udemy as an example, write SQL queries covering common use-cases, and determine necessary indexes to make queries efficient.",
    "gif": "https://media.giphy.com/media/3o6Mbsop5cXzqVqfgA/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/115157736-83bbb980-a0a8-11eb-8974-545b7a9cba9d.png",
    "released_at": "2021-04-18",
    "total_views": 1303,
    "body": "In this essay, we will model [taxonomy](https://en.wikipedia.org/wiki/Taxonomy) on top of a relational database, and as a specific example, we will try to build [Udemy's Taxonomy](https://www.udemy.com/). The primary focus of this essay is to understand how to design taxonomy on top of [SQL based relational DB](https://en.wikipedia.org/wiki/Relational_database), define and write queries that are computationally efficient along with deciding indexes on the designed tables.\n\nIn the process, we will also understand a very interesting SQL construct like Window Functions that helps us solve seemingly complex use-cases with a single SQL query.\n\n# Udemy's Taxonomy\n\n[Udemy's Taxonomy](https://www.udemy.com/) is very simple; it features top-level categories - like Software Engineering, Arts, and Business - each category has multiple sub-categories - like Programming Languages, Databases, Sketching - and each sub-category has niche topics like - Python, Javascript, MySQL, etc.\n\nTo keep things simpler, we restrain that one topic can be part of only one sub-category and one sub-category can belong to only one top-level category; and that makes the maximum levels in this taxonomy as `3`.\n\n![https://user-images.githubusercontent.com/4745789/115139853-fcdbf200-a051-11eb-94f1-00382bd26db1.png](https://user-images.githubusercontent.com/4745789/115139853-fcdbf200-a051-11eb-94f1-00382bd26db1.png)\n\n# Database Design\n\nOut of our intuition, we can have one table for categories, one for holding sub-categories, and one for topics, and a bunch of [foreign keys](https://en.wikipedia.org/wiki/Foreign_key) that weaves them together. But is this the best we can come up with? A few issues with this design is\n\n- all the 3 tables will have an identical schema\n- if we were to introduce a new level, say `concept` that sits between sub-category and topic, we will have to create a new table to accommodate it, making this design cumbersome to future features and extensions.\n- what if for a few topics we want it to be a child of a category, leaving out sub-categories altogether; handling this with this design will be very tricky.\n\nSo, we need a better design, that is robust and extensible and hence we go for a single table called `topics` that holds categories, sub-categories, and topics differentiated with a column called `type` distinguishing between the 3. The schema of this table `topics` would be\n\n![SQL Schema - Taxonomy Udemy](https://user-images.githubusercontent.com/4745789/115140362-8260a180-a054-11eb-8820-2a830dcc025e.png)\n\nNow that we have the table `topics` ready, we see how the following two topics are stored\n\n- Software Engineering > Programming Languages > Python\n- Software Engineering > Programming Languages > Javascript\n\n![Sample Data - Taxonomy Udemy](https://user-images.githubusercontent.com/4745789/115140389-b340d680-a054-11eb-8a6d-b39a9f15fde8.png)\n\n# Indexes on `topics`\n\nPicking the right set of indexes is one of the most critical decisions that you will be taking while designing this system. A good set of indexes boosts the overall performance of the system, while poor and/or missing ones will put your database under a terrible load, especially at scale.\n\nBut how do we pick which indexes do we want on `topics`? The answer here is very simple, it depends on the kind of queries we have to support. So, let's list down queries that we will need and then determine indexes to make them efficient.\n\n## Get topic by ID\n\nThe most common query that we'd need is getting a topic by its `id` and this is very well facilitated by making `id` as a [primary key](https://en.wikipedia.org/wiki/Primary_key) of the table.\n\n```sql\nSELECT * FROM topics WHERE id = 531;\n```\n\n## Get the topic path\n\nGetting a topic path is an interesting use case. While rendering any category, sub-category, or topic page we would need to render breadcrumbs that hold the path of it in the taxonomy. For example, for Python's page, we will need to render a path like\n\n```python\nSoftware Engineering > Programming Languages > Python\n```\n\nThis path helps users explore and discover new categories, sub-categories, or topics. So, with our current schema, how could we compute the topic path for a given topic id.\n\nDoing it on the application side is the first approach that comes to mind but it is a poor one because we would be making `n` selects for `n` levels. In the case of our current system, we will be making `3` selects to compute the topic path; with the application pseudocode looking something like this\n\n```python\ndef get_topicpath(topic_id):\n    path = []\n\n    topic = get_topic_by_id(topic_id)\n    path.append(topic)\n\n    while topic.parent_id:\n        topic = get_topic_by_id(topic.parent_id)\n        path.append(topic)\n    \n    return path\n```\n\nWe can do a lot better than this. Since we know that the hierarchy has at max 3 levels, we can just do this in one SQL query with minor `NULL` handling on the application side.\n\nThe SQL query to get the topic path would have to join `3` instances of the `topics` table, each one handling one level in the hierarchy and joining with its parent on `parent_id`. The SQL query would fetch the `id` and the `name` of the topics in the topic path.\n\n```sql\nSELECT topics_level1.id, topics_level1.name,\n       topics_level2.id, topics_level2.name,\n       topics_level3.id, topics_level3.name\n\nFROM topics AS topics_level3\n    LEFT JOIN topics AS topics_level2\n        ON topics_level2.id = topics_level3.parent_id\n    LEFT JOIN topics AS topics_level1\n        ON topics_level1.id = topics_level2.parent_id\n\nWHERE topics_level3.id = 610;\n```\n\nIn the SQL query above we fetch the topic path for topic id `610`. We join table `topics` twice (3 instances of topics table) each handling a distinct level. Since we are using JOIN, if a `parent_id` is `NULL` and the join parameter would not match anything which would result `NULL` selects for those columns. These `NULL` values come in very handy when we compute the topic path for sub-categories and categories.\n\nIf the topic with `610` id is of type `topic` then\n\n- `topics_level1.id`, `topics_level1.name` will be category\n- `topics_level2.id`, `topics_level2.name` will be sub-category\n- `topics_level3.id`, `topics_level3.name` will be topic\n\nIf the topic with `610` id is of type `sub-category` then\n\n- `topics_level1.id`, `topics_level1.name` will be `NULL`\n- `topics_level2.id`, `topics_level2.name` will be category\n- `topics_level3.id`, `topics_level3.name` will be sub-category\n\nIf the topic with `610` id is of type `category` then\n\n- `topics_level1.id`, `topics_level1.name` will be `NULL`\n- `topics_level2.id`, `topics_level2.name` will be `NULL`\n- `topics_level3.id`, `topics_level3.name` will be category\n\nSo, in the application code, we still access all the selected columns but we create the topic path skipping the `NULL` values accordingly.\n\nTo support this query, our table only requires [Primary Key](https://en.wikipedia.org/wiki/Primary_key) on `id` and [Foreign Key](https://en.wikipedia.org/wiki/Foreign_key) on `parent_id`.\n\n## Get all the children of a category or a sub-category\n\nGetting all the children of a category or a sub-category will be heavily used to drive the \"Browse and Explore\" page, where users would want to drill down and explore the kind of topics Udemy covers. SQL Query for this has to support pagination and will be required to output all children for a given parent, in order of `score` such that more popular children are returned first.\n\n```sql\nSELECT * FROM topics WHERE parent_id = 123 ORDER BY score DESC;\n```\n\nThe SQL query above fetches all the child topics of a given parent topic with `id` = `123`. Since we are ordering by `score`, for this query to be efficient we create a [composite index](https://en.wikipedia.org/wiki/Composite_index_(database)) on `(parent_id, score)`.\n\n## Get category hierarchy\n\nUdemy, on its home page, puts out all the categories under a dropdown menu enabling users to explore top categories and topics in a glimpse.\n\nOne peculiar behavior of this is it shows all categories and top `k` sub-categories within each. Once we hover upon a sub-category it makes a network call to fetch top topics within that sub-category. This means we need to write a query that fetches all categories and `k` sub-categories within each category from the entire `topics` table.\n\nAlthough it looks very complicated at first, it is very easy to do with a single SQL query.\n\n```sql\nSELECT t1_id, t1_name, t2_id, t2_name, t2_score\nFROM (\n    SELECT topics1.id AS t1_id, topics1.name AS t1_name,\n           topics2.id AS t2_id, topics2.name AS t2_name,\n           ROW_NUMBER() OVER (PARTITION BY topics1.id) row_num\n\n    FROM topics AS topics1\n        LEFT JOIN topics AS topics2 ON topics1.id = topics2.parent_id\n\n    WHERE topics1.type = 1 and topics2.type = 2\n\n    ORDER BY topics1.score DESC, topics2.score DESC\n\n) t\nWHERE row_num <= 10;\n```\n\nAbove SQL query picks all categories and top `10` sub-categories from each category and returns it as part of `SELECT`. It uses a very interesting SQL construct called Window Functions, specifically [`ROW_NUMBER`](https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number) and `PARTITION BY`.\n\nWe perform the usual join on `topics` once where the left operand is categories (topics with `type = 1`) and the right one is a sub-category (topics with `type = 2`). We then partition this join by category `id` and then compute `ROW_NUMBER` for sub-categories within it.\n\nThe row numbers are computed for each partition separately so it goes as `1, 2, 3, ..., n` for `n` rows within each category. We then apply a simple `WHERE` clause check on this row number to be `<= k` which then typically matches the first `k` row within each partition i.e category.\n\nNote: to get \"top\" `k` sub-categories we just apply for an additional `ORDER BY` on `score` that sorts the sub-categories ensuring top sub-categories are fetched first. This way the first `k` rows we consider from the partition are essentially the top sub-categories within the category.\n\nTo make this SQL query efficient we would need a [foreign key](https://en.wikipedia.org/wiki/Foreign_key) on `parent_id` and an index on `score` to make `ORDER BY` efficient.\n\n## Summary of indexes we need on `topics`\n\n- Primary Key on `id`\n- Foreign Key on `parents_id`\n- Index on `type`\n- Composite Index on `(parent_id, score)`\n\n# Explore more\n\nAlthough we covered quite a bit of this DB design there is always something interesting in exploring something new around this topic; so\n\n- explore [Nested Set Model](https://en.wikipedia.org/wiki/Nested_set_model) to design Taxonomy on relational databases\n- explore how DB engines behave when there are no indexes, you can use `EXPLAIN` to understand the behavior\n- find if there could be a better alternative to paginate results apart from `LIMIT/OFFSET`\n\nThus we designed a neat [Taxonomy](https://en.wikipedia.org/wiki/Taxonomy) on top of SQL-based relational databases like MySQL, Postgres, etc; wrote queries for some common scenarios, and determined the indexes to make taxonomy efficient.\n\n# References\n\n- [Window Functions - MySQL](https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html)\n- [Partitioning Types - MySQL](https://dev.mysql.com/doc/mysql-partitioning-excerpt/8.0/en/partitioning-types.html)\n",
    "similar": [
      "better-programmer",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "how-sleepsort-helped-me-understand-concurrency-in-golang",
      "multiple-mysql-on-same-server-using-docker"
    ]
  },
  {
    "id": 26,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "the-weird-walrus",
    "title": "The Weird Walrus",
    "description": "In this essay, we alter the Python Grammar and allow it run Assignment Expressions without any parenthesis.",
    "gif": "https://media.giphy.com/media/SbIJjAaist696ywQXx/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/112817071-ac4f2580-909f-11eb-8d56-f89536cfa065.png",
    "released_at": "2021-03-29",
    "total_views": 2377,
    "body": "Python in version 3.8 introduced [Assignment Expressions](https://realpython.com/lessons/assignment-expressions/) which can be used with the help of the Walrus Operator `:=`. This expression does assign and return in the same expression helping in writing a concise code.\n\nSay you are building your own shell in Python. It takes commands and input from the prompt, executes it on your shell, and renders the output. The shell should stop the execution as soon as it receives the `exit` command. This seemingly complicated problem can be built using just 4 lines of Python code.\n\n```python\ncommand = input(\">>> \")\nwhile command != \"exit\":\n    os.system(command)\n    command = input(\">>> \")\n```\n\nAlthough the above code runs perfectly fine, we can see that the `input` is taken twice, once outside the loop and once within the loop. This kind of use case is very common in Python.\n\nWalrus Operator fits perfectly here; now instead of initializing `command` with `input` outside and then checking if `command != 'exit'`, we can merge the two logic in one expression. The 4 lines of code above can be rewritten into the most intuitive 2 lines\n\n```python\nwhile (command := input(\">>> \")) != \"exit\":\n    os.system(command)\n```\n\n# What's weird with the Walrus operator?\n\nNow that we have established how useful the Walrus Operator could be for us, let's dive into the weird stuff. Since the Walrus operator has functioning similar to an assignment operator `=`, we would expect the following code to work fine, but it actually gives an error, not just any but a `SyntaxError`.\n\n```python\n>>> a := 10\n  File \"<stdin>\", line 1\n    a := 10\n      ^\nSyntaxError: invalid syntax\n```\n\nIf you thought, that was weird wait till we wrap the exact same statement with parenthesis and execute it.\n\n```python\n>>> (a := 10)\n10\n```\n\nWhat! it worked! How? What happened here? Just by wrapping the statement by parenthesis made an invalid Syntax valid? Isn't it weird? This behavior is pointed out in a Github repository called [wtf-python](https://github.com/satwikkansal/wtfpython#-first-things-first-). The theoretical explanation for this behavior is simple; Python disallows non-parenthesized Assignment Expressions but it allows non-parenthesized assignment statements.\n\nIn this essay, we dig deep into CPython and find out hows and the whys.\n\n# The hows and the whys\n\nFew points to note:\n\n- The Walrus Operator or Assignment Expressions are called Named Expressions in CPython.\n- The branch of the CPython we are referring to here is for version `3.8`\n\n## The Grammar\n\nIf `a := 10` is giving us a Syntax Error then it must be linked to the Grammar specification of the language. The grammar of Python can be found in the file [Grammar/Grammar](https://github.com/python/cpython/blob/3.8/Grammar/Grammar). So if we grep `namedexpr` in the Grammar file we get the following rules\n\n```\nnamedexpr_test: test [':=' test]\n\natom: ('(' [yield_expr|testlist_comp] ')' |\n       '[' [testlist_comp] ']' |\n       '{' [dictorsetmaker] '}' |\n       NAME | NUMBER | STRING+ | '...' | 'None' | 'True' | 'False')\n\ntestlist_comp: (namedexpr_test|star_expr) ( comp_for | (',' (namedexpr_test|star_expr))* [','] )\n\nif_stmt: 'if' namedexpr_test ':' suite ('elif' namedexpr_test ':' suite)* ['else' ':' suite]\n\nwhile_stmt: 'while' namedexpr_test ':' suite ['else' ':' suite]\n```\n\nThe above Grammar rules give us a good gist of how Named Expressions are supposed to be used. Here are some observations about it -\n\n- can be used in `while` statements\n- can be used along with `if` statements\n- named expressions are part of a rule called `testlist_comp`, which seems related to list comprehensions\n\nWe can see that the `atom` rules put in a hard check that `testlist_comp` should be either surrounded by `()` or `[]` and since `testlist_comp` can have `namedexpr_test` this puts in the check that Named Expressions should be surrounded by `()` or `[]`. \n\n```\n>>> (a := 1)\n1\n>>> [a := 1]\n[1]\n```\n\nSo when we run `a := 1`, none of the Grammar rules is satisfied and hence this results in a `SyntaxError`.\n\n## What about `if` and `while`?\n\nAccording to the rule `if_stmt` and `while_stmt` you can have named expressions right after `if` without needing any brackets surrounding it. This means the following statement is valid, but still chose to put parenthesis around `:=`, why?\n\n```python\nwhile command := input(\">>> \") != \"exit\":\n```\n\nThe answer is simple, [Operator Precedence](https://en.wikipedia.org/wiki/Order_of_operations); because of the configured precedence the above statement sets `command` as `bool` after evaluating `input(\">>> \") != \"exit\"` but we do not want this behaviour. Instead, we want `command` to be set as a command given as an input through `input` call and hence we wrap the expression with parenthesis for specifying explicit precedence.\n\n# Allowing `a := 10`\n\nTill now we saw how doing `a := 10` on a fresh Python prompt gives us a `SyntaxError`, so how about altering the CPython to allow `a := 10`? Sounds fun, isn't it?\n\n## Changing the Grammar\n\nTo achieve what we want to we will have to alter the Grammar rules. A good point to note here is that as a standalone statement, `:=` works and behaves very similar to a regular assignment statement having an `=`. So let's first find out, where have we allowed regular assignment statements \n\n```\nstmt: simple_stmt | compound_stmt\nsimple_stmt: small_stmt (';' small_stmt)* [';'] NEWLINE\nsmall_stmt: (expr_stmt | del_stmt | pass_stmt | flow_stmt |\n             import_stmt | global_stmt | nonlocal_stmt | assert_stmt)\nexpr_stmt: testlist_star_expr (annassign | augassign (yield_expr|testlist) |\n                     [('=' (yield_expr|testlist_star_expr))+ [TYPE_COMMENT]] )\n```\n\nThe regular assignment statements are allowed as per `expr_stmt` rule which is, in turn, a `small_stmt`, `simple_stmt`, and `stmt`. Rules are self-explanatory and skimming them would help you understand what exactly is happening in there.\n\nIn order to mimic the behavior of `:=` to be the same as `=` how about adding a new rule in `expr_stmt` that suggests matching the same pattern as `=`. So we make the following change in `expr_stmt`.\n\n```\nexpr_stmt: testlist_star_expr (annassign | augassign (yield_expr|testlist) |\n                     [('=' (yield_expr|testlist_star_expr))+ [TYPE_COMMENT]] |\n                     [(':=' (yield_expr|testlist_star_expr))+ [TYPE_COMMENT]] )\n```\n\nWhen we change anything in the `Grammar` file, we have to regenerate the parser code; and this can be done using the following command\n\n```\n$ make regen-grammar\n```\n\nOnce the above command is successful, we generate a fresh Python binary and see our changes in action.\n\n```\n$ make && ./python.exe\n```\n\nOn the fresh prompt that would have popped up try putting in `a := 10`, once you do this you will find out that this does not give any error and it executes seamlessly and it works just like a normal assignment statement, the behavior that we were seeking.\n\nSo with these changes, we have our Python interpreter that supports all three statements without any Error.\n\n```python\n>>> a = 10\n>>> (b := 10)\n10\n>>> c := 10\n```\n\nAll of these changes were made on my own [fork of CPython](https://github.com/arpitbbhayani/cpython) and the PR can be found [here](https://github.com/arpitbbhayani/cpython/pull/8).\n\n# References\n\n- [CPython Source Code Guide](https://realpython.com/cpython-source-code-guide/)\n- [Exploring CPython\u2019s Internals](https://devguide.python.org/exploring/)\n- [wtfpython - Github Repository](https://github.com/satwikkansal/wtfpython)\n- [Assignment Expressions: The Walrus Operator](https://realpython.com/lessons/assignment-expressions/)\n",
    "similar": [
      "setting-up-graphite-grafana-using-nginx-on-ubuntu",
      "super-long-integers",
      "making-http-requests-using-netcat",
      "how-sleepsort-helped-me-understand-concurrency-in-golang"
    ]
  },
  {
    "id": 27,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fully-persistent-arrays",
    "title": "Fully Persistent Arrays",
    "description": "Persistent Data Structures allow us to hold multiple versions of a data structure at any given instant of time. This enables us to go back in 'time' and access any version that we want. In this essay, we take a detailed look into the implementation of Fully Persistent Arrays.",
    "gif": "https://media.giphy.com/media/26tPplGWjN0xLybiU/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/107943189-55d7cd00-6fb2-11eb-9e03-a1da3870e718.png",
    "released_at": "2021-02-14",
    "total_views": 618,
    "body": "[Persistent Data Structures](https://arpitbhayani.me/blogs/persistent-data-structures-introduction) preserve previous versions of themselves allowing us to revisit and audit any historical version. While we already did an exhaustive introduction in the [previous essay](https://arpitbhayani.me/blogs/persistent-data-structures-introduction), in this essay, we take a detailed look into how we can implement the ***Fully Persistent*** variant of the most common data structure - ***Arrays***.\n\nThe essay is loosely based on the paper titled [Fully Persistent Arrays for Efficient Incremental Updates and Voluminous Reads](https://link.springer.com/chapter/10.1007/3-540-55253-7_7) by Tyng-Ruey Chuang and we implement it using Backer's trick elaborated in the paper [A Persistent Union-Find Data Structure](https://www.lri.fr/~filliatr/ftp/publis/puf-wml07.pdf).\n\n# Fully Persistent Arrays\n\nAn array is an abstract data structure consisting of `n` slots to hold a maximum of `n` elements such that each element is identified by at least one index. A typical array allows the following functions - `create(n)`, `update(index, value)` and `get(index)`.\n\nThe simplest form of an array, that we all are familiar with, is the Linear Array that is designed to hold elements in consecutive memory locations, leveraging spatial locality for faster and more efficient retrievals and scans. Before we jump into the implementation details of Fully Persistent Arrays, let's reiterate what exactly are Fully Persistent Data Structures.\n\nPersistent Data Structures preserve previous versions of themselves allowing us to revisit and audit any historical version. Fully Persistent Data Structures allows access and modification to all the historical versions as well. It does not restrict any modifications whatsoever. This means we can typically revisit any historical version of the data structure, modify it like we are forking out a new branch.\n\n![https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png](https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png)\n\nFully Persistent Arrays are arrays that support Full Persistence which means it supports usual array operations while also allowing us to go back in time and make updates to any of the previous versions. We define the following operations on Fully Persistent Arrays -\n\n- `create(n)` - returns an array of size `n` having all the slots uninitialized\n- `update(array, index, value)` - returns a new array identical to `array` except for the element at the position `index`. The parent array `array` remains unaffected and is still accessible.\n- `get(array, index)` - returns the element present at the index `index` in array `array`\n\n# Implementing Fully Persistent Array\n\nA naive way of implementing these arrays is to do a [Copy-on-Write](https://arpitbhayani.me/blogs/copy-on-write) and keep track of historical versions. This approach very inefficient as it requires `m` times the memory required to hold `n` elements, where `m` is the total number of versions of the array.\n\n![https://user-images.githubusercontent.com/4745789/107803148-3cebd380-6d88-11eb-9889-bb551e83c00a.png](https://user-images.githubusercontent.com/4745789/107803148-3cebd380-6d88-11eb-9889-bb551e83c00a.png)\n\nA better way of implementing these arrays is by using the [Backer's Trick](https://www.lri.fr/~filliatr/ftp/publis/spds-rr.pdf) which enables the required functionality with just one array and a tree of modifications.\n\n# Fully Persistent Arrays using Backer's Trick\n\nA more efficient way of implementing Fully Persistent Arrays is by using a single instance of an in-memory Array and in conjunction use a tree of modifications. Instead of storing all the versions separately, Backer's trick allows us to compute any version of the array by replaying all the changes asked for.\n\n## Tree of modifications\n\nThe tree of modifications is an `n`-ary tree that holds all the versions of the array by storing only the modifications made to the elements. Each version is derived from a parent version and the root points to the in-memory *cache* array which holds the initial version of it.\n\n![https://user-images.githubusercontent.com/4745789/107856766-71c35d80-6e50-11eb-9f59-c3744cdc884f.png](https://user-images.githubusercontent.com/4745789/107856766-71c35d80-6e50-11eb-9f59-c3744cdc884f.png)\n\nEach node of the tree holds three fields - `index`, `value`, and a pointer to the `parent`, making this tree pointing upwards towards to root. Thus each node holds the changed `value`, where did the change happen `index` and on which version the change happened `parent`.\n\nSay we changed the element at the index `1` of the array `9, 6, 3, 5, 1`  to `7` we get array `9, 7, 3, 5, 1`. The tree of modifications has 2 nodes one root node `a0` pointing to the initial array, and another node `a1` denoting the updated version.\n\n![https://user-images.githubusercontent.com/4745789/107858298-996af380-6e59-11eb-99dc-7a68ea25f5b4.png](https://user-images.githubusercontent.com/4745789/107858298-996af380-6e59-11eb-99dc-7a68ea25f5b4.png)\n\nThe node `a1` has 3 fields, `index` set to `1`, `value` set to `7` and `parent` pointing to `a0`. The node implies that it was derived from `a0` by changing the value of the element at the index `1` to `7`. If we try to branch off `a0` with another change say index `4` set to value `9` we would have 3 nodes in the tree. Thus we see how an update translates into just creating a new node and adding it at the right place in the tree.\n\nNow we see with this design how we implement the three functions of an array `create`, `update`, and `get`.\n\n## Implementing `create`\n\nThe `create` function allocates a linear array of size `n` to hold `n` elements. This is a usual array allocation. While doing this we also create the root node of our tree of modifications. The root node, as established earlier, points to the *cache* array.\n\n```python\n# The function creates a new persistent array of size `n`\ndef create(n: int) -> Array:\n    # 1. allocate in-memory cache\n    # 2. initialize the tree of modifications\n    # 3. make the root of the tree point to the cache\n    pass\n```\n\nThe overall complexity of this operation is `O(n)` space and `O(n)` time.\n\n## Implementing `update`\n\nThe `update` operation takes in the `index` that needs to be updated, the `value`, and the version of the array on which update is to be made.\n\n```python\n# The function updates the element at the index `index` with value\n# `value` on array `array` and returns the newly updated array\n# keeping the old one accessible.\ndef update(array: Array, index: int, value: object) -> Array:\n    # 1. create a node in the tree and store index, the value in it\n    # 2. point this new node to the parent array\n    pass\n```\n\nTo do this efficiently, we create a new node in the tree whose parent is set to the array version on which update is performed, index and value are set what was passed during invocation. Thus we see that the `update` operation takes a constant `O(1)` space and `O(1)` time to create and represent a new version of the array.\n\nWith the update operation being made efficient we have to trade-off `get` operation.\n\n## Implementing `get`\n\nThe `get` operation takes in the `index` that needs to be fetched and the version of the array from which the element is to be fetched. The `get` operation seeks no extra space but takes time proportional to the distance between the array version and the root. In the worst case, this distance will be as long as the total number of versions of the array.\n\n```python\n# The function fetches the element from index `index`\n# from the array `array` and returns it.\ndef get(array: Array, index: int) -> object:\n    # 1. Start from the requested array and traverse to the root node.\n    # 2. Allocate a new register to store the requested value\n    # 3. During traversal, if the node.index == `index` update the\n    # register with the value.\n    # 4. return the value of the register\n    pass\n```\n\nThe overall complexity of this operation is `O(1)` space and `O(n)` time.\n\n# Optimizing successive reads on the same version\n\nWe established that the update operation takes constant time and reads are expensive. If our system is write-heavy, then this is pretty handy but if the system has more reads then operation taking `O(n)` time hampers the overall performance of the system. So as to optimize this use case we take a look at the operation called *Rerooting.*\n\n## Rerooting\n\nThe initial array (the first version of the array) has no significance to be the root forever. We can reroot the entire tree such that any child node could become the root and the value it points to - *cache* - represents the true copy of the array. Rerooting is a sequence of rotations to make the desired array version the root.\n\nThe algorithm for rerooting is a classic Backtracking algorithm that requires updates in all the nodes coming in the path from the old node to the new node.\n\n![https://user-images.githubusercontent.com/4745789/107935855-ee1c8480-6fa7-11eb-9870-2ae90d6460a9.png](https://user-images.githubusercontent.com/4745789/107935855-ee1c8480-6fa7-11eb-9870-2ae90d6460a9.png)\n\nThe rerooting operation takes time proportional to the distance between the old and new root ~ `O(n)`. Since the successive reads are happening on the same version the `get` operation becomes `O(1)` as well. Thus depending on the kind of usage of the system we can add rerooting step in either `get` or `update` operation.\n\n# References\n\n- [Persistent Arrays - Wikipedia](https://en.wikipedia.org/wiki/Persistent_array)\n- [Semi-Persistent Data Structures](https://www.lri.fr/~filliatr/ftp/publis/spds-rr.pdf)\n- [Fully Persistent Arrays for Efficient Incremental Updates and Voluminous Reads](https://link.springer.com/chapter/10.1007/3-540-55253-7_7)\n",
    "similar": [
      "phi-accrual",
      "persistent-data-structures-introduction",
      "copy-on-write",
      "lfu"
    ]
  },
  {
    "id": 28,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "persistent-data-structures-introduction",
    "title": "Persistent Data Structures - An Introduction",
    "description": "Persistent Data Structures allow us to hold multiple versions of a data structure at any given instant of time. This enables us to go back in 'time' and access any version that we want.",
    "gif": "https://media.giphy.com/media/1hVi7JFFzplHW/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/107117196-9347a680-689e-11eb-8d3f-75da045b7baf.png",
    "released_at": "2021-02-07",
    "total_views": 1861,
    "body": "Ordinary data structures are ephemeral implying that any update made to it destroys the old version and all we are left with is the updated latest one. Persistent Data Structures change this notion and allow us to hold multiple versions of a data structure at any given instant of time. This enables us to go back in \"time\" and access any version that we want.\n\nIn this essay, we take a detailed look into the world of Persistent Data Structures and see the basics of its implementation along with where exactly we can find them in action. This essay is meant to act as an exhaustive introduction to the topic and in future essays, we will dive deeper into the specifics of each data structure.\n\nThis essay is loosely based on the iconic paper published in 1986 titled [Making Data Structures Persistent](https://www.cs.cmu.edu/~sleator/papers/making-data-structures-persistent.pdf) by James R. Driscoll, Neil Sarnak, Daniel D. Sleator, and Robert E. Tarjan.\n\n# Persistence\n\nPersistent Data Structures preserve previous versions of themselves allowing us to revisit and audit any historical version. Depending on the operations allowed on the previous versions, persistence is classified into three categories\n\n### Partially Persistent\n\nPartially Persistent Data Structures allows access to all the historical versions but allows modification to only the newest one. This typically makes historical versions of the data structure immutable (read-only).\n\n![https://user-images.githubusercontent.com/4745789/107117960-e4a66480-68a3-11eb-97a0-c8c412527471.png](https://user-images.githubusercontent.com/4745789/107117960-e4a66480-68a3-11eb-97a0-c8c412527471.png)\n\n### Fully Persistent\n\nFully Persistent Data Structures allows access and modification to all the historical versions. It does not restrict any modifications whatsoever. This means we can typically revisit any historical version and modify it and thus fork out a new branch.\n\n![https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png](https://user-images.githubusercontent.com/4745789/107117958-e112dd80-68a3-11eb-971b-58034e693f44.png)\n\n### Confluently Persistent\n\nConfluently Persistent Data Structures allow modifications to historical versions while also allowing them to be merged with existing ones to create a new version from the previous two. \n\n![https://user-images.githubusercontent.com/4745789/107117954-da846600-68a3-11eb-9c34-b9489c170710.png](https://user-images.githubusercontent.com/4745789/107117954-da846600-68a3-11eb-9c34-b9489c170710.png)\n\n# Applications of Persistent Data Structures\n\nPersistent Data Structures find their applications spanning the entire spectrum of Computer Science, including but not limited to - Functional Programming Languages, Computational Geometry, Text Editors, and many more.\n\n## Functional Programming Languages\n\nFunctional Programming Languages are ideal candidates for incorporating Persistent Data Structures as they forbid, while some discourage, the mutability of underlying structures. These languages pass around states within functions and expect that they do not update the existing one but return a new state. Programming languages like Haskell, Clojure, Elm, Javascript, Scala have native Persistent implementations of data structures like Lists, Maps, Sets, and Trees.\n\n## Computational Geometry\n\nOne of the fundamental problems in Computational Geometry is the [Point Location Problem](https://en.wikipedia.org/wiki/Point_location) which deals with identifying the region where the query point lies. A simpler version of the problem statement is to find if a point lies within or outside a given polygon. A popular solution to determine a solution to the Point Location problem statement uses [Persistent Red-Black Trees](https://en.wikipedia.org/wiki/Red\u2013black_tree).\n\n## Text and File Editing\n\nThe most common operation required by any Text or File editing tool is *Undo and Redo* and having persisted all historical versions through a persistent data structure makes these most frequent operations very efficient and a breeze.\n\n# Implementing Partial Persistence\n\nThere are a few generic techniques that help in implementing Partial Persistence. Just to reiterate, partial persistence allows access to all the historical versions but allows modification to only the newest one, to create a newer updated copy of data.\n\n## Copy on Write Semantics\n\nA naive way to implement Partial Persistence is by utilizing the Copy-on-Write semantics and naively creating a deep copy upon every update. This technique is inefficient because upon every writes the entire data structure is deep copied.\n\nThere are certain methods built upon certain storage paradigms that copy only what matters making the entire CoW efficient. I have already gone in-depth of [Copy-on-Write semantics](https://arpitbhayani.me/blogs/copy-on-write) and I encourage you to check that out.\n\n## The Fat Node Method\n\nInstead of creating copies of the entire data structure, the Fat Node method suggests that each cell holding the value within the data structure is modified to hold multiple values (one for each version) making it arbitrarily *fat*. Each value node thus holds a value and a version stamp.\n\n```python\nclass Node:\n    def __init__(self, value: object):\n        # references to other Nodes creating a Node topology\n        # that forms the core of the data structure\n        # this could be `next` and `prev` pointers in a linked list\n        # or `left`, `right` in case of a binary search tree.\n        self.refs: List[] = []\n\n        # holding all the values against the version (key)\n        self.values: Dict[int, object] = {}\n```\n\n## The Node-Copying Method\n\nThe Node-Copying method eliminates all the problems with the Fat Node method. It allows each node to hold only a fixed number of values in it. Upon exhaustion of space, a new copy of Node is created and it holds only the newest values in it. The old node also holds pointers to the newly created node allowing browsing.\n\nHaving this structure helps in making update operation slightly efficient by reducing the number of nodes to be copied during writes, considering the in-degrees to each node is bounded.\n\n## Path-Copying Method\n\nThe path-copying method copies all the nodes coming in the path from the root to the node being modified. This way it tries to minimize the copy and promotes reusing some of the unmodified data. This method comes in super handy in Linked Data Structures like Lists and Trees.\n\n![https://user-images.githubusercontent.com/4745789/107144006-33b0d000-695e-11eb-9e13-959eaeba44f4.png](https://user-images.githubusercontent.com/4745789/107144006-33b0d000-695e-11eb-9e13-959eaeba44f4.png)\n\n> Implementation of Full Persistence is a mammoth of a topic on its own and deserves its own essay. Hence skipping it from the scope of this one.\n\n# Reducing the memory footprint\n\nSince persistent data structures thrive on high memory usage, they require some garbage collection system to prevent memory leaks. Algorithms like [Reference Counting](https://en.wikipedia.org/wiki/Reference_counting) or [Mark and Sweep](https://en.wikipedia.org/wiki/Mark_and_sweep) serves the purpose pretty well.\n\nThus when a historical version is not referenced anymore in the program space, the corresponding objects and nodes are freed up.\n\n# References\n\n- [Making Data Structures Persistent](https://www.cs.cmu.edu/~sleator/papers/making-data-structures-persistent.pdf)\n- [Persistent data structure - Wikipedia](https://en.wikipedia.org/wiki/Persistent_data_structure)\n- [Geometric Data Structures](http://www.sccg.sk/~samuelcik/dgs/geom_structures.pdf)\n- [Point Location Problem - Wikipedia](https://en.wikipedia.org/wiki/Point_location)\n- [Text Editor: Data Structures - HackerNews](https://news.ycombinator.com/item?id=15381886)\n",
    "similar": [
      "bitcask",
      "leaderless-replication",
      "master-replica-replication",
      "rum"
    ]
  },
  {
    "id": 29,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "constant-folding-python",
    "title": "Constant Folding in Python",
    "description": "Every programming language aims to be performant and Python is no exception. In this essay, we dive deep into Python internals and find out how Python makes its interpreter performant using a technique called Constant Folding.",
    "gif": "https://media.giphy.com/media/mMCXtwz1DM3xPKgkOB/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/104118331-23e9a000-534e-11eb-9ec8-8369cdae40b1.png",
    "released_at": "2021-01-10",
    "total_views": 3917,
    "body": "Every programming language aims to be performant in its niche and achieving superior performance requires a lot of compiler level optimizations. One famous optimization technique is [Constant Folding](https://en.wikipedia.org/wiki/Constant_folding) where during compile time the engine tries to recognize constant expressions, evaluate them, and replaces the expression with this newly evaluated value, making the runtime leaner.\n\nIn this essay, we dive deep and find what exactly is Constant Folding, understand the scope of it in the world of Python and finally go through Python's source code - [CPython](https://github.com/python/cpython/) - and find out how elegantly Python actually implements it.\n\n# Constant Folding\n\nIn [Constant Folding](https://en.wikipedia.org/wiki/Constant_folding), the engine finds and evaluates constant expressions at compile time rather than computing them at runtime, making the runtime leaner and faster.\n\n```python\n>>> day_sec = 24 * 60 * 60\n```\n\nWhen the compiler encounters a constant expression, like above, it evaluates the expression and replaces it with the evaluated value. The expression is usually replaced by the evaluated value in the [Abstract Syntax Tree](https://en.wikipedia.org/wiki/Abstract_syntax_tree), but the implementation is totally up to the language. Hence the above expression is effectively executed as\n\n```python\n>>> day_sec = 86400\n```\n\n# Constant Folding in Python\n\nIn Python, we could use the [Disassembler module](https://docs.python.org/3/library/dis.html#module-dis) to get the CPython bytecode giving us a good peek at how things will be executed. When we disassemble the above constant expression using the `dis` module, we get the following bytecode\n\n```python\n>>> import dis\n>>> dis.dis(\"day_sec = 24 * 60 * 60\")\n\n        0 LOAD_CONST               0 (86400)\n        2 STORE_NAME               0 (day_sec)\n        4 LOAD_CONST               1 (None)\n        6 RETURN_VALUE\n```\n\nWe see that the bytecode, instead of having two binary multiply operations followed by one `LOAD_CONST`, is having just one `LOAD_CONST` with the already evaluated value of `86400`. This indicates that the CPython interpreter during parsing and building of Abstract Syntax Tree folded the constant expression, `24 * 60 * 60` and replaced it with the evaluated value `86400`.\n\n### Scope of Constant Folding\n\nPython tries to fold every single constant expression present but there are some cases where even though the expression is constant, but Python chooses not to fold it. For example, Python does not fold `x = 4 ** 64` while it does fold `x = 2 ** 64`. \n\nApart from the arithmetic expressions, Python also folds expressions involving Strings and Tuples, where constant string expressions till the length `4096` are folded.\n\n```python\n>>> a = \"-\" * 4096   # folded\n>>> a = \"-\" * 4097   # not folded\n>>> a = \"--\" * 4096  # not folded\n```\n\n# Internals of Constant Folding\n\nNow we shift our focus to the internals and find exactly where and how CPython implements Constant Folding. All AST optimizations, including Constant Folding, can be found in file [ast_opt.c](https://github.com/python/cpython/blob/master/Python/ast_opt.c). The base function starting it all is `astfold_expr` which folds any and every expression that Python source has. The function recursively goes through the AST and tries to fold every constant expression, as seen in the snippet below.\n\n![https://user-images.githubusercontent.com/4745789/103898628-38922200-511b-11eb-965f-fb4d46d3c45c.png](https://user-images.githubusercontent.com/4745789/103898628-38922200-511b-11eb-965f-fb4d46d3c45c.png)\n\nThe `astfold_expr` before folding the expression at hand, tries to fold its child expressions (operands) and then delegates the folding to the corresponding specific expression folding function. The operation-specific folding function evaluates the expression and returns the evaluated constant value, which is then put into the AST.\n\nFor example, whenever `astfold_expr` encounters a binary operation, it recursively folds the two child operands (expressions) before evaluating the expression at hand using `fold_binop`. The function `fold_binop` returns the evaluated constant value as seen in the snippet below.\n\n![https://user-images.githubusercontent.com/4745789/103898745-670ffd00-511b-11eb-88a9-f741157473b3.png](https://user-images.githubusercontent.com/4745789/103898745-670ffd00-511b-11eb-88a9-f741157473b3.png)\n\n`fold_binop` function folds the binary operation by checking the kind of operator at hand and then invoking the corresponding evaluation function on them. For example, if the operation at hand is an addition then, to evaluate the final value, it invokes `PyNumber_Add` on both its left and right operands.\n\n### What makes this elegant?\n\nInstead of writing special logic to handle certain patterns or types to fold constant expressions efficiently, CPython invokes the same general code. For example, it invokes the same usual `PyNumber_Add` function while folding that it does to perform the usual addition operation.\n\nCPython has thus eradicated the need to write special functions to handle constant folding by making sure its code and evaluation process is structured in such a way that the general-purpose code itself can handle the evaluation of constant expressions.\n\n# References\n\n- [Constant Folding](https://en.wikipedia.org/wiki/Constant_folding)\n- [CPython Optimizations](https://stummjr.org/post/cpython-optimizations/)\n- [Python dis module and constant folding](https://yasoob.me/2019/02/26/python-dis-module-and-constant-folding/)\n- [The simple way CPython does constant folding](https://utcc.utoronto.ca/~cks/space/blog/python/CPythonConstantFolding)\n- [A constant folding optimization pass for the AST](https://bugs.python.org/issue1346238)\n",
    "similar": [
      "python-prompts",
      "python-caches-integers",
      "python-iterable-integers",
      "fsm"
    ]
  },
  {
    "id": 30,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "string-interning",
    "title": "String Interning in Python",
    "description": "Every programming language aims to be performant and Python is no exception. In this essay, we dive deep into Python internals and find out how Python makes its interpreter performant using a technique called String Interning.",
    "gif": "https://media.giphy.com/media/KRNBl0iSS8gg0/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/102710262-1c971f80-42d7-11eb-9ee4-dcefd540e869.png",
    "released_at": "2020-12-20",
    "total_views": 1480,
    "body": "Every programming language aims to be performant in its niche and achieving superior performance requires a bunch of compiler and interpreter level optimizations. Since character Strings are an integral part of any programming language, having the ability to perform string operations quickly elevates the overall performance.\n\nIn this essay, we dive deep into Python internals and find out how Python makes its interpreter performant using a technique called [String Interning](https://en.wikipedia.org/wiki/String_interning). This essay not only aims to put forth Python internals but also aims to make the reader comfortable in navigating through Python's source code; so expect a lot of code snippets taken from [CPython](https://github.com/python/cpython/).\n\n# String Interning\n\nString Interning is a compiler/interpreter optimization method that makes common string processing tasks space and time efficient by [caching](https://en.wikipedia.org/wiki/Cache_(computing)) them. Instead of creating a new copy of string every time, this optimization method dictates to keep just one copy of string for every *appropriate* [immutable](https://en.wikipedia.org/wiki/Immutable_object) distinct value and use the pointer reference wherever referred.\n\nThe single copy of each string is called its ***intern*** and hence the name String Interning. The lookup of string intern, may or may not be exposed as a public interfaced method. Modern programming languages like Java, Python, PHP, Ruby, Julia, and many more, performs String Interning to make their compilers and interpreters performant.\n\n![https://user-images.githubusercontent.com/4745789/102705512-d1691680-42ae-11eb-825f-1e032a7c12c5.png](https://user-images.githubusercontent.com/4745789/102705512-d1691680-42ae-11eb-825f-1e032a7c12c5.png)\n\n## Why should Strings be interned?\n\n*String Interning speeds up string comparisons*. Without interning if we were to compare two strings for equality the complexity of it would shoot up to `O(n)` where we examine every character from both the strings to decide their equality. But if the strings are interned, instead of checking every character, equal strings will have the same object reference so just a pointer quality check would be sufficient to say if two string literals are equal. Since this is a very common operation, this is typically implemented as a pointer equality test, using just a single machine instruction with no memory reference at all.\n\n*String Interning reduces the memory footprint.* Instead of filling memory with redundant String objects, Python optimizes memory footprint by sharing and reusing already defined objects as dictated by the [flyweight design pattern](https://en.wikipedia.org/wiki/Flyweight_pattern).\n\n# String Interning in Python\nJust like most other modern programming languages, Python also does [String Interning](https://en.wikipedia.org/wiki/String_interning) to gain a performance boost. In Python, we can find if two objects are referring to the same in-memory object using the `is` operator. So if two string objects refer to the same in-memory object, the `is` operator yields `True` otherwise `False`.\n\n```bash\n>>> 'python' is 'python'\nTrue\n```\n\nWe can use this particular operator to test which all strings are interned and which are not. In CPython, String Interning is implemented through the following function, declared in [unicodeobject.h](https://github.com/python/cpython/blob/master/Include/unicodeobject.h) and defined in [unicodeobject.c](https://github.com/python/cpython/blob/master/Objects/unicodeobject.c).\n\n```cpp\nPyAPI_FUNC(void) PyUnicode_InternInPlace(PyObject **);\n```\n\nIn order to check if a String is interned, CPython implements a macro named `PyUnicode_CHECK_INTERNED`, again defined in [unicodeobject.h](https://github.com/python/cpython/blob/master/Include/unicodeobject.h). The macro suggests that the Python maintains a member named `interned` in `PyASCIIObject` structure whose value suggests if the corresponding String is interned or not.\n\n```cpp\n#define PyUnicode_CHECK_INTERNED(op) \\\n    (((PyASCIIObject *)(op))->state.interned)\n```\n\n## Internals of String Interning\n\nIn CPython, the String references are stored, accessed, and managed using a Python dictionary named `interned`. This dictionary is lazily initialized upon the first String Intern invocation and holds the reference to all the interned String objects.\n\n### Interning the String\n\nThe core function responsible for interning the String is named `PyUnicode_InternInPlace` defined in [unicodeobject.c](https://github.com/python/cpython/blob/master/Objects/unicodeobject.c) that upon invocation lazily builds the main dictionary `interned` to hold all interned strings and then registers the object into it with the key and the value both set as the same object reference. The following function snippet shows the String Interning process as implemented in Python.\n\n```cpp\nvoid\nPyUnicode_InternInPlace(PyObject **p)\n{\n    PyObject *s = *p;\n\n    .........\n\n    // Lazily build the dictionary to hold interned Strings\n    if (interned == NULL) {\n        interned = PyDict_New();\n        if (interned == NULL) {\n            PyErr_Clear();\n            return;\n        }\n    }\n\n    PyObject *t;\n\n    // Make an entry to the interned dictionary for the\n    // given object\n    t = PyDict_SetDefault(interned, s, s);\n\n    .........\n    \n    // The two references in interned dict (key and value) are\n    // not counted by refcnt.\n    // unicode_dealloc() and _PyUnicode_ClearInterned() take\n    // care of this.\n    Py_SET_REFCNT(s, Py_REFCNT(s) - 2);\n\n    // Set the state of the string to be INTERNED\n    _PyUnicode_STATE(s).interned = SSTATE_INTERNED_MORTAL;\n}\n```\n\n### Cleanup of Interned Strings\n\nThe cleanup function iterates over all the Strings held in the `interned` dictionary, adjusts the reference counts of the object, and marks them as `NOT_INTERNED` allowing them to be garbage collected. Once all the strings are marked as `NOT_INTERNED`, the `interned` dictionary is cleared and deleted. The cleanup function is defined in [unicodeobject.c](https://github.com/python/cpython/blob/master/Objects/unicodeobject.c) by the name `_PyUnicode_ClearInterned`.\n\n```cpp\nvoid\n_PyUnicode_ClearInterned(PyThreadState *tstate)\n{\n    .........\n\n    // Get all the keys to the interned dictionary\n    PyObject *keys = PyDict_Keys(interned);\n\n    .........\n\n    // Interned Unicode strings are not forcibly deallocated;\n    // rather, we give them their stolen references back\n    // and then clear and DECREF the interned dict.\n\n    for (Py_ssize_t i = 0; i < n; i++) {\n        PyObject *s = PyList_GET_ITEM(keys, i);\n\n        .........\n\n        switch (PyUnicode_CHECK_INTERNED(s)) {\n        case SSTATE_INTERNED_IMMORTAL:\n            Py_SET_REFCNT(s, Py_REFCNT(s) + 1);\n            break;\n        case SSTATE_INTERNED_MORTAL:\n            // Restore the two references (key and value) ignored\n            // by PyUnicode_InternInPlace().\n            Py_SET_REFCNT(s, Py_REFCNT(s) + 2);\n            break;\n        case SSTATE_NOT_INTERNED:\n            /* fall through */\n        default:\n            Py_UNREACHABLE();\n        }\n\n        // marking the string to be NOT_INTERNED\n        _PyUnicode_STATE(s).interned = SSTATE_NOT_INTERNED;\n    }\n\n    // decreasing the reference to the initialized and\n    // access keys object.\n    Py_DECREF(keys);\n\n    // clearing the dictionary\n    PyDict_Clear(interned);\n\n    // clearing the object interned\n    Py_CLEAR(interned);\n}\n```\n\n## String Interning in Action\n\nNow that we understand the internals of String Interning and Cleanup, we find out what all Strings are interned in Python. To discover the spots all we do is grep for the function invocation for `PyUnicode_InternInPlace` in the CPython source code and peek at the neighboring code. Here is a list of interesting spots where String Interning happens in Python.\n\n### Variables, Constants, and Function Names\n\nCPython performs String Interning on constants such as Function Names, Variable Names, String Literals, etc. Following is the snippet from [codeobject.c](https://github.com/python/cpython/blob/master/Objects/codeobject.c) that suggests that when a new `PyCode` object is created the interpreter is interning all the compile-time constants, names, and literals.\n\n```cpp\nPyCodeObject *\nPyCode_NewWithPosOnlyArgs(int argcount, int posonlyargcount, int kwonlyargcount,\n                          int nlocals, int stacksize, int flags,\n                          PyObject *code, PyObject *consts, PyObject *names,\n                          PyObject *varnames, PyObject *freevars, PyObject *cellvars,\n                          PyObject *filename, PyObject *name, int firstlineno,\n                          PyObject *linetable)\n{\n\n    ........\n\n    if (intern_strings(names) < 0) {\n        return NULL;\n    }\n\n    if (intern_strings(varnames) < 0) {\n        return NULL;\n    }\n\n    if (intern_strings(freevars) < 0) {\n        return NULL;\n    }\n\n    if (intern_strings(cellvars) < 0) {\n        return NULL;\n    }\n\n    if (intern_string_constants(consts, NULL) < 0) {\n        return NULL;\n    }\n\n    ........\n\n}\n```\n\n### Dictionary Keys\n\nCPython also interns thee Strings which keys of any dictionary object. Upon putting an item in the dictionary the interpreter String Interning on the key against which item is stored. The following code is taken from [dictobject.c](https://github.com/python/cpython/blob/master/Objects/dictobject.c) showcasing the exact behavior.\n\nFun Fact: There is a comment next to the `PyUnicode_InternInPlace` function call that suggests if we really need to intern all the keys in all the dictionaries.\n\n```cpp\nint\nPyDict_SetItemString(PyObject *v, const char *key, PyObject *item)\n{\n    PyObject *kv;\n    int err;\n    kv = PyUnicode_FromString(key);\n    if (kv == NULL)\n        return -1;\n\n    // Invoking String Interning on the key\n    PyUnicode_InternInPlace(&kv); /* XXX Should we really? */\n\n    err = PyDict_SetItem(v, kv, item);\n    Py_DECREF(kv);\n    return err;\n}\n```\n\n### Attributes of any Object\n\nObjects in Python can have attributes that can be explicitly set using `setattr` function or are implicitly set as part of Class members or as pre-defined functions on data types. CPython interns all these attribute names, so as to make lookup blazing fast. Following is the snippet of the function `PyObject_SetAttr` responsible for setting a new attribute to a Python object, as defined in the file [object.c](https://github.com/python/cpython/blob/master/Objects/object.c).\n\n```cpp\nint\nPyObject_SetAttr(PyObject *v, PyObject *name, PyObject *value)\n{\n\n    ........\n\n    PyUnicode_InternInPlace(&name);\n\n    ........\n}\n```\n\n### Explicit Interning\n\nPython also allows explicit String Interning through the function `intern` defined in `sys` module. When this function is invoked with any String object, the provided String is interned. Following is the code snippet from the file [sysmodule.c](https://github.com/python/cpython/blob/master/Python/sysmodule.c) that shows String Interning happening in `sys_intern_impl`.\n\n```cpp\nstatic PyObject *\nsys_intern_impl(PyObject *module, PyObject *s)\n{\n\n    ........\n\n    if (PyUnicode_CheckExact(s)) {\n        Py_INCREF(s);\n        PyUnicode_InternInPlace(&s);\n        return s;\n    }\n\n    ........\n}\n```\n\n## Extra nuggets on String Interning\n\n*Only compile-time strings are interned*. Strings that are specified during interpretation or compile-time are interned while dynamically created strings are not.\n\n*Strings having ASCII letters and underscores are interned*. During compile time when string literals are observed for interning, [CPython](https://github.com/python/cpython/blob/master/Objects/codeobject.c) ensures that it only interns the literals matching the regular expression `[a-zA-Z0-9_]*` as they closely resemble Python identifiers.\n\nComments on how CPython does String Interning internally (as discussed in the [Video](https://youtu.be/QpGK69LzfpY)) can be found in [this PR](https://github.com/arpitbbhayani/cpython/pull/9]).\n\n# References\n\n- [String Interning](https://en.wikipedia.org/wiki/String_interning)\n- [CPython Optimizations](https://stummjr.org/post/cpython-optimizations/)\n- [Python Objects Part III: String Interning](https://medium.com/@bdov_/https-medium-com-bdov-python-objects-part-iii-string-interning-625d3c7319de)\n- [The internals of Python string interning](http://guilload.com/python-string-interning/)\n",
    "similar": [
      "i-changed-my-python",
      "python-caches-integers",
      "python-iterable-integers",
      "super-long-integers"
    ]
  },
  {
    "id": 31,
    "topic": null,
    "uid": "recursion-visualizer",
    "title": "A Simple Recursion Tree Visualizer for Python",
    "description": "One of the most complicated concepts to wrap our heads around has to be recursion; to understand it well it always helps to have some sort of recursion visualization. In this essay, we build a super-simple yet effective visualizer for recursive functions in Python.",
    "gif": "https://media.giphy.com/media/3ov9jQX2Ow4bM5xxuM/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/102006828-0ffc4f80-3d4a-11eb-8ab1-0891dc5ba00f.png",
    "released_at": "2020-12-13",
    "total_views": 4140,
    "body": "One programming paradigm, that is hardest to visualize, as almost every single programmer out there will agree on, is [Recursion](https://en.wikipedia.org/wiki/Recursion_(computer_science)). We usually use pen and paper to visualize the flow and check how recursion is behaving. But what if, this could be done programmatically, and today we address this very problem and try to come up with a simple yet effective solution.\n\nThis essay is going to be a little different from the usuals; instead of taking a look into a research paper or an algorithm, we will implement a simple and easy recursion visualizer for Python.\n\n# Recursion Tree\n\nRecursion helps in solving a larger problem by breaking it into smaller similar ones. The classic implementation of recursion in the world of programming is when a function invokes itself using reduced parameters while having a base terminating condition.\n\n```python\ndef fib(n):\n  # base condition mimicking the first two numbers\n  # in the sequence\n  if n == 0: return 0\n  if n == 1: return 1\n\n  # every number is summation of the previous two\n  return fib(n - 1) + fib(n - 2)\n```\n\nThe most common problem that is solved using recursion is computing the `n`th [Fibonacci Number](https://en.wikipedia.org/wiki/Fibonacci_number). A trivial recursive Python function that spits out `n`th Fibonacci Number is as shown below\n\nThe most effective way of visualizing recursion is by drawing a recursion tree. It is very useful for visualizing what happens when a recurrence is iterated. The recursion tree for the above function `fib` for input `n = 3` is as illustrated below\n\n![https://user-images.githubusercontent.com/4745789/102004754-74fb7980-3d39-11eb-991e-0f54fa7f20c6.png](https://user-images.githubusercontent.com/4745789/102004754-74fb7980-3d39-11eb-991e-0f54fa7f20c6.png)\n\n# Decorating to visualize\n\nInstead of printing an actual tree-like recursion tree, we take some liberty and print a close-enough version of it running top-down. To keep track of recursive function calls we use [Python Decorators](https://realpython.com/primer-on-python-decorators/) that essentially wraps the function allowing us to invoke statements before and after the function call.\n\nThe decorator that wraps the recursive function and prints the recursion tree is as illustrated below.\n\n```python\ndef recviz(fn):\n    \"\"\"Decorator that pretty prints the recursion tree with\n       args, kwargs, and return values.\n    \"\"\"\n\n    # holds the current recursion level\n    recursion_level = 1\n\n    def wrapper(*args, **kwargs):\n\n        # we register a nonlocal recursion_level so that\n        # it binds with the recursion_level variable.\n        # in this case, it will bind to the one defined\n        # in recviz function.\n        nonlocal recursion_level\n\n        # Generate the pretty printed function string\n        fn_str = pretty_func(fn, args, kwargs)\n\n        # Generate the whitespaces as per the recursion level\n        whitespace = \"   \" * (recursion_level - 1)\n\n        # Pretty print the function with the whitespace\n        print(f\"{whitespace} -> {fn_str}\")\n\n        # increment the recursion level\n        recursion_level += 1\n\n        # Invoke the wrapped function and hold the return value\n        return_value = fn(*args, **kwargs)\n\n        # Post function evaluation we decrease the recursion\n        # level by 1\n        recursion_level -= 1\n\n        # Pretty print the return value\n        print(f\"{whitespace} <- {repr(return_value)}\")\n\n        # Return the return value of the wrapped function\n        return return_value\n\n    return wrapper\n```\n\nWe use `recursion_level` to keep track of the current recursion level using which we decide the indentation. The value of this variable is increased every time we are about the invoke the function while it is reduced post the execution. In order to pretty-print the invoked function, we have a helper method called `pretty_func` whose implementation can be found [here](https://github.com/arpitbbhayani/recviz/blob/master/src/recviz/rec.py).\n\nWhen we decorate our previously defined `fib` function and invoke it with `n = 3` we get the following output.\n\n```python\n -> fib(3)\n    -> fib(2)\n       -> fib(1)\n       <- 1\n       -> fib(0)\n       <- 1\n    <- 2\n    -> fib(1)\n    <- 1\n <- 3\n```\n\nThe above output renders how recurrence is evaluated and is pretty printed to make it more human-readable. The right arrow `->` defines a function invocation while the left arrow `<-` indicates the return value post invocation.\n\n## Publishing it on PyPI\n\nEverything mentioned above is published in a Python Package and hosted on [PyPI](https://pypi.org/) at [pypi/recviz](https://pypi.org/project/recviz/). So in order to use this, simply install the package `recviz` like a usual Python package using `pip` and decorate the recursive function.\n\n```python\nfrom recviz import recviz\n\n@recviz\ndef fib(n):\n  # base condition mimicking the first two numbers\n  # in the sequence\n  if n == 0: return 0\n  if n == 1: return 1\n\n  # every number is summation of the previous two\n  return fib(n - 1) + fib(n - 2)\n\nfib(3)\n```\n\n# References\n\n- [The nonlocal statement](https://docs.python.org/3/reference/simple_stmts.html#the-nonlocal-statement)\n- [Primer on Python Decorators](https://realpython.com/primer-on-python-decorators/)\n- [Python Debugging with Decorators](https://paulbutler.org/2008/python-debugging-with-decorators/)\n- [Recursion Trees and the Master Method](https://www.cs.cornell.edu/courses/cs3110/2012sp/lectures/lec20-master/lec20.html)\n- [Easy tracing of nested function calls in Python](https://eli.thegreenplace.net/2012/08/22/easy-tracing-of-nested-function-calls-in-python)\n",
    "similar": [
      "fsm",
      "idf",
      "constant-folding-python",
      "python-iterable-integers"
    ]
  },
  {
    "id": 32,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "flajolet-martin",
    "title": "Approximate Count-Distinct using Flajolet Martin Algorithm",
    "description": "Measuring distinct elements from a stream of values is one of the most common utilities that finds its application across the spectrum. Most of these use cases do not expect accurate count-distinct rather they expect it to be computed very quickly and efficiently. In this essay, we deep dive into one of the first Count-Distinct approximation algorithm called Flajlet-Martin Algorithm.",
    "gif": "https://media.giphy.com/media/cRMgB2wjHhVN2tDD2z/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/101244544-31977e80-372d-11eb-9ab4-d9e879496d39.png",
    "released_at": "2020-12-06",
    "total_views": 3131,
    "body": "Measuring the number of distinct elements from a stream of values is one of the most common utilities that finds its application in the field of Database Query Optimizations, Network Topology, Internet Routing, Big Data Analytics, and Data Mining.\n\nA deterministic count-distinct algorithm either demands a large auxiliary space or takes some extra time for its computation. But what if, instead of finding the cardinality deterministically and accurately we just approximate, can we do better? This was addressed in one of the first algorithms in approximating count-distinct introduced in the seminal paper titled [Probabilistic Counting Algorithms for Data Base Applications](http://algo.inria.fr/flajolet/Publications/FlMa85.pdf) by Philippe Flajolet and G. Nigel Martin in 1984.\n\nIn this essay, we dive deep into this algorithm and find how wittily it approximates the count-distinct by making a single pass on the stream of elements and using a fraction of auxiliary space.\n\n# Deterministic count-distinct\n\nThe problem statement of determining count-distinct is very simple -\n\n*Given a stream of elements, output the total number of distinct elements as efficiently as possible.*\n\n![https://user-images.githubusercontent.com/4745789/101273043-250c3800-37b8-11eb-9e0c-435e386f3529.png](https://user-images.githubusercontent.com/4745789/101273043-250c3800-37b8-11eb-9e0c-435e386f3529.png)\n\nIn the illustration above the stream has the following elements `4`, `1`, `7`, `4`, `2`, `7`, `6`, `5`, `3`, `2`, `4`, `7` and `1`. The stream has in all `7` unique elements and hence it is the count-distinct of this stream.\n\nDeterministically computing count-distinct is an easy affair, we need a data structure to hold all the unique elements as we iterate the stream. Data structures like [Set](https://en.wikipedia.org/wiki/Set_(abstract_data_type)) and [Hash Table](https://en.wikipedia.org/wiki/Hash_table) suit this use-case particularly well. A simple pythonic implementation of this approach is as programmed below\n\n```python\ndef cardinality(elements: int) -> int:\n    return len(set(elements))\n```\n\nAbove deterministic approach demands an auxiliary space of `O(n)` so as to accurately measure the cardinality. But when we are allowed to approximate the count we can do it with a fraction of auxiliary space using the Flajolet-Martin Algorithm.\n\n# The Flajolet-Martin Algorithm\n\nThe Flajolet-Martin algorithm uses the position of the rightmost set and unset bit to approximate the count-distinct in a given stream. The two seemingly unrelated concepts are intertwined using probability. It uses extra storage of order `O(log m)` where `m` is the number of unique elements in the stream and provides a practical estimate of the cardinalities.\n\n## The intuition\n\nGiven a good uniform distribution of numbers, the probability that the rightmost set bit is at position `0` is `1/2`, probability of rightmost set bit is at position `1` is `1/2 * 1/2 = 1/4`, at position `2` it is `1/8` and so on.\n\n![https://user-images.githubusercontent.com/4745789/101275842-e635ac80-37ce-11eb-9e00-357b966cbac6.png](https://user-images.githubusercontent.com/4745789/101275842-e635ac80-37ce-11eb-9e00-357b966cbac6.png)\n\nIn general, we can say, the probability of the rightmost set bit, in binary presentation, to be at the position `k` in a uniform distribution of numbers is\n\n![https://user-images.githubusercontent.com/4745789/101275886-357bdd00-37cf-11eb-9bc6-346332031eb2.png](https://user-images.githubusercontent.com/4745789/101275886-357bdd00-37cf-11eb-9bc6-346332031eb2.png)\n\nThe probability of the rightmost set bit drops by a factor of `1/2` with every position from the Least Significant Bit to the Most Significant Bit.\n\n![https://user-images.githubusercontent.com/4745789/101276356-1cc0f680-37d2-11eb-858d-3f40061988f0.png](https://user-images.githubusercontent.com/4745789/101276356-1cc0f680-37d2-11eb-858d-3f40061988f0.png)\n\nSo if we keep on recording the position of the rightmost set bit, `\u03c1`, for every element in the stream (assuming uniform distribution) we should expect `\u03c1 = 0` to be `0.5`, `\u03c1 = 1` to be `0.25`, and so on. This probability should become `0` when bit position, `b` is `b > log m` while it should be non-zero when `b <= log m` where `m` is the number of distinct elements in the stream.\n\nHence, if we find the rightmost unset bit position `b` such that the probability is `0`, we can say that the number of unique elements will approximately be `2 ^ b`. This forms the core intuition behind the Flajolet Martin algorithm.\n\n## Ensuring uniform distribution\n\nThe above intuition and approximation are based on the assumption that the distribution of the elements in the stream is uniform, which cannot always be true. The elements can be sparse and dense in patches. To ensure uniformity we hash the elements using a multiplicative hash function\n\n![https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png](https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png)\n\nwhere `a` and `b` are odd numbers and `c` is the capping limit of the hash range. This hash function hashes the elements uniformly into a hash range of size `c`.\n\n## The procedure\n\nThe procedure of the Flajolet-Martin algorithm is as elegant as its intuition. We start with defining a closed hash range, big enough to hold the maximum number of unique values possible - something as big as `2 ^ 64`. Every element of the stream is passed through a hash function that permutes the elements in a uniform distribution.\n\nFor this hash value, we find the position of the rightmost set bit and mark the corresponding position in the bit vector as `1`, suggesting that we have seen the position. Once all the elements are processed, the bit vector will have `1`s at all the positions corresponding to the position of every rightmost set bit for all elements in the stream.\n\nNow we find the position, `b`, of the rightmost `0` in this bit vector. This position `b` corresponds to the rightmost set bit that we have not seen while processing the elements. This corresponds to the probability `0` and hence as per the intuition will help in approximating the cardinality as `2 ^ b`.\n\n```python\n# Size of the bit vector\nL = 64\n\ndef hash_fn(x: int):\n    return (3 * x + 5) % (2 ** L)\n\ndef cardinality_fm(stream) -> int:\n    # we initialize the bit vector\n    vector = 0\n\n    # for every element in the stream\n    for x in skream:\n        \n        # compute the hash value bounded by (2 ** L)\n        # this hash value will ensure uniform distribution\n        # of elements of the stream in range [0, 2 ** L)\n        y = hash_fn(x)\n\n        # find the rightmost set bit\n        k = get_rightmost_set_bit(y)\n\n        # set the corresponding bit in the bit vector\n        vector = set_bit(vector, k)\n\n    # find the rightmost unset bit in the bit vector that\n    # suggests that the probability being 0\n    b = rightmost_unset_bit(vector)\n\n    # return the approximate cardinality\n    return 2 ** b\n```\n\nAlthough the above algorithm does a decent job of approximating count-distinct it has a huge error margin, which can be fixed by averaging the approximations with multiple hash functions. The original Flajolet-Martin algorithm also suggests that the final approximation needs a correction by dividing the approximation by the factor `\u03d5 = 0.77351`.\n\nThe algorithm was [run](https://github.com/arpitbbhayani/flajolet-martin/blob/master/flajolet-martin.ipynb) on a stream size of `1048` with a varying number of distinct elements and we get the following plot.\n\n![https://user-images.githubusercontent.com/4745789/101244923-58ef4b00-372f-11eb-9193-8e9d6dc6a227.png](https://user-images.githubusercontent.com/4745789/101244923-58ef4b00-372f-11eb-9193-8e9d6dc6a227.png)\n\nFrom the illustration above we see that the approximated count-distinct using the Flajolet-Martin algorithm is very close to the actual deterministic value.\n\nA great feature of this algorithm is that the result of this approximation will be the same whether the elements appear a million times or just a few times, as we only consider the rightmost set bit across all elements and do not sample.\n\n### Unique words in Thee Jungle Book\n\nThe algorithm was run on the text dump of [The Jungle Book](https://en.wikipedia.org/wiki/The_Jungle_Book) by Rudyard Kipling. The text was converted into a stream of tokens and it was found that the total number of unique tokens was `7150`. The approximation of the same using the Flajolet-Martin algorithm came out to be `7606` which in fact is pretty close to the actual number.\n\n# References\n\n- [Probabilistic Counting Algorithms for Data Base Applications](http://algo.inria.fr/flajolet/Publications/FlMa85.pdf)\n- [Flajolet-Martin algorithm by Ravi Bhide](http://ravi-bhide.blogspot.com/2011/04/flajolet-martin-algorithm.html)\n- [The Jungle Book by Rudyard Kipling - Project Gutenberg](https://www.gutenberg.org/files/236/236-h/236-h.htm)\n",
    "similar": [
      "slowsort",
      "1d-terrain",
      "isolation-forest",
      "morris-counter"
    ]
  },
  {
    "id": 33,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "2q-cache",
    "title": "2Q Cache Management Algorithm",
    "description": "LRU is one of the most widely used cache eviction algorithms suffers from a bunch of limitations especially when used for managing caches in disk-backed databases. 2Q remediates the limitations and improves upon it by adding multiple parallel buffers.",
    "gif": "https://media.giphy.com/media/cfuL5gqFDreXxkWQ4o/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/100538603-bf1f2e00-3256-11eb-87dc-32594b25ae30.png",
    "released_at": "2020-11-29",
    "total_views": 1705,
    "body": "LRU is one of the most widely used cache eviction algorithms that span its utility across multiple database systems. Although popular, it suffers from a bunch of limitations especially when it is used for managing caches in disk-backed databases like MySQL and Postgres.\n\nIn this essay, we take a detailed look into the sub-optimality of LRU and how one of its variants called 2Q addresses and improves upon it. 2Q algorithm was first introduced in the paper - [2Q: A low overhead high-performance buffer management replacement algorithm](https://www.semanticscholar.org/paper/2Q%3A-A-Low-Overhead-High-Performance-Buffer-Johnson-Shasha/5fa357b43c8351a5d8e7124429e538ad7d687abc) by Theodore Johnson and Dennis Shasha.\n\n# LRU\n\nThe [LRU eviction algorithm](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)) evicts the page from the buffer which has not been accessed for the longest. LRU is typically implemented using a [Doubly Linked List](https://en.wikipedia.org/wiki/Doubly_linked_list) and a [Hash Table](https://en.wikipedia.org/wiki/Hash_table). The intuition of this algorithm is so strong and implementation is so simple that until the early '80s, LRU was the algorithm of choice in nearly all the systems. But as stated above, there are certain situations where LRU performs sub-optimal.\n\n![https://user-images.githubusercontent.com/4745789/100534745-43ae8400-3238-11eb-8855-752a6ef2f3c6.png](https://user-images.githubusercontent.com/4745789/100534745-43ae8400-3238-11eb-8855-752a6ef2f3c6.png)\n\n## Sub-optimality during DB scans\n\nIf the database table is bigger than the LRU cache, the DB process, upon scanning the table will wipe out the entire LRU cache and fill it with the pages from just one scanned table. If these pages are not referenced again, this is a total loss and the performance of the database takes a massive hit. The performance will pickup once these pages are evicted from the cache and other pages make an entry.\n\n## Sub-optimality in evictions\n\nLRU algorithm works with a single dimension - recency - as it removes the pages from the buffer on the basis of recent accesses. Since it does not really consider any other factor, it can actually evict a warmer page and replace it with a colder one - a page that could and would be accessed just once.\n\n# 2Q Algorithm\n\n2Q addresses the above-illustrated issues by introducing parallel buffers and supporting queues. Instead of considering just recency as a factor, 2Q also considers access frequency while making the decision to ensure the page that is really warm gets a place in the LRU cache. It admits only hot pages to the main buffer and tests every page for a second reference.\n\nThe golden rule that 2Q is based on is - *Just because a page is accessed once does not entitle it to stay in the buffer. Instead, it should be decided if it is accessed again then only keep it in the buffer.*\n\nBelow we take a detailed look into two versions of the 2Q algorithm - simplified and improved.\n\n## Simplified 2Q\n\nSimplified 2Q algorithm works with two buffers: the primary LRU buffer - `Am`  and a secondary FIFO buffer - `A1`. New faulted pages first go to the secondary buffer `A1` and then when the page is referenced again, it moves to the primary LRU buffer `Am`. This ensures that the page that moves to the primary LRU buffer is hot and indeed requires to be cached.\n\n![https://user-images.githubusercontent.com/4745789/100536835-41a0f100-3249-11eb-920b-0bcaff905906.png](https://user-images.githubusercontent.com/4745789/100536835-41a0f100-3249-11eb-920b-0bcaff905906.png)\n\nIf the page residing in `A1` is never referenced again, it eventually gets discarded, implying the page was indeed cold and did not deserve to be cached. Thus this simplified 2Q provides protection against the two listed sub-optimality of the simple LRU scheme by adding a secondary buffer and testing pages for a second reference. The pseudocode for the Simplified 2Q algorithm is as follows:\n\n```python\ndef access_page(X: page):\n    # if the page already exists in the LRU cache\n    # in buffer Am\n    if X in Am:\n         Am.move_front(X)\n\n    # if the page exists in secondary storage\n    # and not it gets access.\n    # since the page is accessed again, indicating interest\n    # and long-term need, move it to Am.\n    elif X in A1:\n         A1.remove(X)\n         Am.add_front(X)\n\n    # page X is accessed for the first time\n    else:\n         # if A1 is full then free a slot.\n         if A1.is_full():\n             A1.pop()\n\n         # add X to the front of the FIFO A1 queue\n         A1.add_front(X)\n```\n\nTuning Simplified 2Q buffer is difficult - if the maximum size of `A1` is too small, the test for hotness becomes too strong and if it is too large then due to memory constraint `Am` will get relatively smaller memory making the primary LRU cache smaller, eventually degrading the database performance.\n\nThe full version 2Q algorithm remediates this limitation and eliminates tuning to a massive extent without taking any hit in performance.\n\n## 2Q Full Version\n\nAlthough Simplified 2Q algorithm does a decent job there is still scope of improvement when it comes to handling common database access pattern, that suggests, a page generally receives a lot of references for a short period of time and then no reference for a long time. If a page truly needs to be cached then after it receives a lot (not just one) of references in a short span it continues to receive references and hits on regular intervals.\n\nTo handle this common database access pattern, the 2Q algorithm splits the secondary buffer `A1` into two buffers `A1-In` and `A1-Out`, where the new element always enters `A1-In` and continues to stay in `A1-In` till it gets accesses ensuring that the most recent first accesses happen in the memory.\n\nOnce the page gets old, it gets thrown off the memory but its disk reference is stored in the `A1-Out` buffer. If the page, whose reference is, residing in `A1-Out` is accessed again the page is promoted to `Am` LRU implying it indeed is a hot page that will be accessed again and hence required to be cached.\n\n![https://user-images.githubusercontent.com/4745789/100538168-0bb53a00-3254-11eb-8f69-ddcaf8d33a84.png](https://user-images.githubusercontent.com/4745789/100538168-0bb53a00-3254-11eb-8f69-ddcaf8d33a84.png)\n\nThe `Am` buffer continues to be the usual LRU which means when any page residing in `Am` is accessed it is moved to the head and when a page is needed to be discarded the eviction happens from the tail end.\n\n# 2Q in Postgres\n\nPostgres uses 2Q as its cache management algorithm due to [patent issues](http://www.varlena.com/GeneralBits/96.php) with IBM. Postgres used to have [ARC](https://en.wikipedia.org/wiki/Adaptive_replacement_cache) as its caching algorithm but with IBM getting a patent over it, Postgres moved to 2Q. Postgres also claims that the performance of 2Q is similar to ARC.\n\n# References\n\n- [LRU - Wikipedia](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU))\n- [The Saga of the ARC Algorithm and Patent](http://www.varlena.com/GeneralBits/96.php)\n- [2Q: A low overhead high-performance buffer management replacement algorithm](https://www.semanticscholar.org/paper/2Q%3A-A-Low-Overhead-High-Performance-Buffer-Johnson-Shasha/5fa357b43c8351a5d8e7124429e538ad7d687abc)\n",
    "similar": [
      "mysql-cache",
      "israeli-queues",
      "ts-smoothing",
      "phi-accrual"
    ]
  },
  {
    "id": 34,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "israeli-queues",
    "title": "Israeli Queues",
    "description": "Israeli Queues are fondly named after a peculiar behavior observed in Israel. This behavior was mimicked to solve a very particular problem in Polling Systems. In this essay, we find what makes the Israeli Queue different than the traditional FIFO Queue and how it efficiently addresses the problem at hand.",
    "gif": "https://media.giphy.com/media/3orif9DJNKDfXqXbBS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/99896424-64b22a80-2cb6-11eb-92b2-ed1f44a76ba1.png",
    "released_at": "2020-11-22",
    "total_views": 7250,
    "body": "A queue is a data structure that holds up elements for a brief period of time until a peripheral processing system is ready to process them. The most common implementation of a queue is a FIFO queue - First In First Out - that evicts the element that was inserted the first i.e. it evicts the one that has spent the most time in the queue. There are other variations of Queues one of which is called Priority Queue.\n\nIn Priority Queue, every element is associated with a priority, usually provided by the user during enqueueing; This associated priority is used during eviction where the element with the highest priority is evicted first during dequeuing.\n\nIn this essay, we take a detailed look into a variation of Priority Queue, fondly called Israeli Queues, where the priority of the element is defined by the affinity of it with one of its \"friends\" in the queue. Israeli Queues were first introduced in the paper [Polling with batch service](https://pure.tue.nl/ws/files/2152975/632939.pdf) by Boxma, O. J., Wal, van der, J., & Yechiali, U in the year 2007.\n\n# Israeli Queues\n\nQueues in Israel are usually unorganized, due to which people tend to find their friends, who are already waiting, and instead of adhering to the usual protocol of joining at the back end, they cut through and directly join their friends. Israeli Queues mimic this behavior and hence get this [punny name](https://www.tandfonline.com/doi/abs/10.1080/15326340802427497).\n\n![https://user-images.githubusercontent.com/4745789/99894937-fddc4380-2cac-11eb-8a73-a4dc5c490d2b.png](https://user-images.githubusercontent.com/4745789/99894937-fddc4380-2cac-11eb-8a73-a4dc5c490d2b.png)\n\nIsraeli Queues are a variation of [Priority Queues](https://en.wikipedia.org/wiki/Priority_queue) where instead of associating priority with the element to be enqueued, the priority is implicitly derived using the \"friend\" element and it joins right at the back end of the group that the friend belongs to. The function signature of the enqueue operation is as shown below, while other operations like `dequeue` and `peek` remains fairly similar.\n\n```c\n// Enqueues the element `e`, a friend of element `f`,\n// into the queue `q`.\nvoid enqueue(israeli_queue * q, element * e, element * f);\n```\n\n## How could this help?\n\nEvery Data Structures is designed to solve a niche use case efficiently and Israeli Queues are no different as they prove to be super-efficient where one could batch and process similar elements or where the *set-up* cost for a task is high.\n\nConsider a system where a queue is used to hold up heterogeneous tasks and there is a single machine taking care of processing. Now if some of these tasks are similar and have a high *set-up or preparation cost*, for example downloading large metafiles, or spinning up a parallel infrastructure, or even setting up persistent connections with device farms, queuing them closer and processing them sequentially or in batch helps in reducing redundant processing and computation by promoting reuse.\n\n## Issue of starvation\n\nBy enqueuing elements in between Israeli Queues reduces redundant processing, but by doing that it makes itself vulnerable to the classical case of starvation. Elements stuck at the rear end of the list could potentially starve for longer durations if elements having \"friends\" in the queue keep coming in at high frequency.\n\nThe original implementation of Israeli Queues suggests batch processing where instead of processing tasks one at a time, it processes a batch (a group of friends) in one go. This proves to be super-handy when the time required to processes a single task is much lower than the set-up cost for it.\n\n## Implementation Guidelines\n\nThe best way to implement Israeli Queues is by using a [Doubly Linked List](https://en.wikipedia.org/wiki/Doubly_linked_list) with a bunch of pointers pointing to the head and tail of groups within it. Insertion to an existing group happens at the tail of it while if the element has no friend element, then it goes at the tail end of the list and forms its own group.\n\nA constraint that could be added during implementation is that the friend element should always be the leader (head) element of the group. Details of the implementation could be tweaked so long the core concept remains unaltered.\n\n# The original use case of Israeli Queues\n\nIsraeli Queues were the outcome of a problem statement dealing with Polling Systems. Polling System usually contains `N` queues `Q1`, `Q2`, ..., `Qn` where the processing unit visits each queue in cyclic order processing one element at a time i.e. `Q1`, `Q2`, ..., `Qn`, `Q1`, `Q2`, ..., `Qn`, etc.\n\nWhen the server attends a queue instead of processing just one element from it, it processes the entire batch present in the queue utilizing the setup-cost efficiently assuming that time to process an element from a queue is much lesser than the set-up cost.\n\n# References\n\n- [Polling with batch service](https://pure.tue.nl/ws/files/2152975/632939.pdf)\n- [The Israeli Queue with priorities](http://www.math.tau.ac.il/~uriy/Papers/IQ-with-Priorities.pdf)\n- [Israeli Queues: Exploring a bizarre data structure](https://rapidapi.com/blog/israeli-queues-exploring-a-bizarre-data-structure/)\n",
    "similar": [
      "2q-cache",
      "phi-accrual",
      "morris-counter",
      "copy-on-write"
    ]
  },
  {
    "id": 35,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "1d-terrain",
    "title": "1D Procedural Terrain Generation",
    "description": "Terrains are at the heart of every Computer Game - be it Counter-Strike, Age of Empires, or even Minecraft. The virtual world that these games generate is the key to a great gaming experience. Generating terrain, manually, requires a ton of effort and hence it makes sense to auto-generate a pseudorandom terrain using some procedure. In this essay, we take a detailed look into generating pseudorandom one-dimensional terrain that is very close to real ones.",
    "gif": "https://media.giphy.com/media/xT5LMTf2CRdBhetWPC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/99261379-a9ffd380-2842-11eb-8f50-36b36ed42a95.png",
    "released_at": "2020-11-16",
    "total_views": 1062,
    "body": "Terrains are at the heart of every Computer Game - be it Counter-Strike, Age of Empires, or even Minecraft. The virtual world that these games generate is the key to a great gaming experience. Generating terrain, manually, requires a ton of effort and hence it makes sense to auto-generate a pseudorandom terrain using some procedure. In this essay, we take a detailed look into generating pseudorandom one-dimensional terrain that is very close to real ones.\n\n# 1D Terrain\n\nA one-dimensional terrain is a bunch of heights defined at every point along the X-axis. An example of this could be seen in games like Flappy Bird, Galaxy, and many more. Such terrain could also be traced as the skylines of mountain ranges. \n\n![https://user-images.githubusercontent.com/4745789/99189082-70b55e00-2785-11eb-87c0-6d95568f709a.png](https://user-images.githubusercontent.com/4745789/99189082-70b55e00-2785-11eb-87c0-6d95568f709a.png)\n\nThe illustration above shows a one-dimensional terrain and is taken as a sketch of a distant mountain range. The procedure we define should generate terrains as close to natural ones as possible.\n\n# Generating terrain using `random` values\n\nA naive solution to generate a random terrain is by using the `random` function for each point along the X-axis. The `random` function yields random values in the interval `[0, 1]`, and we map these values to the required height, for example `[0, 100]`.  The 1D terrain generation algorithm using the default random function is thus defined as\n\n```python\nimport random\n\ndef mapv(v, ol, oh, nl, nh):\n    \"\"\"maps the value `v` from old range [ol, oh] to new range [nl, nh]\n    \"\"\"\n    return nl + (v * ((nh - nl) / (oh - ol)))\n\ndef terrain_naive(count) -> List[float]:\n    \"\"\"returns the list of integers representing height at each point.\n    \"\"\"\n    return [\n        mapv(random.random(), 0, 1, 0, 100)\n        for i in range(count)\n    ]\n```\n\nThe `mapv` function defined above comes in very handy as it helps us to map value from a source range to a target range. The terrain generation function `terrain_naive` takes in `count` as input, suggesting the number of points along the X-axis, and it returns a list of float values representing height at each of the `count` number of points along the terrain.\n\n![https://user-images.githubusercontent.com/4745789/99189731-9db74000-2788-11eb-98d3-0138a7378d77.png](https://user-images.githubusercontent.com/4745789/99189731-9db74000-2788-11eb-98d3-0138a7378d77.png)\n\nThe above illustration shows the plot of the one-dimensional terrain using the above `terrain_naive` function. The terrain generated using this procedure has a lot of spikes and abrupt changes in height and it clearly does not mimic the terrains in the real world. Real-world terrains, although random, does not have a lot of sharp spikes, instead, the changes in height are very gradual and ensure some degree of smoothness. \n\nIn order to bring smoothness to `terrain_naive` generated terrains, we take a look at a famous estimation technique called [interpolation](https://en.wikipedia.org/wiki/Interpolation) which estimates intermediate values given a bunch of known points. \n\n# Interpolation\n\nInterpolation is a method of constructing new data points within a range of a discrete set of known data points. Interpolation methods estimate the intermediate data points ensuring a \"smoothened\" transition from one known point to another. There are several interpolation methods but we restrict our focus to [Linear](https://en.wikipedia.org/wiki/Linear_interpolation) and [Cosine](https://en.wikipedia.org/wiki/Trigonometric_interpolation) interpolation methods.\n\n### Linear Interpolation\n\nLinear interpolation estimates the intermediate points between the known points assuming collinearity. Thus given two known points `a` and `b`, using linear interpolation, we estimate an intermediate point `c` at a relative distance of `mu` from `a` using the function defined below\n\n```python\ndef linp(a, b, mu):\n    \"\"\"returns the intermediate point between `a` and `b`\n    which is `mu` factor away from `a`.\n    \"\"\"\n    return a * (1 - mu) + b * mu\n```\n\nThe value of the parameter `mu` ranges in the interval `[0, 1]` where `0` implies the point being estimated and interpolated is at `a` while `1` implies it is at the second point `b`.\n\n![https://user-images.githubusercontent.com/4745789/99184683-d0ead680-276a-11eb-9b4d-6c78dbf3c197.png](https://user-images.githubusercontent.com/4745789/99184683-d0ead680-276a-11eb-9b4d-6c78dbf3c197.png)\n\n### Cosine Interpolation\n\nLinear interpolation is not always desirable as the plot sees a lot of discontinuous and sharp transitions. When the plot is expected to be smoother, it is where the Cosine Interpolation comes in handy. Instead of assuming intermediate points are collinear with the known ones, Cosine Interpolation, plots them on a cosine curve passing through the known points, providing a much smoother transition. This is could be seen in the illustration above.\n\n```python\nimport math\n\ndef cosp(a, b, mu):\n    \"\"\"returns the intermediate point between `a` and `b`\n    which is `mu` factor away from `a`.\n    \"\"\"\n    mu2 = (1 - math.cos(mu * math.pi)) / 2\n    return a * (1 - mu2) + b * mu2\n```\n\nThe above code snippet computes and estimates intermediate point `c` at a relative distance of `mu` from the first point `a` using Cosine Interpolation. Note there are other interpolation methods, but we can solve all major of our use cases using these two.\n\n# Smoothing via Interpolation\n\nWe can apply interpolation to our naively generated terrain and make transitions smoother leading to fewer spikes. In the real-world, the transitions in the terrain are gradual with peaks being widespread; we can mimic the pattern by sampling `k` points from the naive terrain which could become our desired peaks, and interpolate the rest of the points lying between them.\n\nThis should ideally help us reduce the sudden spikes and make the terrain look much closer to really like. A simple python code that outputs linearly interpolated terrain that samples every `sample` points from the naive one is as follows\n\n```python\ndef terrain_linp(naive_terrain, sample=4) -> List[float]:\n    \"\"\"Using naive terrain `naive_terrain` the function generates\n    Linearly Interpolated terrain on sample data.\n    \"\"\"\n    terrain = []\n\n    # get every `sample point from the naive terrain.\n    sample_points = naive_terrain[::sample]\n\n    # for every point in sample point denoting \n    for i in range(len(sample_points)):\n\n        # add current peak (sample point) to terrain.\n        terrain.append(sample_points[i])\n\n        # fill in `sample - 1` number of intermediary points using\n        # linear interpolation.\n        for j in range(sample - 1):\n            # compute relative distance from the left point\n            mu = (j + 1)/sample\n          \n            # compute interpolated point at relative distance of mu\n            a = sample_points[i]\n            b = sample_points[(i + 1) % len(sample_points)]\n            v = linp(a, b, mu)\n\n            # add an interpolated point to the terrain terrain.append(v)\n\n    # return the terrain\n    return terrain\n```\n\nThe above code snippet generates intermediate points using Linear Interpolation but we can very easily change the interpolation function to Cosine Interpolation and see the effect in action. The sampling and interpolating on a naively generated terrain for different values of sample is as shown below\n\n![https://user-images.githubusercontent.com/4745789/99227920-e7983880-2811-11eb-9c79-bed5cad2eed3.png](https://user-images.githubusercontent.com/4745789/99227920-e7983880-2811-11eb-9c79-bed5cad2eed3.png)\n\nFor each interpolation, the 5 plots shown above are sampled for every `1`, `2`, `3`, `4`, and `5` points respectively. We can clearly see the plots of Cosine Interpolation are much smoother than Linear Interpolated ones. The technique does a good job over naive implementation but it still does not mimic what we see in the real world. To make things as close to the real-world as possible we use the concept of [Superposition](https://en.wikipedia.org/wiki/Superposition_principle).\n\n# Superposition Sampled Terrains\n\nSampling and Interpolation are effective in reducing the spikes and making transitions gradual. The concern with this approach is that sudden changes are not really gone. In order to address this situation, we use the principle of Superposition upon multiple such sampled terrains.\n\nThe approach we take here is to generate `k` such terrains with different sampling frequencies and then perform a normalized weighted sum. This way we get the best of both worlds i.e smoothness from the terrain with the least sampled points and aberrations from the one with the most sampled points.\n\nNow the only piece that remains is choosing the weights. The weights and sampling frequency varies by the power of `2`. The number of terrains to be sampled depends on the kind of terrain needed and is to be left for experimentation, but we can assume it to be 6 for most use cases.\n\nThe terrain that samples all the points should be given the least weight as it aims to contribute sudden spikes; and as we increase the sampling frequency by the power of `2` we increase the weight by the power of `2` as well. Hence for the first terrain, the scale is `0.03125` where were sample all the points `256`, the next terrain is sampled with `128` points and has a scale of `0.0625`, and so on till we reach sampled points to be `8` with scale as `1`, giving it the highest weight.\n\nOnce these terrains are generated we perform a normalized weighted sum and generate the final terrain as shown in the illustration below.\n\n![https://user-images.githubusercontent.com/4745789/99264852-0107a780-2847-11eb-9bd1-2e2994c56028.png](https://user-images.githubusercontent.com/4745789/99264852-0107a780-2847-11eb-9bd1-2e2994c56028.png)\n\nThe illustration above shows 6 scaled sampled terrains with different sampling frequencies along with the final super-positioned terrain generated from the procedure. It is very clear that the terrain generated using this procedure is much more closer to the real world terrain than any other we have seen before. Python code that generated the above terrain is as below\n\n```python\ndef terrain_superpos_linp(naive_terrain, iterations=8) -> List[float]:\n    \"\"\"Using naive terrain `naive_terrain` the function generates\n    Linearly Interpolated Superpositioned terrain that looks real world like.\n    \"\"\"\n    terrains = []\n\n    # holds the sum of weights for normalization\n    weight_sum = 0\n\n    # for every iteration\n    for z in range(iterations, 0, -1):\n        terrain = []\n\n        # compute the scaling factor (weight)\n        weight = 1 / (2 ** (z - 1))\n\n        # compute sampling frequency suggesting every `sample`th\n        # point to be picked from the naive terrain.\n        sample = 1 << (iterations - z)\n\n        # get the sample points\n        sample_points = naive_terrain[::sample]\n        \n        weight_sum += weight\n\n        for i in range(len(sample_points)):\n\n            # append the current sample point (scaled) to the terrain\n            terrain.append(weight * sample_points[i])\n\n            # perform interpolation and add all interpolated values to\n            # to the terrain.\n            for j in range(sample - 1):\n                # compute relative distance from the left point\n                mu = (j + 1) / sample\n\n                # compute interpolated point at relative distance of mu\n                a = sample_points[i]\n                b = sample_points[(i + 1) % len(sample_points)]\n                v = linp(a, b, mu)\n\n                # add interpolated point (scaled) to the terrain\n                terrain.append(weight * v)\n\n        # append this terrain to list of terrains preparing\n        # it to be superpositioned.\n        terrains.append(terrain)\n\n    # perform super position and normalization of terrains to\n    # get the final terrain\n    return [sum(x)/weight_sum for x in zip(*terrains)]\n```\n\nIf the terrain to be generated is needed to be smoother then instead of using Linear Interpolation switch to Cosine Interpolation and the resultant terrain will be much smoother and curvier as seen in the illustration below.\n\n![https://user-images.githubusercontent.com/4745789/99264869-06fd8880-2847-11eb-83d7-80d0ab5509da.png](https://user-images.githubusercontent.com/4745789/99264869-06fd8880-2847-11eb-83d7-80d0ab5509da.png)\n\n> This approach is very similar to [Perlin Noise](https://en.wikipedia.org/wiki/Perlin_noise) that is used for generating multi-dimensional terrains. [Ken Perlin](https://en.wikipedia.org/wiki/Ken_Perlin) was awarded an Academy Award for Technical Achievement for creating the algorithm.\n\n# References\n\n- [Superposition](https://en.wikipedia.org/wiki/Superposition_principle)\n- [Interpolation Methods](http://paulbourke.net/miscellaneous/interpolation/)\n- [One Lone Coder - Perlin-like Noise](https://github.com/OneLoneCoder/videos/blob/master/OneLoneCoder_PerlinNoise.cpp)\n- [IPython notebook with the source code](https://github.com/arpitbbhayani/1d-terrain/)",
    "similar": [
      "slowsort",
      "flajolet-martin",
      "jaccard-minhash",
      "idf"
    ]
  },
  {
    "id": 36,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "jaccard-minhash",
    "title": "Set Similarity using Jaccard Similarity Coefficient and MinHash",
    "description": "Set similarity measure finds its application spanning the Computer Science spectrum; some applications being - user segmentation, finding near-duplicate webpages/documents, clustering, recommendation generation, sequence alignment, and many more. In this essay, we take a detailed look into a set-similarity measure called - Jaccard's Similarity Coefficient and how its computation can be optimized using a neat technique called MinHash.",
    "gif": "https://media.giphy.com/media/yvXWADxQxRMkQ4eEID/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/98465225-c4580280-21ed-11eb-9f1f-6508ed229925.png",
    "released_at": "2020-11-08",
    "total_views": 1481,
    "body": "Set similarity measure finds its application spanning the Computer Science spectrum; some applications being - user segmentation, finding near-duplicate webpages/documents, clustering, recommendation generation, sequence alignment, and many more. In this essay, we take a detailed look into a set-similarity measure called - Jaccard's Similarity Coefficient and how its computation can be optimized using a neat technique called MinHash.\n\n# Jaccard Similarity Coefficient\n\nJaccard Similarity Coefficient quantifies how similar two *finite* sets really are and is defined as the size of their intersection divided by the size of their union. This similarity measure is very intuitive and we can clearly see that it is a real-valued measure bounded in the interval `[0, 1]`.\n\n![https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png](https://user-images.githubusercontent.com/4745789/98461673-302d7180-21d4-11eb-9722-41f473c1fe84.png)\n\nThe coefficient is `0` when the two sets are mutually exclusive (disjoint) and it is `1` when the sets are equal. Below we see the one-line python function that computes this similarity measure.\n\n```python\ndef similarity_jaccard(a: set, b: set) -> float:\n    return len(a.intersection(b)) / len(a.union(b))\n```\n\n## Jaccard Similarity Coefficient as Probability\n\nJaccard Coefficient can also be interpreted as the probability that an element picked at random from the universal set `U` is present in both sets `A` and `B`. \n\n![https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png](https://user-images.githubusercontent.com/4745789/98462221-8dc3bd00-21d8-11eb-95bf-5a9267e88b97.png)\n\nAnother analogy for this probability is the chances of throwing a dart and it hitting the intersection. Thus we see how we can transform the Jaccard Similarity Coefficient into a simple probability statement. This will come in very handy when we try to optimize the computation at scale.\n\n## Problem at Scale\n\nComputing Jaccard Similarity Coefficient is very simple, all we require is a union operation and an intersection operation on the participating sets. But these computations go haywire when things run at scale.\n\nComputing set similarity is usually a subproblem fitting in a bigger picture, for example, near-duplicate detection which finds near-duplicate articles across millions of documents. When we tokenize the documents and apply raw Jaccard Similarity Coefficient for every two combinations of documents we find that the computation will take [years](https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/).\n\nInstead of finding the true value for this coefficient, we can rely on an approximation if we can get a considerable speedup and this is where a technique called MinHash fits well.\n\n# MinHash\n\nMinHash algorithm gives us a fast approximation to the Jaccard Similarity Coefficient between any two finite sets. Instead of computing the unions and the intersections every single time, this method once creates *MinHash Signature* for each set and use it to approximate the coefficient.\n\n## Computing single MinHash\n\nMinHash `h` of the set `S` is the index of the first element, from a permuted Universal Set, that is present in the set `S`. But since permutation is a computation heavy operation especially for large sets we use a hashing/mapping function that typically reorders the elements using simple math operation. One such hashing function is\n\n![https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png](https://user-images.githubusercontent.com/4745789/98463097-f7df6080-21de-11eb-8b61-a84ff7ad85de.png)\n\nIf `u` is the total number of elements in the Universal Set `U` then `a` and `b` are the random integers less than `u` and `c` is the prime number slightly higher than `u`.  A sample permute function could be\n\n```python\ndef permute_fn(x: int) -> int:\n    return (23 * x + 67) % 199\n```\n\nNow that we have defined permutation as a simple mathematical operation that spits out the new row index, we can find MinHash of a set as the element that has the minimum new row number. Hence we can define the MinHash function as \n\n```python\ndef minhash(s: set) -> int:\n    return min([permute_fn(e) for e in s])\n```\n\n## A surprising property of MinHash\n\nMinHash has a surprising property, according to which, the probability that the MinHash of random permutation produces the same value for the two sets equals the Jaccard Similarity Coefficient of those sets.\n\n![https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png](https://user-images.githubusercontent.com/4745789/98463732-8229c380-21e3-11eb-9b26-04ec08bc8753.png)\n\nThe above equality holds true because the probability of MinHash of two sets to be the same is the number of elements present in both the sets divided by the total number of elements in both the sets combined; which in fact is the definition of Jaccard Similarity Coefficient.\n\nHence to approximate Similarity Coefficient using MinHash all we have to do is find the Probability of MinHash of two sets to be the same, and this is where the MinHash Signature comes in to play.\n\n## MinHash Signature\n\nMinHash Signature of a set `S` is a collection of `k` MinHash values corresponding to `k` different MinHash functions. The size `k` depends on the error tolerance, keeping it higher leads to more accurate approximations.\n\n```python\ndef minhash_signature(s: set):\n    return [minhash(s) for minhash in minhash_fns]\n```\n\n> MinHash functions usually differ in the permutation parameters i.e. coefficients `a`, `b` and `c`.\n\nNow in order to compute `Pr[h(A) = h(B)]` we have to compare the MinHash Signature of the participating sets `A` and `B` and find how many values in their signatures match; dividing this number by the number of hash functions `k` will give the required probability and in turn an approximation of Jaccard Similarity Coefficient.\n\n```python\ndef similarity_minhash(a: set, b: set) -> float:\n    sign_a = minhash_signature(a)\n    sign_b = minhash_signature(b)\n    return sum([1 for a, b in zip(sign_a, sign_b) if a == b]) / len(sign_a)\n```\n\n> MinHash Signature could well be computed just once per set.\n\nThus to compute set similarity, we need not perform heavy computation like Union and Intersection and that too across millions of sets at scale, rather we can simply compare `k` items of in their signatures and get a fairly good estimate of it.\n\n# How good is the estimate?\n\nIn order to find how close the estimate is we compute the Jaccard Similarity Coefficient and its approximate using MinHash on two disjoint sets having equal cardinality. One of the sets will undergo a transition where one element of it will be replaced with one element of the other set. So with time, the sets will go from disjoint to being equal.\n\n![https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png](https://user-images.githubusercontent.com/4745789/98465023-860e1380-21ec-11eb-8813-7cb6920bc1fd.png)\n\nThe illustration above shows the two plots and we can clearly see that the MinHash technique provides a fairly good estimate of Jaccard Similarity Coefficient with much fewer computations.\n\n# References\n\n- [Jaccard Index](https://en.wikipedia.org/wiki/Jaccard_index)\n- [MinHash Wikipedia](https://en.wikipedia.org/wiki/MinHash)\n- [Using of Jaccard Coefficient for Keywords Similarity](https://www.researchgate.net/profile/Ekkachai_Naenudorn/publication/317248581_Using_of_Jaccard_Coefficient_for_Keywords_Similarity/links/592e560ba6fdcc89e759c6d0/Using-of-Jaccard-Coefficient-for-Keywords-Similarity.pdf)\n- [MinHash Tutorial with Python Code](https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/)\n",
    "similar": [
      "idf",
      "constant-folding-python",
      "publish-python-package-on-pypi",
      "rule-30"
    ]
  },
  {
    "id": 37,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "ts-smoothing",
    "title": "Time Series Smoothing - Making Aberrations Stand Out",
    "description": "Time Series smoothing algorithms removes short-term irregularities from the plot while preserving long-term trends. But as an observer, it is important that such smoothing techniques or irregularities do not mask anomalies that need attention. In this essay, we take a look at a smoothing algorithm that smooths out a time series plot while making aberrations and anomalies standout.",
    "gif": "https://media.giphy.com/media/3orieVe5VYqTdT16qk/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/97785060-cbb55580-1bc8-11eb-9d6d-91ff16cc2ddc.png",
    "released_at": "2020-11-01",
    "total_views": 499,
    "body": "Time series is a collection of numerical data points (often measurements), gathered in discrete time intervals, and indexed in order of the time. Common examples of time series data are CPU utilization metrics, Temperature of some geolocation, New User Signups of a product, etc.\n\nObserving time-series of critical metrics helps in spotting trends, aberrations, and anomalies. Time series forecasting helps in predicting future demand and thus aids in altering and adjusting the supply to match that. Software companies continuously monitor hundreds of time series plots for anomalies that, if unattended, could result in downtime or a loss in revenue.\n\nUnfortunately, time series data have a lot of short-term irregularities, often making it harder for the observer to spot the sudden spikes and true anomalies; and which is where the need for *smoothing* arises. By smoothing the plot we get rid of the irregularities, to some extent, while enabling the observer to clearly see the patterns, trends, and anomalies.\n\nIn this essay, we take a detailed look into how we can optimally smooth the time series data to prioritize the user's attention i.e. making it easier for the observer to spot the aberrations. The approach we discuss was introduced in the paper [ASAP: Automatic Smoothing for Attention Prioritization in Streaming Time Series Visualization](https://arxiv.org/abs/1703.00983) by Kexin Rong, Peter Bailis.\n\n# Time Series and need of Smoothing\n\nTime Series, more often than not, is very irregular in nature. Below is the plot of India's Average Temperature - Monthly since 1870. We can clearly see the plot being very irregular making it harder for us to deduce any information out of it whatsoever. Probably the only fact we can point out is that the Monthly Average temperature in India is always between 15 - 30 degrees celsius, which everyone can agree, is not that informative enough.\n\n![https://user-images.githubusercontent.com/4745789/94363195-3cef7d80-00de-11eb-9280-cf0ab83f2230.png](https://user-images.githubusercontent.com/4745789/94363195-3cef7d80-00de-11eb-9280-cf0ab83f2230.png)\n\nIn order to make sense of such an irregular plot and find a pattern or a trend out of it, we have to get rid of **short-term irregularities** without substantial information loss; and this process is called \"smoothing\". Aggregation doesn't work well here because it will not only hide the anomaly but will also reduce the data density making the resultant plot sparse; hence in order to spot anomalies and see long-term trends smoothing is preferred.\n\nIf we smooth the above raw plot using one of the simplest techniques out there, we get the following plot which, everyone would agree, not only looks cleaner but it also clearly shows us the long-term trend while being rich in information.\n\n![https://user-images.githubusercontent.com/4745789/94363189-32cd7f00-00de-11eb-9012-773b42105020.png](https://user-images.githubusercontent.com/4745789/94363189-32cd7f00-00de-11eb-9012-773b42105020.png)\n\n# Time Series Smoothing using Moving Average\n\nThe technique we used to smooth the temperature plot is known as [Simple Moving Average (SMA)](https://en.wikipedia.org/wiki/Moving_average) and it is the simplest, most effective, and one of the most popular smoothing techniques for time series data. Moving Average, very instinctively, smooths out short-term irregularities and highlights longer-term trends and patterns. Computing it is also very simple - each point in the smoothened plot is just an unweighted mean of the data points lying in the sliding window of length `n`. Because of the Sliding Window, SMA ensures that there is no substantial loss of data resolution in the smoothened plot.\n\n![https://user-images.githubusercontent.com/4745789/94834298-d3e56e00-042d-11eb-8c1d-1b339478a7c9.png](https://user-images.githubusercontent.com/4745789/94834298-d3e56e00-042d-11eb-8c1d-1b339478a7c9.png)\n\nWe apply SMA, with window length `11`, to another time series plot and we clearly find the smoothened plot to be visually cleaner with fewer short-term irregularities.\n\n![https://user-images.githubusercontent.com/4745789/94832462-8f58d300-042b-11eb-8d39-f9a12e441519.png](https://user-images.githubusercontent.com/4745789/94832462-8f58d300-042b-11eb-8d39-f9a12e441519.png)\n\n# Making Aberrations Stand Out\n\nWhen an observer is looking at the plot, the primary motive is to spot any aberrations and anomalies. If the plot has irregularities (i.e. it is not smooth enough), spotting anomalies or aberrations becomes tough, and hence smoothing plays a vital role here.\n\nSimple Moving Average is a very effective smoothing technique but choosing the optimal window size is a challenge. Picking a smaller window size will not help in getting rid of irregularities while picking the window size that is too large will mask all the anomalies.\n\n![https://user-images.githubusercontent.com/4745789/94897527-76910180-04ad-11eb-92ab-d38574428dbe.png](https://user-images.githubusercontent.com/4745789/94897527-76910180-04ad-11eb-92ab-d38574428dbe.png)\n\nFrom the over-smoothened plot illustrated above it is clear that having a large window size leads to a heavy information loss and in most cases hides the anomalies and aberrations. Hence we reduce our problem statement to *find the optimal window size for a given plot such that we make anomalies and aberrations standout*.\n\n## Aberrations and Anomalies\n\nIn any data distribution, the anomalies and aberrations form in the long tail which means they are some extreme values that are far away from the mean. Being part of the long tail makes these anomalies - outliers i.e. data points that do not really fit the distribution.\n\nHence in order to find out optimal window size that gets rid of short-term irregularities but makes anomalies stand out, we have to make the resultant distribution \"tail heavy\" implying the presence of anomalies. This is exactly where [Kurtosis](https://en.wikipedia.org/wiki/Kurtosis) - a famous concept from Statistics comes into the picture.\n\n## Kurtosis\n\n[Kurtosis](https://en.wikipedia.org/wiki/Kurtosis) is the measure of \"tailedness\" of the probability distribution (data distribution) and it helps in describing the shape of the plot. Kurtosis is the fourth standardized moment and is defined as\n\n![https://user-images.githubusercontent.com/4745789/94909588-0a1ffd80-04c1-11eb-9b7d-c89bf9dbfb39.png](https://user-images.githubusercontent.com/4745789/94909588-0a1ffd80-04c1-11eb-9b7d-c89bf9dbfb39.png)\n\nThe high value of kurtosis implies that the distribution is heavy on either tail and this is evident when we compute Kurtosis of various distributions with and without any tail noise - mimicking anomalies.\n\n![https://user-images.githubusercontent.com/4745789/94403183-ac22ab80-018a-11eb-9bca-72f6b2e5f98e.png](https://user-images.githubusercontent.com/4745789/94403183-ac22ab80-018a-11eb-9bca-72f6b2e5f98e.png)\n\nIn the illustration above, a small variation (anomaly) is added to the tail of the individual distribution and is encircled in red; and we can clearly see that even a tiny tailedness (anomaly and aberration) that makes the distribution deviate from the mean has a heavy impact on the Kurtosis, making it go much higher.\n\n## Finding the Optimal Window Size\n\nAs established earlier, anomalies and aberrations are extreme values that largely deviate from the mean and hence occupy a position on either tail of the distribution. Hence in order to find the optimal window size that neither under-smooths nor over-smooths the plot while ensuring that it makes anomalies and aberrations stand out, we need to **find the window size that maximizes the Kurtosis**.\n\n```python\nfrom scipy.stats import kurtosis\n\noptimal_window, max_kurt = 1, kurtosis(raw_plot)\n\nfor window_size in range(2, len(raw_plot), 1):\n    # we get the smoothened plot from the `raw_plot` by applying\n    # Simple Moving Average for a window of length `window_size`\n    smoothened_plot = moving_average_plot(raw_plot, window=window_size)\n\n    # measure the kurtosis of the smoothened_plot\n    kurt = kurtosis(smoothened_plot)\n\n    # if kurtosis of the current smoothened plot is greater than the\n    # max we have seen, then we update the optimal window the max_kurt\n    if kurt > max_kurt:\n        max_kurt, optimal_window = kurt, window_size\n```\n\nThe pseudocode above computes the optimal window size that maximizes the Kurtosis and in turn ensuring that the smoothened plot has a heavy tail, making anomalies and aberrations stand out.\n\nFinding the global optimal window size, that maximizes Kurtosis, is not always a good idea, because doing so can totally distort the plot leading to heavy information loss. A better way is to find local optimum within pre-defined limits; for example, an optimal point for window size between 10 and 40. These limits totally depend on the data at hand. Doing this not only leads to a smooth plot that highlights anomalies but also converges the computation to a local optimum much quicker.\n\n# References\n\n- [Kurtosis - Wikipedia](https://en.wikipedia.org/wiki/Kurtosis)\n- [Moving Average - Wikipedia](https://en.wikipedia.org/wiki/Moving_average)\n- [ASAP: Automatic Smoothing for Attention Prioritization in Streaming Time Series Visualization](https://arxiv.org/abs/1703.00983)\n- [Climate Change Earth Surface Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "similar": [
      "better-programmer",
      "mysql-cache",
      "2q-cache",
      "atomicity"
    ]
  },
  {
    "id": 38,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "lfu",
    "title": "Constant Time LFU",
    "description": "The most popular implementation of the LFU Cache Eviction Scheme, using a min-heap, implements all three operations with running time complexity of O(log n) and this makes LFU sub-optimal. In this essay, we take a detailed look at a clever algorithm that implements LFU such that all the operations happen with O(1) running time complexity.",
    "gif": "https://media.giphy.com/media/fWqGY1AC4HVIN3lRyB/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/89711582-cf11ba00-d9a8-11ea-9173-c7069b1537b5.png",
    "released_at": "2020-08-23",
    "total_views": 6035,
    "body": "A common strategy to make any system super-performant is *[Caching](https://en.wikipedia.org/wiki/Cache_(computing)).* Almost all software products, operating at scale, have multiple layers of caches in their architectures. Caching, when done right, does wonder to the response time and is one of the main reasons why products work so well at a massive scale. Cache engines are limited by the amount of memory available and hence once it gets full the engine has to decide which item should be evicted and that is where an eviction algorithm, like [LFU](https://en.wikipedia.org/wiki/Least_frequently_used) and [LRU](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)). kicks in.\n\n[LRU](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)) (Least Recently Used) cache eviction strategy is one of the most popular strategies out there. LFU (Least Frequently Used) strategy fares well in some use cases but a concern with its most popular implementation, where it uses a min-heap, is that it provides a running time complexity of `O(log n)` for all the three operations - insert, update and delete; because of which, more often than not, it is replaced with a sub-optimal yet extremely fast `O(1)` LRU eviction scheme.\n\nIn this essay, we take a look at Constant Time LFU implementation based on the paper [An O(1) algorithm for implementing the LFU cache eviction scheme](http://dhruvbird.com/lfu.pdf) by Prof. Ketan Shah, Anirban Mitra and Dhruv Matani, where instead of using a min-heap, it uses a combination of [doubly-linked lists](https://en.wikipedia.org/wiki/Doubly_linked_list) and [hash table](https://en.wikipedia.org/wiki/Hash_table) to gain a running time complexity of `O(1)` for all the three core operations.\n\n# The LFU cache eviction strategy\n\nLFU, very commonly, is implemented using a [min-heap](https://en.wikipedia.org/wiki/Min-max_heap) which is organized as per the frequency of access of each element. Each element of this heap holds a pair - cached value and the access frequency; and is structured in order of this frequency such that the cached value with the minimum access frequency sits at the top, making it quick to identify the element to be evicted.\n\n![min-heap LFU](https://user-images.githubusercontent.com/4745789/89717235-0fd1f900-d9d2-11ea-968d-9ed67f52a2db.png)\n\nAlthough the identification of the element to be evicted is quick, but in order for the heap to maintain its property - element with lowest access frequency be at the top - it demands a rebalance, and this rebalancing process has a running complexity of `O(log n)`. To make things worse, rebalancing is required every single time the frequency of an item is changed; which means that in the cache that implements LFU, every time an item is either inserted, accessed or evicted, a rebalance is required - making all the three core operations to have the time complexity of `O(log n)`.\n\n# Constant time implementation\n\nThe LFU cache can be implemented with `O(1)` complexity for all the three operations by using one [Hash Table](https://en.wikipedia.org/wiki/Hash_table) and a bunch of [Doubly Linked Lists](https://en.wikipedia.org/wiki/Doubly_linked_list). As stated by the [RUM Conjecture](https://arpitbhayani.me/blogs/rum), in order to get a constant time reads and updates operations, we have to make a compromise with memory utilization. This is exactly what we observe in this implementation.\n\n## The Hash Table\n\nThe Hash Table stores the mapping of the cached key to the Value Node holding the cached value. The value against the key is usually a pointer to the actual Value Node. Given that the lookup complexity of the hash table is `O(1)`, the operation to access the value given the key from this Hash Table could be accomplished in constant time.\n\n![LFU hash table](https://user-images.githubusercontent.com/4745789/90469594-e2561f80-e136-11ea-9ff4-8369a7ea3df3.png)\n\nThe illustration above depicts that the Hash Table holding cache keys `k1`, `k2`, etc are mapped to the nodes holding the values `v1` and `v2` through direct pointers. The nodes are allocated on the heap using dynamic allocation, hence are a little disorganized. The Value Node to which the key maps to, not only hold the cached value, but it also holds a few pointers pointing to different entities in the system, enabling constant-time operations.\n\n## Doubly Linked Lists\n\nThis implementation of LFU requires us to maintain one Doubly Linked List of frequencies, called `freq_list`, holding one node for each unique frequency spanning all the caches values. This list is kept sorted on the frequency the node represents such that, the node representing the lowest frequency is on the one end while the node representing the highest frequency is at the other.\n\nEvery Frequency Node holds the frequency that it represents in the member `freq` and the usual `next` and `prev` pointers pointing to the adjacent Frequency Nodes; it also keeps a `values_ptr` which points to another doubly-linked list holding Value Nodes (referred in the hash table) having the same access frequency `freq`.\n\n![lists LFU](https://user-images.githubusercontent.com/4745789/90469593-e08c5c00-e136-11ea-995b-e4590981dd89.png)\n\nThe overall schematic representation of doubly-linked lists and its arrangement is as shown in the illustration above. The doubly-linked list holding Frequency Nodes is arranged horizontally while the list holding the Value Nodes is arranged vertically, for clearer view and understanding.\n\nSince the cached values `v1` and `v7` both have been accessed `7` times, they both are chained in a doubly-linked list and are hooked with the Frequency Node representing the frequency of `7`. Similarly, the Value Nodes holding values `v5`, `v3`, and `v9` are chained in another doubly-linked list and are hooked with the Frequency Node representing the frequency of `18`.\n\nThe Value Node contains the cached value in member `data`, along with the usual `next` and `prev` pointers pointing to the adjacent Value Nodes in the list. It also holds a `freq_pointer` pointing back to the Frequency Node to which the list if hooked at. Having all of these pointers helps us ensure all the three operations happen in constant time.\n\nNow that we have put all the necessary structures in place, we take a look at the 3 core operations along with their pseudo implementation.\n\n## Adding value to the cache\nAdding a new value to the cache is a relatively simpler operation that requires a bunch of pointer manipulations and does the job with a constant time running complexity. While inserting the value in the cache, we first check the existence of the key in the table, if the key is already present and we try to put it again the function raises an error. Then we ensure the presence of the Frequency Node representing the frequency of `1`, and in the process, we might also need to create a new frequency node also. Then we wrap the value in a Value Node and adds it to the `values_list` of this Frequency Node; and at last, we make an entry in the table acknowledging the completion of the caching process.\n\n```python\ndef add(key: str, value: object):\n    # check if the key already present in the table,\n    # if so then return the error\n    if key in table:\n        raise KeyAlreadyExistsError\n\n    # create the Value Node out of value\n    # holding all the necessary pointers\n    value_node = make_value_node(value)\n\n    first_frequency_node = freq_list.head\n    if first_frequency_node.freq != 1:\n        # since the first node in the freq_list does not represent\n        # frequency of 1, we create a new node\n        first_frequency_node = make_frequency_node(1)\n\n        # update the `freq_list` that holds all the Frequency Nodes\n        # such that the first node represents the frequency 1\n        # and other list stays as is.\n        first_frequency_node.next = freq_list.head\n        freq_list.head.prev = first_frequency_node\n        freq_list.head = first_frequency_node\n\n    # Value Node points back to the Frequency Node to which it belongs\n    value_node.freq_pointer = first_frequency_node\n\n    # add the Value Node in `first_frequency_node`\n    first_frequency_node.values.add(value_node)\n\n    # update the entry in the hash table\n    table[key] = value_node\n```\n\nAs seen in the pseudocode above the entire procedure to add a new value in the cache is a bunch of memory allocation along with some pointer manipulations, hence we observe that the running complexity of `O(1)` for this operation.\n\n## Evicting an item from the cache\n\nEviction, similar to insertion, is a trivial operation where we simply pick the frequency node with lowest access frequency (the first node in the `freq_list`) and remove the first Value Node present in its `values_list`. Since the entire eviction also requires pointer manipulations, it also exhibits a running complexity of `O(1)`.\n\n```python\ndef evict():\n    if freq_list.head and freq_list.head.values:\n        first_value_node = freq_list.head.values.first\n        second_value_node = first_value_node.next\n\n        # make the second element as first\n        freq_list.head = second_value_node\n\n        # ensure second node does not have a dangling prev link\n        second_value_node.prev = None\n\n        # delete the first element\n        delete_value_node(first_value_node)\n\n    if freq_list.head and not freq_list.head.values:\n        # if the Frequency Node after eviction does not hold\n        # any values we get rid of it\n        delete_frequency_node(freq_list.head)\n```\n\n## Getting a value from the cache\n\nAccessing an item from the cache has to be the most common operation of any cache. In the LFU scheme, before returning the cached value, the engine also has to update its access frequency. Ensuring the change in access frequency of one cached value does not require some sort of rebalancing or restructuring to maintain the integrity, is what makes this implementation special.\n\nThe engine first makes a get call to the Hash Table to check that the key exists in the cache. Before returning the cached value from the retrieved Value Node, the engine performs the following operations - it accesses the Frequency Node and its sibling corresponding to the retrieved Value Node. It ensures that the frequency of the sibling is 1 more than that of the Frequency Node; if not it creates the necessary Frequency Node and place it as the new sibling. The Value Node then changes its affinity to this sibling Frequency Node so that it correctly matches the access frequency. In the end, the back pointer from the Value Node to the new Frequency Node is set and the value is returned.\n\n```python\ndef get(key: str) -> object:\n    # get the Value Node from the hash table\n    value_node = table.get(key, None)\n\n    # if the value does not exist in the cache then return an error\n    # stating Key Not Found\n    if not value_node:\n        raise KeyNotFoundError\n\n    # we get the Frequency Node from the Value Node using the\n    # freq_pointer member.\n    frequency_node = value_node.freq_pointer\n\n    # we also get the next Frequency Node to the current\n    # Frequency Node so that we could make a call about\n    # the need to create a new node.\n    next_frequency_node = frequency_node.next\n\n    if next_frequency_node.freq != frequency_node.freq + 1:\n        # create a new Frequency Node\n        new_frequency_node = make_frequency_node(frequency_node.freq + 1)\n        \n        # place the Frequency Node at the correct position in the list\n        frequency_node.next = new_frequency_node\n        new_frequency_node.prev = frequency_node\n\n        next_frequency_node.prev = new_frequency_node\n        new_frequency_node.next = frequency_node\n\n        # going forward we call the new Frequency Node as next\n        # because it represents the the next Frequency Node\n        next_frequency_node = new_frequency_node\n\n    # we add the Value Node in the nex\n    next_frequency_node.values.add(value_node)\n    \n    # we change the parent and adjecent nodes to this Value Nodes\n    value_node.freq_pointer = next_frequency_node\n    value_node.next = None\n    value_node.prev = next_frequency_node.values.last\n    \n    # if the Frequency Node has no elements then deleting the node\n    # so as to avoid the memory leak\n    if len(frequency_node.values) == 0:\n        delete_frequency_node(frequency_node)\n\n    # returning the value\n    return value_node.value\n```\n\nAgain, since this operation also only deals with pointer manipulations through direct pointers, the running time complexity of this operation is also constant time. Thus we see the Constant Time LFU implementation where the necessary time complexity is achieved by using Hash Tables and Doubly-Linked Lists.\n\n# References\n\n- [An O(1) algorithm for implementing the LFU cache eviction scheme](http://dhruvbird.com/lfu.pdf)\n- [When and Why to use a Least Frequently Used (LFU) cache with an implementation in Golang](https://ieftimov.com/post/when-why-least-frequently-used-cache-implementation-golang/)\n",
    "similar": [
      "fully-persistent-arrays",
      "isolation-forest",
      "flajolet-martin",
      "consistent-hashing"
    ]
  },
  {
    "id": 39,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "morris-counter",
    "title": "Morris's Algorithm for Approximate Counting",
    "description": "Morris' Algorithm counts a large number of events using a very small space O(log log n). The algorithm uses probabilistic techniques to increment the counter and in this essay, we take a detailed look at Morris' Algorithm and the math behind it.",
    "gif": "https://media.giphy.com/media/uWXxbGPMbFkxsni3tc/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/89116441-b9d0f300-d4b1-11ea-99e5-cca7c4cef5fb.png",
    "released_at": "2020-08-02",
    "total_views": 2120,
    "body": "Approximate Counting algorithms are techniques that allow us to count a large number of events using a very small amount of memory. It was invented by [Robert Morris](https://en.wikipedia.org/wiki/Robert_Tappan_Morris) in 1977 and was published through his paper [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf). The algorithm uses probabilistic techniques to increment the counter, although it does not guarantee the exactness it does provide a fairly good estimate of the true value while inducing a minimal and yet fairly constant relative error. In this essay, we take a detailed look at Morris' Algorithm and the math behind it.\n\nRobert Morris, while working at Bell Laboratories, had a problem at hand; he was supposed to write a piece of code that counts a large number of events and all he had was one 8-bit counter. Since the number of events easily crossed 256, counting them was infeasible using ordinary methods and this constraint led him to build this approximate counter that instead of providing the exact count, does a pretty good job providing an approximate one.\n\n# Counting and coin flips\n\nA very simple solution, to build an approximate counter, is to count every alternate event. In order to make the decision of whether to count an event or not - we use a coin flip, which means, every time we see a new event, we flip a coin and if it lands *heads* we increase the count otherwise we don't. This way the value preset in our counter register will, on average, represent half of the total events. When we multiply the count (in the register) by 2, we get a good approximation of the actual number of events.\n\nThis coin flip based counting technique is a Binomial Distribution with parameters `(n, p)` where `n` is the total number of events seen and `p` is the success probability i.e. probability of getting *heads* during a coin flip. The expected value `v` in the counter register corresponding to the number of events `n` is given by\n\n![value in coin-flip counter](https://user-images.githubusercontent.com/4745789/89116526-a6725780-d4b2-11ea-9143-a562d6c3ca93.png)\n\nThe standard deviation of this binomial distribution will help us find the error in our measurement i.e. value retrieved using our counter vs the actual number of the events seen. For a binomial distribution twice the standard deviation on either side of the mean covers **95%** of the distribution; and we use this to find the relative and absolute error in our counter value as illustrated below.\n\n![coin-flip binomial distribution](https://user-images.githubusercontent.com/4745789/89117327-82b30f80-d4ba-11ea-8346-a39ae6cb639a.png)\n\nAs illustrated in the image we can say that if the counter holds the value `200`, the closest approximate to the actual number of events that we can make is `2 * 200` = `400`. Even though `400` might not be the actual number of events seen, we can say, with **95%** confidence, that the true value lies in the range `[180, 220]`.\n\nWith this coin flip based counter, we have actually doubled our capacity to count and have also ensured that our memory requirements stay constant. On an 8-bit register, ordinarily, we would have counted till 256, but with this counter in place, we can approximately count till 512.\n\nThis approach can be extended to count even larger numbers by changing the value of `p`. The absolute error observed here is small but the relative error is very high for smaller counts and hence this creates a need for a technique that has a near-constant relative error, something that is independent of `n`.\n\n# The Morris' Algorithm\n\nInstead of keeping track of the total number of events `n` or some constant multiple of `n`, Morris' algorithm suggests that the value we store in the register is\n\n![value in counter Morris' algorithm](https://user-images.githubusercontent.com/4745789/89117993-edb31500-d4bf-11ea-9879-1f0032950ff4.png)\n\nHere we try to exploit the core property of *logarithm* - *the growth of logarithmic function is inverse of the exponential* - which means the value `v` will grow faster for the smaller values of `n` - providing better approximations. This ensures that the relative error is near-constant i.e. independent of `n` and it does not matter if the number of events is fewer or larger.\n\nNow that we have found a function that suits the needs of a good approximate counter, it is time we define what exactly would happen to the counter when we see a new event.\n\n## Incrementing the value\n\nSince we are building a counter, all we know is the value of the counter `v` and have no knowledge of the actual number of events seen. So when the new event comes in we have to decide if `v` needs to change or not. Given the above equation, our best estimate of `n` given `v` can be computed as\n\n![estimate n](https://user-images.githubusercontent.com/4745789/89120289-c7e33b80-d4d2-11ea-92b8-d307b0aa9032.png)\n\nNow that we have seen a new event we want to find the new value `v` for the counter. This value should always, on an average, converge to `n + 1` and we find it as\n\n![next value](https://user-images.githubusercontent.com/4745789/89120467-2eb52480-d4d4-11ea-89ac-d8cacd14d952.png)\n\nSince the value computed using the above method is generally not an integer, performing either round-up or round-down every time will induce a serious error in counting.\n\nFor us to determine if we should increment the value of `v` or not, we need to find the cost (inaccuracy) that we might incur if we made an incorrect call. We establish a heuristic that if the change in the value of `n` by change in `v` is huge, we should have a lower probability of making an increment to `v` and vice versa. We this define `d` to be reciprocal of this jump i.e. difference between `n` corresponding to `v + 1` and `v`.\n\n![defining d](https://user-images.githubusercontent.com/4745789/89120957-38d92200-d4d8-11ea-975f-323b36da325c.png)\n\nThe value of `d` will always be in the interval `(0, 1)` . Smaller the jump between two `n`s larger will be the value of `d` and larger the jump, smaller will be the value of `d`. This also implies that as `n` grows the value of `d` will become smaller and smaller making it harder for us to make the change in `v`.\n\nSo we pick a random number `r` uniformly generated in the interval `[0, 1)` and using this random number `r` and previously defined `d` we state that if this `r` is less than `d` increase the counter `v` otherwise, we keep it as is. As `n` increases, `d` decreases making it tougher for the odds of pick `r` in the range `[0, d)`.\n\n![defiing d 2](https://user-images.githubusercontent.com/4745789/89120929-f9aad100-d4d7-11ea-8f0e-066fc059c066.png)\n\nThe proof that the expected value of `n`, post this probabilistic decision, is `n + 1` can be found in the paper - [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf). After tweaking some parameters and making process stabler the function that Morris came up was\n\n![morris function v](https://user-images.githubusercontent.com/4745789/89121058-3fb46480-d4d9-11ea-9d93-5af712ac08e7.png)\n\nWhen we plot values produced by Morris' Algorithm vs the actual number of events we find that Morris' algorithm indeed generates better approximate values to smaller values of `n` but as `n` increases the absolute error grows but the relative error remains fairly constant. The illustrations shown below describe these facts.\n\n![Morris comparison](https://user-images.githubusercontent.com/4745789/89123322-13eeaa00-d4ec-11ea-9539-ada7f5de9af1.png)\n\n\n> _Python based implementation of Morris' algorithm can be found at [github.com/arpitbbhayani/morris-counter](https://github.com/arpitbbhayani/morris-counter/blob/master/morris-counter.ipynb)_\n\n\n# Space Complexity\n\nIn order to count till `n` the Morris' algorithm requires the counter to go up to `log(n)` and hence the number of bits required to count from `0 to log(n)` ordinarily is `log(log(n))` and hence we say that the space complexity of this technique is `O(log log n)`. Morris' algorithm thus provides a very efficient way to manage cardinalities where we can afford to have approximations.\n\n# References\n\n- [Approximate Counting Algorithm - Wikipedia](https://en.wikipedia.org/wiki/Approximate_counting_algorithm)\n- [Approximate Counting with Morris's Algorithm](http://gregorygundersen.com/blog/2019/11/11/morris-algorithm/)\n- [Counting large number of events in small registers](http://www.inf.ed.ac.uk/teaching/courses/exc/reading/morris.pdf)\n- [Probabilistic Counting and Morris' Algorithm - Texas A&M University](http://cesg.tamu.edu/wp-content/uploads/2014/09/ECEN689-lec11.pdf)\n- [Python-based implementation of Morris' algorithm](https://github.com/arpitbbhayani/morris-counter/blob/master/morris-counter.ipynb)\n",
    "similar": [
      "israeli-queues",
      "slowsort",
      "fast-and-efficient-pagination-in-mongodb",
      "phi-accrual"
    ]
  },
  {
    "id": 40,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "slowsort",
    "title": "Slowsort - A Pessimal Sorting Algorithm",
    "description": "Slowsort is a pessimal sorting algorithm based on the Multiply and Surrender paradigm. The algorithm is designed to be deterministically sub-optimal and it could easily be the worst way anyone could sort an array.",
    "gif": "https://media.giphy.com/media/3NtY188QaxDdC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/88473025-fabb8b80-cf36-11ea-8390-a807fcd57d93.png",
    "released_at": "2020-07-26",
    "total_views": 6485,
    "body": "Slowsort is a sorting algorithm that is designed to be deterministically sub-optimal. The algorithm was published in 1986 by Andrei Broder and Jorge Stolfi in their paper [Pessimal Algorithms and Simplexity Analysis](https://www.researchgate.net/publication/2805500_Pessimal_Algorithms_and_Simplexity_Analysis) where they expressed a bunch of very in-efficient algorithms. These techniques and algorithms are special because they never make a wrong move while solving a problem, instead, they find ways to delay the success. In this essay, we put our focus on the Slowsort algorithm based on the *Multiply and Surrender* paradigm.\n\n# Multiply and Surrender\n\nThe *Multiply and Surrender (MnS)* paradigm, is a humorous take on the popular *Divide and Conquer (DnC)* technique. MnS splits the problem into subproblems, slightly simpler than the original, and continues doing so recursively for as long as possible. The subproblems are multiplied and the evaluation is delayed till the state that the solving could not be further postponed and then it surrenders.\n\nMnS paradigm is very similar to DnC; but where the DnC actually splits the problems into subproblems to reach the solution quicker, MnS does it to procrastinate, making the entire process very inefficient but yet convergent.\n\nAlthough Slowsort is a classic example, recursive Fibonacci with no memoization also fares under the MnS paradigm. The recursive code to find `n`th Fibonacci number is as illustrated below\n\n```python\ndef fib(n):\n    \"\"\"the function returns the `n`th fibonacci number\n    \"\"\"\n\n    # the subproblem could not be postponed anymore\n    # hence we surrender and return the answer\n    if n < 2:\n        return 1\n\n    # we split the problem into subproblems and invoke\n    # the function recursively\n    return fib(n - 1) + fib(n - 2)\n```\n\nWhile computing the Fibonacci numbers, we split the problem into subproblems and do this recursively till we are left with elemental states i.e. `0` or `1` and which is when we return the initial values which are `1` and `1`. This approach is not DnC because we are not splitting the problem into subproblems to achieve optimality, instead are doing a lot of repetitive work and taking a non-polynomial time to generate the Fibonacci numbers.\n\n# Slowsort Algorithm\n\nSlowsort algorithm draws a lot of similarities to the very popular Mergesort, but while Mergesort operates in `O(n . log(n))` the complexity of Slowsort is non-polynomial `O(n ^ log(n))` and its best case performs worse than the worst case of bubble sort.\n\nSlowsort algorithm recursively breaks the array sorting problem into two subarray sorting problems and a few extra processing steps. Once the two subarrays are sorted, the algorithm swaps the rightmost elements such that the greatest among the two becomes the rightmost element of the array i.e. the greatest among the two is placed at the correct position relative to each other, and then it invokes the sorting for all elements except this fixed maximum.\n\nThe algorithm could this be expressed as following broad steps\n\n- sort the first half recursively\n- sort the second half recursively\n- find the maximum of the whole array by comparing the last elements of both the sorted halves and place it at the end of the array\n- recursively sort the entire array except for the maximum one\n\nThe in-place implementation of the Slowsort algorithm is as illustrated below\n\n```python\ndef _slowsort(a, i, j):\n    \"\"\"in-place sorts the integers in the array\n    spanning indexes [i, j].\n    \"\"\"\n    # base condition; then no need of sorting if\n    #  - there is one element to sort\n    #  - when start and end of the array flipped positions\n    if i >= j:\n        return\n\n    # find the mid index of the array so that the\n    # problem could be divided intto sub-problems of\n    # smaller spans\n    m = (i + j) // 2\n\n    # invoke the slowsort on both the subarrays\n    _slowsort(a, i, m)\n    _slowsort(a, m + 1, j)\n  \n    # once both the subproblems are solved, check if\n    # last elements of both subarrays and move the\n    # higher among the both to end of the right subarray\n    # ensuring that the highest element is placed at the\n    # correct relative position\n    if a[m] > a[j]:\n        a[m], a[j] = a[j], a[m]\n  \n    # now that the rightmost element of the array is at\n    # the relatively correct position, we invoke Slowsort on all\n    # the elements except the last one.\n    _slowsort(a, i, j - 1)\n\n\ndef slowsort(a):\n    \"\"\"in-place sorts the array `a` using Slowsort.\n    \"\"\"\n    _slowsort(a, 0, len(a) - 1)\n```\n\nThe Slowsort algorithm typically replaces the `merge` function of the Mergesort with a simple swap that correctly places the largest element (local maxima) and then invokes the sort function on all but this element. So on every invocation, we keep correctly placing the largest element but in a recursive manner.\n\n*A visualization of this algorithm could be found in this [youtube video](https://www.youtube.com/watch?v=QbRoyhGdjnA).*\n\n# Asymptotic Analysis\n\nThe runtime of Slowsort could be computed by the following recurrence relation\n\n![slowsort recurrence relation](https://user-images.githubusercontent.com/4745789/88473102-cb594e80-cf37-11ea-9015-217c3eda50d6.png)\n\nWhen the above recurrence relation is solved and we find that the asymptotic lower bound for `T(n)` is given by\n\n![lower bound of slowsort](https://user-images.githubusercontent.com/4745789/88473128-14a99e00-cf38-11ea-905b-f3f473a0d74c.png)\n\nThe above expression suggests that lower bound of Slowsort is non-polynomial in nature and for a sufficiently large `n` this would be more than `n ^ 2` implying that even the best case of Slowsort is worse than the worst case of Bubble sort. The illustration below compares the time taken by Slowsort and the recursive implementation of Bubblesort.\n\n![slowsort vs recursive bubblesort](https://user-images.githubusercontent.com/4745789/88475549-8e4c8680-cf4e-11ea-8ee4-9f7ed345ff5d.png)\n\n> The iterative implementation of Bubblesort stood no chance in terms of time taken for smaller sets of integers, hence the chart was plotted against the recursive implementation of it. The iterative bubble sort for smaller arrays is nearly flat.\n\n# Slowsort vs Bogosort\n\n[Bogosort](https://en.wikipedia.org/wiki/Bogosort) is a sorting algorithm that has an average case time complexity of `O(n!)` and an unbounded time in the worst case. The algorithm keeps on permuting (shuffling) the array till it is sorted which introduces an unboundedness in its implementation and hence the algorithm is considered to be the worst sorting algorithm ever.\n\n```python\nimport random\n\ndef is_sorted(a):\n    return all(a[i] <= a[i + 1] for i in range(len(a) - 1))\n\ndef bogosort(a):\n    while not is_sorted(a):\n        random.shuffle(a)\n```\n\nThe reason that we should rate Slowsort highly is the fact the Bogosort could sort the list in its first shuffle while Slowsort is deterministic and will take `O(n ^ log(n))` time in best case scenario.\n\nA rule that every algorithm follows is that every step that it takes actually makes some progress towards the final goal. Bogosort does not guarantee progress, and since it randomly shuffles the array, at one iteration we could end up at a nearly sorted array while the next shuffle takes us afar.\n\nSlowsort is deterministic and convergent. The number of swaps made (inversions) during the execution is non-increasing which means once two items are swapped they are in the correct order relative to each other. In other words, we say that Slowsort never makes a wrong move.\n\n# References\n\n- [Slowsort](https://wiki.c2.com/?SlowSort)\n- [Slowsort - Wikipedia](https://en.wikipedia.org/wiki/Slowsort)\n- [Multiply and Surrender](https://wiki.c2.com/?MultiplyAndSurrender)\n- [Pessimal Algorithms and Simplexity Analysis](https://www.researchgate.net/publication/2805500_Pessimal_Algorithms_and_Simplexity_Analysis)\n",
    "similar": [
      "flajolet-martin",
      "1d-terrain",
      "morris-counter",
      "isolation-forest"
    ]
  },
  {
    "id": 41,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "bitcask",
    "title": "Bitcask - A Log-Structured Fast KV Store",
    "description": "Bitcask is a Key-Value store that persists its data in append-only log files and still reaps super-performant read-write throughputs. In this essay, we take a detailed look into Bitcask, its design, and find the secret sauce that makes it so performant.",
    "gif": "https://media.giphy.com/media/l0NwF1dnk7GRz3pK0/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/87868516-144b5900-c9b4-11ea-91e3-2de14e80817b.png",
    "released_at": "2020-07-19",
    "total_views": 574,
    "body": "Bitcask is one of the most efficient embedded Key-Value (KV) Databases designed to handle production-grade traffic. The paper that introduced Bitcask to the world says it is a *[Log-Structured](https://en.wikipedia.org/wiki/Log-structured_file_system) [Hash Table](https://en.wikipedia.org/wiki/Hash_table) for Fast Key/Value Data* which, in a simpler language, means that the data will be written sequentially to an append-only log file and there will be pointers for each `key` pointing to the `position` of its log entry. Building a KV store off the append-only log files seems like a really weird design choice, but Bitcask does not only make it efficient but it also gives a really high Read-Write throughput.\n\nBitcask was introduced as the backend for a distributed database named [Riak](https://riak.com/) in which each node used to run one instance of Bitcask to hold the data that it was responsible for. In this essay, we take a detailed look into Bitcask, its design, and find the secret sauce that makes it so performant.\n\n# Design of Bitcask\n\nBitcask uses a lot of principles from [log-structured file systems](https://en.wikipedia.org/wiki/Log-structured_file_system) and draws inspiration from a number of designs that involve log file merging, for example - merging in LSM Trees. It essentially is just a directory of append-only (log) files with a fixed structure and an in-memory index holding the keys mapped to a bunch of information necessary for point lookups - referring to the entry in the datafile.\n\n## Datafiles\n\nDatafiles are append-only log files that hold the KV pairs along with some meta-information. A single Bitcask instance could have many datafiles, out of which just one will be active and opened for writing, while the others are considered immutable and are only used for reads.\n\n![Bitcask Datafiles](https://user-images.githubusercontent.com/4745789/87866701-78fdb800-c9a2-11ea-9c35-9a706ac96d97.png)\n\nEach entry in the datafile has a fixed structure illustrated above and it stores `crc`, `timestamp`, `key_size`, `value_size`, actual `key`, and the actual `value`. All the write operations - create, update and delete - made on the engine translates into entries in this active datafile. When this active datafile meets a size threshold, it is closed and a new active datafile is created; and as stated earlier, when closed (intentionally or unintentionally), the datafile is considered immutable and is never opened for writing again.\n\n## KeyDir\n\nKeyDir is an in-memory hash table that stores all the keys present in the Bitcask instance and maps it to the offset in the datafile where the log entry (value) resides; thus facilitating the point lookups. The mapped value in the Hash Table is a structure that holds `file_id`, `offset`, and some meta-information like `timestamp`, as illustrated below.\n\n![Bitcask KeyDir](https://user-images.githubusercontent.com/4745789/87866707-96cb1d00-c9a2-11ea-9730-fc7f8cb79b92.png)\n\n# Operations on Bitcask\n\nNow that we have seen the overall design and components of Bitcask, we can jump into exploring the operations that it supports and details of their implementations.\n\n### Putting a new Key Value\n\nWhen a new KV pair is submitted to be stored in the Bitcask, the engine first appends it to the active datafile and then creates a new entry in the KeyDir specifying the offset and file where the value is stored. Both of these actions are performed atomically which means either the entry is made in both the structures or none.\n\nPutting a new Key-Value pair requires just one atomic operation encapsulating one disk write and a few in-memory access and updates. Since the active datafile is an append-only file, the disk write operation does not have to perform any disk seek whatsoever making the write operate at an optimum rate providing a high write throughput.\n\n### Updating an existing Key Value\n\nThis KV store does not support partial update, out of the box, but it does support full value replacement. Hence the update operation is very similar to putting a new KV pair, the only change being instead of creating an entry in KeyDir, the existing entry is updated with the new position in, possibly, the new datafile.\n\nThe entry corresponding to the old value is now dangling and will be garbage collected explicitly during merging and compaction.\n\n### Deleting a Key\n\nDeleting a key is a special operation where the engine atomically appends a new entry in the active datafile with value equalling a tombstone value, denoting deletion, and deleting the entry from the in-memory KeyDir. The tombstone value is chosen as something very unique so that it does not interfere with the existing value space.\n\nDelete operation, just like the update operation, is very lightweight and requires a disk write and an in-memory update. In delete operation as well, the older entries corresponding to the deleted keys are left dangling and will be garbage collected explicitly during merging and compaction.\n\n### Reading a Key-Value\n\nReading a KV pair from the store requires the engine to first find the datafile and the offset within it for the given key; which is done using the KeyDir. Once that information is available the engine then performs one disk read from the corresponding datafile at the offset to retrieve the log entry. The correctness of the value retrieved is checked against the CRC stored and the value is then returned to the client.\n\nThe operation is inherently fast as it requires just one disk read and a few in-memory accesses, but it could be made faster using Filesystem read-ahead cache.\n\n# Merge and Compaction\n\nAs we have seen during Update and Delete operations the old entries corresponding to the key remain untouched and dangling and this leads to Bitcask consuming a lot of disk space. In order to make things efficient for the disk utilization the engine once a while compacts the older closed datafiles into one or many merged files having the same structure as the existing datafiles.\n\nThe merge process iterates over all the immutable files in the Bitcask and produces a set of datafiles having only *live* and *latest* versions of each present key. This way the unused and non-existent keys are ignored from the newer datafiles saving a bunch of disk space. Since the record now exists in a different merged datafile and at a new offset, its entry in KeyDir needs an atomic updation.\n\n# Performant bootup\n\nIf the Bitcask crashes and needs a boot-up, it will have to read all the datafiles and build a new KeyDir. Merging and compaction here do help as it reduces the need to read data that is eventually going to be evicted. But there is another operation that could help in making the boot times faster.\n\nFor every datafile a *hint* file is created which holds everything in the datafile except the value i.e. it holds the key and its meta-information. This *hint* file, hence, is just a file containing all the keys from the corresponding datafile. This *hint* file is very small in size and hence by reading this file the engine could quickly create the entire KeyDir and complete the bootup process faster.\n\n# Strengths and Weaknesses of Bitcask\n\n## Strengths\n\n- Low latency for read and write operations\n- High Write Throughput\n- Single disk seek to retrieve any value\n- Predictable lookup and insert performance\n- Crash recovery is fast and bounded\n- Backing up is easy - Just copy the directory would suffice\n\n## Weaknesses\n\nThe KeyDir holds all the keys in memory at all times and this adds a huge constraint on the system that it needs to have enough memory to contain the entire keyspace along with other essentials like Filesystem buffers. Thus the limiting factor for a Bitcask is the limited RAM available to hold the KeyDir.\n\nAlthough this weakness sees a major one but the solution to this is fairly simple. We can typically shard the keys and scale it horizontally without losing much of the basic operations like Create, Read, Update, and Delete.\n\n# References\n\n- [Bitcask Paper](https://riak.com/assets/bitcask-intro.pdf)\n- [Bitcask - Wikipedia](https://en.wikipedia.org/wiki/Bitcask)\n- [Riak's Bitcask - High Scalability](http://highscalability.com/blog/2011/1/10/riaks-bitcask-a-log-structured-hash-table-for-fast-keyvalue.html/)\n- [Implementation of the Bitcask storage model-merge and hint files](https://topic.alibabacloud.com/a/implementation-of-the-bitcask-storage-model-merge-and-hint-files_8_8_31516931.html)\n",
    "similar": [
      "durability",
      "persistent-data-structures-introduction",
      "isolation",
      "atomicity"
    ]
  },
  {
    "id": 42,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "phi-accrual",
    "title": "Phi \u03c6 Accrual Failure Detection",
    "description": "Phi \u03c6 Accrual Failure Detection algorithm, unlike conventional algorithms, is an adaptive failure detection algorithm that instead of providing output as a boolean (system being up or down), outputs the suspicion information (level) on a continuous scale.",
    "gif": "https://media.giphy.com/media/gLoMzjGQB2tQlQtB9P/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/87240958-c5de0d80-c43b-11ea-8e1b-2a7c70586b9b.png",
    "released_at": "2020-07-12",
    "total_views": 478,
    "body": "One of the most important virtues of any distributed system is its ability to detect failures in any of its subsystems before things go havoc. Early detection of failures helps in taking preventive actions and ensuring that the system stays fault-tolerant. The conventional way of failure detection is by using a bunch of heartbeat messages with a fixed timeout, indicating if a subsystem is down or not.\n\nIn this essay, we take a look into an adaptive failure detection algorithm called *Phi Accrual Failure Detection*, which was introduced in a [paper](https://pdfs.semanticscholar.org/11ae/4c0c0d0c36dc177c1fff5eb84fa49aa3e1a8.pdf) by Naohiro Hayashibara, Xavier D\u00e9fago, Rami Yared, and Takuya Katayama. The algorithm uses historical heartbeat information to make the threshold adaptive. Instead of generating a binary value, like conventional methods, it generates continuous values suggesting the confidence level it has in stating if the system crashed or not.\n\n# Conventional Failure Detection\n\nAccurately detecting failures is an impossible problem to solve as we cannot ever say if a system crashed or is just very slow in responding. Conventional Failure Detection algorithms output a boolean value stating if the system is down or not; there is no middle ground.\n\n## Heartbeats with constants timeouts\n\nThe conventional Failure Detection algorithms use *heartbeat* messages with a fixed timeout in order to determine if a system is alive or not. The monitored system periodically sends a heartbeat message to the monitoring system, informing that it is still alive. The monitoring system will suspect that the process crashed if it fails to receive any heartbeat message within a configured timeout period.\n\nHere the value of timeout is very crucial as keeping it short means we detect failures quickly but with a lot of false positives; and while keeping it long means we reduce the false positives but the detection time takes a toll.\n\n# Phi Accrual Failure Detection\n\nPhi Accrual Failure Detection is an adaptive Failure Detection algorithm that provides a building block to implementing failure detectors in any distributed system. A generic Accrual Failure Detector, instead of providing output as a boolean (system being up or down), outputs the suspicion information (level) on a continuous scale such that higher the suspicion value, the higher are the chances that the system is down.\n\n## Detailing \u03c6\n\nWe define \u03c6 as the suspicion level output by this failure detector and as the algorithm is adaptive, the value will be dynamic and will reflect the current network conditions and system behavior. As we established earlier - lower are the chances of receiving the heartbeat, higher are the chances that the system crashed hence higher should be the value of \u03c6; the details around expressing \u03c6 mathematically are as illustrated below.\n\n![Phi Accrual Failure Detection](https://user-images.githubusercontent.com/4745789/87240784-469c0a00-c43a-11ea-8689-9dc41eb1ccf1.png)\n\nThe illustration above mathematically expresses our establishments and shows how we can use `-log10(x)` function applied to the probability to get a gradual negative slope indicating a decline in the value of \u03c6. We observe how, as the probability of receiving heartbeat increases, the value of \u03c6 decreases and approaches `0`, and when the probability of receiving heartbeat decreases and approaches `0`, the value of \u03c6 tends to infinity \u221e.\n\nThe \u03c6 value computed using `-log10(x)` also suggests our likeliness of making mistakes decreases exponentially as the value of \u03c6 increases. So if we say a system is down if \u03c6 crosses a certain threshold `X` where `X` is `1`, it implies that our decision will be contradicted in the future by the reception of a late heartbeat is about `10%`. For `X = 2`, the likelihood of the mistake will be `1%`, for `X = 3` it will be `0.1%`, and so on.\n\n## Estimating the probability of receiving another heartbeat\n\nNow that we have defined what \u03c6 is, we need a way to compute the probability of receiving another heartbeat given we have seen some heartbeats before. This probability is proportional to the probability that the heartbeat will arrive more than `t` units after the previous one i.e. longer the wait lesser are the chances of receiving the heartbeat.\n\nIn order to implement this, we keep a sampled Sliding Window holding arrival times of past heartbeats. Whenever a new heartbeat arrives, its arrival time is stored into the window, and the data regarding the oldest heartbeat is deleted.\n\nWe observe that the arrival intervals follow a [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution) indicating most of the heartbeats arrive within a specific range while there are a few that arrive late due to various network or system conditions. From the information stored in the window, we can easily compute the arrival intervals, mean, and variance which we require to estimate the probability.\n\nSince arrival intervals follow a Normal Distribution, we can integrate the [Probability Density Function](https://en.wikipedia.org/wiki/Probability_density) over the interval `(t, \u221e)` to get the probability of receiving heartbeat after `t` units of time. Thus the expression for deriving this can be illustrated below.\n\n![Estimating probability of receiving another heartbeat](https://user-images.githubusercontent.com/4745789/87231591-fbe8a680-c3d5-11ea-9427-d4cd66e8e717.png)\n\nWe observe that if the process actually crashes, the value is guaranteed to accrue (accumulate) over time and will tend to infinity \u221e. Since the accrual failure detectors output value in a continuous range, we need to explicitly define thresholds crossing which we say that the system crashed.\n\n# Benefits of using Accrual Failure Detectors\n\nWe can define multiple thresholds, crossing which we can take precautionary measures defined for it. As the threshold becomes steeper the action could become more drastic. Another major benefit of using this system is that it favors a nearly complete decoupling between application requirements and monitoring as it leaves the applications to define threshold according to their QoS requirements.\n\n# References\n\n- [The \u03c6 Accrual Failure Detector](https://pdfs.semanticscholar.org/11ae/4c0c0d0c36dc177c1fff5eb84fa49aa3e1a8.pdf)\n- [Normal Distribution](https://en.wikipedia.org/wiki/Normal_distribution)\n",
    "similar": [
      "israeli-queues",
      "copy-on-write",
      "2q-cache",
      "morris-counter"
    ]
  },
  {
    "id": 43,
    "topic": null,
    "uid": "decipher-repeated-key-xor",
    "title": "Deciphering Repeated-key XOR Ciphertext",
    "description": "Deciphering is the process of recovering the original message from an encrypted byte stream, usually, without having any knowledge of the encryption key. In this essay, we look at how we can use Hamming Distance, Linguistics to recover the original message from a Repeated-key XORed Ciphertext.",
    "gif": "https://media.giphy.com/media/b5Hcaz7EPz26I/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/86505243-d52bde00-bddf-11ea-9fb2-bd9c25e51bd4.png",
    "released_at": "2020-07-04",
    "total_views": 2797,
    "body": "Encryption is a process of encoding messages such that it can only be read and understood by the intended parties. The process of extracting the original message from an encrypted one is called Decryption. Encryption usually scrambles the original message using a key, called the encryption key, that the involved parties agree on.\n\nIn the previous essay, we went through the [Single-byte XOR cipher](https://arpitbhayani.me/blogs/decipher-single-xor) and found a way to decipher it without having any knowledge of the encryption key. In this essay, we find how to break a [Repeating-key XOR cipher](https://en.wikipedia.org/wiki/XOR_cipher) with variable key length. The problem statement, defined above, is based on [Cryptopals Set 1 Challenge 6](https://cryptopals.com/sets/1/challenges/6).\n\n# Repeating-key XOR Cipher\n\nThe Repeating-key XOR cipher algorithm works with an encryption key with no constraint on its length, which makes it much stronger than a Single-byte XOR Cipher, where the encryption key length was restricted to a single byte.\n\n## Encryption\n\nA plain text is encrypted using an encryption key by performing a bitwise [XOR](https://en.wikipedia.org/wiki/Exclusive_or) operation on every character. The encryption key is repeated until it XORs every single character of the plain text and the resultant stream of bytes is again translated back as characters and sent to the other party. These encrypted bytes need not be among the usual printable characters and should ideally be interpreted as a stream of bytes. Following is the python-based implementation of this encryption process.\n\n```python\ndef repeating_key_xor(text: bytes, key: bytes) -> bytes:\n    \"\"\"Given a plain text `text` as bytes and an encryption key `key`\n    as bytes, the function encrypts the text by performing\n    XOR of all the bytes and the `key` (in repeated manner) and returns\n    the resultant XORed byte stream.\n    \"\"\"\n    \n    # we update the encryption key by repeating it such that it\n    # matches the length of the text to be processed.\n    repetitions = 1 + (len(text) // len(key))\n    key = key * repetitions\n    key = key[:len(text)]\n\n    # XOR text and key generated above and return the raw bytes\n    return bytes([b ^ k for b, k in zip(text, key)])\n```\n\nAs an example, we encrypt the plain text - `secretattack` - with encryption key `$^!` and as per the algorithm, we first repeat the encryption key until it matches the length of the plain text and then XOR it against the plain text. The illustration below shows the entire encryption process.\n\n![https://user-images.githubusercontent.com/4745789/85919742-d1520600-b88b-11ea-8d71-aa36c58dc48a.png](https://user-images.githubusercontent.com/4745789/85919742-d1520600-b88b-11ea-8d71-aa36c58dc48a.png)\n\nFor the first character in plain text - `s` - the byte i.e. ASCII value is `115` which when XORed with `$` results in `87` whose character equivalent is `W`, similarly for the second character `e` the encrypted byte is `;`, for `c` it is `B`, for the fourth character `r`, since the key repeats, the XOR is taken with `$` to get `V` and the process continues. The resultant encrypted text using repeated-key XOR on the plain text `secretattack` with key `$^!` is `W;BV;UE*UE=J`.\n\n## Decryption\n\nDecryption is a process of extracting the original message from the encrypted ciphertext given the encryption key. XOR has a [property](https://brainly.in/question/3038497) - if `a = b ^ c` then `b = a ^ c`, hence the decryption process is exactly the same as the encryption i.e. we first repeat the encryption key till it matches the length and then perform bitwise XOR with the ciphertext - the resultant bytes stream will be the original message.\n\nSince encryption and decryption both have an exact same implementation - we pass the ciphertext to the function `repeating_key_xor`, defined above, to get the original message back.\n\n```python\n>>> repeating_key_xor(b'W;BV;UE*UE=J', b'$^!')\nb'secretattack'\n```\n\n# Deciphering without the encryption key\n\nThings become really interesting when, given the encryption algorithm, we have to recover the original message from the ciphertext with no knowledge of the encryption key. Just like solving any other problem, the crux of deciphering the message encrypted using repeated-key XOR cipher is to break it down into manageable sub-problems and tackle them independently. We break this deciphering problem into the following two sub-problems:\n\n- Finding the length of the Encryption Key\n- Bruteforce with all possible keys and finding the \"most English\" plain text\n\n## Finding the length of the Encryption Key\n\nIn order to recover the original text from the cipher, we first find the length of the encryption key used and then apply brute force with all possible keys of the estimated length and deduce the plain text. Finding the length of the Encryption key makes the deciphering process quicker as it eliminates a lot of false keys and thus reducing the overall effort required during the brute force. In order to find the length of the Encryption Key, we need to have a better understanding of a seemingly unrelated topic - Hamming Distance.\n\n### Hamming Distance\n\nHamming distance between two bytes is the number of positions at which the corresponding bits differ. For a stream of bytes, of equal lengths, it is the sum of Hamming Distances between the corresponding bytes. Finding differences between bits can be efficiently done using bitwise XOR operation as the operation yields `0` when both the bits are the same and `1` when they differ. So for computing Hamming Distance between two bytes we XOR the bytes and count the number of `1` in its binary representation.\n\n```python\ndef hamming_distance_bytes(text1: bytes, text2: bytes) -> int:\n    \"\"\"Given two stream of bytes, the function returns the Hamming Distance\n    between the two.\n    Note: If the two texts have unequal lengths, the hamming distance is\n    computed only till one of the text exhausts, other bytes are not iterated.\n    \"\"\"\n    dist = 0\n    for byte1, byte2 in zip(text1, text2):\n        dist += bin(byte1 ^ byte2).count('1')\n    return dist\n\n>>> hamming_distance_bytes(b'ab', b'zb')\n4\n```\n\nIn the example above, we find that the hamming distance between two bytestreams `ab` and `zb` is `4`, which implies that the byte streams `ab` and `zb` differ at `4` different bits in their binary representations.\n\n### Hamming Score\n\nHamming distance is an absolute measure, hence in order to compare hamming distance across byte streams of varying lengths, it has to be normalized with the number of pairs of bits compared. We name this measure - Hamming Score - which thus is defined as the Hamming Distance per unit bit-pair. In python, Hamming Score is implemented as:\n\n```python\ndef hamming_score_bytes(text1: bytes, text2: bytes) -> float:\n    \"\"\"Given two streams of bytes, the function computes a normalized Hamming\n    Score based on the Hamming distance.\n    Normalization is done by dividing the Hamming Distance by the number of bits\n    present in the shorter text.\n    \"\"\"\n    return hamming_distance_bytes(text1, text2) / (8 * min(len(text1), len(text2)))\n\n>>> hamming_score_bytes(b'ab', b'zb')\n0.25\n```\n\n### What can we infer through Hamming Distance?\n\nHamming Distance is an interesting measure; it effectively tells us the minimum number of bit flips required to convert one bytestream into another. It also implies that (on average) if the numerical values of two bytestreams are closer then their Hamming Distance and Hamming Score will be lower i.e it would take fewer bit flips to convert one into another.\n\nThis is evident from the fact that the average Hamming distance between any two bytes `[0-256)` picked at random is `3.9999` while that of any two lowercased English characters `[97, 122]` is just `2.45`. Similar ratios are observed for Hamming Score where `0.4999` is of the former while `0.3072` is of the later.\n\nThis inference comes in handy when we want to find out the length of Encryption Key in Repeating-key XOR Cipher as illustrated in the section below.\n\n### Formal definition of encryption and decryption processes\n\nSay if `p` denotes the plaintext, `k` denotes the encryption key which is repeated to match the length of the plain text, and `c` denotes the ciphertext, we could define encryption and decryption processes as\n\n```python\nencryption: c[i] = p[i] XOR k[i]   for i in [0, len(c))\ndecryption: p[i] = c[i] XOR k[i]   for i in [0, len(p))\n```\n\nThe above definitions, along with the rules of XOR operations, implying that if we XOR two bytes of the ciphertext, encrypted (XORed) using the same byte of the encryption key, we are effectively XORing the corresponding bytes of the plain text. If `k'` is the byte of the encryption key `k` which was used to encrypt (XOR) the bytes `p[i]` and `p[j]` of the plain text to generate `c[i]` and `c[j]` of the ciphertext, we could derive the following relation\n\n```\n# k' is the common byte of the key i.e. k' = k[i] = k[j]\n\nc[i] XOR c[j] = (p[i] XOR k') XOR (p[j] XOR k')\n              = p[i] XOR k' XOR k' XOR p[j]\n              = p[i] XOR 0 XOR p[j]\n              = p[i] XOR p[j]\n```\n\nThe above relation, `c[i] XOR c[j]` equal to `p[i] XOR p[j]`, holds true only because both the bytes were XORed with the same byte `k'` of the encryption key; which in fact helped reduce the expression. If the byte from the encryption key which was used to XOR the pain texts were different then the relation was irreducible and we could not have possibly setup this relation.\n\n### Chunking of ciphertext\n\nChunking is the process where the ciphertext is split into smaller chunks (segments) of almost equal lengths. For example, chunking the ciphertext `W;BV;UE*UE=J` for chunk length `4` would create `3` chunks `W;BV`, `;UE*` and `UE=J`. The illustration below shows the chunks that would be formed for `W;BV;UE*UE=J` with chunks lengths varying from 2 to 6.\n\n![https://user-images.githubusercontent.com/4745789/86434084-24a7d680-bd1a-11ea-8346-aad7b42bab1c.png](https://user-images.githubusercontent.com/4745789/86434084-24a7d680-bd1a-11ea-8346-aad7b42bab1c.png)\n\n### XOR of the chunks\n\nSomething very interesting happens when we compute the Average Hamming Score for all possible chunk lengths. If we consider the ciphertext `b'W;BV;UE*UE=J` and we chunk it with lengths varying from 2 to 6, we get the following distribution for the Average Hamming Score for each of the chunk length.\n\n![https://user-images.githubusercontent.com/4745789/86473899-6149f100-bd5f-11ea-908a-d4adabff1cf0.png](https://user-images.githubusercontent.com/4745789/86473899-6149f100-bd5f-11ea-908a-d4adabff1cf0.png)\n\nFrom the distribution above it is evident that the score was minimum at chunk length equalling `3`, which actually was the length of the Encryption Key used on the plain text. Is this mere coincidence or are we onto something?\n\nWhen chunk length is equal to the length of the encryption key, the XOR operation on any two chunks will reduce the expression to XOR of the corresponding plain texts (as seen above), because there will be a perfect alignment of bytes from ciphertext and bytes from the keys i.e every `i`th byte from both the chunks would have been XORed with `i`th byte from the encryption key.\n\nWe have established that for chunk length equal to the length of the encryption key `c[i] XOR c[j]` is effectively `p[i] XOR p[j]`. Since we have assumed that the plain text is a lowercased English sentence the XOR is happening between bytes residing numerically closer to each other and hence has a lower Average Hamming Score between them; because of which we see a minimum at this particular chunk length. The Hamming Score will be much higher for lengths other than the length of Encryption Key because during XOR operation the expression stays irreducible and hence hamming distance is computed panning the entire range of bytes `[0, 256)`.\n\n### Something far more interesting\n\nThis minimum does not only hold true for chunk length equal to the length of the encryption key, but it also holds true when the length of the chunk is a multiple of the length of the encryption key. This happens because for repeated keys when the chunk length is a multiple of Encryption Key there will be a perfect alignment of bytes such that every `i`th byte of chunks is XORed with `i`th byte of the encryption key; which sets up the relation `c[i] XOR c[j]` equalling `p[i] XOR p[j]`.\n\n![https://user-images.githubusercontent.com/4745789/86473953-7cb4fc00-bd5f-11ea-83af-f22413e1ecf9.png](https://user-images.githubusercontent.com/4745789/86473953-7cb4fc00-bd5f-11ea-83af-f22413e1ecf9.png)\n\nThe above distribution shows a lot of sharp drops of Average Hamming Score for chunk lengths that are multiples of `7` - the length of the encryption key used.\n\n## Computing Encryption Key Length\n\nNow that we understand the theory and concept behind the process of finding the length of the Encryption Key, we can compile the logic into a function that accepts `text` and bytes and returns the length of the Encryption Key as illustrated below\n\n```python\ndef compute_key_length(text: bytes) -> int:\n    \"\"\"The function returns the length of the encryption key\n    by chunking and minimizing the Average Hamming Score\n    \"\"\"\n    min_score, key_len = None, None\n\n    # We check for chunk lengths from 2 till the half the length of the\n    # plain text. Here we assume that the Encryption Key had to be\n    # repeated at least twice to match the length of the plaintext\n    for klen in range(2, math.ceil(len(text)/2)):\n\n        # We create chunks such that length of each chunk if `klen`\n        chunks = [\n            text[i: i+klen]\n            for i in range(0, len(text), klen)\n        ]\n\n        # To gain better accuracy we get rid of the last chunk that had\n        # length smaller than klen/2\n        if len(chunks) >= 2 and len(chunks[-1]) <= len(chunks[-2])/2:\n            chunks.pop()\n        \n        # For each chunk length, for every pair of chunks we compute the\n        # Hamming Score and keep piling it in a list.\n        _scores = []\n        for i in range(0, len(chunks) - 1, 1):\n            for j in range(i+1, len(chunks), 1):\n                score = hamming_score_bytes(chunks[i], chunks[j])\n                _scores.append(score)\n\n        # The Hamming Score for a chunk length is the average\n        # hamming score computed over all possible pairs of chunks\n        score = sum(_scores) / len(_scores)\n        \n        # Keep track of the minimum score we have seen and the key length\n        # corresponding to it.\n        if min_score is None or score < min_score:\n            min_score, key_len = score, klen\n\n    # return the key length corresponding to the minimum score\n    return key_len\n```\n\n## Bruteforce to recover the original text\n\nThe function `compute_key_length` returns the length of the Encryption Key used to encrypt the plain text. Once we know the length, we can apply Bruteforce with all possible keys of that length and try to decipher the ciphertext. The approach of deciphering will be very similar to how it was done to [Decipher single-byte XOR Ciphertext](https://arpitbhayani.me/blogs/decipher-single-xor) i.e. by using [Letter Frequency Distribution](https://en.wikipedia.org/wiki/Letter_frequency) and Fitting Quotient to find which key leads to the plain text that is closest to a genuine English sentence.\n\nA test was run on 100 random English sentences with random Encryption keys of varying lengths and it was found that this deciphering technique worked with an accuracy of 99%. Even though the approach is not fool-proof, it does pretty well in eliminating keys that would definitely not result in a correct plain text.\n\n# Conclusion\n\nDeciphering a repeated-key XOR Cipher could also be done using [Kasiski examination](https://en.wikipedia.org/wiki/Kasiski_examination); the method we saw in this essay was Friedman Test using Hamming Distance and Frequency Analysis. The main purpose of this essay was to showcase how seemingly unrelated concepts work together to solve an interesting problem efficiently.\n\n# References\n\n- [Vigen\u00e8re Cipher](https://en.wikipedia.org/wiki/Vigen%C3%A8re_cipher)\n- [Repeating-key XOR Cipher](https://en.wikipedia.org/wiki/XOR_cipher)\n- [Cryptopals Set 1 Challenge 6](https://cryptopals.com/sets/1/challenges/6)\n",
    "similar": [
      "decipher-single-xor",
      "bayesian-average",
      "lfsr",
      "isolation-forest"
    ]
  },
  {
    "id": 44,
    "topic": null,
    "uid": "decipher-single-xor",
    "title": "Deciphering Single-byte XOR Ciphertext",
    "description": "Deciphering is the process of recovering the original message from an encrypted byte stream, usually, without having any knowledge of the encryption key. In this essay, we look at how we can use linguistics to recover the original message from a Single-byte XORed Ciphertext.",
    "gif": "https://media.giphy.com/media/l3vRmVv5P01I5NDAA/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/85223233-0ddeb700-b3df-11ea-9bb8-5bd1cf0cabcb.png",
    "released_at": "2020-06-21",
    "total_views": 1999,
    "body": "Encryption is a process of encoding messages such that it can only be read and understood by the intended parties. The process of extracting the original message from an encrypted one is called Decryption. Encryption usually scrambles the original message using a key, called encryption key, that the involved parties agree on.\n\nThe strength of an encryption algorithm is determined by how hard it would be to extract the original message without knowing the encryption key. Usually, this depends on the number of bits in the key - bigger the key, the longer it takes to decrypt the enciphered data.\n\nIn this essay, we will work with a very simple cipher (encryption algorithm) that uses an encryption key with a size of one byte, and try to decipher the ciphered text and retrieve the original message without knowing the encryption key. The problem statement, defined above, is based on [Cryptopals Set 1 Challenge 3](https://cryptopals.com/sets/1/challenges/3).\n\n# Single-byte XOR cipher\n\nThe Single-byte XOR cipher algorithm works with an encryption key of size 1 byte - which means the encryption key could be one of the possible 256 values of a byte. Now we take a detailed look at how the encryption and decryption processes look like for this cipher.\n\n## Encryption\n\nAs part of the encryption process, the original message is iterated bytewise and every single byte `b` is XORed with the encryption key `key` and the resultant stream of bytes is again translated back as characters and sent to the other party. These encrypted bytes need not be among the usual printable characters and should ideally be interpreted as a stream of bytes. Following is the python-based implementation of the encryption process.\n\n```python\ndef single_byte_xor(text: bytes, key: int) -> bytes:\n    \"\"\"Given a plain text `text` as bytes and an encryption key `key` as a byte\n    in range [0, 256) the function encrypts the text by performing\n    XOR of all the bytes and the `key` and returns the resultant.\n    \"\"\"\n    return bytes([b ^ key for b in text])\n```\n\nAs an example, we can try to encrypt the plain text - `abcd` - with encryption key `69` and as per the algorithm, we perform XOR bytewise on the given plain text. For character `a`, the byte i.e. ASCII value is `97` which when XORed with `69` results in `36` whose character equivalent is `$`, similarly for `b` the encrypted byte is `'`, for `c` it is `&` and for `d` it is `!`. Hence when `abcd` is encrypted using single-byte XOR cipher and encryption key `69`, the resultant ciphertext i.e. the encrypted message is `$'&!`.\n\n![https://user-images.githubusercontent.com/4745789/85209379-0b377f80-b355-11ea-8206-54ad558b4a6f.png](https://user-images.githubusercontent.com/4745789/85209379-0b377f80-b355-11ea-8206-54ad558b4a6f.png)\n\n## Decryption\n\nDecryption is the process of extracting the original message from the encrypted ciphertext given the encryption key. XOR has a [property](https://brainly.in/question/3038497) - if `a = b ^ c` then `b = a ^ c`, hence the decryption process is exactly the same as the encryption i.e. we iterate through the encrypted message bytewise and XOR each byte with the encryption key - the resultant will be the original message.\n\nSince encryption and decryption both have an exact same implementation - we pass the ciphertext to the function `single_byte_xor`, defined above, to get the original message back.\n\n```python\n>>> single_byte_xor(b\"$'&!\", 69)\nb'abcd'\n```\n\n# Deciphering without the encryption key\n\nThings become really interesting when we have to recover the original message given the ciphertext and having no knowledge of the encryption key; although we do know the encryption algorithm.\n\nAs a sample plain text, we take the last couple of messages, sent across on their German military radio network during World War II. These messages were intercepted and decrypted by the British troops. During wartime, the messages were encrypted using [Enigma Machine](https://en.wikipedia.org/wiki/Enigma_machine) and [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing) famously [cracked the Enigma Code](https://www.iwm.org.uk/history/how-alan-turing-cracked-the-enigma-code) (similar to encryption key) that was used to encipher German messages.\n\n![https://user-images.githubusercontent.com/4745789/85209533-72096880-b356-11ea-8a84-97f2feb86b44.png](https://user-images.githubusercontent.com/4745789/85209533-72096880-b356-11ea-8a84-97f2feb86b44.png)\n\n> In this essay, instead of encrypting the message using the Enigma Code, we are going to use Single-byte XOR cipher and try to recover the original message back without any knowledge of the encryption key.\n\nHere, we assume that the original message, to be encrypted, is a genuine English lowercased sentence. The ciphertext that we would try to decipher can be obtained as\n\n```python\n>>> key = 82\n>>> plain_text = b'british troops entered cuxhaven at 1400 on 6 may - from now on all radio traffic will cease - wishing you all the best. lt kunkel.'\n>>> single_byte_xor(plain_text, key)\nb'0 ;&;!:r& ==\"!r7<&7 76r1\\'*:3$7<r3&rcfbbr=<rdr?3+r\\x7fr4 =?r<=%r=<r3>>r 36;=r& 344;1r%;>>r173!7r\\x7fr%;!:;<5r+=\\'r3>>r&:7r07!&|r>&r9\\'<97>|'\n```\n\n## Bruteforce\n\nThere are a very limited number of possible encryption keys - 256 to be exact - we can, very conveniently, go for the Bruteforce approach and try to decrypt the ciphered text with every single one of it. So we start iterating over all keys in the range `[0, 256)` and decrypt the ciphertext and see which one resembles the original message the most.\n\n![https://user-images.githubusercontent.com/4745789/85209704-ad586700-b357-11ea-8b7c-4d4616af609a.png](https://user-images.githubusercontent.com/4745789/85209704-ad586700-b357-11ea-8b7c-4d4616af609a.png)\n\nIn the illustration above, we see that the message decrypted through key `82` is, in fact, our original message, while the other retrieved plain texts look scrambled and garbage. Doing this visually is very easy; we, as humans, are able to comprehend familiarity but how will a computer recognize this?\n\nWe need a way to quantify the closeness of a text to a genuine English sentence. Closer the decrypted text is to be a genuine English sentence, the closer it would be to our original plain text.\n\n> We can do this only because of our assumption - that the original plain text is a genuine English sentence.\n\n## ETAOIN SHRDLU\n\nLetter Frequency is the number of times letters of an alphabet appear on average in written language. In the English language the letter frequency of letter `a` is `8.239%`, for `b` it is `1.505%` which means out of 100 letters written in English, the letter `a`, on an average, will show up `8.239%` of times while `b` shows up `1.505%` of times. Letter frequency (in percentage) for other letters is as shown below.\n\n```python\noccurance_english = {\n    'a': 8.2389258,    'b': 1.5051398,    'c': 2.8065007,    'd': 4.2904556,\n    'e': 12.813865,    'f': 2.2476217,    'g': 2.0327458,    'h': 6.1476691,\n    'i': 6.1476691,    'j': 0.1543474,    'k': 0.7787989,    'l': 4.0604477,\n    'm': 2.4271893,    'n': 6.8084376,    'o': 7.5731132,    'p': 1.9459884,\n    'q': 0.0958366,    'r': 6.0397268,    's': 6.3827211,    't': 9.1357551,\n    'u': 2.7822893,    'v': 0.9866131,    'w': 2.3807842,    'x': 0.1513210,\n    'y': 1.9913847,    'z': 0.0746517\n}\n```\n\nThis Letter Frequency analysis is a rudimentary way for language identification in which we see if the current letter frequency distribution of a text matches the average letter frequency distribution of the English language. [ETAOIN SHRDLU](https://en.wikipedia.org/wiki/Etaoin_shrdlu) is the approximate order of frequency of the 12 most commonly used letters in the English language.\n\nThe following chart shows Letter Frequency analysis for decrypted plain texts with encryption keys from `79` to `84`.\n\n![https://user-images.githubusercontent.com/4745789/85209804-5a32e400-b358-11ea-8e1b-2b6bb3e22868.png](https://user-images.githubusercontent.com/4745789/85209804-5a32e400-b358-11ea-8e1b-2b6bb3e22868.png)\n\nIn the illustration above, we could clearly see how well the Letter Frequency distribution for encryption key `82` fits the distribution of the English language. Now that our hypothesis holds true, we need a way to quantify this measure and we call if the Fitting Quotient.\n\n## Fitting Quotient\n\nFitting Quotient is the measure that suggests how well the two Letter Frequency Distributions match. Heuristically, we define the Fitting Quotient as the average of the absolute difference between the frequencies (in percentage) of letters in `text` and the corresponding letter in the English Language. Thus having a smaller value of Fitting Quotient implies the text is closer to the English Language.\n\n![https://user-images.githubusercontent.com/4745789/85219888-f2ff4900-b3c4-11ea-933a-96e26580a3fb.png](https://user-images.githubusercontent.com/4745789/85219888-f2ff4900-b3c4-11ea-933a-96e26580a3fb.png)\n\nPython-based implementation of the, above defined, Fitting Quotient is as shown below. The function first computes the relative frequency for each letter in `text` and then takes an average of the absolute difference between the two distributions.\n\n```python\ndist_english = list(occurance_english.values())\n\ndef compute_fitting_quotient(text: bytes) -> float:\n    \"\"\"Given the stream of bytes `text` the function computes the fitting\n    quotient of the letter frequency distribution for `text` with the\n    letter frequency distribution of the English language.\n\n    The function returns the average of the absolute difference between the\n    frequencies (in percentage) of letters in `text` and the corresponding\n    letter in the English Language.\n    \"\"\"\n    counter = Counter(text)\n    dist_text = [\n        (counter.get(ord(ch), 0) * 100) / len(text)\n        for ch in occurance_english\n    ]\n    return sum([abs(a - b) for a, b in zip(dist_english, dist_text)]) / len(dist_text)\n```\n\n## Deciphering\n\nNow that we have everything we require to directly get the plain text out of the given ciphertext we wrap it in a function that iterates over all possible encryption keys in the range `[0, 256)`, decrypts the ciphertext, computes the fitting quotient for the plain text and returns the one that minimizes the quotient as the original message. Python-based implementation of this deciphering logic is as illustrated below.\n\n```python\ndef decipher(text: bytes) -> Tuple[bytes, int]:\n    \"\"\"The function deciphers an encrypted text using Single Byte XOR and returns\n    the original plain text message and the encryption key.\n    \"\"\"\n    original_text, encryption_key, min_fq = None, None, None\n    for k in range(256):\n        # we generate the plain text using encryption key `k`\n        _text = single_byte_xor(text, k)\n        \n        # we compute the fitting quotient for this decrypted plain text\n        _fq = compute_fitting_quotient(_text)\n        \n        # if the fitting quotient of this generated plain text is lesser\n        # than the minimum seen till now `min_fq` we update.\n        if min_fq is None or _fq < min_fq:\n            encryption_key, original_text, min_fq = k, _text, _fq\n\n    # return the text and key that has the minimum fitting quotient\n    return original_text, encryption_key\n```\n\nThis approach was also tested against 100 random English sentences with random Encryption keys and it was found that this deciphering technique fared well for all the samples. The approach would fail if the sentence is very short or contains a lot of symbols. The source code for this entire deciphering process is available in a Jupyter notebook at [arpitbhayani.me/decipher-single-byte-xor](https://github.com/arpitbbhayani/decipher-single-byte-xor/blob/master/decipher-single-byte-xor.ipynb).\n\n# References\n\n- [Etaoin shrdlu](https://en.wikipedia.org/wiki/Etaoin_shrdlu)\n- [English Letter Frequency](https://en.wikipedia.org/wiki/Letter_frequency)\n- [Single-byte XOR encryption](https://wiki.bi0s.in/crypto/xor/#single-byte-xor-cipher)\n- [Cryptopals Challenge - Set 1 Challenge 3](https://cryptopals.com/sets/1/challenges/3)\n",
    "similar": [
      "decipher-repeated-key-xor",
      "bayesian-average",
      "lfsr",
      "isolation-forest"
    ]
  },
  {
    "id": 45,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "python-iterable-integers",
    "title": "Making Python Integers Iterable",
    "description": "In Python, Integers are not iterables but we can make them iterable by implementing __iter__ function. In this essay, we change Python's source code and implement iter function for integers.",
    "gif": "https://media.giphy.com/media/k4ta29T68xlfi/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/84585100-bf06af80-ae29-11ea-8797-16c70aee5cc4.png",
    "released_at": "2020-06-14",
    "total_views": 1790,
    "body": "Iterables in Python are objects and containers that could be stepped through one item at a time, usually using a `for ... in` loop. Not all objects can be iterated, for example - we cannot iterate an integer, it is a singular value. The best we can do here is iterate on a range of integers using the `range` type which helps us iterate through all integers in the range `[0, n)`.\n\nSince integers, individualistically, are not iterable, when we try to do a `for x in 7`, it raises an exception stating `TypeError: 'int' object is not iterable`. So what if, we change the Python's source code and make integers iterable, say every time we do a `for x in 7`, instead of raising an exception it actually iterates through the values `[0, 7)`. In this essay, we would be going through exactly that, and the entire agenda being:\n\n- What is a Python iterable?\n- What is an iterator protocol?\n- Changing Python's source code and make integers iterable, and\n- Why it might be a bad idea to do so?\n\n# Python Iterables\n\nAny object that could be iterated is an Iterable in Python. The list has to be the most popular iterable out there and it finds its usage in almost every single Python application - directly or indirectly. Even before the first user command is executed, the Python interpreter, while booting up, has already created `406` lists, for its internal usage.\n\nIn the example below, we see how a list `a` is iterated through using a `for ... in` loop and each element can be accessed via variable `x`. \n\n```python\n>>> a = [2, 3, 5, 7, 11, 13]\n>>> for x in a: print(x, end=\" \")\n2 3 5 7 11 13\n```\n\nSimilar to `list`, `range` is a python type that allows us to iterate on integer values starting with the value `start` and going till `end` while stepping over `step` values at each time. `range` is most commonly used for implementing a C-like `for` loop in Python. In the example below, the `for` loop iterates over a `range` that starts from `0`, goes till `7` with a step of  `1` - producing the sequence `[0, 7)`.\n\n```python\n# The range(0, 7, 1) will iterate through values 0 to 6 and every time\n# it will increment the current value by 1 i.e. the step.\n>>> for x in range(0, 7, 1): print(x, end=\" \")\n0 1 2 3 4 5 6\n```\n\nApart from `list` and `range` other [iterables](https://docs.python.org/3/library/stdtypes.html#sequence-types-list-tuple-range) are - `tuple`, `set`, `frozenset`, `str`, `bytes`, `bytearray`, `memoryview`, and `dict`. Python also allows us to create custom iterables by making objects and types follow the [Iterator Protocol](https://docs.python.org/3/c-api/iter.html).\n\n# Iterators and Iterator Protocol\n\nPython, keeping things simple, defines iterable as any object that follows the [Iterator Protocol](https://docs.python.org/3/c-api/iter.html); which means the object or a container implements the following functions\n\n- `__iter__` should return an iterator object having implemented the `__next__` method\n- `__next__` should return the next item of the iteration and if items are exhausted then raise a `StopIteration` exception.\n\nSo, in a gist, `__iter__` is something that makes any python object iterable; hence to make integers iterable we need to have `__iter__` function set for integers.\n\n# Iterable in CPython\n\nThe most famous and widely used implementation of Python is [CPython](https://github.com/python/cpython/) where the core is implemented in pure C. Since we need to make changes to one of the core datatypes of Python, we will be modifying CPython, add `__iter__` function to an Integer type, and rebuild the binary. But before jumping into the implementation, it is important to understand a few fundamentals.\n\n## The `PyTypeObject`\n\nEvery object in Python is associated with a type and each [type](https://docs.python.org/3/c-api/typeobj.html#type-objects) is an instance of a struct named [PyTypeObject](https://docs.python.org/3/c-api/typeobj.html). A new instance of this structure is effectively a new type in python. This structure holds a few meta information and a bunch of C function pointers - each implementing a small segment of the type's functionality. Most of these \"slots\" in the structure are optional which could be filled by putting appropriate function pointers and driving the corresponding functionality.\n\n## The `tp_iter` slot\n\nAmong all the slots available, the slot that interests us is the `tp_iter` slot which can hold a pointer to a function that returns an iterator object. This slot corresponds to the `__iter__` function which effectively makes the object iterable. A non `NULL` value of this slot indicates iterability. The `tp_iter` holds the function with the following signature\n\n```cpp\nPyObject * tp_iter(PyObject *);\n```\n\nIntegers in Python do not have a fixed size; rather the size of integer depends on the value it holds. [How Python implements super long integers](https://arpitbhayani.me/blogs/super-long-integers) is a story on its own but the core implementation can be found at [longobject.c](https://github.com/python/cpython/blob/master/Objects/longobject.c). The instance of `PyTypeObject` that defines integer/long type is `PyLong_Type` and has its `tp_iter` slot set to `0` i.e. `NULL` which asserts the fact that Integers in python are not iterable.\n\n```cpp\nPyTypeObject PyLong_Type = {\n    ...\n\n    \"int\",                                      /* tp_name */\n    offsetof(PyLongObject, ob_digit),           /* tp_basicsize */\n    sizeof(digit),                              /* tp_itemsize */\n    ...\n    0,                                          /* tp_iter */\n    ...\n};\n```\n\nThis `NULL` value for `tp_iter` makes `int` object not iterable and hence if this slot was occupied by an appropriate function pointer with the aforementioned signature, this could well make any integer iterable.\n\n# Implementing `long_iter`\n\nNow we implement the `tp_iter` function on integer type, naming it `long_iter`, that returns an iterator object, as required by the convention. The core functionality we are looking to implement here is - when an integer `n` is iterated, it should iterate through the sequence `[0, n)` with step `1`. This behavior is very close to the pre-defined `range` type, that iterates over a range of integer values, more specifically a `range` that starts at `0`, goes till `n` with a step of `1`.\n\nWe define a utility function in `rangeobject.c` that, given a python integer, returns an instance of `longrangeiterobject` as per our specifications. This utility function will instantiate the `longrangeiterobject` with start as `0`, ending at the long value given in the argument, and step as `1`. The utility function is as illustrated below.\n\n```cpp\n/*\n *  PyLongRangeIter_ZeroToN creates and returns a range iterator on long\n *  iterating on values in the range [0, n).\n *\n *  The function creates and returns a range iterator from 0 till the\n *  provided long value.\n */\nPyObject *\nPyLongRangeIter_ZeroToN(PyObject *long_obj)\n{\n    // creating a new instance of longrangeiterobject\n    longrangeiterobject *it;\n    it = PyObject_New(longrangeiterobject, &PyLongRangeIter_Type);\n\n    // if unable to allocate memoty to it, return NULL.\n    if (it == NULL)\n        return NULL;\n\n    // we set the start to 0\n    it->start = _PyLong_Zero;\n\n    // we set the step to 1\n    it->step = _PyLong_One;\n\n    // we set the index to 0, since we want to always start from the first\n    // element of the iteration\n    it->index = _PyLong_Zero;\n\n    // we set the total length of iteration to be equal to the provided value\n    it->len = long_obj;\n\n    // we increment the reference count for each of the values referenced\n    Py_INCREF(it->start);\n    Py_INCREF(it->step);\n    Py_INCREF(it->len);\n    Py_INCREF(it->index);\n\n    // downcast the iterator instance to PyObject and return\n    return (PyObject *)it;\n}\n```\n\nThe utility function `PyLongRangeIter_ZeroToN` is defined in `rangeobject.c` and will be declared in `rangeobject.h` so that it can be used across the CPython. Declaration of function in `rangeobject.h` using standard Python macros goes like this\n\n```cpp\nPyAPI_FUNC(PyObject *)   PyLongRangeIter_ZeroToN(PyObject *);\n```\n\nThe function occupying the `tp_iter` slot will receive the `self` object as the input argument and is expected to return the iterator instance. Hence, the `long_iter` function will receive the python integer object (self) that is being iterated as an input argument and it should return the iterator instance. Here we would use the utility function `PyLongRangeIter_ZeroToN`, we just defined, which is returning us an instance of range iterator. The entire `long_iter` function could be defined as\n\n```cpp\n/*\n *  long_iter creates an instance of range iterator using PyLongRangeIter_ZeroToN\n *  and returns the iterator instance.\n *\n *  The argument to the `tp_iter` is the `self` object and since we are trying to\n *  iterate an integer here, the input argument to `long_iter` will be the\n *  PyObject of type PyLong_Type, holding the integer value.\n */\nstatic PyObject * long_iter(PyObject *long_obj)\n{\n    return PyLongRangeIter_ZeroToN(long_obj);\n}\n```\n\nNow that we have `long_iter` defined, we can place the function on the `tp_iter` slot of `PyLong_Type` that enables the required iterability on integers.\n\n```cpp\nPyTypeObject PyLong_Type = {\n    ...\n\n    \"int\",                                      /* tp_name */\n    offsetof(PyLongObject, ob_digit),           /* tp_basicsize */\n    sizeof(digit),                              /* tp_itemsize */\n    ...\n    long_iter,                                  /* tp_iter */\n    ...\n};\n```\n\n## Consolidated flow\n\nOnce we have everything in place, the entire flow goes like this -\n\nEvery time an integer is iterated, using any iteration method - for example `for ... in`, it would check the `tp_iter` of the `PyLongType` and since now it holds the function pointer `long_iter`, the function will be invoked. This invocation will return an iterator object of type `longrangeiterobject` with a fixed start, index, and step values - which in pythonic terms is effectively a `range(0, n, 1)`.  Hence the `for x in 7` is inherently evaluated as `for x in range(0, 7, 1)` allowing us to iterate integers.\n\n> These changes are also hosted on a remote branch [cpython@02-long-iter](https://github.com/arpitbbhayani/cpython/tree/02-long-iter) and Pull Request holding the `diff` can be found [here](https://github.com/arpitbbhayani/cpython/pull/7).\n\n# Integer iteration in action\n\nOnce we build a new python binary with the aforementioned changes, we can see iterable integers in actions. Now when we do `for x in 7`, instead of raising an exception, it actually iterates through values `[0, 7)`.\n\n```cpp\n>>> for i in 7: print(i, end=\" \");\n0 1 2 3 4 5 6\n\n# Since integers are now iterable, we can create a list of [0, 7) using `list`\n# Internally `list` tries to iterate on the given object i.e. `7`\n# now that the iteration is defined as [0, 7) we get the list from\n# from iteration, instead of an exception\n>>> list(7)\n[0, 1, 2, 3, 4, 5, 6]\n```\n\n# Why it is not a good idea\n\nAlthough it seems fun, and somewhat useful, to have iterable integers, it is really not a great idea. The core reason for this is that it makes unpacking unpredictable. Unpacking is when you unpack an iterable and assign it to multiple variables. For example: `a, b = 3, 4` will assign 3 to a and 4 to b. So assigning `a, b = 7` should be an error because there is just one value on the right side and multiple on the left.\n\nUnpacking treats right-hand size as iterable and tries to iterate on it; and now since Integers are iterable the right-hand side, post iteration yields 7 values which the left-hand side has mere 2 variables; Hence it raises an exception `ValueError: too many values to unpack (expected 2)`.\n\nThings would work just fine if we do `a, b = 2` as now the right-hand side, post iteration, has two values, and the left-hand side has two variables. Thus two very similar statements result in two very different outcomes, making unpacking unpredictable.\n\n```python\n>>> a, b = 7\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: too many values to unpack (expected 2)\n\n>>> a, b = 2\n>>> a, b\n0, 1\n```\n\n# Conclusion\n\nIn this essay, we modified the Python's source code and made integers iterable. Even though it is not a good idea to do so, but it is fun to play around with the code and make changes in our favorite programming language. It helps us get a detailed idea about core python implementation and may pave the way for us to become a Python core developer. This is one of many articles in Python Internals series - [How python implements super long integers?](https://arpitbhayani.me/blogs/super-long-integers) and [Python Caches Integers](https://arpitbhayani.me/blogs/python-caches-integers).\n\n# References\n - [PyTypeObject](https://docs.python.org/3/c-api/type.html#c.PyTypeObject)\n - [Python Type Objects](https://docs.python.org/3/c-api/typeobj.html)\n - [Python Iterator Protocol](https://docs.python.org/3/c-api/iter.html)\n - [CPython with long_iter](https://github.com/arpitbbhayani/cpython/pull/7)\n",
    "similar": [
      "python-caches-integers",
      "python-prompts",
      "constant-folding-python",
      "i-changed-my-python"
    ]
  },
  {
    "id": 46,
    "topic": null,
    "uid": "inheritance-c",
    "title": "Powering Inheritance in C using Structure Composition",
    "description": "C language does not support inheritance however it does support Structure Compositions which can be tweaked to serve use-cases requiring parent-child relationships. In this article we find out how Structure Compositions help us emulate inheritance in C and keep our code extensible. We will also find how it powers two of the most important things to have ever been invented in the field of computer science.",
    "gif": "https://media.giphy.com/media/3o6Mbk7C7HVuhyqX3G/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/83960768-2bbdff00-a8aa-11ea-9e82-16b928d2a244.png",
    "released_at": "2020-06-07",
    "total_views": 3850,
    "body": "C language does not support inheritance however it does support Structure Compositions which can be tweaked to serve use-cases requiring parent-child relationships. In this article, we find out how Structure Compositions help us emulate inheritance in C and keep our code extensible. We will also find how it powers two of the most important things to have ever been invented in the field of computer science.\n\n# What is structure composition?\n\nStructure Composition is when we put one structure within another, not through its pointer but as a native member - something like this\n\n```cpp\n// this structure defines a node of a linked list and\n// it only holds the pointers to the next and the previous\n// nodes in the linked list.\nstruct list_head {\n\tstruct list_head *next; // pointer to the node next to the current one\n\tstruct list_head *prev; // pointer to the node previous to the current one\n};\n\n// list_int holds an list_head and an integer data member\nstruct list_int {\n\tstruct list_head list;  // common next and prev pointers\n\tint value;              // specific member as per implementation\n};\n\n// list_int holds an list_head and an char * data member\nstruct list_str {\n\tstruct list_head list;  // common next and prev pointers\n\tchar * str;             // specific member as per implementation\n};\n```\n\nIn the example above, we define a node of a linked list using structure composition. Usually, a linked list node has 3 members - two pointers to adjacent nodes (next and previous) and a third one could either be the data or a pointer to it.  The defining factor of a linked list is the two pointers that logically form a chain of nodes. To keep things abstract we create a struct named `list_head` which holds these two pointers  `next` and `prev` and omits the specifics i.e. data.\n\nUsing `list_head` structure, if we were to define a node of a linked list holding an integer value we could create another struct, named `list_int` that holds a member of type `list_head` and an integer value `value`. The next and previous pointers are brought into this struct through `list_head list` and could be referred to as `list.next` and `list.prev`.\n\n> There is a very genuine reason for picking such weird names for a linked list node and members of structures; the reason to do so will be cleared in the later sections of this essay.\n\nBecause of the above structure definition, building a linked list node holding of any type becomes a breeze. For example, a node holding string could be quickly defined as a struct `list_str` having `list_head` and a `char *`. This ability to extend `list_head` and build a node holding data of any type and any specifics make low-level code simple, uniform, and extensible.\n\n## Memory Representation of `list_int`\n\nStructures in C are not padded and they do not even hold any meta information, not even for the member names; hence during allocation, they are allocated the space just enough to hold the actual data.\n\n![https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png](https://user-images.githubusercontent.com/4745789/83953834-694a6a00-a861-11ea-8ff7-fa69af6af7d6.png)\n\nIn the illustration above we see how members of `list_int` are mapped on the allocated space - required by its individual members. It is allocated a contiguous space of 12 bytes - 4 bytes for each of the two pointers and another 4 bytes for the integer value. The contiguity of space allocation and order of members during allocation could be verified by printing out their addresses as shown below.\n\n```cpp\nvoid print_addrs() {\n    // creating a node of the list_int holding value 41434\n    struct list_int *ll = new_list_int(41434);\n\n    // printing the address of individual members\n    printf(\"%p: head\\n\",             head);\n    printf(\"%p: head->list.next\\n\",  &((head->list).next));\n    printf(\"%p: head->list.prev\\n\",  &((head->list).prev));\n    printf(\"%p: head->value\\n\",      &(head->value));\n}\n\n~ $ make && ./a.out\n0x4058f0: head\n0x4058f0: head->list.next\n0x4058f4: head->list.prev\n0x4058f8: head->value\n```\n\nWe clearly see all the 3 members, occupying 12 bytes contiguous memory segments in order of their definition within the struct.\n\n> The above code was executed on a machine where the size of integer and pointers were 4 bytes each. The results might differ depending on the machine and CPU architecture.\n\n## Casting pointers pointing to struct\n\nIn C language, when a pointer to a struct is cast to a pointer to another struct, the engine maps the individual members of a target struct type, depending on their order and offsets, on to the slice of memory of the source struct instance.\n\nWhen we cast `list_int *` into `list_head *`, the engine maps the space required by target type i.e. `list_head` on space occupied by `list_int`. This means it maps the 8 bytes required by `list_head` on the first 8 bytes occupied by `list_int` instance. Going by the memory representation discussed above, we find that the first 8 bytes of `list_int` are in fact `list_head`, and hence casting `list_int *` to `list_head *` is effectively just referencing the `list_head` member of `list_int` through a new variable.\n\n![https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png](https://user-images.githubusercontent.com/4745789/83943610-2e254800-a81b-11ea-8b25-056e1b1df85e.png)\n\nThis effectively builds a parent-child relationship between the two structs where we can safely typecast a child `list_int` to its parent `list_head`.\n\n> It is important to note here that the parent-child relationship is established only because the first member of `list_int` is of type `list_head`. it would not have worked if we change the order of members in `list_int`.\n\n# How does this drive inheritance?\n\nAs established above, by putting one struct within another as its first element we are effectively creating a parent-child relationship between the two. Since this gives us an ability to safely typecast child to its parent we can define functions that accept a pointer to parent struct as an argument and perform operations that do not really require to deal with specifics. This allows us to **NOT** rewrite the functional logic for every child extensions and thus avoid redundant code.\n\nFrom the context we have set up, say we want to write a function that adds a node between the two in a linked list. The core logic to perform this operation does not really need to deal with any specifics all it takes is a few pointer manipulations of `next` and `prev`. Hence, we could just define the function accepting arguments of type `list_head *`  and write the function as\n\n```cpp\n/*\n * Insert a new entry between two known consecutive entries.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic void __list_add(struct list_head *new,\n                       struct list_head *prev,\n                       struct list_head *next)\n{\n    next->prev = new;\n    new->next = next;\n    new->prev = prev;\n    prev->next = new;\n}\n```\n\nSince we can safely typecase `list_int *` and `list_str *` to `list_head *` we can pass any of the specific implementations the function `__list_add` and it would still add the node between the other two seamlessly.\n\nSince the core operations on linked lists only require pointer manipulations, we can define these operations as functions accepting `list_head *` instead of specific types like `list_int *`.  Thus we need not write similar functions for specifics. A function to delete a node could be written as\n\n```cpp\n/*\n * Delete a list entry by making the prev/next entries\n * point to each other.\n *\n * This is only for internal list manipulation where we know\n * the prev/next entries already!\n */\nstatic inline void __list_del(struct list_head * prev, struct list_head * next)\n{\n    next->prev = prev;\n    prev->next = next;\n}\n```\n\nOther linked list utilities like *adding a node to tail*, *swapping nodes*, *splicing the list*, *rotating the list*, etc only require manipulations of `next` and `prev` pointers. Hence they could also be written in a very similar way i.e accepting `list_head *` and thus eliminating the need to reimplement function logic for every single child implementation.\n\nThis behavior is very similar to how inheritance in modern OOP languages, like Python and Java, work where the child is allowed to invoke any parent function.\n\n# Who uses structure compositions?\n\nThere are a ton of practical usage of using Structure Compositions but the most famous ones are\n\n## Linux Kernel\n\nIn order to keep things abstract and extensible, Linux Kernel uses Structure Composition at several places. One of the most important places where it uses composition is for managing and maintaining Linked Lists, exactly how we saw things above. The struct definitions and code snippets are taken as-is from the [Kernel's source code](https://elixir.bootlin.com/linux/latest/source/include/linux/list.h), and hence the structure and variable names look different than usual.\n\n## Python Type and Object Hierarchy\n\nPython, one of the most important languages in today's world, uses Structure Composition to build Type Hierarchy. Python defines a root structure called `PyObject` which holds reference count, defining the number of places from which the object is referenced - and object type - determining the type of the object i.e. `int`, `str`, `list`, `dict`, etc.\n\n```cpp\ntypedef struct _object {\n    Py_ssize_t     ob_refcnt;  // holds reference count of the object\n    PyTypeObject   *ob_type;   // holds the type of the object\n} PyObject;\n```\n\nSince Python wants these fields to be present in every single object that is created during runtime, it uses structure composition to ensure that objects like integers, floats, string, etc put `PyObject` as their first element and thus establishing a parent-child relationship. A Float object in Python is defined as\n\n```cpp\n#define PyObject_HEAD PyObject ob_base;\n\ntypedef struct {\n    PyObject_HEAD\n    double ob_fval;    // holds the actual float value\n} PyFloatObject;\n```\n\nNow writing utility functions that increments and decrements references count on every access of any object could be written as just a single function accepting `PyObject *` as shown below\n\n```cpp\nstatic inline void _Py_INCREF(PyObject *op) {\n    op->ob_refcnt++;\n}\n```\n\nThus we eradicate a need of rewriting `INCREF` for every single object type and just write it once for `PyObject` and it will work for every single Python object type that is extended through `PyObject`.\n\n# References\n\n- [LinkedList in Linux Source Code](https://elixir.bootlin.com/linux/latest/source/include/linux/list.h)\n- [PyObject - Python Internals Documentation](https://docs.python.org/3/c-api/structures.html#c.PyObject)\n- [PyFloatObject - Python Internals Documentation](https://docs.python.org/3/c-api/float.html)\n",
    "similar": [
      "mongodb-cursor-skip-is-slow",
      "fork-bomb",
      "fast-and-efficient-pagination-in-mongodb",
      "idf"
    ]
  },
  {
    "id": 47,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "rum",
    "title": "The RUM Conjecture",
    "description": "While designing any storage system the three main aspects we optimize for are Reads, Updates, and auxiliary Memory. RUM Conjecture states that these three form a competing triangle and we could only optimize two at the expense of the third.",
    "gif": "https://media.giphy.com/media/1n7B7bJ917pqZGHG9m/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/83344735-64009300-a328-11ea-858a-587d440136f1.png",
    "released_at": "2020-05-31",
    "total_views": 670,
    "body": "The RUM Conjecture states that we cannot design an access method for a storage system that is optimal in all the following three aspects - Reads, Updates, and, Memory. The conjecture puts forth that we always have to trade one to make the other two optimal and this makes the three constitutes a competing triangle, very similar to the famous [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem).\n\n![https://user-images.githubusercontent.com/4745789/83323578-6eb21e00-a27d-11ea-941b-43e875169c97.png](https://user-images.githubusercontent.com/4745789/83323578-6eb21e00-a27d-11ea-941b-43e875169c97.png)\n\n# Access Method\n\nData access refers to an ability to access and retrieve data stored within a storage system driven by an optional storage engine. Usually, a storage system is designed to be optimal for serving a niche use case and achieve that by carefully and judiciously deciding the memory and disk storage requirements, defining well-structured access and retrieval pattern, designing data structures for primary and auxiliary data and picking additional techniques like compression, encryption, etc. These decisions define, and to some extent restricts, the possible ways the storage engine can read and update the data in the system.\n\n# RUM Overheads\n\nAn ideal storage system would be the one that has an access method that provides lowest Read Overhead, minimal Update Cost, and does not require any extra Memory or Storage space, over the main data. In the real-world, achieving this is near impossible and that is something that is dictated by this conjecture.\n\n### Read Overhead\n\nRead Overhead occur when the storage engine performs reads on auxiliary data to fetch the required intended main data. This usually happens when we use an auxiliary data structure like a Secondary Index to speed up reads. The reads happening on this auxiliary structure constitutes read overheads.\n\nRead Overhead is measured through Read Amplification and it is defined as the ratio between the total amount of data read (main + auxiliary) and the amount of main data intended to be read.\n\n### Update Overhead\n\nUpdate Overhead occur when the storage engine performs writes on auxiliary data or on some unmodified main data along with intended updates on the main data. A typical example of Update Overheads is the writes that happen on an auxiliary structure like Secondary Index alongside the write happening on intended main data. \n\nUpdate Overhead is measured through Write Amplification and it is defined as the ratio between the total amount of data written (main + auxiliary) and the amount of main data intended to be updated.\n\n### Memory Overhead\n\nMemory overhead occurs when the storage system uses an auxiliary data structure to speed up reads, writes, or to serve common access patterns. This storage is in addition to the storage needs of the main data.\n\nMemory Overhead is measured through Space Amplification and it is defined as the ratio between the space utilized for auxiliary and main data and space utilized by the main data. \n\n# The Conjecture\n\nThe RUM Conjecture, in a formal way, states that\n\n> An access method that can set an upper bound for two out of the read, update, and memory overheads, also sets a lower bound for the third overhead.\n\nThis is not a hard rule that is followed and hence it is not a theorem but a conjecture - widely observed but not proven. But we can safely keep this in mind while designing the next big storage system serving a use case.\n\n# Categorizing Storage Systems\n\nNow that we have seen RUM overheads and the RUM Conjecture we take a look at examples of Storage Systems that classify into one of the three types.\n\n## Read Optimised\n\nRead Optimised storage systems offer very low read overhead but require some extra auxiliary space to gain necessary performance that again comes at a cost of updates required to keep auxiliary data in sync with main data which adds to update overheads. When the updates, on main data, become frequent the performance of a read optimized storage system takes a dip.\n\nA fine example of a read optimized storage system is the one that supports Point Indexes, also called Hash-based indexes, offering constant time access. The systems that provide logarithmic time access, like [B-Trees](https://en.wikipedia.org/wiki/B-tree) and [Skiplists](https://en.wikipedia.org/wiki/Skip_list), also fall into this category.\n\n## Update Optimised\n\nUpdate Optimised storage systems offer very low Update Overhead by usually using an auxiliary space holding differential data (delta) and flushing them over main data in a bulk operation. The need of having an auxiliary data to keep track of delta to perform a bulk update adds to Memory Overhead.\n\nA few examples of Update Optimised systems are [LSM Trees](https://en.wikipedia.org/wiki/Log-structured_merge-tree), [Partitioned B Trees](http://cs.emis.de/LNI/Proceedings/Proceedings26/GI-Proceedings.26-47.pdf), and [FD Tree](http://pages.cs.wisc.edu/~yinan/fdtree.html). These structures offer very good performance for an update-heavy system but suffer from an increased read and space overheads. While reading data from LSM Tree, the engine needs to perform read on all the tiers and then perform a conflict resolution, and maintaining tiers of data itself is a huge Space Overhead.\n\n## Memory Optimised\n\nMemory Optimised storage systems are designed to minimize auxiliary memory required for access and updates on the main data. To be memory-optimized the systems usually use compress the main data and auxiliary storages, or allow some error rate, like false positives.\n\nA few examples of Memory Optimises systems are lossy index structures like [Bloom Filters](https://en.wikipedia.org/wiki/Bloom_filter), [Count-min sketches](https://en.wikipedia.org/wiki/Count%E2%80%93min_sketch), Lossy encodings, and Sparse Indexes. Keeping either main or auxiliary data compressed, to be memory efficient, the system takes a toll on writes and reads as they now have additionally performed compression and decompressions adding to the Update and Read overheads.\n\n![https://user-images.githubusercontent.com/4745789/83323560-55a96d00-a27d-11ea-9d33-4001c672b920.png](https://user-images.githubusercontent.com/4745789/83323560-55a96d00-a27d-11ea-9d33-4001c672b920.png)\n\nStorage System examples for RUM Conjecture\n\n# Block-based Clustered Indexing\n\nBlock-based Clustered Indexing, sits comfortably between these three optimized systems types. It is not read Read efficient but also efficient on Updates and Memory. It builds a very short tree for its auxiliary data, by storing a few pointers to pages and since the data is clustered i.e. the main data itself is stored in the index,  the system does not go to fetch the main data from the main storage and hence provides a minimal Read overhead.\n\n# Being RUM Adaptive\n\nStorage systems have always been rigid with respect to the kind of use cases it aims to solve. the application, the workload, and the hardware should dictate how we access our data, and not the constraints of our systems. Storage systems could be designed to be RUM Adaptive and they should possess an ability to be tuned to reduce the RUM overheads depending on the data access pattern and computation knowledge. RUM Adaptive storage systems are part of the discussion for some other day.\n\n# Conclusion\n\nThere will always be trade-offs, between Read, Update, and Memory, while either choosing one storage system over others; the RUM conjecture facilitates and to some extent formalizes the entire process. Although this is just a conjecture, it still helps us disambiguate and make an informed, better and viable decision that will go a long way.\n\nThis essay was heavily based on the original research paper introducing The RUM Conjecture.\n\n# References\n\n- [Designing Access Methods: The RUM Conjecture](https://stratos.seas.harvard.edu/files/stratos/files/rum.pdf)\n",
    "similar": [
      "master-replica-replication",
      "leaderless-replication",
      "replication-formats",
      "multi-master-replication"
    ]
  },
  {
    "id": 48,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "consistent-hashing",
    "title": "Consistent Hashing",
    "description": "Consistent Hashing is one of the most sought after techniques when it comes to designing highly scalable distributed systems. In this article, we dive deep into the need for Consistent Hashing, the internals of it, and more importantly along the way implement it using arrays and binary search.",
    "gif": "https://media.giphy.com/media/3ofSBqzxwsiN0npCak/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/82760647-92efa400-9e12-11ea-9533-5003bc3c46df.png",
    "released_at": "2020-05-24",
    "total_views": 1262,
    "body": "Consistent hashing is a hashing technique that performs really well when operated in a dynamic environment where the distributed system scales up and scales down frequently. The core concept of Consistent Hashing was introduced in the paper [Consistent Hashing and RandomTrees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web](https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf) but it gained popularity after the famous paper introducing DynamoDB - [Dynamo: Amazon\u2019s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf). Since then the consistent hashing gained traction and found a ton of use cases in designing and scaling distributed systems efficiently. The two famous examples that exhaustively use this technique are Bit Torrent, for their peer-to-peer networks and Akamai, for their web caches. In this article we dive deep into the need of Consistent Hashing, the internals of it, and more importantly along the way implement it using arrays and [Binary Search](https://en.wikipedia.org/wiki/Binary_search_algorithm).\n\n# Hash Functions\nBefore we jump into the core Consistent Hashing technique we first get a few things cleared up, one of which is Hash Functions. Hash Functions are any functions that map value from an arbitrarily sized domain to another fixed-sized domain, usually called the Hash Space. For example, mapping URLs to 32-bit integers or web pages' HTML content to a 256-byte string. The values generated as an output of these hash functions are typically used as keys to enable efficient lookups of the original entity.\n\nAn example of a simple hash function is a function that maps a 32-bit integer into an 8-bit integer hash space. The function could be implemented using the arithmetic operator `modulo` and we can achieve this by taking a `modulo 256` which yields numbers in the range `[0, 255]` taking up 8-bits for its representation. A hash function, that maps keys to such integer domain, more often than not applies the `modulo N` so as to restrict the values, or the hash space, to a range `[0, N-1]`.\n\nA good hash function has the following properties\n - The function is computationally efficient and the values generated are easy for lookups\n - The function, for most general use cases, behaves like a pseudorandom generator that spreads data out evenly without any noticeable correlation\n\nNow that we have seen what a hash function is, we take a look into how we could use them and build a somewhat scalable distributed system.\n\n# Building a distributed storage system\nSay we are building a distributed storage system in which users can upload files and access them on demand. The service exposes the following APIs to the users\n\n - `upload` to upload the file\n - `fetch` to fetch the file and return its content\n\nBehind the scenes the system has Storage Nodes on which the files are stored and accessed. These nodes expose the functions `put_file` and `fetch_file` that puts and gets the file content to/from the disk and sends the response to the main API server which in turn sends it back to the user.\n\nTo sustain the initial load, the system has 5 Stogare Nodes which stores the uploaded files in a distributed manner. Having multiple nodes ensures that the system, as a whole, is not overwhelmed, and the storage is distributed almost evenly across.\n\nWhen the user invokes `upload` function with the path of the file, the system first needs to identify the storage node that will be responsible for holding the file and we do this by applying a hash function to the path and in turn getting the storage node index. Once we get the storage node, we read the content of the file and put that file on the node by invoking the `put_file` function of the node.\n\n```py\n# storage_nodes holding instances of actual storage node objects\nstorage_nodes = [\n    StorageNode(name='A', host='10.131.213.12'),\n    StorageNode(name='B', host='10.131.217.11'),\n    StorageNode(name='C', host='10.131.142.46'),\n    StorageNode(name='D', host='10.131.114.17'),\n    StorageNode(name='E', host='10.131.189.18'),\n]\n\n\ndef hash_fn(key):\n    \"\"\"The function sums the bytes present in the `key` and then\n    take a mod with 5. This hash function thus generates output\n    in the range [0, 4].\n    \"\"\"\n    return sum(bytearray(key.encode('utf-8'))) % 5\n\n\ndef upload(path):\n    # we use the hash function to get the index of the storage node\n    # that would hold the file\n    index = hash_fn(path)\n\n    # we get the StorageNode instance\n    node = storage_nodes[index]\n\n    # we put the file on the node and return\n    return node.put_file(path)\n\n\ndef fetch(path):\n    # we use the hash function to get the index of the storage node\n    # that would hold the file\n    index = hash_fn(path)\n\n    # we get the StorageNode instance\n    node = storage_nodes[index]\n\n    # we fetch the file from the node and return\n    return node.fetch_file(path)\n```\n\nThe hash function used over here simply sums the bytes and takes the modulo by `5` (since there are 5 storage nodes in the system) and thus generating the output in the hash space `[0, 4]`. This output value now represents the index of the storage engine that will be responsible for holding the file.\n\nSay we have 5 files 'f1.txt', 'f2.txt', 'f3.txt', 'f4.txt', 'f5.txt' if we apply the hash function to these we find that they are stored on storage nodes E, A, B, C, and D respectively.\n\nThings become interesting when the system gains some traction and it needs to be scaled to 7 nodes, which means now the hash function should do `mod 7` instead of a `mod 5`. Changing the hash function implies changing the mapping and association of files with storage nodes. We first need to administer the new associations and see which files required to be moved from one node to another.\n\nWith the new hash function the same 5 files 'f1.txt', 'f2.txt', 'f3.txt', 'f4.txt', 'f5.txt' will now be associated with storage nodes D, E, F, G, A. Here we see that changing the hash function requires us to move every single one of the 5 files to a different node.\n\n![File association changed](https://user-images.githubusercontent.com/4745789/82746677-16c47480-9db0-11ea-8dea-7b5a3cb73e91.png)\n\nIf we have to change the hash function every time we scale up or down and if this requires us to move not all but even half of the data, the process becomes super expensive and in longer run infeasible. So we need a way to minimize the data movement required during scale-ups or scale-downs, and this is where Consistent Hashing fits in and minimizes the required data transfer.\n\n# Consistent Hashing\nThe major pain point of the above system is that it is prone to events like scale-ups and scale-downs as it requires a lot of alterations in associations. These associations are purely driven by the underlying Hash Function and hence if we could somehow make this hash function independent of the number of the storage nodes in the system, we address this flaw.\n\nConsistent Hashing addresses this situation by keeping the Hash Space huge and constant, somewhere in the order of `[0, 2^128 - 1]` and the storage node and objects both map to one of the slots in this huge Hash Space. Unlike in the traditional system where the file was associated with storage node at index where it got hashed to, in this system the chances of a collision between a file and a storage node are infinitesimally small and hence we need a different way to define this association.\n\nInstead of using a collision-based approach we define the association as - the file will be associated with the storage node which is present to the immediate right of its hashed location. Defining association in this way helps us\n\n - keep the hash function independent of the number of storage nodes\n - keep associations relative and not driven by absolute collisions\n\n![Associations in Consistent Hashing](https://user-images.githubusercontent.com/4745789/82748149-4d54bc00-9dbd-11ea-8f06-6710a5c98f20.png)\n\n> Consistent Hashing on an average requires only k/n units of data to be migrated during scale up and down; where k is the total number of keys and n is the number of nodes in the system.\n\nA very naive way to implement this is by allocating an array of size equal to the Hash Space and putting files and storage node literally in the array on the hashed location. In order to get association we iterate from the item's hashed location towards the right and find the first Storage Node. If we reach the end of the array and do not find any Storage Node we circle back to index 0 and continue the search. The approach is very easy to implement but suffers from the following limitations\n\n - requires huge memory to hold such a large array\n - finding association by iterating every time to the right is `O(hash_space)`\n\nA better way of implementing this is by using two arrays: one to hold the Storage Nodes, called `nodes` and another one to hold the positions of the Storage Nodes in the hash space, called `keys`. There is a one-to-one correspondence between the two arrays - the Storage Node `nodes[i]` is present at position `keys[i]` in the hash space. Both the arrays are kept sorted as per the `keys` array.\n\n## Hash Function in Consistent Hashing\nWe define `total_slots` as the size of this entire hash space, typically of the order `2^256` and the hash function could be implemented by taking [SHA-256](https://en.wikipedia.org/wiki/SHA-2) followed by a `mod total_slots`. Since the `total_slots` is huge and a constant the following hash function implementation is independent of the actual number of Storage Nodes present in the system and hence remains unaffected by events like scale-ups and scale-downs.\n\n```py\ndef hash_fn(key: str, total_slots: int) -> int:\n    \"\"\"hash_fn creates an integer equivalent of a SHA256 hash and\n    takes a modulo with the total number of slots in hash space.\n    \"\"\"\n    hsh = hashlib.sha256()\n\n    # converting data into bytes and passing it to hash function\n    hsh.update(bytes(key.encode('utf-8')))\n\n    # converting the HEX digest into equivalent integer value\n    return int(hsh.hexdigest(), 16) % total_slots\n```\n\n## Adding a new node in the system\nWhen there is a need to scale up and add a new node in the system, in our case a new Storage Node, we\n\n - find the position of the node where it resides in the Hash Space\n - populate the new node with data it is supposed to serve\n - add the node in the Hash Space\n\nWhen a new node is added in the system it only affects the files that hash at the location to the left and associated with the node to the right, of the position the new node will fit in. All other files and associations remain unaffected, thus minimizing the amount of data to be migrated and mapping required to be changed.\n\n![Adding a new node in the system - Consistent Hashing](https://user-images.githubusercontent.com/4745789/82751279-c959fe80-9dd3-11ea-86de-62d162519262.png)\n\nFrom the illustration above, we see when a new node K is added between nodes B and E, we change the associations of files present in the segment B-K and assign them to node K. The data belonging to the segment B-K could be found at node E to which they were previously associated with. Thus the only files affected and that needs migration are in the segment B-K; and their association changes from node E to node K.\n\nIn order to implement this at a low-level using `nodes` and `keys` array, we first get the position of the new node in the Hash Space using the hash function. We then find the index of the smallest key greater than the position in the sorted `keys` array using binary search. This index will be where the key and the new Storage node will be placed in `keys` and `nodes` array respectively.\n\n```py\ndef add_node(self, node: StorageNode) -> int:\n    \"\"\"add_node function adds a new node in the system and returns the key\n    from the hash space where it was placed\n    \"\"\"\n\n    # handling error when hash space is full.\n    if len(self._keys) == self.total_slots:\n        raise Exception(\"hash space is full\")\n\n    key = hash_fn(node.host, self.total_slots)\n\n    # find the index where the key should be inserted in the keys array\n    # this will be the index where the Storage Node will be added in the\n    # nodes array.\n    index = bisect(self._keys, key)\n\n    # if we have already seen the key i.e. node already is present\n    # for the same key, we raise Collision Exception\n    if index > 0 and self._keys[index - 1] == key:\n        raise Exception(\"collision occurred\")\n\n    # Perform data migration\n\n    # insert the node_id and the key at the same `index` location.\n    # this insertion will keep nodes and keys sorted w.r.t keys.\n    self.nodes.insert(index, node)\n    self._keys.insert(index, key)\n\n    return key\n```\n\n## Removing a new node from the system\nWhen there is a need to scale down and remove an existing node from the system, we\n\n - find the position of the node to be removed from the Hash Space\n - populate the node to the right with data that was associated with the node to be removed\n - remove the node from the Hash Space\n\nWhen a node is removed from the system it only affects the files associated with the node itself. All other files and associations remain unaffected, thus minimizing the amount of data to be migrated and mapping required to be changed.\n\n![Removing a new node from the system - Consistent Hashing](https://user-images.githubusercontent.com/4745789/82751261-b0e9e400-9dd3-11ea-81ee-3fd3f0187857.png)\n\nFrom the illustration above, we see when node K is removed from the system, we change the associations of files associated with node K to the node that lies to its immediate right i.e. node E. Thus the only files affected and needs migration are the ones associated with node K.\n\nIn order to implement this at a low-level using `nodes` and `keys` array, we get the index where the node K lies in the `keys` array using binary search. Once we have the index we remove the key from the `keys` array and Storage Node from the `nodes` array present on that index.\n\n```py\ndef remove_node(self, node: StorageNode) -> int:\n    \"\"\"remove_node removes the node and returns the key\n    from the hash space on which the node was placed.\n    \"\"\"\n\n    # handling error when space is empty\n    if len(self._keys) == 0:\n        raise Exception(\"hash space is empty\")\n\n    key = hash_fn(node.host, self.total_slots)\n\n    # we find the index where the key would reside in the keys\n    index = bisect_left(self._keys, key)\n\n    # if key does not exist in the array we raise Exception\n    if index >= len(self._keys) or self._keys[index] != key:\n        raise Exception(\"node does not exist\")\n\n    # Perform data migration\n\n    # now that all sanity checks are done we popping the\n    # keys and nodes at the index and thus removing the presence of the node.\n    self._keys.pop(index)\n    self.nodes.pop(index)\n\n    return key\n```\n\n## Associating an item to a node\nNow that we have seen how consistent hashing helps in keeping data migration, during scale-ups and scale-downs, to a bare minimum; it is time we see how to efficiently we can find the \"node to the right\" for a given item. The operation to find the association has to be super fast and efficient as it is something that will be invoked for every single read and write that happens on the system.\n\nTo implement this at low-level we again take leverage of binary search and perform this operation in `O(log(n))`. We first pass the item to the hash function and fetch the position where the item is hashed in the hash space. This position is then binary searched in the `keys` array to obtain the index of the first key which is greater than the position (obtained from the hash function). if there are no keys greater than the position, in the `keys` array, we circle back and return the 0th index. The index thus obtained will be the index of the storage node in the `nodes` array associated with the item.\n\n```py\ndef assign(self, item: str) -> str:\n    \"\"\"Given an item, the function returns the node it is associated with.\n    \"\"\"\n    key = hash_fn(item, self.total_slots)\n\n    # we find the first node to the right of this key\n    # if bisect_right returns index which is out of bounds then\n    # we circle back to the first in the array in a circular fashion.\n    index = bisect_right(self._keys, key) % len(self._keys)\n\n    # return the node present at the index\n    return self.nodes[index]\n```\n\nThe source code with the implementation of Consistent Hashing in Python could be found at [github.com/arpitbbhayani/consistent-hashing](https://github.com/arpitbbhayani/consistent-hashing/blob/master/consistent-hashing.ipynb).\n\n# Conclusion\nConsistent Hashing is one of the most important algorithms to help us horizontally scale and manage any distributed system. The algorithm does not only work in sharded systems but also finds its application in load balancing, data partitioning, managing server-based sticky sessions, routing algorithms, and many more. A lot of databases owe their scale, performance, and ability to handle the humongous load to Consistent Hashing.\n\n# References\n - [Hash Functions - Wikipedia](https://en.wikipedia.org/wiki/Hash_function)\n - [Consistent Hashing - Wikipedia](https://en.wikipedia.org/wiki/Consistent_hashing)\n - [Consistent Hashing - Stanford](https://web.stanford.edu/class/cs168/l/l1.pdf)\n - [Consistent Hashing and RandomTrees](https://www.akamai.com/us/en/multimedia/documents/technical-publication/consistent-hashing-and-random-trees-distributed-caching-protocols-for-relieving-hot-spots-on-the-world-wide-web-technical-publication.pdf)\n - [Dynamo: Amazon\u2019s Highly Available Key-value Store](https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)\n",
    "similar": [
      "fully-persistent-arrays",
      "lfu",
      "leaderless-replication",
      "persistent-data-structures-introduction"
    ]
  },
  {
    "id": 49,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "python-caches-integers",
    "title": "Integer Caching in Python",
    "description": "To gain a performance boost and avoid reallocation of frequently used integers, Python creates singleton instances of small integer values and uses them by reference.",
    "gif": "https://media.giphy.com/media/l378kmO7gdbXaesXS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/82141979-a620d380-9856-11ea-9a4a-32399c0c01b8.png",
    "released_at": "2020-05-17",
    "total_views": 1824,
    "body": "An integer in Python is not a traditional 2, 4, or 8-byte implementation but rather it is implemented as an array of digits in base 2<sup>30</sup> which enables Python to support [super long integers](https://arpitbhayani.me/blogs/super-long-integers). Since there is no explicit limit on the size, working with integers in Python is extremely convenient as we can carry out operations on very long numbers without worrying about integer overflows. This convenience comes at a cost of allocation being expensive and trivial operations like addition, multiplication, division being inefficient.\n\nEach integer in python is implemented as a C structure illustrated below.\n\n```cpp\nstruct _longobject {\n    ...\n    Py_ssize_t    ob_refcnt;      // <--- holds reference count\n    ...\n    Py_ssize_t    ob_size;        // <--- holds number of digits\n    digit         ob_digit[1];    // <--- holds the digits in base 2^30\n};\n```\n\nIt is observed that smaller integers in the range -5 to 256, are used very frequently as compared to other longer integers and hence to gain performance benefit Python preallocates this range of integers during initialization and makes them singleton and hence every time a smaller integer value is referenced instead of allocating a new integer it passes the reference of the corresponding singleton.\n\nHere is what [Python's official documentation](https://docs.python.org/3/c-api/long.html#c.PyLong_FromLong) says about this preallocation\n\n> The current implementation keeps an array of integer objects for all integers between -5 and 256 when you create an int in that range you actually just get back a reference to the existing object.\n\nIn the CPython's [source code](https://github.com/python/cpython/) this optimization can be traced in the macro `IS_SMALL_INT` and the function [`get_small_int`](https://github.com/python/cpython/blob/master/Objects/longobject.c#L40) in [longobject.c](https://github.com/python/cpython/blob/master/Objects/longobject.c). This way python saves a lot of space and computation for commonly used integers.\n\n# Verifying smaller integers are indeed a singleton\nFor a CPython implementation, the in-built [`id` function](https://docs.python.org/3/library/functions.html#id) returns the address of the object in memory. This means if the smaller integers are indeed singleton then the `id` function should return the same memory address for two instances of the same value while multiple instances of larger values should return different ones, and this is indeed what we observe\n\n```py\n>>> x, y = 36, 36\n>>> id(x) == id(y)\nTrue\n\n\n>>> x, y = 257, 257\n>>> id(x) == id(y)\nFalse\n```\n\nThe singletons can also be seen in action during computations. In the example below, we reach the same target value `6` by performing two operations on three different numbers, 2, 4, and 10, and we see the `id` function returning the same memory reference in both the cases.\n\n```py\n>>> a, b, c = 2, 4, 10\n>>> x = a + b\n>>> y = c - b\n>>> id(x) == id(y)\nTrue\n```\n\n# Verifying if these integers are indeed referenced often\nWe have established that Python indeed is consuming smaller integers through their corresponding singleton instances, without reallocating them every time. Now we verify the hypothesis that Python indeed saves a bunch of allocations during its initialization through these singletons. We do this by checking the reference counts of each of the integer values.\n\n## Reference Counts\nReference count holds the number of different places there are that have a reference to the object. Every time an object is referenced the `ob_refcnt`, in its structure, is increased by `1`, and when dereferenced the count is decreased by `1`. When the reference count becomes `0` the object is garbage collected.\n\nIn order to get the current reference count of an object, we use the function `getrefcount` from the `sys` module.\n\n```py\n>>> ref_count = sys.getrefcount(50)\n11\n```\n\nWhen we do this for all the integers in range -5 to 300 we get the following distribution\n\n![Reference counts of interger values](https://user-images.githubusercontent.com/4745789/82141240-1e38ca80-9852-11ea-8133-fd8e1b26fc01.png)\n\nThe above graph suggests that the reference count of smaller integer values is high indicating heavy usage and it decreases as the value increases which asserts the fact that there are many objects referencing smaller integer values as compared to larger ones during python initialization.\n\nThe value `0` is referenced the most - `359` times while along the long tail we see spikes in reference counts at powers of 2 i.e. 32, 64, 128, and 256. Python during its initialization itself requires small integer values and hence by creating singletons it saves about `1993` allocations.\n\nThe reference counts were computed on a freshly spun python which means during initialization it requires some integers for computations and these are facilitated by creating singleton instances of smaller values.\n\nIn usual programming, the smaller integer values are accessed much more frequently than larger ones, having singleton instances of these saves python a bunch of computation and allocations.\n\n# References\n - [Python Object Types and Reference Counts](https://docs.python.org/3/c-api/intro.html#objects-types-and-reference-counts)\n - [How python implements super-long integers](https://arpitbhayani.me/blogs/super-long-integers)\n - [Why Python is Slow: Looking Under the Hood](http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/)\n",
    "similar": [
      "python-prompts",
      "python-iterable-integers",
      "constant-folding-python",
      "i-changed-my-python"
    ]
  },
  {
    "id": 50,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fractional-cascading",
    "title": "Fractional Cascading - Speeding up Binary Searches",
    "description": "The performance of binary search when applied on k lists independently can be improved using bridges and the technique is called Fractional Cascading. Fractional Cascading also sees its application in Geometric Data Structures, Segment Trees, and Databases.",
    "gif": "https://media.giphy.com/media/1k5k3J5K3BywQOrpNA/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/82142043-352deb80-9857-11ea-9801-878d771248da.png",
    "released_at": "2020-05-10",
    "total_views": 486,
    "body": "Binary Search is an algorithm that finds the position of a target value in a sorted list. The algorithm exploits the fact that the list is sorted, and is devised such that is does not have to even look at all the `n` elements, to decide if a value is present or not. In the worst case, the algorithm checks the `log(n)` number of elements to make the decision.\n\nBinary Search could be tweaked to output the position of the target value, or return the position of the smallest number greater than the target value i.e. position where the target value should have been present in the list.\n\nThings become more interesting when we have to perform an iterative binary search on `k` lists in which we find the target value in each of the `k` lists independently. The problem statement could be formally defined as\n\n> Given `k` lists of `n` sorted integers each, and a target value `x`, return the position of the smallest value greater than or equal to `x` in each of the `k` lists. Preprocessing of the list is allowed before answering the queries.\n\n# The naive approach - k binary searches\nThe expected output of this iterative search is the position of the smallest value greater than or equal to `x` in each of the `k` lists. This is a classical Binary Search problem and hence in this approach, we fire `k` binary searches on `k` lists for the target value `x` and collect the positions.\n\n![k-binary searches](https://user-images.githubusercontent.com/4745789/81492614-dbf21500-92b6-11ea-9f75-29eb3522186f.png)\n\nPython has an in-built module called `bisect` which has the function `bisect_left` which outputs the smallest value greater than or equal to `x` in a list which is exactly what we need to output and hence python-based solution using this k-binary searches approach could be \n\n```py\nimport bisect\n\narr = [\n    [21, 54, 64, 79, 93],\n    [27, 35, 46, 47, 72],\n    [11, 44, 62, 66, 94],\n    [10, 35, 46, 79, 83],\n]\n\ndef get_positions_k_bin_search(x): \n    return [bisect.bisect_left(l, x) for l in arr]\n\n>>> get_positions_k_bin_search(60)\n[2, 4, 2, 3]\n```\n\n## Time and Space Complexity\nEach of the `k` lists have size `n` and we know the time complexity of performing a binary search in one list of `n` elements is `O(log(n))`. Hence we deduce that the time complexity of this k-binary searches approach is `O(klog(n))`. \n\nThis approach does not really require any additional space and hence the space complexity is `O(1)`.\n\nThe k-binary searches approach is thus super-efficient on space but not so much on time. Hence by trading some space, we could reap some benefits on time, and on this exact principle, the unified binary search approach is based.\n\n# Unified binary search\nThis approach uses some extra space, preprocessing and computations to reduce search time. The preprocessing actually involves precomputing the positions of all elements in all the `k` lists. This precomputation enables us to perform just one binary search and get the required precalculated positions in one go.\n\n## Preprocess\nThe preprocessing is done in two phases; in the first phase, we compute a position tuple for each element and associate it with the same. In phase two of preprocessing, we create an auxiliary list containing all the elements of all the lists, on which we then perform a binary search for the given target value.\n\n### Computing position tuple for each element\nPosition tuple is a `k` item tuple where every `i`th item denotes the position of the associated element in the `i`th list. We compute this tuple by performing a binary search on all the `k` lists treating the element as the target value.\n\nFrom the example above, the position tuple of 4th element in the 4th list i.e 79 will be `[3, 5, 4, 3]` which denotes its position in all 4 lists. In list 1, 79 is at index `3`, in list 2, 79 is actually out of bounds but would be inserted at index `5` hence the output `5`, we could also have returned a value marking out of bounds, like `-2`, in list 3, 79 is not present but the smallest number greater than 79 is 94 and which is at index `4` and in list 4, 79 is present at index `3`. This makes the position tuple for 79 to be `[3, 5, 4, 3]`.\n\nGiven a 2-dimensional array `arr` we compute the position tuple for an element `(i, j)` by performing a binary search on all `k` lists as shown in python code below\n\n```py\nfor i, l in enumerate(arr):\n    for j, e in enumerate(l):\n        for k, m in enumerate(arr):\n            positions[i][j][k] = int(bisect.bisect_left(m, e))\n```\n\n### Creating a huge list\nOnce we have all the position tuples and they are well associated with the corresponding elements, we create an auxiliary list of size `k * n` that holds all the elements from all the `k` lists. This auxiliary list is again kept sorted so that we could perform a binary search on it.\n\n## Working\nGiven a target value, we perform a binary search in the above auxiliary list and get the smallest element greater than or equal to this target value. Once we get the element, we now get the associated position tuple. This position tuple is precisely the position of the target element in all the `k` lists. Thus by performing one binary search in this huge list, we are able to get the required positions.\n\n![unified binary search](https://user-images.githubusercontent.com/4745789/81492609-ca107200-92b6-11ea-8fdf-999852f4d9b1.png)\n\n## Complexity\nWe are performing binary search just once on the list of size `k * n` hence, the time complexity of this approach is `O(log(kn))` which is a huge improvement over the k-binary searches approach where it was `O(klog(n))`.\n\nThis approach, unlike k-binary searches, requires an additional space of `O(k.kn)` since each element holds `k` item position tuple and there are in all `k * n` elements.\n\nFractional cascading is something that gives us the best of both worlds by creating bridges between the lists and narrowing the scope of binary searches on subsequent iterations. Let's find out how.\n\n# Fractional Cascading\nFractional cascading is a technique through which we speed up the iterative binary searches by creating bridges between the lists. The main idea behind this approach is to dampen the need to perform binary searches on subsequent lists after performing the search on one.\n\nIn the k-binary searches approach, we solved the problem by performing `k` binary searches on `k` lists. If, after the binary search on the first list, we would have known a range within which the target value was present in the 2nd list, we would have limited our search within that subset which helps us save a bunch of computation time. The bridges, defined above, provides us with a shortcut to reach the subset of the other list where that target value would be present.\n\n![Fractional Cascading the Idea](https://user-images.githubusercontent.com/4745789/81495324-241c3200-92cd-11ea-9d7d-9c9b0911071b.png)\n\nFractional cascading is just an idea through which we could speed up binary searches, implementations vary with respect to the underlying data. The bridges could be implemented using pointers, graphs, or array indexes.\n\n## Preprocess\nPreprocessing is a super-critical step in fractional cascading because it is responsible for speeding up the iterative binary searches. Preprocessing actually sets up all the bridges from all the elements from one list to the range of items in the lower list where the element could be found. These bridges then cascade to all the lists on the lower levels.\n\n### Create Auxiliary Lists\nThe first step in pre-processing is to create `k` auxiliary lists from `k` original lists. These lists are created bottom-up which means lists on the lower levels are created first - `M(i+1)` is created before `M(i)`. An auxiliary list `M(i)` is created as a sorted list of elements of the original list `L(i)` and half of the previously created auxiliary list `M(i+1)`. The half elements of auxiliary lists are chosen by picking every other element from it.\n\n![Create Auxiliary Lists](https://user-images.githubusercontent.com/4745789/81494077-8112ea80-92c3-11ea-9416-bb2422334744.png)\n\nBy picking every other element from lower-level lists, we fill the gaps in value ranges in the original list `L(i)`, giving us a uniform spread of values across all auxiliary lists. Another advantage of picking every other element is that we eradicate the need for performing binary searches on subsequent lists altogether. Now we only need to perform a binary search for list `M(0)` and for every other list, we only need to check the element we reach via the bridge and an element before that - a constant time comparison.\n\n### Position tuples\nA position tuple for Fractional Cascading is a 2 item tuple, associated with each element of the auxiliary list, where the first item is the position of the element in the original list on the same level - serving as the required position - and the second element is the position of the element in the auxiliary list on the lower level - serving as the bridge from one level to another.\n\n![Create position pointerss](https://user-images.githubusercontent.com/4745789/89712282-83adda80-d9ad-11ea-888f-2b20d839252f.png)\n\nThe position tuple for each element in the auxiliary array can be created by doing a binary search on the original list and the auxiliary list on the lower level. Given a 2-dimensional array `arr` and auxiliary lists `m_arr` we compute the position tuples for element `(i, j)` by performing a binary search on all `k` original and auxiliary lists as shown in python code below\n\n```py\nfor i, l in enumerate(m_arr):\n    for j, m in enumerate(m_arr[i]):\n        pointers[i][j] = [\n            bisect.bisect_left(arr[i], m_arr[i][j]),\n            bisect.bisect_left(m_arr[i+1], m_arr[i][j]),\n        ]\n```\n\n## Fractional Cascading in action\nWe start by performing a binary search on the first auxiliary list `M(0)` from which we get the element corresponding to the target value. The position tuple for this element contains the position corresponding to the original list `L(0)` and bridge that will take us to the list `M(1)`. Now when we move to the list `M(1)` through the bridge and have reached the index `b`.\n\nSince auxiliary lists have uniform range spread, because of every other element being promoted, we are sure that the target value should be checked again at the index `b` and `b - 1`; because if the value was any lower it would have been promoted and bridged to other value and hence the trail we trace would be different from what we are tracing now.\n\nOnce we know which of the `b` and `b-1` index to pick (depending on the values at the index and the target value) we add the first item of the position tuple to the solution set and move the auxiliary list on the lower level and the entire process continues.\n\nOnce we reach the last auxiliary list and process the position tuple there and pick the element, our solution set contains the required positions and we can stop the iteration.\n\n```py\ndef get_locations_fractional_cascading(x): \n    locations = []\n\n    # the first and only binary search on the auxiliary list M[0]\n    index = bisect.bisect_left(m_arr[0], x)\n\n    # loc always holds the required location from the original list on same level\n    # next_loc holds the bridge index on the lower level\n    loc, next_loc = pointers[0][index]\n\n    # adding loc to the solution\n    locations.append(loc)\n\n    for i in range(1, len(m_arr)):\n        # we check for the element we reach through the bridge\n        # and the one before it and make the decision to go with one\n        # depending on the target value.\n        if x <= m_arr[i][next_loc-1]:\n            loc, next_loc = pointers[i][next_loc-1]\n        else:\n            loc, next_loc = pointers[i][next_loc]\n\n        # adding loc to the solution\n        locations.append(loc)\n\n    # returning the required locations\n    return locations\n```\n\nThe entire working code could be found here [github.com/arpitbbhayani/fractional-cascading](https://github.com/arpitbbhayani/fractional-cascading/blob/master/fractional-cascading.ipynb)\n\n## Time and space complexity\nIn Fractional Cascading, we perform binary search once on the auxiliary list `M(0)` and then make `k` constant comparisons for each of the subsequent levels; hence the time complexity is `O(k + log(n))`.\n\nThe auxiliary lists could at most contain all the elements from the original list plus `1/2 |L(n)| + 1/4 |L(n-1)| + 1/8 |L(n-2)| + ...` which is less than all elements of the original list combined. Thus the total size of the auxiliary list cannot exceed twice the original lists. The position tuple for each of the elements is also a constant 2 item tuple thus the space complexity of Fractional Cascading is `O(kn)`.\n\nThus Fractional Cascading has time complexity very close to the k-binary searches approach with a very low space complexity as compared to the unified binary searches approach; thus giving us the best of both worlds.\n\n## Fractional Cascading in real world\nFractional Cascading is used in [FD-Trees](http://pages.cs.wisc.edu/~yinan/fdtree.html) which are used in databases to address the asymmetry of read-write speeds in tree indexing on the flash disk. Fractional cascading is typically used in [range search](https://en.wikipedia.org/wiki/Range_searching) data structures like [Segment Trees](https://en.wikipedia.org/wiki/Segment_tree) to speed up lookups and filters.\n\n# References\n - [Fractional Cascading - Wikipedia](https://en.wikipedia.org/wiki/Fractional_cascading)\n - [Fractional Cascading - Original Paper by Bernard Chazelle and Leonidas Guibas](https://www.cs.princeton.edu/~chazelle/pubs/FractionalCascading1.pdf)\n - [Fractional Cascading Revisited](http://www.cse.iitd.ernet.in/~ssen/journals/frac.pdf)\n - [Fractional Cascading - Brown University](http://cs.brown.edu/courses/cs252/misc/resources/lectures/pdf/notes08.pdf)\n",
    "similar": [
      "genetic-knapsack",
      "flajolet-martin",
      "slowsort",
      "1d-terrain"
    ]
  },
  {
    "id": 51,
    "topic": null,
    "uid": "copy-on-write",
    "title": "Copy-on-Write Semantics",
    "description": "Copy-on-write is used to model Time Travel, build databases with no locks, and makes the fork system call super-efficient.",
    "gif": "https://media.giphy.com/media/GIrIC3g657AYg/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/80913860-73121680-8d65-11ea-87f8-d02460f05a22.png",
    "released_at": "2020-05-03",
    "total_views": 559,
    "body": "Copy-On-Write, abbreviately referred to as CoW suggests deferring the copy process until the first modification. A resource is usually copied when we do not want the changes made in either to be visible to the other. A resource here could be anything - an in-memory page, a database disk block, an item in a structure, or even the entire data structure.\n\nCoW suggests that we first copy by reference and let both instances share the same resource and just before the first modification we clone the original resource and then apply the updates.\n\n# Deep copying\nThe process of creating a pure clone of the resource is called [Deep Copying](https://en.wikipedia.org/wiki/Object_copying#Deep_copy) and it copies not only the immediate content but also all the remote resources that are referenced within it. Thus if we were to deep copy a [Linked List](https://en.wikipedia.org/wiki/Linked_list) we do not just copy the head pointer, rather we clone all the nodes of the list and create an entirely new list from the original one. A C++ function for deep copying a Linked List is as illustrated below\n\n```cpp\nstruct node* copy(struct node *head) {\n    if (!head) {\n        return NULL;\n    }\n\n    struct node *nhead = (struct node *) calloc(sizeof(struct node))\n    nhead->val = head->val;\n\n    struct node *p = head;\n    struct node *q = nhead;\n\n    while(p -> next) {\n        q -> next = (struct node *) calloc(sizeof(struct node));\n        q -> next -> val = p -> next -> val;\n        p = p -> next;\n        q = q -> next;\n    }\n\n    return nhead;\n}\n```\n\n![deep copying a linked list](https://user-images.githubusercontent.com/4745789/80907205-76d87580-8d32-11ea-88a8-153a94d92d72.png)\n\nGoing by the details, we understand that deep copying is a very memory-intensive operation, and hence we try to not do it very often.\n\n# Why Copy-on-Write\nCopy-on-Write, as established earlier, suggests we defer the copy operation until the first modification is requested. The approach suits the best when the traversal and access operations vastly outnumber the mutations. CoW has a number of advantages, some of them are\n\n## Perceived performance gain\nBy having a CoW, the process need not wait for the deep copy to happen, instead, it could directly proceed by just doing a copy-by-reference, where the resource is shared between the two, which is much faster than a deep copy and thus gaining a performance boost. Although we cannot totally get rid of deep copy because once some modification is requested the deep copy has to be triggered.\n\nA particular example where we gain a significant performance boost is during the `fork` system call.\n\n`fork` system call creates a child process that is a spitting copy of its parent. During this call, if the parent's program space is huge and we trigger a deep copy, the time taken to create the child process will shoot up. But if we just do copy-by-reference the child process could be spun super fast. Once the child decides to make some modifications to its program space, then we trigger the deep copy.\n\n## Better resource management\nCoW gives us an optimistic way to manage memory. One peculiar property that CoW exploits are how, before any modifications to the copied instance, both the original and the copied resources are exactly the same. The readers, thus, cannot distinguish if they are reading from the original resource or the copied one.\n\nThings change when the first modification is made to the copied instance and that's where readers of the corresponding resource would expect to see things differently. But what if the copied instance is never modified?\n\nSince there are no modifications, in CoW, the deep copy would never happen and hence the only operation that ever happened was a super-fast copy-by-reference of the original resource; and thus we just saved an expensive deep copy operation.\n\nOne very common pattern in OS is called [fork-exec](https://en.wikipedia.org/wiki/Fork%E2%80%93exec) in which a child process is forked as a spitting copy of its parent but it immediately executes another program, using `exec` family of functions, replacing its entire program space. Since the child does not intend to modify its program space ever, inherited from the parent, and just wants to replace it with the new program, deep copy plays no part and is a waste. So if we defer the deep copy operation until modification, the deep copy would never happen and we thus save a bunch of memory and CPU cycles.\n\n```cpp\n#include <stdio.h>\n\nint main( void ) {\n    char * argv[2] = {\".\", NULL};\n\n    // fork spins the child process and both child and the parent\n    // continues to co-exist from this point with the same\n    // program space.\n    int pid = fork();\n\n    if ( pid == 0 ) {\n        // The entire child program space is replace by the\n        // execvp function call.\n        // The child continues to execute the `ls` command.\n        execvp(\"ls\", argv);\n    }\n\n    // Child process will never reach here.\n    // hence all memory that was copied from its parent's\n    // program space is of no use.\n    \n    // The parent will continue its execution and print the\n    // following message.\n    printf(\"parent finishes...\\n\");\n    return 0;\n}\n```\n\n## Updating without locks\nLocks are required when we have in-place updates. Multiple writers try to modify the same instance of the resource and hence we need to define a [critical section](https://en.wikipedia.org/wiki/Critical_section) where the updations happen. This critical section is bounded by locks and any writer who wishes to modify would have to acquire the lock. This streamlines the writers and ensures only one writer could enter the critical section at any point in time, creating a chokepoint.\n\nIf we follow CoW aggressively, which suggests we copy before we write, there will be no in-place updates. All variables during every single write will create a clone, apply updates to it and then in one atomic [compare-and-swap](https://en.wikipedia.org/wiki/Compare-and-swap) operation switch and start pointing to this newer version; thus eradicating the need for locking entirely. Garbage collection on unused items, with old values, could happen from time to time.\n\n![Updating variables without locks](https://user-images.githubusercontent.com/4745789/80912595-9fc13080-8d5b-11ea-9b73-599b673e6715.png)\n\n## Versioning and point in time snapshots\nIf we aggressively follow CoW then on every write we create a clone of the original resource and apply updates to it. If we do not garbage collect the older unused instances, what we get is the history of the resource that shows us how it has been changing with time (every write operation).\n\nEach update creates a new version of the resource and thus we get resource versioning; enabling us to take point-in-time snapshots. This particular behavior is used by all collaborative document tools, like [Google Docs](https://en.wikipedia.org/wiki/Google_Docs), to provide document versioning. Point-in-time snapshots are also used in the databases to take timely backups allowing us to have a rollback and recovery plan in case of some data loss or worse a database failure.\n\n# Implementing CoW\nCoW is just a technique and it tells us what and not how. The implementation is all in the hands of the system and depending on the type of resource being CoW'ed the implementation details differ.\n\nThe naive way to perform copy operation is by doing a deep copy which, as established before, is a super inefficient way. We can do a lot better than this by understanding the nuances of the underlying resource. To gain a deeper understanding we see how efficiently we could make CoW Binary Tree [Binary Tree](https://en.wikipedia.org/wiki/Binary_tree).\n\n## Efficient Copy-on-write on a Binary Tree\nGiven a Binary Tree `A` we create a copy `B` such that any modifications by `A` are not visible to `B` and any modifications on `B` are not visible to `A`. The simplest way to achieve this is by cloning all the nodes of the tree, their pointer references, and create a second tree which is then pointed by `B` - as illustrated in the diagram below. Any modifications made to either tree will not be visible to the other because their entire space is mutually exclusive.\n\n![Deep Copying a Binary Tree](https://user-images.githubusercontent.com/4745789/80859895-b3986400-8c81-11ea-9ebe-829540df77d5.png)\n\nCopy-on-Write semantics suggest an optimistic approach where `B` instead of pointing to the cloned `A`, shares the same reference as `A` which means it also points to the exact same tree as `A`. Now say, we modify the node `2` in tree `B` and change its value to `9`.\n\nObserving closely we find that a lot of pointers could be reused and hence a better approach would be to only copy the path from the updating node till the root, keeping all other pointers references same, and let `B` point to this new root, as shown in the illustration.\n\n![Copy-on-Write a Binary Tree](https://user-images.githubusercontent.com/4745789/80869877-7606fb80-8cc0-11ea-8a9b-2b7312a59f11.png)\n\nThus instead of maintaining two separate mutually exclusive trees, we make space partially exclusive depending on which node is updated and in the process make things efficient with respect to memory and time. This behavior is core to a family of data structures called [Persistent Data Structures](https://en.wikipedia.org/wiki/Persistent_data_structure).\n\n> Fun fact: You can model Time Travel using Copy-on-Write semantics.\n\n# Why shouldn't we Copy-on-Write\nCoW is an expensive process if done aggressively. If on every single write, we create a copy then in a system that is write-heavy, things could go out of hand very soon. A lot of CPU cycles will be occupied for doing garbage collections and thus stalling the core processes. Picking which battles to win is important while choosing something as critical as Copy-on-Write.\n\n# References\n - [Copy on Write](https://en.wikipedia.org/wiki/Copy-on-write)\n - [Persistent Data Structures](https://en.wikipedia.org/wiki/Persistent_data_structure)\n - [Fork Exec Pattern](https://en.wikipedia.org/wiki/Fork%E2%80%93exec)\n",
    "similar": [
      "phi-accrual",
      "israeli-queues",
      "mysql-cache",
      "ts-smoothing"
    ]
  },
  {
    "id": 52,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "mysql-cache",
    "title": "Midpoint Insertion Strategy in MySQL LRU Cache",
    "description": "The MySQL InnoDB Storage engine uses LRU cache but it suffers from a notorious problem. In this article, we find how by using Midpoint Insertion Strategy and changing one aspect of LRU, MySQL becomes scan resistant and super performant.",
    "gif": "https://media.giphy.com/media/daUOBsa1OztxC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/80304802-0ed7db80-87d6-11ea-98db-bc5d4afe965b.png",
    "released_at": "2020-04-26",
    "total_views": 466,
    "body": "Disk reads are 4x (for SSD) to 80x (for magnetic disk) [slower](https://gist.github.com/hellerbarde/2843375) as compared to main memory (RAM) reads and hence it becomes extremely important for a database to utilize main memory as much as it can, and be super-performant while keeping its latencies to a bare minimum. Engines cannot simply replace disks with RAM because of volatility and cost, hence it needs to strike a balance between the two - maximize main-memory utilization and minimize the disk access.\n\nThe database engine virtually splits the data files into pages. A page is a unit which represents how much data the engine transfers at any one time between the disk (the data files) and the main memory. It is usually a few kilobytes 4KB, 8KB, 16KB, 32KB, etc. and is configurable via engine parameters. Because of its bulky size, a page can hold one or multiple rows of a table depending on how much data is in each row i.e. the length of the row.\n\n# Locality of reference\nDatabase systems exhibit a strong and predictable behaviour called [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference) which suggests the access pattern of a page and its neighbours.\n\n## Spatial Locality of Reference\nThe spatial locality of reference suggests if a row is accessed, there is a high probability that the neighbouring rows will be accessed in the near future.\n\nHaving a larger page size addresses this situation to some extent. As one page could fit multiple rows, this means when that page is cached in main memory, the engine saves a disk read if the neighbouring rows residing on the same page are accessed.\n\nAnother way to address this situation is to [read-ahead](https://dev.mysql.com/doc/refman/8.0/en/innodb-disk-io.html) pages that are very likely to be accessed in the future and keep them available in the main memory. This way if the read-ahead pages are referenced, the engine needs to go to the disk to fetch the page, rather it will find the page residing in the main memory and thus saving a bunch of disk reads.\n\n## Temporal Locality of Reference\nThe temporal locality of reference suggests that if a page is recently accessed, it is very likely that the same page will be accessed again in the near future.\n\nCaching exploits this behaviour by putting every single page accessed from the disk into main-memory (cache). Hence the next time a page which is available in the cache is referenced, the engine need not make a disk read to get the page, rather it could reference it from the cache directly, again saving a disk read.\n\n![Disk cache-control flow](https://user-images.githubusercontent.com/4745789/80286313-4e57e680-8748-11ea-88c2-dcb67f6ac566.png)\n\nSince the cache is very costly, it is in magnitude smaller in capacity than the disk. It can only hold some fixed number of pages which means the cache suffers from the problem of getting full very quickly. Once the cache gets full, the engine needs to evict an old page so that the new page, which according to the temporal locality of reference is going to be accessed in the near future, could get a place in the cache.\n\nThe most common strategy that decides the page that will be evicted from the cache is the [Least Recently Used cache eviction strategy](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)). This strategy uses Temporal Locality of Reference to the core and hence evicts the page which was not accessed the longest, thus maximizing the time the most-recently accessed pages are held in the cache.\n\n# LRU Cache\nThe LRU cache holds the items in the order of its last access, allowing us to identify which item is not being used the longest. When the cache is full and a newer item needs to make an entry in the cache, the item which is not accessed the longest is evicted and hence the name Least Recently Used.\n\nThe one end (head) of the list holds the most-recently referenced page while the fag end (tail) of the list holds the least-recently referenced one. A new page, being most-recently accessed, is always added at the head of the list while the eviction happens at the tail. If a page from the cache is referenced again, it is moved to the head of the list as it is now the most-recently referenced.\n\n## Implementation\nLRU cache is often implemented by pairing a [doubly-linked list](https://en.wikipedia.org/wiki/Doubly_linked_list) with a [hash map](https://en.wikipedia.org/wiki/Hash_table). The cache is thus just a linked list of pages and the hashmap maps the `page_id` to the node in the linked list, enabling `O(1)` lookups.\n\n![LRU Cache](https://user-images.githubusercontent.com/4745789/80288324-d7751a80-8754-11ea-96ab-6a8e25730bff.png)\n\n## InnoDB's Buffer Pool\nMySQL InnoDB's cache is called [Buffer Pool](https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html) which does exactly what has been established earlier. Pseudocode implementation of `get_page` function, using which the engine gets the page for further processing, could be summarized as\n\n```py\ndef get_page(page_id:int) -> Page:\n    # Check if the page is available in the cache\n    page = cache.get_page(page_id)\n\n    # if the page is retrieved from the main memory\n    # return the page.\n    if page:\n        return page\n\n    # retrieve the page from the disk\n    page = disk.get_page(page_id)\n\n    # put the page in the cache,\n    # if the cache is full, evict a page which is\n    # least recently used.\n    if cache.is_full():\n        cache.evict_page()\n\n    # put the page in the cache\n    cache.put_page(page)\n\n    # return the pages\n    return page\n```\n\n## A notorious problem with Sequential Scans\nAbove caching strategy works wonders and helps the engine to be super-performant. [Cache hit ratio](https://www.stix.id.au/wiki/Cache_Hit_Ratio) is usually more than 80% for mid-sized production-level traffic, which means 80% of the times the pages were served from the main memory (cache) and the engine did not require to make the disk read.\n\nWhat would happen if an entire table is scanned? say, while talking a [DB dump]((https://dev.mysql.com/doc/refman/8.0/en/mysqldump.html)), or running a `SELECT` without `WHERE` to perform some statistical computations.\n\nGoing by the MySQL's aforementioned behaviour, the engine iterates on all the pages and since each page which is accessed now is the most recent one, it puts it at the head of the cache while evicting one from the tail.\n\nIf the table is bigger than the cache, this process will wipe out the entire cache and fill it with the pages from just one table. If these pages are not referenced again, this is a total loss and performance of the database takes a hit. The performance will pickup once these pages are evicted from the cache and other pages make an entry.\n\n# Midpoint Insertion Strategy\nMySQL InnoDB Engine ploys an extremely smart solution to solve the notorious problem with Sequential Scans. Instead of keeping its Buffer Pool a strict LRU, it tweaks it a little bit.\n\nInstead of treating the Buffer Pool as a single doubly-linked list, it treats it as a combination of two smaller sublists - usually 5/8th and 3/8th of the total size. One sublist holds the younger data while the other one holds the older data. The head of the Young sublist holds the most recent pages and the recency decreases as we reach the tail of the Old sublist.\n\n![MySQL InnoDB Midpoint Insertion Strategy](https://user-images.githubusercontent.com/4745789/80299447-138a9880-87b2-11ea-9b0a-888e0ccf4b49.png)\n\n## Eviction\nThe tail of the Old Sublist holds the Least Recently Used page and the eviction thus happens as per the LRU Strategy i.e. at the tail of the Old Sublist.\n\n## Insertion\nThis is where this strategy differs from Strict LRU. The insertion, instead of happening at \"newest\" end of the list i.e. head of Young sublist, happens at the head of Old sublist i.e. in the \"middle\" of the list. This position of the list where the tail of the Young sublist meets the head of the Old sublist is referred to as the \"midpoint\", and hence the name of the strategy is Midpoint Insertion Strategy.\n\n> By inserting in the middle, the pages that are only read once, such as during a full table scan, can be aged out of the Buffer Pool sooner than with a strict LRU algorithm.\n\n## Moving page from Old to the Young sublist\nIn this strategy, like in Strict LRU implementation, whenever the page is accessed it moves to the newest end of the list i.e. the head of the Young sublist. During the first access, the pages make an entry in the cache in the \"middle\" position.\n\nIf the page is referenced the second time it is moved to the head of Young sublist and hence stays in the cache for a longer time. If the page, after being inserted in the middle, is never referenced again (during full scans), it is evicted sooner because the Old sublist is usually shorter than the Young sublist.\n\nThe Young sublist thus remains unaffected by table scans bringing in new blocks that might or might not be accessed afterwards. The engine thus remains performant as more frequently accessed pages continue to remain in the cache (Young sublist).\n\n## MySQL parameter to tune the midpoint\nInnoDB allows us to tune the midpoint of the buffer pool through the parameter `innodb_old_blocks_pct`. This parameter controls the percentage of Old sublist to Buffer Pool. The default value is 37 which corresponds to the ratio 3/8.\n\nIn order to get greater insights about Buffer Pool we can invoke the following command as\n\n```\n$ SHOW ENGINE INNODB STATUS\n\n----------------------\nBUFFER POOL AND MEMORY\n----------------------\nTotal memory allocated 137363456; in additional pool allocated 0\nDictionary memory allocated 159646\nBuffer pool size   8191\nFree buffers       7741\nDatabase pages     449\nOld database pages 0\n\n...\n\nPages made young 12, not young 0\n43.00 youngs/s, 27.00 non-youngs/s\n\n...\n\nBuffer pool hit rate 997 / 1000, young-making rate 0 / 1000 not 0 / 1000\nPages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s\n\n...\n```\n\nThe command `SHOW ENGINE INNODB STATUS` outputs a lot of interesting metrics but the most interesting and critical ones, w.r.t Memory and Buffer Pool, are\n\n - number of pages that were made young\n - rate of eviction without access\n - cache hit ratio\n - read ahead rate\n\n# Conclusion\nWe see how by changing just one aspect of LRU cache, MySQL InnoDB makes itself Scan Resistant. Sequential scanning was a critical issue for the cache but it was addressed in a very elegant way.\n\n# References\n - [Latency numbers](https://gist.github.com/hellerbarde/2843375)\n - [Locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference)\n - [InnoDB: Making Buffer Cache Scan Resistant](https://serge.frezefond.com/2009/12/innodb-making-buffer-cache-scan-resistant/)\n - [MySQL Dev - Buffer Pool](https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html)\n - [MySQL Dev - Making the Buffer Pool Scan Resistant](https://dev.mysql.com/doc/refman/8.0/en/innodb-performance-midpoint_insertion.html)\n - [MySQL Dev - InnoDB Disk I/O](https://dev.mysql.com/doc/refman/8.0/en/innodb-disk-io.html)\n",
    "similar": [
      "2q-cache",
      "ts-smoothing",
      "israeli-queues",
      "copy-on-write"
    ]
  },
  {
    "id": 53,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "fsm",
    "title": "Building Finite State Machines with Python Coroutines",
    "description": "The most intuitive way of building and implementing Finite State Machines is by using Python Coroutines and in this article, we find how and why.",
    "gif": "https://media.giphy.com/media/KhdQ2Ia3FJuKs/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/79684359-3ebf3600-824e-11ea-97cc-0f06c2919aeb.png",
    "released_at": "2020-04-19",
    "total_views": 7625,
    "body": "Finite State Machine is a mathematical model of computation that models a sequential logic. FSM consists of a finite number of states, transition functions, input alphabets, a start state and end state(s). In the field of computer science, the FSMs are used in designing Compilers, Linguistics Processing, Step workflows, Game Design, Protocols Procedures (like TCP/IP), Event-driven programming, Conversational AI and many more.\n\nTo understand what a finite machine is, we take a look at Traffic Signal. Finite State Machine for a Traffic Signal is designed and rendered below. `Green` is the start/initial state, which upon receiving a trigger moves to `Yellow`, which, in turn, upon receiving a trigger, transitions to `Red`. The `Red` then circles back to `Green` and the loop continues.\n\n![traffic signal fsm](https://user-images.githubusercontent.com/4745789/79678813-d572ff00-821c-11ea-8437-b4a3b7fd1a60.png)\n\nAn FSM must be in exactly one of the finite states at any given point in time and then in response to an input, it receives, the machine transitions to another state. In the example above, the traffic signal is exactly in one of the 3 states - `Green`, `Yellow` or `Red`. The transition rules are defined for each state which defines what sequential logic will be played out upon input.\n\nImplementing an FSM is crucial to solving some of the most interesting problems in Computer Science and in this article, we dive deep into modeling a Finite State Machine using Python coroutines.\n\n# Python Coroutines\nBefore diving into the implementation we take a detour and look at what Generators and Coroutines are, how they keep implementation intuitive and fits into the scheme of things.\n\n## Generators\nGenerators are **resumable functions** that yield values as long as someone, by calling `next` function, keeps asking it. If there are no more values to yield, the generator raises a `StopIteration` exception.\n\n```py\ndef fib():\n    a, b = 0, 1\n    while True:\n        yield a\n        a, b = b, a+b\n```\n\nThe `yield` statement is where the magic happens. Upon reaching the `yield` statement, the generator function execution is paused and the yielded value is returned to the caller and the caller continues its execution. The flow returns back to the generator when the caller function asks from the next value. Once the next value is requested by calling `next` (explicitly or implicitly), the generator function resumes from where it left off i.e. `yield` statement.\n\n```py\n>>> fgen = fib()\n>>> [next(fgen) for _ in range(10)]\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n```\n\nUsing a Fibonacci generator is memory-efficient as now we need not compute a lot of Fibonacci numbers and hold them in memory, in a list, rather the requesting process could ask for as many values as it needs and the generator would keep on yielding values one by one.\n\n## Coroutines\nCoroutines, just like generators, are resumable functions but instead of generating values, they consume values on the fly. The working of it is very similar to the generator and again the `yield` statement is where the magic happens. When a coroutine is paused at the `yield` statement, we could send the value it using `send` function and the value could be used using the assignment operator `=` on `yield` as shown below\n\n```py\ndef grep(substr):\n    while True:\n        line = yield\n        if substr in line:\n            print(f\"found {substr}\")\n```\n\nIn the example above, we wrote a simple `grep` utility that checks for a substring in a given stream of text. When the coroutine `grep` is paused at the `yield` statement, using the `send` function, we send the text to it, and it will be referenced by the variable `line`. The coroutine then continues its execution to check if `substr` is in `line` or not. Once the flow reaches the `yield` statement again, the coroutine pauses and waits for the caller to `send` it a new value.\n\nNote that, this is not a thread that keeps on running and hogging the CPU. It is just a function whose execution is paused at the `yield` statement waiting for the value; the state is persisted and the control is passed back to the caller. When resumed the coroutine starts from the same state where it left off.\n\n> Before sending the value to a coroutine we need to \"prime\" it so that the flow reaches the yield statement and the execution is paused while waiting for the value to be sent.\n\n```py\n>>> g = grep(\"users/created\")\n>>> next(g)  # priming the generator\n>>>\n>>> g.send(\"users/get api took 1 ms.\")\n>>> g.send(\"users/created api took 3 ms.\")\nfound users/created\n>>> g.send(\"users/get api took 1 ms.\")\n>>> g.send(\"users/created api took 4 ms.\")\nfound users/created\n>>> g.send(\"users/get api took 1 ms.\")\n```\n\nIn the function invocations above we see how we could keep on sending the text to the coroutine and it continues to spit out if it found the given substring `users/created` in the text. This ability of coroutine to pause the execution and accept input on the fly helps us model FSM in a very intuitive way.\n\n# Building a Finite State Machine\nWhile building FSMs, the most important thing is how we decide to model and implement states and transition functions. States could be modeled as Python Coroutines that run an infinite loop within which they accept the input, decides the transition and updates the current state of the FSM. The transition function could be as simple as a bunch of `if` and `elif` statements and in a more complex system it could be a decision function.\n\nTo dive into low-level details, we build an FSM for the regular expression `ab*c`, which means if the given string matches the regex then the machine should end at the end state, only then we say that the string matches the regex.\n\n![fsm for ab*c](https://user-images.githubusercontent.com/4745789/79634655-84fe9180-8189-11ea-9b94-f9ee563394bf.png)\n\n## State\nFrom the FSM above we model the state `q2` as\n\n```py\ndef _create_q2():\n    while True:\n        # Wait till the input is received.\n        # once received store the input in `char`\n        char = yield\n\n        # depending on what we received as the input\n        # change the current state of the fsm\n        if char == 'b':\n            # on receiving `b` the state moves to `q2`\n            current_state = q2\n        elif char == 'c':\n            # on receiving `c` the state moves to `q3`\n            current_state = q3\n        else:\n            # on receiving any other input, break the loop\n            # so that next time when someone sends any input to\n            # the coroutine it raises StopIteration\n            break\n```\n\nThe coroutine runs as an infinite loop in which it waits for the input token at the `yield` statement. Upon receiving the input, say `b` it changes the current state of FSM to `q2` and on receiving `c` changes the state to `q3` and this precisely what we see in the FSM diagram.\n\n## FSM Class\nTo keep things encapsulated we will define a class for FSM which holds all the states and maintains the current state of the machine. It will also have a method called `send` which reroutes the received input to the current state. The current state upon receiving this input makes a decision and updates the `current_state` of the FSM as shown above.\n\nDepending on the use-case the FSM could also have a function that answers the core problem statement, for example, does the given line matches the regular expression? or is the number divisible by 3?\n\nThe FSM class for the regular expression `ab*c` could be modeled as\n\n```py\nclass FSM:\n    def __init__(self):\n        # initializing states\n        self.start = self._create_start()\n        self.q1 = self._create_q1()\n        self.q2 = self._create_q2()\n        self.q3 = self._create_q3()\n        \n        # setting current state of the system\n        self.current_state = self.start\n\n        # stopped flag to denote that iteration is stopped due to bad\n        # input against which transition was not defined.\n        self.stopped = False\n\n    def send(self, char):\n        \"\"\"The function sends the curretn input to the current state\n        It captures the StopIteration exception and marks the stopped flag.\n        \"\"\"\n        try:\n            self.current_state.send(char)\n        except StopIteration:\n            self.stopped = True\n        \n    def does_match(self):\n        \"\"\"The function at any point in time returns if till the current input\n        the string matches the given regular expression.\n\n        It does so by comparing the current state with the end state `q3`.\n        It also checks for `stopped` flag which sees that due to bad input the iteration of FSM had to be stopped.\n        \"\"\"\n        if self.stopped:\n            return False\n        return self.current_state == self.q3\n\n    ...\n    \n    @prime\n    def _create_q2(self):\n        while True:\n            # Wait till the input is received.\n            # once received store the input in `char`\n            char = yield\n\n            # depending on what we received as the input\n            # change the current state of the fsm\n            if char == 'b':\n                # on receiving `b` the state moves to `q2`\n                self.current_state = self.q2\n            elif char == 'c':\n                # on receiving `c` the state moves to `q3`\n                self.current_state = self.q3\n            else:\n                # on receiving any other input, break the loop\n                # so that next time when someone sends any input to\n                # the coroutine it raises StopIteration\n                break\n    ...\n\n```\n\nSimilar to how we have defined the function `_create_q2` we could define functions for the other three states `start`, `q1` and `q3`. You can find the complete FSM modeled at [arpitbbhayani/fsm/regex-1](https://github.com/arpitbbhayani/fsm/blob/master/regex-1.ipynb)\n\n## Driver function\nThe motive of this problem statement is to define a function called `grep_regex` which tests a given `text` against the regex `ab*c`. The function will internally create an instance of `FSM` and will pass the stream of characters to it. Once all the characters are exhausted, we invoke `does_match` function on the FSM which suggests if the given `text` matches the regex `ab*c` or not.\n\n```py\ndef grep_regex(text):\n    evaluator = FSM()\n    for ch in text:\n        evaluator.send(ch)\n    return evaluator.does_match()\n\n>>> grep_regex(\"abc\")\nTrue\n\n>>> grep_regex(\"aba\")\nFalse\n```\n\n> The entire execution is purely running sequential - and that's because of Coroutines. All states seem to run in parallel but they that are all executing in one thread concurrently. The coroutine of the current state is executing while all others are suspended on their corresponding `yield` statements. When a new input is sent to the coroutine it is unblocked completes its execution, changes the current state of FSM and pauses itself on its `yield` statement again.\n\n# More FSMs\nWe have seen how intuitive it is to build Regular expression FSMs using Python Coroutines, but if our hypothesis is true things should equally intuitive when we are implementing FSMs for other use cases and here we take a look at two examples and see how a state is implemented in each\n\n## Divisibility by 3\nHere we build an FSM that tells if a given stream of digits of a number is divisible by 3 or not. The state machine is as shown below.\n\n![div3](https://user-images.githubusercontent.com/4745789/79641628-564ae000-81b6-11ea-9c84-147cae3a30a6.png)\n\nWe can implement the state `q1` as a coroutine as\n\n```py\ndef _create_q1(self):\n    while True:\n        digit = yield\n        if  digit in [0, 3, 6, 9]:\n            self.current_state = self.q1\n        elif  digit in [1, 4, 7]:\n            self.current_state = self.q2\n        elif  digit in [2, 5, 8]:\n            self.current_state = self.q0\n```\n\nWe can see the similarity between the coroutine implementation and the transition function for a state. The entire implementation of this FSM can be found at [arpitbbhayani/fsm/divisibility-by-3](https://github.com/arpitbbhayani/fsm/blob/master/divisibility-by-3.ipynb).\n\n## SQL Query Validator\nHere we build an FSM for a SQL Query Validator, which for a given a SQL query tells if it is a valid SQL query or not. The FSM for the validator that covers all the SQL queries will be massive, hence we just deal with the subset of it where we support the following SQL queries\n\n```\nSELECT * from TABLE_NAME;\nSELECT column, [...columns] from TABLE_NAME;\n```\n\n![fsm for sql query validator](https://user-images.githubusercontent.com/4745789/79635523-1c1a1800-818f-11ea-8afe-fe8065b55791.png)\n\nWe can implement the state `explicit_cols` as a coroutine as\n\n```py\ndef _create_explicit_cols(self):\n    while True:\n        token = yield\n        if token == 'from':\n            self.current_state = self.from_clause\n        elif token == ',':\n            self.current_state = self.more_cols\n        else:\n            break\n```\n\nAgain the coroutine through which the state is implemented is very similar to the transition function of the state keeping things intuitive. The entire implementation of this FSM can be found at [arpitbbhayani/fsm/sql-query-validator](https://github.com/arpitbbhayani/fsm/blob/master/sql-query-validator.ipynb).\n\n# Conclusion\nEven though this may not be the most efficient way to implement and build FSM but it is the most intuitive way indeed. The edges and state transitions, translate well into `if` and `elif` statements or the decision functions, while each state is being modeled as an independent coroutine and we still do things in a sequential manner. The entire execution is like a relay race where the baton of execution is being passed from one coroutine to another.\n\n# References and Readings\n\n - [Finite State Machines - Wikipedia](https://en.wikipedia.org/wiki/Finite-state_machine)\n - [Finite State Machines - Brilliant.org](https://brilliant.org/wiki/finite-state-machines/)\n - [FSM Applications](https://web.cs.ucdavis.edu/~rogaway/classes/120/spring13/eric-applications.pdf)\n - [What Are Python Coroutines?](https://realpython.com/lessons/what-are-python-coroutines/)\n - [How to Use Generators and yield in Python](https://realpython.com/introduction-to-python-generators/)\n",
    "similar": [
      "recursion-visualizer",
      "constant-folding-python",
      "python-prompts",
      "python-iterable-integers"
    ]
  },
  {
    "id": 54,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "bayesian-average",
    "title": "Better Ranking using Bayesian Average",
    "description": "Ranking a list of movies, products, books or even restaurants is tricky and in this article, we find what works for such a rating system and the math behind it.",
    "gif": "https://media.giphy.com/media/dJ4vNQ7r72pb4nDhN5/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/78990379-ebfdc400-7b53-11ea-88b7-cd64e39eabf6.png",
    "released_at": "2020-04-12",
    "total_views": 526,
    "body": "Almost every single website, app or platform on the internet has some sort of rating system in place. Whenever you purchase a product or use a service, you are asked to rate it on a scale, say 1 to 5. The platform then uses this data to generate a score and build a ranking system around it. The score is the measure of quality for each product or service. By surfacing the most quality content on top of the list, the platform tries to up their sales and ensure better engagement with their users.\n\nComing up with an aggregated score is not an easy thing - we need to crunch a million ratings and then see that the score is, in fact, the true measure of quality. If it isn't then it would directly affect the business. Today we discuss how we should define this score in a rating based system; spoiler alert! the measure is called [Bayesian Average](https://en.wikipedia.org/wiki/Bayesian_average).\n\nTo keep things simple we define the problem statement as\n\n> Given the ratings, on a scale of 1 to 5, that users give to a movie, we generate a score that is a measure of how good a movie is which then helps us get the top 10 movies of all time.\n\nWe will use the [MovieLens Dataset](https://grouplens.org/datasets/movielens/) to explore various scoring functions in this article. In the dataset, we get user ratings for each movie and the ratings are made on a scale of 1 to 5.\n\n# Generating the score\nThe score we generate for each item should be proportional to the quality quotient which means higher the score, superior is the item. Hence we say that the score of an item is the function of all the `m` ratings that it received.\n\n![score function](https://user-images.githubusercontent.com/4745789/79067003-cf8b9400-7cd9-11ea-9b16-c1875933725a.png)\n\n## Arithmetic Mean\nThe simplest and the most common strategy to compute this aggregated score for an item is by taking an [Arithmetic Mean (average)](https://en.wikipedia.org/wiki/Arithmetic_mean) of all the ratings it received. Hence for each item we sum all the ratings that it received and divide it by its cardinality, giving us the average value.\n\n![arithmetic mean](https://user-images.githubusercontent.com/4745789/79049349-b387e400-7c40-11ea-9adf-b40aa377778f.png)\n\n### Issues with arithmetic mean\nThe arithmetic mean falls apart pretty quickly. Let's say there is an item with just one rating of 5 on 5, the item would soar high on the leaderboard ranking. But does it deserve that place? probably not. Because of low cardinality (number of ratings), the score (and hence the rank) of the item will fluctuate more and will not give a true measure of quality.\n\nWith the movie dataset, we are analyzing here are the top 10 movies ranked using Arithmetic Mean.\n\n![top 10 movies arithmetic mean](https://user-images.githubusercontent.com/4745789/79049814-58a3bc00-7c43-11ea-980e-a12ae10379f7.png)\n\nThrough this measure, all of the top 10 movies have a score of 5 (out of 5) and all of them have just 1 rating. Are these really the top 10 movies of all time? Probably not. Looks like we need to do a lot better than the Arithmetic Mean.\n\n## Cumulative Rating\nTo remedy the issue with Arithmetic Mean, we come up with an approach of using Cumulative Rating as the scoring function hence instead of taking the average we only consider the sum of all the ratings as the final score.\n\n![cumulative rating as scoring function](https://user-images.githubusercontent.com/4745789/79050470-e1245b80-7c47-11ea-824b-ecd5cbb40912.png)\n\nCumulative Rating actually does a pretty decent job, it makes popular items with a large number of ratings bubble up to the top of the leaderboard. When we rank the movies in our dataset using Cumulative Ratings we get the following as the top 10.\n\n![top 10 movies through cunulative rating](https://user-images.githubusercontent.com/4745789/79050520-2d6f9b80-7c48-11ea-8e48-1c12fbbc0a88.png)\n\nThe top 10 movies now feature Shawshank Redemption, Forrest Gump, Pulp Fiction, etc. which are in fact considered as the top movies of all times. But is Cumulative Rating fool-proof?\n\n### Issues with cumulative rating\nCumulative Rating favors high cardinality. Let's say there is an extremely poor yet popular item `A` that got 10000 ratings of 1 on 5, and there is another item `B` which is very good but it got 1000 rating of 5 on 5 Cumulative Rating thus gives a score of 10000 * 1 = 10000 to item `A` and 1000 * 5 = 5000 to item `B`, but `B` clearly is far superior of an item than `A`.\n\nAnother issue with Cumulative Rating is the fact that it generates an unbounded score. Ideally, any ranking system expects a normalized bounded score so that the system becomes predictable and consistent.\n\nWe established that Cumulative Rating is better than Arithmetic Mean but it is not fool-proof and that's where the Bayesian Average comes to the rescue.\n\n# The Bayesian Average\nBayesian Average computes the mean of a population by not only using the data residing in the population but also considering some outside information, like a pre-existing belief - a derived property from the dataset, for example, prior mean.\n\n## The intuition\nThe major problem with Arithmetic Mean as the scoring function was how unreliable it was when we had a low number of data points (cardinality) to compute the score. Bayesian Average plays a part here by introducing pre-belief into the scheme of things.\n\nWe start by defining the requirements of our scoring function\n - for an item with a fewer than average number of ratings - the score should be around the system's arithmetic mean\n - for an item with a substantial number of ratings - the score should be the item's arithmetic mean\n - as the number of ratings that an item receives increases, the score should gradually move from system's mean to item's mean\n\nBy ensuring the above we neither prematurely promote nor demote an item in the leaderboard. An item is given a fair number of chances before its score falls to its own Arithmetic mean. This way we use the prior-belief - System's Arithmetic mean, to make the scoring function more robust and fair to all items.\n\n## The formula\nGiven the intuition and scoring rules, we come up with the following formula\n\n![bayesian average formula for rating system](https://user-images.githubusercontent.com/4745789/79066315-ab798400-7cd4-11ea-804b-e5e8479824b2.png)\n\nIn the above formula, `w` indicates the weight that needs to be given the item's Arithmetic Mean `A` while `S` represents the System's Arithmetic Mean. If `A` and `S` are bounded then the final score `s` will also be bounded in the same range, thus solving the problem with Cumulative Rating.\n\nSuppose the number of ratings that an item `i` receives is denoted by `m` and the average number of ratings that any item in the system receives is denoted by `m_avg`, we define the requirements of weight `w` as follows\n\n - `w` is bounded in the range [0, 1]\n - `w` should be monotonically increasing\n - `w` should be close to 0 when `m` is close to 0\n - `w` should reach 0.5 when number `m` reaches `m_avg`\n - `w` tries to get closer to 1 as `m` increases\n\nFrom the above requirements, it is clear that `w` is acting like a knob which decides in what proportions we should consider an item's mean versus the system's mean. As `w` increases we tilt more towards item's mean. We define the `w` as\n\n![weight function for bayesian average](https://user-images.githubusercontent.com/4745789/79066802-4162de00-7cd8-11ea-8068-467ce3305810.png)\n\nWhen we combine all of the above we get the final scoring function as\n\n![scoring function for bayesian average rating system](https://user-images.githubusercontent.com/4745789/79066769-111b3f80-7cd8-11ea-979e-6437334ccbba.png)\n\nOne of the most important properties of Bayesian Average is the fact that the pre-existing belief acts as support which oversees that the score does not fluctuate too abruptly and it smoothens with more number of ratings.\n\n## Applying Bayesian Average to movies dataset\nAfter applying the above mentioned Bayesian Average scoring function to our Movie dataset, we get the following movies as top 10\n\n![top 10 movies by Basysian Average](https://user-images.githubusercontent.com/4745789/79066961-686ddf80-7cd9-11ea-87d7-7e7e582ab9ac.png)\n\nPretty impressive list! The list contains almost all the famous movies that we all think make the cut. Bayesian average thus provides a bounded score that is a measure of the quality of the item, by using prior-belief i.e. system's mean.\n\n## Analyzing how Bayesian Average changes the rank\nNow that we have seen that the Bayesian Average is, in fact, an excellent way to rank items in a rating system, we find how the rank of an item changes as it receives more ratings. Below we plot the change in the percentile rank of the movies: [Kingsman](https://en.wikipedia.org/wiki/Kingsman:_The_Secret_Service), [Logan](https://en.wikipedia.org/wiki/Logan_(film)) and [The Scorpion King](https://en.wikipedia.org/wiki/The_Scorpion_King).\n\n![Kingsman position with ratings](https://user-images.githubusercontent.com/4745789/79068414-53e31480-7ce4-11ea-884a-90e7aee326d8.png)\n\n![Logan rankings](https://user-images.githubusercontent.com/4745789/79068443-7f65ff00-7ce4-11ea-9623-6f03451235de.png)\n\n![Scorpion King](https://user-images.githubusercontent.com/4745789/79068524-35c9e400-7ce5-11ea-8726-d1836a6b9c23.png)\n\nWe observe that the fluctuations in percentile rank are more in the case of Arithmetic Mean. Sometimes even after receiving a good number of reviews, the rank fluctuates sharply. In the case of Bayesian Average after an initial set of aberrations, the rank smoothens and converges.\n\n# A note on Bayesian Average\nBayesian Average is not a fixed formula that we have seen above, but it is a concept where we make the scoring function \"smoother\" by using a pre-existing belief as support. Hence we can tweak the formula as per our needs, or use multiple prior beliefs and still it would classify as a Bayesian Average.\n\n# References\n\n - [Bayesian Average](https://en.wikipedia.org/wiki/Bayesian_average)\n - [How not to sort by Average Rating](https://evanmiller.org/how-not-to-sort-by-average-rating.html)\n - [How to Rank (Restaurants)](http://www.ebc.cat/2015/01/05/how-to-rank-restaurants/)\n - [Of Bayesian average and star ratings](https://fulmicoton.com/posts/bayesian_rating/)\n - [Code to compute Bayesian Average](https://github.com/arpitbbhayani/ranking-on-ratings/blob/master/movie-lens.ipynb)\n",
    "similar": [
      "fork-bomb",
      "fast-and-efficient-pagination-in-mongodb",
      "efficient-way-to-stop-an-iterating-loop",
      "the-weird-walrus"
    ]
  },
  {
    "id": 55,
    "topic": {
      "id": 0,
      "uid": "system-design",
      "name": "Backend System Design",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT27BuTnJ_trF7BsaTpYLqst",
      "bgcolor": "#FFE7E7",
      "themecolor": "#f00",
      "course_url": null
    },
    "uid": "sliding-window-ratelimiter",
    "title": "Sliding Window based Rate Limiter",
    "description": "A rate limiter is used to control the rate of traffic sent or received on the network and in this article we dive deep and design a sliding window based rate limiter.",
    "gif": "https://media.giphy.com/media/5YuhLwDgrgtRVwI7OY/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/78276848-3c13cf80-7531-11ea-8186-99cb1da58e50.png",
    "released_at": "2020-04-05",
    "total_views": 1056,
    "body": "A rate limiter restricts the intended or unintended excessive usage of a system by regulating the number of requests made to/from it by discarding the surplus ones. In this article, we dive deep into an intuitive and heuristic approach for rate-limiting that uses a sliding window. The other algorithms and approaches include [Leaky Bucket](https://en.wikipedia.org/wiki/Leaky_bucket), [Token Bucket](https://en.wikipedia.org/wiki/Token_bucket) and Fixed Window.\n\nRate limiting is usually applied per access token or per user or per region/IP. For a generic rate-limiting system that we intend to design here, this is abstracted by a configuration key `key` on which the capacity (limit) will be configured; the key could hold any of the aforementioned value or its combinations. The limit is defined as the number of requests `number_of_requests` allowed within a time window `time_window_sec` (defined in seconds).\n\n# The algorithm\nThe algorithm is pretty intuitive and could be summarized as follow\n\n> If the number of requests served on configuration key `key` in the last `time_window_sec` seconds is more than `number_of_requests` configured for it then discard, else the request goes through while we update the counter.\n\nAlthough the above description of the algorithm looks very close to the core definition of any rate limiter, it becomes important to visualize what is happening here and implement it in an extremely efficient and resourceful manner.\n\n## Visualizing sliding window\nEvery time we get a request, we make a decision to either serve it or not; hence we check the `number_of_requests` made in last `time_window_sec` seconds. So this process of checking for a fixed window of `time_window_sec` seconds on every request, makes this approach a sliding window where the fixed window of size `time_window_sec` seconds is moving forward with each request. The entire approach could be visualized as follows\n\n![Sliding window visualization](https://user-images.githubusercontent.com/4745789/78364339-eac01a80-75da-11ea-8f65-633fd779afac.png)\n\n## The pseudocode\nThe core of the algorithm could be summarized in the following Python pseudocode. It is not recommended to put this or similar code in production as it has a lot of limitations (discussed later), but the idea here is to design the rate limiter ground up including low-level data models, schema, data structures, and a rough algorithm.\n\n```py\ndef is_allowed(key:str) -> Bool:\n\"\"\"The function decides is the current request should be served or not.\nIt accepts the configuration key `key` and checks the number of requests made against it\nas per the configuration.\n\nThe function returns True if the request goes through and False otherwise.\n\"\"\"\n    current_time = int(time.time())\n\n    # Fetch the configuration for the given key\n    # the configuration holds the number of requests allowed in a time window.\n    config = get_ratelimit_config(key)\n\n    # Fetch the current window for the key\n    # The window returned, holds the number of requests served since the start_time\n    # provided as the argument.\n    start_time = current_time - config.time_window_sec\n    window = get_current_window(key, start_time)\n\n    if window.number_of_requests > config.capacity:\n        return False\n    \n    # Since the request goes through, register it.\n    register_request(key, current_time)\n    return True\n```\n\nA naive implementation of the above pseudocode is trivial but the true challenge lies in making the implementation horizontally scalable, with low memory footprint, low CPU utilization, and low time complexity.\n\n# Design\nDesigning a rate limiter has to be super-efficient because the rate limiter decision engine will be invoked on every single request and if the engine takes a long time to decide this, it will add some overhead in the overall response time of the request. A better design will not only help us keep the response time to a bare minimum, but it also ensures that the system is extensible with respect to future requirement changes.\n\n## Components of the Rate limiter\nThe Rate limiter has the following components\n\n - Configuration Store - to keep all the rate limit configurations\n - Request Store - to keep all the requests made against one configuration key\n - Decision Engine - it uses data from the Configuration Store and Request Store and makes the decision\n\n## Deciding the datastores\nPicking the right data store for the use case is extremely important. The kind of datastore we choose determines the core performance of a system like this.\n\n### Configuration Store\nThe primary role of the Configuration Store would be to\n\n - efficiently store configuration for a key\n - efficiently retrieve the configuration for a key\n\nIn case of machine failure, we would not want to lose the configurations created, hence we choose a disk-backed data store that has an efficient `get` and `put` operation for a key. Since there would be billions of entries in this Configuration Store, using a SQL DB to hold these entries will lead to a performance bottleneck and hence we go with a simple key-value NoSQL database like [MongoDB](https://mongodb.com) or [DynamoDB](https://aws.amazon.com/dynamodb/) for this use case.\n\n### Request Store\nRequest Store will hold the count of requests served against each key per unit time. The most frequent operations on this store will be\n\n - registering (storing and updating) requests count served against each key - _write heavy_\n - summing all the requests served in a given time window - _read and compute heavy_\n - cleaning up the obsolete requests count - _write heavy_\n\nSince the operations are both read and write-heavy and will be made very frequently (on every request call), we chose an in-memory store for persisting it. A good choice for such operation will be a datastore like [Redis](https://redis.io) but since we would be diving deep with the core implementation, we would store everything using the common data structures available.\n\n## Data models and data structures\nNow we take a look at data models and data structures we would use to build this generic rate limiter.\n\n### Configuration Store\nAs decided before we would be using a NoSQL key-value store to hold the configuration data. In this store, the key would be the configuration key (discussed above) which would identify the user/IP/token or any combination of it; while the value will be a tuple/JSON document that holds `time_window_sec` and `capacity` (limit).\n\n```json\n{\n    \"user:241531\": {\n        \"time_window_sec\": 1,\n        \"capacity\": 5\n    }\n}\n```\n\nThe above configuration defines that the user with id `241531` would be allowed to make `5` requests in `1` second.\n\n## Request Store\nRequest Store is a nested dictionary where the outer dictionary maps the configuration key `key` to an inner dictionary, and the inner dictionary maps the epoch second to the request counter. The inner dictionary is actually holding the number of requests served during the corresponding epoch second. This  way we keep on aggregating the requests per second and then sum them all  during aggregation to compute the number of requests served in the required time window.\n\n![Request Store for sliding window rate limiter](https://user-images.githubusercontent.com/4745789/78384914-b0657600-75f8-11ea-8158-981ac3ecd46d.png)\n\n\n## Implementation\nNow that we have defined and designed the data stores and structures, it is time that we implement all the helper functions we saw in the pseudocode.\n\n### Getting the rate limit configuration\nGetting the rate limit configuration is a simple get on the Configuration Store by `key`. Since the information does not change often and making a disk read every time is expensive, we cache the results in memory for faster access.\n\n```py\ndef get_ratelimit_config(key):\n    value = cache.get(key)\n\n    if not value:\n        value = config_store.get(key)\n        cache.put(key, value)\n\n    return value\n```\n\n### Getting requests in the current window\nNow that we have the configuration for the given key, we first compute the `start_time` from which we want to count the requests that have been served by the system for the `key`. For this, we iterate through the data from the inner dictionary second by second and keep on summing the requests count for the epoch seconds greater than the `start_time`. This way we get the total requests served from start_time till now.\n\nIn order to reduce the memory footprint, we could delete the items from the inner dictionary against the time older than the `start_time` because we are sure that the requests for a timestamp older than `start_time` would never come in the future.\n\n```python\ndef get_current_window(key, start_time):\n    ts_data = requests_store.get(key)\n    if not key:\n        return 0\n    \n    total_requests = 0\n    for ts, count in ts_data.items():\n        if ts > start_time:\n            total_requests += count\n        else:\n            del ts_data[ts]\n\n    return total_requests\n```\n\n### Registering the request\nOnce we have validated that the request is good to go through, it is time to register it in the store and the defined function `register_request` does exactly that.\n\n```python\ndef register_request(key, ts):\n    store[key][ts] += 1\n```\n\n## Potential issues and performance bottlenecks\nAlthough the above code elaborates on the overall low-level implementation details of the algorithm, it is not something that we would want to put in production as there are lots of improvements to be made.\n\n### Atomic updates\nWhile we register a request in the Request Store we increment the request counter by 1. When the code runs in a multi-threaded environment, all the threads executing the function for the same key `key`, all will try to increment the same counter. Thus there will be a classical problem where multiple writers read the same old value and updates. To fix this we need to ensure that the increment is done atomically and to do this we could use one of the following approaches\n\n - optimistic locking (compare and swap)\n - pessimistic locks (always taking lock before incrementing)\n - utilize atomic hardware instructions (fetch-and-add instruction)\n\n### Accurately computing total requests\nSince we are deleting the keys from the inner dictionary that refers to older timestamps (older than the `start_time`), it is possible that a request with older `start_time` is executing while a request with newer `start_time` deleted the entry and lead to incorrect `total_request` calculation. To remedy this we could either\n\n - delete entries from the inner dictionary with a buffer (say older than 10 seconds before the start_time),\n - take locks while reading and block the deletions\n\n### Non-static sliding window\nThere would be cases where the `time_window_sec` is large - an hour or even a day, suppose it is an hour, so if in the Request Store we hold the requests count against the epoch seconds there will be 3600 entries for that key and on every request, we will be iterating over at least 3600 keys and computing the sum. A faster way to do this is, instead of keeping granularity at seconds we could do it at the minute-level and thus we sub-aggregate the requests count at per minute and now we only need to iterate over about 60 entries to get the total number of  requests and our window slides not per second but per minute.\n\nThe granularity configuration could be persisted in the configuration as a new attribute which would help us take this call.\n\n### Other improvements\nThe solution described above is not the most optimal solution but it aims to prove a rough idea on how we could implement a sliding window rate limiting algorithm. Apart from the improvements mentioned above there some approaches that would further improve the performance\n\n - use a data structure that is optimized for range sum, like segment tree\n - use a running aggregation algorithm that would prevent from recomputing redundant sums\n\n## Scaling the solution\n\n### Scaling the Decision engine\nThe decision engine is the one making the call to each store to fetch the data and taking the call to either accept or discard the request. Since decision engine is a typical service engine we would put it behind a load balancer that takes care of distributing requests to decision engine instances in a round-robin fashion ensuring it scales horizontally.\n\nThe scaling policy of the decision engine will be kept on following metrics\n\n - number of requests received per second\n - time taken to make a decision (response time)\n - memory consumption\n - CPU utilization\n\n### Scaling the Request Store\nSince the Request Store is doing all the heavy lifting and storing a lot of data in memory, this would not scale if kept on a single instance. We would need to horizontally scale this system and for that, we shard the store using configuration key key and use consistent hashing to find the machine that holds the data for the key.\n\nTo facilitate sharding and making things seamless for the decision engine we will have a Request Store proxy which will act as the entry point to access Request Store data. It will abstract out all the complexities of distributed data, replication, and failures.\n\n### Scaling the Configuration Store\nThe number of configurations would be high but it would be relatively simple to scale since we are using a NoSQL solution, sharding on configuration key `key` would help us achieve horizontal scalability.\n\nSimilar to Request Store proxy we will have a proxy for Configuration Store that would be an abstraction over the distributed Configuration Stores.\n\n## High-level design\nThe overall high-level design of the entire system looks something like this\n\n![Rate limiter high-level design diagram](https://user-images.githubusercontent.com/4745789/78460031-1cb8a600-76db-11ea-94f4-b821244993b3.png)\n\n## Deploying in production\nWhile deploying it to production we could use a memory store like Redis whose features, like Key expiration, transaction, locks, sorted, would come in handy. The language we chose for explaining and pseudocode was Python but in production to make things super-fast and concurrent we would prefer a language like Java or Golang. Picking this stack will keep our server cost down and would also help us make optimum utilization of the resources.\n\n# References\n - [Rate Limiting - Wikipedia](https://en.wikipedia.org/wiki/Rate_limiting)\n - [Rate-limiting strategies and techniques](https://cloud.google.com/solutions/rate-limiting-strategies-techniques)\n - [An alternative approach to rate limiting](https://www.figma.com/blog/an-alternative-approach-to-rate-limiting/)\n - [Building a sliding window rate limiter with Redis](https://engagor.github.io/blog/2017/05/02/sliding-window-rate-limiter-redis/)\n - [Everything You Need To Know About API Rate Limiting](https://nordicapis.com/everything-you-need-to-know-about-api-rate-limiting/)\n",
    "similar": [
      "consistency",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "image-steganography",
      "multiple-mysql-on-same-server-using-docker"
    ]
  },
  {
    "id": 56,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "idf",
    "title": "Inverse Document Frequency",
    "description": "TF-IDF is extensively used in search engines and in various document classification and clustering techniques. Instead of taking the formula by the word, we take a detour and dive deep into the better half of it and find its connection with Probability, the role it plays in document relevance and the intuition behind it.",
    "gif": "https://media.giphy.com/media/3ornjWIRSzXEw61KH6/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/76207579-10e4db80-6224-11ea-91ba-b67359125156.png",
    "released_at": "2020-03-06",
    "total_views": 417,
    "body": "[TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) is one of the most popular measures that quantify document relevance for a given term. It is extensively used in [Information Retrieval](https://en.wikipedia.org/wiki/Information_retrieval) (ex: Search Engines), Text Mining and even for text-heavy Machine Learning use cases like Document Classification and Clustering. Today we explore the better half of TF-IDF and see its connection with Probability, the role it plays in TF-IDF and even the intuition behind it.\n\nInverse Document Frequency (IDF) is a measure of term rarity which means it quantifies how rare the term, in the corpus, really is (document collection); higher the IDF, rarer the term. A rare term helps in discriminating, distinguishing and ranking documents and it contributes more information to the corpus than what a more frequent term (like `a`, `and` and `the`) does.\n\nThe IDF was heuristically proposed in the paper \"[A statistical interpretation of term specificity and its application in retrieval](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.115.8343&rep=rep1&type=pdf)\" (Sp\u00e4rck Jones, 1972) and was originally called Term Specificity.\n\n# The intuition behind IDF\nIn order to quantify the term rarity, the heuristic says we need to give higher weight to the term that occurs in fewer documents and lesser weights to the frequent ones. Thus this measure (weight) `w` of the term is __inversely proportional__ to the number of documents in which it is present (called Document Frequency) - and hence the measure is called Inverse Document Frequency.\n\n![IDF Inversely proportional to Document Frequency](https://user-images.githubusercontent.com/4745789/76211536-85237d00-622c-11ea-82f5-c0b655634839.png)\n\nAny function that adheres to the requirement of being inversely proportional to the document frequency i.e. a decreasing function, would do the job; it may not yield optimality but could be used as an IDF for some use cases. Some decreasing functions that could be used as an IDF for some use cases are shown below\n\n![Decreasing functions](https://user-images.githubusercontent.com/4745789/76213296-63c49000-6230-11ea-9d24-94ce048732bc.png)\n\nThe more frequent words, like `a`, `and` and `the` will lie on the far right of the plot and will have a smaller value of IDF.\n\n# The most common IDF\nA widely adapted IDF measure that performs better in most use cases is defined below\n\n![common idf function](https://user-images.githubusercontent.com/4745789/76239930-633fef80-6258-11ea-823a-2011c04a1e97.png)\n\nwhere\n\n - `N` is the number of documents in the corpus\n - `df(t)` is the number of documents that has an occurrence of the term `t`\n\nIf we plot the above IDF function against the document frequency we get a nice smooth decreasing function as shown below. For lower values of X i.e. Document Frequency we see the IDF is very high as it suggests a good discriminator and as the Document Frequency increases the plot smoothly descends and reaches 0 for `df(t) = N`.\n\n![IDF Graph](https://user-images.githubusercontent.com/4745789/76215908-ae94d680-6235-11ea-8e50-498aae029ea2.png)\n\n# IDF and Probability\nWhat would be the probability that a random document picked from a corpus of `N` documents contains the term `t`? The answer to this question is the fraction of documents, out of N, that contains the term `t` and, as seen above, this is its Document Frequency.\n\n![Probability](https://user-images.githubusercontent.com/4745789/76229411-29ff8380-6248-11ea-9518-6cbc4c6947da.png)\n\nThe fraction inside the logarithm in the IDF function is oddly similar to the above probability, in fact, it is the inverse of probability defined above. Hence we could redefine IDF using this probability as \n\n![IDF as probability](https://user-images.githubusercontent.com/4745789/76229704-a09c8100-6248-11ea-9960-0cfd5f45dcce.png)\n\nBy defining IDF as a probability, we could now estimate the true IDF of a term by observing a random sample instead and computing IDF on this sampled data.\n\n# IDF of conjunction\nComputing IDF for a single term is fine but what happens when we have multiple terms? How would that fare out? This is a very common use case in Information Retrieval where we need to rank documents for a given search query, and the search query more often than not contains multiple terms.\n\nFor finding IDF of multiple terms in conjunction we make an assumption - the occurrences of terms are statistically independent and because of this the equation below holds true\n\n![Probability of conjunction](https://user-images.githubusercontent.com/4745789/76239792-2d9b0680-6258-11ea-8da2-56899540cab0.png)\n\nGiven this, we could derive the IDF of two terms in conjunction as follows\n\n![IDF derivation](https://user-images.githubusercontent.com/4745789/76232475-c2980280-624c-11ea-8a3a-37d17704a221.png)\n\nFrom the derivation above we see that the IDF of conjunction is just the summation of IDF of individual terms. Extending this to search engines we could say that the score of a document for a given search query is the summation of scores that document gets for individual terms of the query.\n\n> Note: IDF on conjunction could be made much more complex by not assuming statistical independence.\n\n# Other measures of IDF\nThe decreasing functions we see in the first section of this article were just some examples of possible IDF functions. But there are IDF functions that are not just examples but are also used in some specific use cases and some of them are:\n\n![Other IDF Measures](https://user-images.githubusercontent.com/4745789/76232678-0db21580-624d-11ea-864c-1094559e0790.png)\n\nMost of the IDF functions only differ in the bounds they produce for a given range of document frequency. The plots of 3 IDF functions namely - Common IDF, Smooth IDF, and Probabilistic IDF, are shown below:\n\n![Plot IDF Functions](https://user-images.githubusercontent.com/4745789/76232756-2de1d480-624d-11ea-81cb-8d29109bd594.png)\n\nBy observing the plots of 3 different IDF functions it becomes clear that we should use Probabilistic IDF function when we want to penalize a term that occurs in more than 50% of document by giving it a negative weight; and use a Smooth IDF when we do not want a bounded IDF value and not `undefined` (for `DF(t) = 0`) and `0` (for `DF(t) = N`) as such values ruins a function where IDF is multiplied with some other scalar (like Term Frequency).\n\nSimilarly, we could define our own IDF function by deciding when and how the penalty to be applied and defining the parameters accordingly.\n\n# Role of IDF in TF-IDF\nTF-IDF suggests how important a word is to a document in a collection (corpus). It helps search engines identify what it is that makes a given document special for a given query. It is defined as the product of Term Frequency (number of occurrences of the term in the document) and Inverse Document Frequency.\n\nFor the document to have a high TF-IDF score (high relevance) it needs to have high term frequency and a high inverse document frequency (i.e. low document frequency) of the term. Thus IDF primarily downscales the frequent occurring of common words and boosts the infrequent words with high term frequency.\n\n# References\nThis article is mostly based on the wonderful paper [Understanding Inverse Document Frequency: On theoretical arguments for IDF](https://pdfs.semanticscholar.org/8397/ab573dd6c97a39ff4feb9c2d9b3c1e16c705.pdf) by Stephen Robertson.\n\nOther references:\n\n - [TF-IDF - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n - [Inverse Document Frequency and the Importance of Uniqueness](https://moz.com/blog/inverse-document-frequency-and-the-importance-of-uniqueness)\n\nImages used in other measures of IDF are taken from [Wikipedia page of TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
    "similar": [
      "jaccard-minhash",
      "inheritance-c",
      "recursion-visualizer",
      "constant-folding-python"
    ]
  },
  {
    "id": 57,
    "topic": null,
    "uid": "better-programmer",
    "title": "Eight Rituals to be a Better Programmer",
    "description": "\"How to get better at programming?\" is the question I had been asked quite a few times, and today I lay down the 8 rituals I have been following, and action items for each, to be good and get better at programming.",
    "gif": "https://media.giphy.com/media/14gQ5vtwD050LC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/75548769-0cfecf80-5a54-11ea-8dd2-1e452ba1298b.png",
    "released_at": "2020-02-28",
    "total_views": 2709,
    "body": "\"How to get better at programming?\" is the question I had been asked quite a few times, and today I lay down the 8 rituals I have been following, and action items for each, to be good and get better at programming.\n\n# Code exhaustively\nDoing something repeatedly always helps and writing a lot of code will develop our ability to\n\n - write code while we think\n - think faster, think better\n - foresee requirement changes and possible logic extensions\n\n### Action Items\n\n - One significant contribution to a project every two weeks\n - Solve at least two programming questions (from [Codechef](https://www.codechef.com/), [Spoj](https://www.spoj.com/) or [HackerRank](https://www.hackerrank.com/)) every week, till we solve at least 300 questions\n\n# Code consistently\nIf we don't do something repeatedly, it becomes extremely hard to get good at it. Writing code consistently helps us\n\n - define the programmatic and algorithmic flow quickly\n - build a habit of programming and thinking analytically\n\n### Action Items\n\n - make one small contribution to anyone project every three days\n\n# Once a while build a complex system\nSolving programming questions is about developing logic but things become a little trickier when we build a complex system, as it requires us to take our programming skills to go up a notch. Some examples of complex systems are - a Library management system, a [Twitter](https://twitter.com) clone, an [Instagram](https://www.instagram.com/) clone, etc. Building a complex system\n\n - widens our tech stack\n - makes us keep our code flexible, extensible and reusable\n - helps us understand how to split our code into independent segments that work in harmony\n\n### Action Items\n\n - build one complex system every 4 months\n\n# Once a while build something inspired by the real world\nAfter we spend some time writing programs and solving problems, things become monotonous and do not seem to challenge us anymore, so to spice things up a bit we should model something from the real world, like\n\n  - [projectile motion](https://en.wikipedia.org/wiki/Projectile_motion)\n  - [double pendulum](https://en.wikipedia.org/wiki/Double_pendulum)\n  - [solar system simulation](https://en.wikipedia.org/wiki/Numerical_model_of_the_Solar_System)\n\nThere are lots of libraries and framework like [p5.js](https://p5js.org) that makes visual programming simple.\n\n### Action Items\n\n - once every 6 months model a physical phenomenon\n\n# Read super exhaustively\nIt is not only writing code that improves our programming skills but it is reading some quality code written by expert programmers that make the difference. Reading code written by experts improve our programming vocabulary and by doing this we\n\n - learn the best programming practices\n - discover the new programming paradigms\n - find ways to properly structure our code for extensibility\n\nThe best way to start doing it is by picking up an open-source project and start skimming the code. It is okay to not understand it in the first go but it is important to skim it a few times and get acquainted. After a few skim, everything will fall in place, the code becomes familiar and we start to understand the flow and business logic.\n\n### Action Items\n\n - pick an open-source project every 6 months and skim its code once every two months\n - pick a tiny open-source utility, from an experienced developer, every month and skim it\n\n# Collaborate with a stranger\nThere is always someone sitting on the other side of the globe, who knows a thing or two more than us. Look for them and collaborate on a project. The developer community is filled with super smart and super enthusiastic developers who love to share and collaborate. Use websites like [Dev.to](https://dev.to/), [Hashnode](https://hashnode.com/) and [Twitter](https://twitter.com/) to find and interact with like-minded people.\n\n### Action Items\n\n - collaborate on a project once a year\n - be active on platforms like [Dev.to](https://dev.to/), [Hashnode](https://hashnode.com/) and [Twitter](https://twitter.com/)\n\n# Fundamentals go a long way\nA programming language is just a tool to express business logic. While learning a programming language we should try to understand the constructs and paradigms used - for example: [Functional programming](https://en.wikipedia.org/wiki/Functional_programming), [Polymorphism](https://en.wikipedia.org/wiki/Polymorphism_(computer_science)), [Event driven programming](https://en.wikipedia.org/wiki/Event-driven_programming), [Actor model](https://en.wikipedia.org/wiki/Actor_model), etc. It is important to do so because we could pick constructs from one language and use it in another to solve our problem. For example: picking Functional programming (Callbacks) from Javascript and using it in Python to create generic action functions.\n\n### Action Items\n\n - learn one design pattern every month and build a simulation around it\n - pick a language construct and implement it in some other language\n\n# We think before we code\nWriting code before putting in some thought is degraded the code more often than not. The code written like this lacks simplicity, reusability, and extensibility. Spending some time thinking about problem statement or task at hand and having a rough execution plan always helps.\n\n### Action Items\n\n - always define the scope of implementation, create an execution plan and then code\n\n# Conclusion\nThese rituals have helped me get better at programming with time and in parallel, I pick at max 3 and act on the action items. Programming is simple but being better than most is difficult. Doing it consistently makes one get better by the day.\n",
    "similar": [
      "udemy-sql-taxonomy",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "ts-smoothing",
      "atomicity"
    ]
  },
  {
    "id": 58,
    "topic": null,
    "uid": "python-prompts",
    "title": "Personalize your Python Prompt",
    "description": "Personalization is what we all love. In this article we find how we could personalize the Python interpreter prompt >>>",
    "gif": "https://media.giphy.com/media/TFTqzyOQwT2zS/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/74850923-8ade1f00-5360-11ea-8ed5-1072c8a718c2.png",
    "released_at": "2020-02-21",
    "total_views": 1763,
    "body": "The `>>> ` we see when the Python interactive shell starts, is called the Prompt String. Usually, the prompt string suggests that the interactive shell is now ready to take new commands.\n\n```py\nPython 2.7.10 (default, Feb 22 2019, 21:55:15)\n[GCC 4.2.1 Compatible Apple LLVM 10.0.1 (clang-1001.0.37.14)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>>\n```\n\nPython has 2 prompt strings, one primary `>>>` and one secondary `...` which we usually see when an execution unit (statement) spans multiline, for example: while defining a function\n\n```py\n>>> def foo(a, b):\n...     return a + b\n...\n>>>\n```\n\n# Personalizing the prompt strings\nThe prompt strings are defined in the [sys](https://docs.python.org/3/library/sys.html) module as [ps1](https://docs.python.org/3/library/sys.html#sys.ps1) and [ps2](https://docs.python.org/3/library/sys.html#sys.ps2) and just like any other attribute we can change the values of `sys.ps1` and `sys.ps2` and the changes take effect immediately and as a result, the prompt we see in the shell changes to the new value.\n\n```py\n>>> import sys\n>>> sys.ps1 = '::: '\n:::\n```\n\nFrom the example above we see that changing the value of `sys.ps1` to `::: ` changes the prompt to `::: `.\n\nAs the interactive shell runs in a terminal, we can color and format it using [bash color format](https://misc.flogisoft.com/bash/tip_colors_and_formatting) as shown below\n\n```py\nimport sys\nsys.ps1 = \"\\033[1;33m>>>\\033[0m \"\nsys.ps2 = \"\\033[1;34m...\\033[0m \"\n```\n\nThe code snippet above makes our primary prompt string yellow and secondary prompt string blue. Here's how it looks\n\n![Python colored prompt](https://user-images.githubusercontent.com/4745789/74897098-03be9480-53bc-11ea-8395-7b3bbb1814dd.png)\n\n## Dynamic prompt strings\n\nThe [documentation](https://docs.python.org/3/library/sys.html#sys.ps2) states that if we assign a non-string object to `ps1` or `ps2` then Python prompts by calling `str()` on the object every time a prompt is shown. Now we create some stateful and dynamic prompt by defining a class and overriding the `__str__` method.\n\nBelow we implement [IPython](https://ipython.org/) like prompt where execution statement number is stored in member `line` of the class and is incremented every time the primary prompt renders.\n\n```py\n# -*- coding: utf-8 -*-\nimport sys\n\nclass IPythonPromptPS1(object):\n  def __init__(self):\n    self.line = 0\n\n  def __str__(self):\n    self.line += 1\n    return \"\\033[92mIn [%d]:\\033[0m \" % (self.line)\n\nsys.ps1 = IPythonPromptPS1()\nsys.ps2 = \"    \\033[91m...\\033[0m \"\n```\n\nThe above code snippet makes prompt look like this\n\n![ipython prompt](https://user-images.githubusercontent.com/4745789/74897125-18029180-53bc-11ea-86e6-9d0ca6753fb9.png)\n\n# Setting new prompt strings every time the shell starts\nWe would not want to run this code snippet every time we start the shell and hence we use an environment variable [PYTHONSTARTUP](https://docs.python.org/3/using/cmdline.html#envvar-PYTHONSTARTUP) which holds the path of a readable file and is executed before the first prompt is displayed in interactive mode.\n\nSo we dump the code snippet in a file, say `ipython.py` and export `PYTHONSTARTUP` as\n\n```sh\nexport PYTHONSTARTUP=\"$HOME/ipython.py\"\n```\n\nNow every time, we start our Python interactive shell, it will execute the file `ipython.py` and set the required prompt strings.\n\n# Conclusion\nCombining everything mentioned above I have created a utility called [py-prompts](https://github.com/arpitbbhayani/py-prompts). Here is a glimpse of the themes that the package holds.\n\n![Pretty Python Prompts GIF](https://user-images.githubusercontent.com/4745789/74897216-539d5b80-53bc-11ea-8cdd-91177b6553b5.gif)\n\nI hope you found this piece interesting. Python being an exhaustively extensible language made it super-easy for us to change the prompt strings and be creative with it. If you have a theme idea or have already personalized your prompt, share it with me [@arpit_bhayani](https://twitter.com/arpit_bhayani), I will be thrilled to learn more about it.\n",
    "similar": [
      "python-caches-integers",
      "constant-folding-python",
      "python-iterable-integers",
      "i-changed-my-python"
    ]
  },
  {
    "id": 59,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "rule-30",
    "title": "Pseudorandom Number Generation using Cellular Automata - Rule 30",
    "description": "Generating pseudorandom numbers is an interesting problem in Computer Science. In this article, we dive deep into an algorithm for generating pseudorandom numbers using Rule 30 of Cellular Automaton.",
    "gif": "https://media.giphy.com/media/26uflDxU6cEhrhmUg/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/74463952-b07aac80-4eb8-11ea-8d8e-6f286767ec7e.png",
    "released_at": "2020-02-14",
    "total_views": 1011,
    "body": "A pseudorandom number generator produces numbers deterministically but they seem aperiodic (random) most of the time for most use-cases. The generator accepts a seed value (ideally a true random number) and starts producing the sequence as a function of this seed and/or a previous number of the sequence. These are Pseudorandom (not truly random) because if seed value is known they can be determined algorithmically. True random numbers are hardware generated or generated from blood volume pulse, atmospheric pressure, thermal noise, quantum phenomenon, etc.\n\nThere are lots of [techniques](https://en.wikipedia.org/wiki/List_of_random_number_generators#Pseudorandom_number_generators_(PRNGs)) to generate Pseudorandom numbers, namely: [Blum Blum Shub algorithm](https://en.wikipedia.org/wiki/Blum_Blum_Shub), [Middle-square method](https://en.wikipedia.org/wiki/Middle-square_method), [Lagged Fibonacci generator](https://en.wikipedia.org/wiki/Lagged_Fibonacci_generator), etc. Today we dive deep into [Rule 30](https://en.wikipedia.org/wiki/Rule_30) that uses a controversial science called [Cellular Automaton](https://en.wikipedia.org/wiki/Cellular_automaton). This method passes many standard tests for randomness and was used in [Mathematica](https://www.wolfram.com/mathematica/online/) for generating random integers.\n\n# Cellular Automaton\nBefore we dive into Rule 30, we will spend some time understanding [Cellular Automaton](https://en.wikipedia.org/wiki/Cellular_automaton). A Cellular Automaton is a discrete model consisting of a regular grid, of any dimension, with each cell of the grid having a finite number of states and a neighborhood definition. There are rules that determine how these cells interact and transition into the next generation (state). The rules are mostly mathematical/programmable functions that depend on the current state of the cell and its neighborhood.\n\n![Cellular Automata](https://user-images.githubusercontent.com/4745789/74360178-9bcfe300-4dea-11ea-8c87-91005e89c881.png)\n\nIn the above Cellular Automaton, each cell has 2 finite states `0` (shown in red), `1` (shown in black). Each cell transitions into the next generation by XORing the state values of its 8 neighbors. The first generation (initial state) of the grid is allocated at random and the state transitions, of the entire grid, is as below\n\n![Cellular Automata Demo](https://media.giphy.com/media/J27aUn6QIWZFnVWzEB/giphy.gif)\n\nCellular Automata was originally conceptualized in the 1940s by [Stanislaw Ulam](https://en.wikipedia.org/wiki/Stanislaw_Ulam) and [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann); it finds its application in computer science, mathematics, physics, complexity science, theoretical biology and microstructure modeling. In the 1980s, [Stephen Wolfram](https://en.wikipedia.org/wiki/Stephen_Wolfram) did a systematic study of one-dimensional cellular automata (also called elementary cellular automata) on which Rule 30 is based.\n\n# Rule 30\nRule 30 is an elementary (one-dimensional) cellular automaton where each cell has two possible states `0` (shown in red) and `1` (shown in black). The neighborhood of a cell is its two immediate neighbors, one on its left and other on right. The next state (generation) of the cell depends on its current state and the state of its neighbors; the transition rules are as illustrated below\n\n![Rule 30](https://user-images.githubusercontent.com/4745789/74396927-78805480-4e39-11ea-8349-b6774d05a600.png)\n\nThe above transition rules could be simplified as `left XOR (central OR right)`.\n\nWe visualize Rule 30 in a 2-dimensional grid where each row represents one generation (state). The next generation (state) of the cells is computed and populated in the row below. Each row contains a finite number of cells which \"wraps around\" at the end.\n\n![Rule 30 in action](https://media.giphy.com/media/d9YuURGwsOD8qVt8uE/giphy.gif)\n\nThe above pattern emerges from an initial state (row 0) in a single cell with state 1 (shown as black) surrounded by cells with state 0 (red). The next generation (as seen in row 1) is computed using the rule chart mentioned above. The vertical axis represents time and any horizontal cross-section of the image represents the state of all the cells in the array at a specific point in the pattern's evolution.\n\n![Chaos in Rule 30](https://user-images.githubusercontent.com/4745789/74433188-f1a59900-4e85-11ea-970d-c60af22568ea.png)\n\nAs the pattern evolves, frequent red triangles of varying sizes pop up but the structure as a whole has no recognizable pattern. The above snapshot of the grid was taken at a random point of time and we could observe chaos and aperiodicity. This property is exploited to generate pseudorandom numbers.\n\n## Pseudorandom Number Generation\nAs established earlier, Rule 30 is exhibits aperiodic and chaotic behavior and hence it produces complex, seemingly random patterns from simple, well-defined rules. To generate random numbers from using Rule 30 we use the center column and pick a batch of `n` random bits and form the required `n` bit random number from it. The next random number is built using the next `n` bits from the column.\n\n![Pseudorandom Number Rule 30](https://user-images.githubusercontent.com/4745789/74435575-c2455b00-4e8a-11ea-835b-ca5f722dae9e.png)\n\nIf we always start from the first row, the sequence of the numbers we generate will always be predictable - which is not what we want. To make things pseudorandom, we take a random seed value (ex: current timestamp) and skip that number of bits and then pick batches of `n` and build random numbers.\n\n> The pseudorandom numbers generated using Rule 30 are not cryptographically secure but are suitable for simulation as long as we do not use bad seed like `0`.\n\nOne major advantage of using Rule 30 to generate pseudorandom numbers is that we could generate multiple random numbers in parallel by picking multiple columns to batch `n` bits each at random. A sample 8-bit random integer sequence generated using this method with seed `0` is `220`, `197`, `147`, `174`, `117`, `97`, `149`, `171`, `240`, `241`, etc.\n\nThe seed value could also be used as the initial state (row 0) for Rule 30 and random numbers are then simply the `n` bits batches picked from the center column starting from row 0. This approach is more efficient but is heavily dependent on the quality of seed value, as a bad seed value could make things extremely predictable. A demonstration of this approach could be found on [Wolfram Cloud Demonstration Page](https://demonstrations.wolfram.com/UsingRule30ToGeneratePseudorandomRealNumbers/).\n\n## Rule 30 in the real world\n\nRule 30 is also seen in nature, on the shell of code snail species [Conus textile](https://en.wikipedia.org/wiki/Conus_textile). The [Cambridge North railway station](https://en.wikipedia.org/wiki/Cambridge_North_railway_station#Facilities) is decorated with architectural panels displaying the evolution of Rule 30.\n\n# Conclusion\nIf you found Rule 30 interesting I urge you to write your own simulation of using [p5 library](https://p5js.org/); you could keep it generic enough to so that the program could generate patterns for different rules like 90, 110, 117, etc. The patterns generated using these rules are quite interesting. If you want, you could things to the next level and extend rule to work in 3 dimensions and see how patterns evolve. I believe programming is fun when it is visual.\n\nIt is exciting when two seemingly unrelated fields, Cellular Automata and Cryptography, come together and create something wonderful. Although this algorithm is not widely used anymore, because of more efficient algorithms, it urges us to be creative in using Cellular Automata in more ways than one. This article is first in the series of Cellular Automata, so stay tuned and watch this space for more.\n",
    "similar": [
      "efficient-way-to-stop-an-iterating-loop",
      "publish-python-package-on-pypi",
      "jaccard-minhash",
      "fast-and-efficient-pagination-in-mongodb"
    ]
  },
  {
    "id": 60,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "function-overloading",
    "title": "Function Overloading in Python",
    "description": "Python natively does not support function overloading - having multiple functions with the same name. Today we see how we can implement and add this functionality to Python by using common language constructs like decorators and dictionaries.",
    "gif": "https://media.giphy.com/media/WtCHRSPCuqS8E/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/73909201-04423000-48d2-11ea-8bd0-d7c25f6435c1.png",
    "released_at": "2020-02-07",
    "total_views": 12359,
    "body": "Function overloading is the ability to have multiple functions with the same name but with different signatures/implementations. When an overloaded function `fn` is called, the runtime first evaluates the arguments/parameters passed to the function call and judging by this invokes the corresponding implementation.\n\n```cpp\nint area(int length, int breadth) {\n  return length * breadth;\n}\n\nfloat area(int radius) {\n  return 3.14 * radius * radius;\n}\n```\n\nIn the above example (written in C++), the function `area` is overloaded with two implementations; one accepts two arguments (both integers) representing the length and the breadth of a rectangle and returns the area; while the other function accepts an integer radius of a circle. When we call the function `area` like `area(7)` it invokes the second function while `area(3, 4)` invokes the first.\n\n### Why no Function Overloading in Python?\nPython does not support function overloading. When we define multiple functions with the same name, the later one always overrides the prior and thus, in the namespace, there will always be a single entry against each function name. We see what exists in Python namespaces by invoking functions `locals()` and `globals()`, which returns local and global namespace respectively.\n\n```py\ndef area(radius):\n  return 3.14 * radius ** 2\n\n>>> locals()\n{\n  ...\n  'area': <function area at 0x10476a440>,\n  ...\n}\n```\n\nCalling the function `locals()` after defining a function we see that it returns a dictionary of all variables defined in the local namespace. The key of the dictionary is the name of the variable and value is the reference/value of that variable. When the runtime encounters another function with the same name it updates the entry in the local namespace and thus removes the possibility of two functions co-existing. Hence python does not support Function overloading. It was the design decision made while creating language but this does not stop us from implementing it, so let's overload some functions.\n\n# Implementing Function Overloading in Python\nWe know how Python manages namespaces and if we would want to implement function overloading, we would need to\n\n - manage the function definitions in a maintained virtual namespace\n - find a way to invoke the appropriate function as per the arguments passed to it\n\nTo keep things simple, we will implement function overloading where the functions with the same name are distinguished by the **number of arguments** it accepts.\n\n## Wrapping the function\nWe create a class called `Function` that wraps any function and makes it callable through an overridden `__call__` method and also exposes a method called `key` that returns a tuple which makes this function unique in entire codebase.\n\n```py\nfrom inspect import getfullargspec\n\nclass Function(object):\n  \"\"\"Function is a wrap over standard python function.\n  \"\"\"\n  def __init__(self, fn):\n    self.fn = fn\n\n  def __call__(self, *args, **kwargs):\n    \"\"\"when invoked like a function it internally invokes\n    the wrapped function and returns the returned value.\n    \"\"\"\n    return self.fn(*args, **kwargs)\n\n  def key(self, args=None):\n    \"\"\"Returns the key that will uniquely identify\n    a function (even when it is overloaded).\n    \"\"\"\n    # if args not specified, extract the arguments from the\n    # function definition\n    if args is None:\n      args = getfullargspec(self.fn).args\n\n    return tuple([\n      self.fn.__module__,\n      self.fn.__class__,\n      self.fn.__name__,\n      len(args or []),\n    ])\n```\n\nIn the snippet above, the `key` function returns a tuple that uniquely identifies the function in the codebase and holds\n\n - the module of the function\n - class to which the function belongs\n - name of the function\n - number of arguments the function accepts\n\nThe overridden `__call__` method invokes the wrapped function and returns the computed value (nothing fancy here right now). This makes the instance callable just like the function and it behaves exactly like the wrapped function.\n\n```py\ndef area(l, b):\n  return l * b\n\n>>> func = Function(area)\n>>> func.key()\n('__main__', <class 'function'>, 'area', 2)\n>>> func(3, 4)\n12\n```\n\nIn the example above, the function `area` is wrapped in `Function` instantiated in `func`. The `key()` returns the tuple whose first element is the module name `__main__`, second is the class `<class 'function'>`, the third is the function name `area` while the fourth is the number of arguments that function `area` accepts which is `2`.\n\nThe example also shows how we could just call the instance `func`, just like the usual `area` function, with arguments `3` and `4` and get the response `12`, which is exactly what we'd get is we would have called `area(3, 4)`. This behavior would come in handy in the later stage when we play with decorators.\n\n## Building the virtual Namespace\nVirtual Namespace, we build here, will store all the functions we gather during the definition phase. As there be only one namespace/registry we create a singleton class that holds the functions in a dictionary whose key will not be just a function name but the tuple we get from the `key` function, which contains elements that uniquely identify function in the entire codebase. Through this, we will be able to hold functions in the registry even if they have the same name (but different arguments) and thus facilitating function overloading.\n\n```py\nclass Namespace(object):\n  \"\"\"Namespace is the singleton class that is responsible\n  for holding all the functions.\n  \"\"\"\n  __instance = None\n\n  def __init__(self):\n    if self.__instance is None:\n      self.function_map = dict()\n      Namespace.__instance = self\n    else:\n      raise Exception(\"cannot instantiate a virtual Namespace again\")\n\n  @staticmethod\n  def get_instance():\n    if Namespace.__instance is None:\n      Namespace()\n    return Namespace.__instance\n\n  def register(self, fn):\n    \"\"\"registers the function in the virtual namespace and returns\n    an instance of callable Function that wraps the\n    function fn.\n    \"\"\"\n    func = Function(fn)\n    self.function_map[func.key()] = fn\n    return func\n```\n\nThe `Namespace` has a method `register` that takes function `fn` as an argument, creates a unique key for it, stores it in the dictionary and returns `fn` wrapped within an instance of `Function`. This means the return value from the `register` function is also callable and (till now) its behavior is exactly the same as the wrapped function `fn`.\n\n```py\ndef area(l, b):\n  return l * b\n\n>>> namespace = Namespace.get_instance()\n>>> func = namespace.register(area)\n>>> func(3, 4)\n12\n```\n\n## Using decorators as a hook\nNow that we have defined a virtual namespace with an ability to register a function, we need a hook that gets called during function definition; and here use Python decorators. In Python, a decorator wraps a function and allows us to add new functionality to an existing function without modifying its structure. A decorator accepts the wrapped function `fn` as an argument and returns another function that gets invoked instead. This function accepts `args` and `kwargs` passed during function invocation and returns the value.\n\nA sample decorator that times execution of a function is demonstrated below\n\n```py\nimport time\n\n\ndef my_decorator(fn):\n  \"\"\"my_decorator is a custom decorator that wraps any function\n  and prints on stdout the time for execution.\n  \"\"\"\n  def wrapper_function(*args, **kwargs):\n    start_time = time.time()\n\n    # invoking the wrapped function and getting the return value.\n    value = fn(*args, **kwargs)\n    print(\"the function execution took:\", time.time() - start_time, \"seconds\")\n\n    # returning the value got after invoking the wrapped function\n    return value\n\n  return wrapper_function\n\n\n@my_decorator\ndef area(l, b):\n  return l * b\n\n\n>>> area(3, 4)\nthe function execution took: 9.5367431640625e-07 seconds\n12\n```\n\nIn the example above we define a decorator named `my_decorator` that wraps function `area` and prints on `stdout` the time it took for the execution.\n\nThe decorator function `my_decorator` is called every time (so that it wraps the decorated function and store this new wrapper function in Python's local or global namespace) the interpreter encounters a function definition, and it is an ideal hook, for us, to register the function in our virtual namespace. Hence we create our decorator named `overload` which registers the function in virtual namespace and returns a callable to be invoked.\n\n```py\ndef overload(fn):\n  \"\"\"overload is the decorator that wraps the function\n  and returns a callable object of type Function.\n  \"\"\"\n  return Namespace.get_instance().register(fn)\n```\n\nThe `overload` decorator returns an instance of `Function`, as returned by `.register()` the function of the namespace. Now whenever the function (decorated by `overload`) is called, it invokes the function returned by the `.register()` function - an instance of `Function` and the `__call__` method gets executed with specified `args` and `kwargs` passed during invocation. Now what remains is implementing the `__call__` method in class `Function` such that it invokes the appropriate function given the arguments passed during invocation.\n\n## Finding the right function from the namespace\nThe scope of disambiguation, apart from the usuals module class and name, is the number of arguments the function accepts and hence we define a method called `get` in our virtual namespace that accepts the function from the python's namespace (will be the last definition for the same name - as we did not alter the default behavior of Python's namespace) and the arguments passed during invocation (our disambiguation factor) and returns the disambiguated function to be invoked.\n\nThe role of this `get` function is to decide which implementation of a function (if overloaded) is to be invoked. The process of getting the appropriate function is pretty simple - from the function and the arguments create the unique key using `key` function (as was done while registering) and see if it exists in the function registry; if it does then fetch the implementation stored against it.\n\n```py\ndef get(self, fn, *args):\n  \"\"\"get returns the matching function from the virtual namespace.\n\n  return None if it did not fund any matching function.\n  \"\"\"\n  func = Function(fn)\n  return self.function_map.get(func.key(args=args))\n```\n\nThe `get` function creates an instance of `Function` just so that it could use the `key` function to get a unique key and not replicate the logic. The key is then used to fetch the appropriate function from the function registry.\n\n## Invoking the function\nAs stated above, the `__call__` method within class `Function` is invoked every time a function decorated with an `overload` decorator is called. We use this function to fetch the appropriate function using the `get` function of namespace and invoke the required implementation of the overloaded function. The `__call__` method is implemented as follows\n\n```py\ndef __call__(self, *args, **kwargs):\n  \"\"\"Overriding the __call__ function which makes the\n  instance callable.\n  \"\"\"\n  # fetching the function to be invoked from the virtual namespace\n  # through the arguments.\n  fn = Namespace.get_instance().get(self.fn, *args)\n  if not fn:\n    raise Exception(\"no matching function found.\")\n\n  # invoking the wrapped function and returning the value.\n  return fn(*args, **kwargs)\n```\n\nThe method fetches the appropriate function from the virtual namespace and if it did not find any function it raises an `Exception` and if it does, it invokes that function and returns the value.\n\n## Function overloading in action\nOnce all the code is put into place we define two functions named `area`: one calculates the area of a rectangle and the other calculate the area of a circle. Both functions are defined below and decorated with an `overload` decorator.\n\n```py\n@overload\ndef area(l, b):\n  return l * b\n\n@overload\ndef area(r):\n  import math\n  return math.pi * r ** 2\n\n\n>>> area(3, 4)\n12\n>>> area(7)\n153.93804002589985\n```\n\nWhen we invoke `area` with one argument it returns the area of a circle and when we pass two arguments it invokes the function that computes the area of a rectangle thus overloading the function `area`. You can find the entire working demo [here](https://repl.it/@arpitbbhayani/Python-Function-Overloading).\n\n> Python supports function overloading using [functools.singledispatch](https://docs.python.org/3/library/functools.html#functools.singledispatch) since Python 3.4 and supports overloading on class and instance methods using [functools.singledispatchmethod](https://docs.python.org/3/library/functools.html#functools.singledispatchmethod) since Python 3.8. Thanks [Harry Percival](https://twitter.com/hjwp) for the correction.\n\n# Conclusion\nPython does not support function overloading but by using common language constructs we hacked a solution to it. We used decorators and a user-maintained namespace to overload functions and used the number of arguments as a disambiguation factor. We could also use data types (defined in decorator) of arguments for disambiguation - which allows functions with the same number of arguments but different types to overload. The granularity of overload is only limited by function `getfullargspec` and our imagination. A neater, cleaner and more efficient approach is also possible with the above constructs so feel free to implement one and tweet me [@arpit_bhayani](https://twitter.com/arpit_bhayani), I will be thrilled to learn what you have done with it.\n",
    "similar": [
      "python-iterable-integers",
      "recursion-visualizer",
      "fsm",
      "string-interning"
    ]
  },
  {
    "id": 61,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "isolation-forest",
    "title": "Isolation Forest Algorithm for Anomaly Detection",
    "description": "Anomaly detection is an age-old problem and in this article, we dive deep into an unsupervised algorithm, Isolation Forest, that beautifully exploits the characteristics of anomalies. Instead of profiling normal points and labeling others as anomalies, the algorithm is actually is tuned to detect anomalies.",
    "gif": "https://media.giphy.com/media/xGdvlOVSWaDvi/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/73280907-34743980-4215-11ea-89f0-eac4a71df6e5.png",
    "released_at": "2020-01-28",
    "total_views": 1134,
    "body": "Anomaly detection is identifying something that could not be stated as \"normal\"; the definition of \"normal\" depends on the phenomenon that is being observed and the properties it bears. In this article, we dive deep into an unsupervised anomaly detection algorithm called [Isolation Forest](https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/icdm08b.pdf). This algorithm beautifully exploits the characteristics of anomalies, keeping it independent of data distributions making the approach novel.\n\n### Characteristics of anomalies\nSince anomalies deviate from normal, they are few in numbers (minority) and/or have attribute values that are very different from those of normal. The paper nicely puts it as **few and different**. These characteristics of anomalies make them more susceptible to isolation than normal points and form the guiding principle of the Isolation Forest algorithm.\n\n# The usual approach for detecting anomalies\nThe existing models train to see what constitutes \"normal\" and then label everything that does not conform to this definition as anomalies. Almost every single algorithm has its own way of defining a normal point/instance; some do it through statistical methods, some use classification or clustering but in the end, the process remains the same - define normal and filter out everything else.\n\n### The issue with the usual approach\n\nThe usual methods are not optimized to detect anomalies, instead, they are optimized to find normal instances, because of which the result of anomaly detection either contains too many false positives or might detect too few anomalies.\nMany of these methods are computationally complex and hence suit low dimensional and/or small-sized data.\n\nIsolation Forest algorithm addresses both of the above concerns and provides an efficient and accurate way to detect anomalies.\n\n# The algorithm\nNow we take a go through the algorithm, and dissect it stage by stage and in the process understand the math behind it. Fasten your seat belts, it's going to be a bumpy ride.\n\n## The core principle\nThe core of the algorithm is to \"isolate\" anomalies by creating decision trees over random attributes. The random partitioning produces noticeable shorter paths for anomalies since\n\n - fewer instances (of anomalies) result in smaller partitions\n - distinguishable attribute values are more likely to be separated in early partitioning\n\nHence, when a forest of random trees collectively produces shorter path lengths for some particular points, then they are highly likely to be anomalies.\n\n![Decision tree splits for normal points and anomalies](https://user-images.githubusercontent.com/4745789/73243800-804fc000-41ce-11ea-826f-14cbc407af99.png)\n\nThe diagram above shows the number of splits required to isolate a normal point and an anomaly. Splits, represented through blue lines, happens at random on a random attribute and in the process building a decision tree. The number of splits determines the level at which the isolation happened and will be used to generate the anomaly score.\n\nThe process is repeated multiple times and we note the isolation level for each point/instance. Once the iterations are over, we generate an anomaly score for each point/instance, suggesting its likeliness to be an anomaly. The score is a function of the average level at which the point was isolated. The top `m` gathered on the basis of the score, are labeled as anomalies.\n\n## Construction of decision tree\nThe decision tree is constructed by splitting the sub-sample points/instances over a split value of a randomly selected attribute such that the instances whose corresponding attribute value is smaller than the split value goes left and the others go right, and the process is continued recursively until the tree is fully constructed. The split value is selected at random between the minimum and maximum values of the selected attribute.\n\nThere are two types of node in the decision tree\n\n### Internal Node\n\nInternal nodes are non-leaf and contain the split value, split attribute and pointers to two child sub-trees. An internal node is always a parent to two child sub-trees making the entire decision tree a proper binary tree.\n\n### External Node\n\nExternal nodes are leaf nodes that could not be split further and reside at the bottom of the tree. Each external node will hold the size of the un-built subtree which is used to calculate the anomaly score.\n\n![Decision tree with internal and external nodes](https://user-images.githubusercontent.com/4745789/73272711-d5a8c300-4208-11ea-9bb7-80894312f16c.png)\n\n## Why sub-sampling helps\nThe Isolation Forest algorithm works well when the trees are created, not from the entire dataset, but from a sub-sampled data set. This is very different from almost all other techniques where they thrive on data and demands more of it for greater accuracy. Sub-sampling works wonder in this algorithm because normal instances can interfere with the isolation process by being a little closer to the anomalies.\n\n![Importance of sub-sampling in Isolation Forest](https://user-images.githubusercontent.com/4745789/73296518-df91ec80-422f-11ea-8c6b-2a2fcbf8afc8.png)\n\nThe image above shows how sub-sampling actually makes a clear separation between normal points and anomalies. In the original dataset, we see that normal points and very close to anomalies making detection tougher and inaccurate (with a lot of false negatives). Because of sub-sampling, we could see a clear separation of anomalies and normal instances. This makes the entire process of anomaly detection efficient and accurate.\n\n### Optimizing decision tree construction\n\nSince anomalies are susceptible to isolation and have a tendency to reside closer to the root of the decision tree, we construct the decision tree till it reaches a certain height `max_height` and not split points further. This height is the height post which we are (almost) sure that there could not be any anomalies.\n\n```py\ndef construct_tree(X, current_height, max_height):\n  \"\"\"The function constructs a tree/sub-tree on points X.\n\n  current_height: represents the height of the current tree to\n    the root of the decision tree.\n  max_height: the max height of the tree that should be constructed.\n\n  The current_height and max_height only exists to make the algorithm efficient\n  as we assume that no anomalies exist at depth >= max_height.\n  \"\"\"\n  if current_height >= max_height:\n    # here we are sure that no anomalies exist hence we\n    # directly construct the external node.\n    return new_external_node(X)\n\n  # pick any attribute at random.\n  attribute = get_random_attribute(X)\n\n  # for set of inputs X, for the tree we get a random value\n  # for the chosen attribute. preferably around the median.\n  split_value = get_random_value(max_value, min_value)\n\n  # split X instances based on `split_values` into Xl and Xr\n  Xl = filter(X, lambda x: X[attribute] < split_value)\n  Xr = filter(X, lambda x: X[attribute] >= split_value)\n\n  # build an internal node with its left subtree created from Xl\n  # and right subtree created from Xr, recursively.\n  return new_internal_node(\n    left=construct_tree(Xl, current_height + 1, max_height),\n    right=construct_tree(Xr, current_height + 1, max_height),\n    split_attribute=attribute,\n    split_value=split_value,\n  )\n```\n\n## Constructing the forest\nThe process of tree construction is repeated multiple times and each time we pick a random sub-sample and construct the tree. There are no strict rules to determine the number of iterations, but in general, we could say the more the merrier. The sub-sampling count is also a parameter and could change depending on the data set.\n\nThe pseudocode for forest construction is as follows\n\n```py\ndef construct_forest(X, trees_count, subsample_count):\n  \"\"\"The function constructs a forest from given inputs/data points X.\n  \"\"\"\n  forest = []\n  for i in range(0, trees_count):\n    # max_height is in fact the average height of the tree that would be\n    # constructed from given points. This acts as max_height for the\n    # construction because we are only interested in data points that have\n    # shorter-than-average path lengths, as those points are more likely\n    # to be anomalies.\n    max_height = math.ceil(math.log2(subsample_count))\n\n    # create a sample with cardinality of `subsample_count` from X\n    X_sample = get_sample(X, subsample_count)\n\n    # construct the decision tree from the sample\n    tree = construct_tree(X_sample, 0, max_height)\n\n    # add the tree to the forest\n    forest.append(tree)\n\n  return forest\n```\n\nWhile constructing the tree we pass `max_height` as `log2(nodes_count)` as that is the average height of a proper binary tree that could be constructed from `nodes_count` number of nodes. Since anomalies reside closer to the root node it is highly unlikely that any anomaly will isolate after the tree has reached height `max_height`. This helps us save a lot of computation and tree construction making it computationally and memory efficient.\n\n## Scoring the anomalies\nEvery anomaly detection algorithm has to score its data points/instances and quantify the confidence the algorithm has on its potential anomalies. The generated anomaly score has to be bounded and comparable. In Isolation Forest, that fact that anomalies always stay closer to the root, becomes our guiding and defining insight that will help us build a scoring function. The anomaly score will a function of path length which is defined as\n\n> Path Length `h(x)` of a point `x` is the number of edges `x` traverses from the root node.\n\nAs the maximum possible height of the tree grows by order of `n`, the average height grows by `log(n)` - this makes normalizing of the scoring function a little tricky. To remedy this we use the insights from the structure of the decision tree. The decision tree has two types of nodes internal and external such that external has no child while internal is a parent to exactly two nodes - which means the decision tree is a proper binary tree and hence we conclude\n\n> The average path length `h(x)` for external node termination is the same as the average path length of unsuccessful search in BST.\n\nIn a BST, an unsuccessful search always terminates at a `NULL` pointer and if we treat external node of the decision tree as `NULL` of BST, then we could say that average path length of external node termination is same as average path length of unsuccessful search in BST (constructed only from internal nodes of the decision tree), and it is given by\n\n![BST unsuccessful search estimation](https://user-images.githubusercontent.com/4745789/73191802-198ac200-414e-11ea-9500-039483b6e780.png)\n\nwhere `H(i)` is the [harmonic number](https://en.wikipedia.org/wiki/Harmonic_number) and it can be estimated by `ln(i) + 0.5772156649` ([Euler\u2013Mascheroni constant](https://en.wikipedia.org/wiki/Euler%E2%80%93Mascheroni_constant)). `c(n)` is the average of path length `h(x)` given `n`, we use it to normalize `h(x)`.\n\n_To understand the derivation in detail refer to the references at the end of this article._\n\nThe anomaly score of an instance `x` is defined as\n\n![scoring function](https://user-images.githubusercontent.com/4745789/73192432-075d5380-414f-11ea-86dc-ae6acda7b7d4.png)\n\nwhere `E(h(x))` is the average path length (average of `h(x)`) from a collection of isolation trees. From the scoring function defined above, we could deduce that if\n\n - the score is very close to 1, then they are definitely anomalies\n - the score is much smaller than 0.5, then they are quite safe to be regarded as normal instances, and\n - all the instances return around 0.5, then the entire sample does not really have any distinct anomaly\n\n## Evaluating anomalies\n\nIn the evaluation stage, an anomaly score is derived from the expected path length `E(h(x))` for each test instance. Using `get_path_length` function (pseudocode below), a single path length `h(x)` is calculated by traversing through the decision tree.\n\nIf iteration terminates at an external node where `size > 1` then the return value is `e` (number of edges traversed till current node) plus an adjustment `c(size)`, estimated from the formula above. This adjustment is for the unbuilt decision tree (for efficiency) beyond the max height. When `h(x)` is obtained for each node of each tree, an anomaly score is produced by computing `s(x, sample_size)`. Sorting instances by the score `s` in descending order and getting top `m` will yield us `m` anomalies.\n\n```py\ndef get_path_length(x, T, e):\n  \"\"\"The function returns the path length h(x) of an instance\n  x in tree `T`.\n\n  here e is the number of edges traversed from the root till the current\n  subtree T.\n  \"\"\"\n  if is_external_node(T):\n    # when T is the root of an external node subtree\n    # we estimate path length and return.\n\n    # here c is the function which estimates the average path length\n    # for external node termination.\n    return e + c(len(T))\n\n  # T is the root of an internal node then we\n  if x[T.split_attribute] < T[split_value]:\n    # instance x may lie in left subtree\n    return get_path_length(x, T.left, e + 1)\n  else:\n    # instance x may lie in right subtree\n    return get_path_length(x, T.right, e + 1)\n```\n\n## References for BST unsuccessful search estimation\n - [IIT KGP, Algorithms, Lecture Notes - Page 7](https://cse.iitkgp.ac.in/~pb/algo-1-pb-10.pdf)\n - [What is real big-O of search in BST?](https://www.cs.csustan.edu/~john/classes/previous_semesters/cs3100_datastructures/2000_04_Fall/Examples/Trees/averageSearchInBST.html)\n - [CMU CMSC 420: Lecture 5 - Slide 13](https://www.cs.cmu.edu/~ckingsf/bioinfo-lectures/trees.pdf)\n - [CISE UFL: Data Structures, Algorithms, & Applications - 1st Proof](https://www.cise.ufl.edu/~sahni/dsaac/public/exer/c18/e47.htm)\n\n\n# Conclusion\nThe isolation forest algorithm thrives on sub-sampled data and does not need to build the tree from the entire data set; it works well with sub-sampled data. While constructing the tree, we need not build tree taller than `max_height` (very cheap to compute), making it low on memory footprint. Since the algorithm does not depend on computationally expensive operations like distance or density calculation, it executes really fast. The training stage has a linear time complexity with a low constant and hence could be used in a real-time online system.\n\nI hope this article helped you to understand Isolation Forest, an unsupervised anomaly detection algorithm. I stumbled upon this through an engineering [blog](https://lambda.grofers.com/anomaly-detection-using-isolation-forest-80b3a3d1a9d8) of [Grofers](https://grofers.com/). This algorithm was very interesting to me because of its novel approach and hence I dived deep into it. FYI: In 2018, Isolation Forest was extended by [Sahand Hariri, Matias Carrasco Kind, Robert J. Brunner](https://arxiv.org/pdf/1811.02141.pdf). I have not read the Extended Isolation Forest algorithm but have definitely added it to my reading list. I recommend that if you liked this algorithm you should definitely give the extended version a skim.\n",
    "similar": [
      "slowsort",
      "flajolet-martin",
      "phi-accrual",
      "morris-counter"
    ]
  },
  {
    "id": 62,
    "topic": null,
    "uid": "image-steganography",
    "title": "Image Steganography",
    "description": "Steganography has been around since at least 440 BCE but with the rise of computers, the techniques have evolved to handle digital data. In this article, we see the science behind image steganography, how it exploits various properties of images to hide secrets and walkthrough a few techniques in detail.",
    "gif": "https://media.giphy.com/media/V1NxC1YoNEHBe/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/72659471-2a1c9900-39e6-11ea-9b93-c11bf3eefc43.png",
    "released_at": "2020-01-17",
    "total_views": 304,
    "body": "Would you shave your head and get it tattooed? Probably no, but a slave in ancient Greece was made to do so in the 440 BCE by a ruler named [Histiaeus](https://en.wikipedia.org/wiki/Histiaeus). The text that was tattooed was a secret message that Histiaeus wanted to send to his son-in-law Aristagoras in Miletus. After his hair grew back the slave left for Miletus and upon his arrival, his head was shaved again and the message was revealed which told Aristagoras to revolt against the Persians and start the [Ionian revolt](https://en.wikipedia.org/wiki/Ionian_Revolt).\n\nThis art of concealing message is called Steganography. The word is derived from the Greek word \u201c\u03c3\u03c4\u03b5\u03b3\u03b1\u03c5\u03c9\u201d which means \"secret or covered writing\". In modern times, steganography can be looked into as the study of the art and science of communicating in a way that hides the presence of the communication.\n\nSteganography continued over time to develop into new levels. Invisible inks, microdots, writing behind postal stamps are all examples of steganography in its physical form. Most of these early developments happened during World War I and II where everyone was trying to outsmart each other. The left half of the image below is a bunch of microdots, sent by German spies and intercepted by Allied intelligence, and the right half is the camera that was used to print such microdots.\n\n![Microdots and Microdot Camera](https://user-images.githubusercontent.com/4745789/72497176-da0ccd80-3851-11ea-96b0-759d7e62f451.png)\n\n### Steganography and Cryptography\n\nSince the rise of the Internet, making communication more secure has been a priority. This lead to the development of the field of Cryptography that deals with hiding the meaning of a message. The techniques of cryptography try to ensure that it becomes extremely difficult to extract the true meaning of the message when it goes into the wrong hands.\n\nSometimes, it becomes necessary to not only hide the meaning of the message but also hide its existence, and the field that deals with this is called Steganography. Both cryptography and steganography, protect the information in their own way but neither alone is perfect and can be compromised. Hence a hybrid approach where we encrypt the message and then hide its presence amplifies the security.\n\nToday steganography is mostly used on computers with digital data, like Image, Audio, Video, Network packets, etc, acting as the carriers. There are a bunch of techniques for each of them but this article aims to provide an exhaustive overview of Image Steganography.\n\n# Image Steganography\n\nImages are an excellent medium for concealing information because they provide a high degree of redundancy - which means that there are lots of bits that are there to provide accuracy far greater than necessary for the object's use (or display). Steganography techniques exploit these redundant bits to hide the information/payload by altering them in such a way that alterations cannot be detected easily by humans or computers.\n\n## Color depth and definition\n\nAn image is a collection of numbers that defines color intensities in different areas of the image. It is arranged in a gird, which is the resolution of the image, and each point on the grid is called a pixel. Each pixel is defined by a fixed number of bits and this is its color scheme. The smallest color depth is 8 bit (in monochrome and greyscale images) and it displays 256 different colors or shades of grey as shown below.\n\n![8-bit grayscale monochrome image](https://user-images.githubusercontent.com/4745789/72497072-8e5a2400-3851-11ea-9b28-8705bbfea070.png)\n\nDigital color images are typically stored in 24-bit pixel depth and uses the RGB color model. All color variations for the pixels of a 24-bit image are derived from three primary colors: red, green and blue, and each primary color is represented by 8 bits. Thus each pixel takes one from a palette of 16-million colors.\n\n![24-bit color palette](https://user-images.githubusercontent.com/4745789/72497287-23f5b380-3852-11ea-96e8-e5c8ffca0c9f.png)\n\n## Compression\n\nWhen working with high-resolution images with greater color depth, the size of the raw file can become big and it becomes impossible to transmit it over a standard internet connection. To remedy this, compressed image formats were developed which, as you would have guessed, compresses the pixel information and keeps file sizes fairly small, making it efficient for transmission.\n\nCompression techniques can be broadly classified into the following two classes\n\n### Lossy Compression\nLossy compression removes redundancies that are too small for the human eye to differentiate which makes the compressed files a close approximate, but not an exact duplicate of the original one. A famous file format that does lossy compression is [JPEG](https://en.wikipedia.org/wiki/JPEG).\n\n### Lossless Compression\nLossless compression never removes any information from the original image, but instead represents data in mathematical formulas maintaining the integrity of the original image and when uncompressed, the file is a bit-by-bit copy of the original. Formats that do lossless compression are [PNG](https://en.wikipedia.org/wiki/Portable_Network_Graphics), [GIF](https://en.wikipedia.org/wiki/GIF), and [BMP](https://en.wikipedia.org/wiki/BMP_file_format).\n\nSteganographic techniques take into account file formats, compression methods, and picture semantics and exploit them to find redundancies and use them to conceal secret information and can be broadly classified into two: spatial domain and frequency domain techniques, and we take a deeper look into both.\n\n# Spatial Domain Techniques\nSpatial domain techniques embed the secret message/payload in the intensity of the pixels directly; which means they update the pixel data by either inserting or substituting bits. Lossless images are best suited for these techniques as compression would not alter the embedded data. These techniques have to be aware of the image format to make concealing information fool-proof.\n\n## LSB Substitution\nThis technique converts the secret message/payload into a bitstream and substitutes them into a least significant bit (the 8th bit) of some or all bytes inside an image. The alterations happen on the least significant bit which changes the intensity by +-1 which is extremely difficult for the human eye to detect.\n\n![LSB substitution](https://user-images.githubusercontent.com/4745789/72587393-1eb06b80-391b-11ea-89ce-eb72be220a89.png)\n\nWhen using a 24-bit image, a bit of each of the red, green and blue color components is substituted. Since there are 256 possible intensities of each primary color, changing the LSB of pixel results in small changes in the intensity of the colors.\n\n![24-bit image LSB substitution](https://user-images.githubusercontent.com/4745789/72589054-2de5e800-3920-11ea-9c0a-c9f878fcd525.png)\n\nSee if you can spot what has changed in the images below. The image on the right has about 1KB long text message embedded through LSB substitution but looks the same as the original image.\n\n![LSB substitution cat image difference](https://user-images.githubusercontent.com/4745789/72535218-31d12600-389e-11ea-9463-011fa42e430c.png)\n\nIn a 24 bit image we can store 3 bits in each pixel hence an 800 \u00d7 600 pixel image, can thus store a total amount of 1,440,000 bits or 180,000 bytes ~ 175KB of embedded data.\n\n## Extending LSB to k-LSB\nTo hold more data into the image we can substitute not `1` but `k` least significant bits. But when we do so the image starts to distort which is never a good sign but a well-chosen image could do the trick and you wouldn't notice any difference.\n\n## Randomized LSB\nA regular LSB substitution technique starts substituting from pixel `0` and goes till `n` making this method highly predictable. To make things slightly challenging sender and receiver could share a secret key through which they agree on the certain pixels that will be altered making the technique more robust.\n\n## Adaptive LSB\nAdaptive LSB uses k-bit LSB and varies `k` as per the sensitivity of the image region over which it is applied. The method analyzes the edges, brightness, and texture of the image and calculates the value of `k` for that region and then does regular k-LSB substitution on it. It keeps the value of `k` high at a not-so-sensitive image region and low at the sensitive region. These alterations ensure that the overall quality of the image is balanced and distortions harder to detect.\n\n**Pixel-value differencing (PVD)** scheme is a concrete implementation of adaptive LSB and it uses the difference of values between two consecutive pixels in a block to determine the number of secret bits to be embedded.\n\n## LSB and Palette Based Images\nThe persistence of Palette Based Images is very interesting. There is a color lookup table which holds all the colors that are used in the image. Each pixel is represented as a single byte and the pixel data is an index to the color palette. [GIF](https://en.wikipedia.org/wiki/GIF) images work on this principle; it cannot have a bit depth greater than 8, thus the maximum number of colors that a GIF can store is 256. Now if we perform LSB substitution to pixel data then it changes the index in the lookup table (palette) and the new value (after substitution), that points to the index on the lookup table (palette), could point to a different color and the change will be evident. We could still do steganography on palette-based images using following workarounds\n\n![palette-based image](https://user-images.githubusercontent.com/4745789/72600791-4ebb3700-393a-11ea-8e3e-2ddf389e85d1.png)\n\n### Sorting the palette\nThe LSB substitution alters the value by +-1 and hence it will always point to a neighboring entry in the table. Hence we sort the palette by color then this will make adjacent lookup table entries similar to each other and minimize the distortion.\n\n### Add new colors to the palette\nIf the original image has fewer colors then we could add similar colors in color palette/lookup table and then perform regular LSB substitution. Again the +-1 alteration will make that pixel point to some similar color in the lookup table.\n\n## Other techniques\nApart from the above-mentioned LSB substitution technique, there are techniques that exploit some aspect of the image and embeds data. I would highly recommend you at least give a skim to each of the below:\n\n - [Edges based data embedding method (EBE)](https://link.springer.com/article/10.1186/1687-417X-2014-8)\n - [Random pixel embedding method (RPE)](https://ieeexplore.ieee.org/abstract/document/8276335)\n - [Mapping pixel to hidden data method](https://www.researchgate.net/publication/26623039_Image_Steganography_by_Mapping_Pixels_to_Letters)\n - [Labeling or connectivity method](https://www.researchgate.net/publication/239551978_Labeling_Method_in_Steganography)\n\n# Frequency Domain Techniques\nSpatial domain techniques directly start putting in data from payload into an image but Frequency-domain techniques will first transform the image and then embed the data. The transformation step ensures that the message is hidden in less sensitive areas of the image, making the hiding more robust and makes the entire process independent of the image format. The areas in which the information is hidden are usually less exposed to compression, cropping, and image processing.\n\nThese techniques are relatively complex to comprehend and require a bit of advanced mathematics to understand thoroughly. Images with lossy compression are ideal candidates and hence we dive a little deep into how JPEG steganography works.\n\n## JPEG steganography\nTo understand how steganography works for JPEG files, we will look into: how the raw data is compressed by JPEG and then we see how we could hide data in it.\n\n### JPEG Compression\nAccording to research, the human eye is more sensitive to changes in the brightness (luminance) of a pixel than to changes in its color. We interpret brightness and color by contrast with adjacent regions. The compression phase takes advantage of this insight and transforms the image from RGB color to [YCbCr](https://en.wikipedia.org/wiki/YCbCr) representation - separating brightness from color. In YCbCr representation, the Y component corresponds to luminance (brightness - black-white) and Cb (yellow-blue) and Cr (green-red) components for chrominance (color). Now we discard some of the color data by downsampling it to half in both horizontal and vertical directions thus directly reducing the size of the file by a factor of 2.\n\n![YCbCr transformation](https://user-images.githubusercontent.com/4745789/72549559-f1ca6d00-38b6-11ea-9760-bd1f35dbf455.png)\n\nNow the image, in YCbCr representation, is processed in blocks of 8 x 8 and we perform [Discrete Cosine Transform (DCT)](https://en.wikipedia.org/wiki/Discrete_cosine_transform) on each, then quantized (rounding) 64 values into 1 by taking the average. The quantization step is the one that removes redundant information from the image. To dive more into DCT on JPEG, I would recommend you watch this [Computerphile video](https://www.youtube.com/watch?v=Q2aEzeMDHMA).\n\nThis is the first stage of JPEG compression which is lossy. Now this image data is then losslessly compressed using the standard [Huffman encoding](https://en.wikipedia.org/wiki/Huffman_coding).\n\n### JPEG Steganography\nSince JPEG images are already lossily compressed (redundant bits are already thrown out) it was thought that steganography would not be possible on it. So if we would try to hide or embed any message in it, it might get either lost, destroyed or altered during compression, adding some noticeable changes to the image. The complete JPEG encoding process is as shown in the diagram below\n\n![JPEG Process](https://user-images.githubusercontent.com/4745789/72615006-a87f2980-3959-11ea-872f-733c9523a411.png)\n\nThe entire process could be split into two stages, the first is where redundancy is removed and the second is where the data is encoded using Huffman encoding. During the DCT transformation phase, rounding errors occur in the coefficient data that are not noticeable and this makes the algorithm lossy. Once this stage is over we have a chance to perform usual LSB substitution and embed the message. Since stage 2 of JPEG compression is lossless, due to Huffman encoding, we are sure that none of our substituted data will be lost. Thus we sandwich the steganography between the lossy and lossless stages of JPEG compression.\n\n## Other techniques\nApart from the above-mentioned DCT technique, there are techniques that use a different form of transform signal and embeds secret data. To name a few\n\n - [Discrete Fourier transformation technique (DFT)](https://link.springer.com/chapter/10.1007/978-3-642-20998-7_39)\n - [Discrete Wavelet transformation technique (DWT)](https://www.insight-centre.org/sites/default/files/publications/17.197_a_steganography_technique_for_images_based_on_wavelet_transform.pdf)\n - [Lossless or reversible method (DCT)](https://www.researchgate.net/publication/330565811_Hiding_data_in_images_using_DCT_steganography_techniques_with_compression_algorithms)\n - [Embedding in coefficient bits](http://www.ijcee.org/papers/533-P0025.pdf)\n\n# Conclusion\nThis is the first article in the series of Steganography that detailed out Image Steganography. I hope you reaped some benefits out of it. The future articles on Steganography will talk about how it is done on carriers like Audio, Network, [DNA](https://www.sciencedirect.com/science/article/pii/S1877050917319804) and [Quantum states](https://arxiv.org/abs/1006.1934) and will also dive into one of the most interesting applications of Steganography - a [Steganographic File System](https://en.wikipedia.org/wiki/Steganographic_file_system). So stay tuned and watch this space for more.\n",
    "similar": [
      "consistency",
      "isolation",
      "atomicity",
      "durability"
    ]
  },
  {
    "id": 63,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "super-long-integers",
    "title": "Super Long Integers in Python",
    "description": "Python must be doing something beautiful internally to support super long integers and today we find out what's under the hood. The article goes in-depth to explain design, storage, and operations on super long integers as implemented by Python.",
    "gif": "https://media.giphy.com/media/SKGo6OYe24EBG/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/72040055-55f49c00-32cd-11ea-9190-8f5a67c2f3d9.png",
    "released_at": "2020-01-10",
    "total_views": 3584,
    "body": "When you code in a low-level language like C, you worry about picking the right data type and qualifiers for your integers; at every step, you need to think if `int` would suffice or should you go for a `long` or even higher to a `long double`. But while coding in python, you need not worry about these \"trivial\" things because python supports integers of arbitrary size.\n\nIn C, when you try to compute 2<sup>20000</sup> using builtin `powl` function it gives you `inf` as the output.\n\n```c\n#include <stdio.h>\n#include <math.h>\n\nint main(void) {\n  printf(\"%Lf\\n\", powl(2, 20000));\n  return 0;\n}\n\n$ ./a.out\ninf\n```\n\nBut for python, it is a piece of cake \ud83c\udf82\n\n```\n>>> 2 ** 20000\n39802768403379665923543072061912024537047727804924259387134 ...\n...\n... 6021 digits long ...\n...\n6309376\n```\n\nPython must be doing something beautiful internally to support integers of arbitrary sizes and today we find out what's under the hood!\n\n# Representation and definition\nAn integer in Python is a C struct defined as following\n\n```c\nstruct _longobject {\n    PyObject_VAR_HEAD\n    digit ob_digit[1];\n};\n```\n\n`PyObject_VAR_HEAD` is a macro that expands into a `PyVarObject` that has the following structure\n\n```c\ntypedef struct {\n    PyObject ob_base;\n    Py_ssize_t ob_size; /* Number of items in variable part */\n} PyVarObject;\n```\n\nOther types that has `PyObject_VAR_HEAD` are\n - `PyBytesObject`\n - `PyTupleObject`\n - `PyListObject`\n\nThis indicates that an integer, just like a `tuple` or a `list`, is variable in length and this is our first insight into how it could support gigantically long integers. The `_longobject` after macro expansion could be roughly seen as\n\n```c\nstruct _longobject {\n    PyObject ob_base;\n    Py_ssize_t ob_size; /* Number of items in variable part */\n    digit ob_digit[1];\n};\n```\n\n> These are some meta fields in the `PyObject` struct, used for reference counting (garbage collection), but that we would require a separate article. The field that we will focus on is `ob_digit` and to some extent `ob_size`.\n\n### Decoding `ob_digit`\n\n`ob_digit` is an array of type `digit`, typedef'ed from `uint32_t`, statically allocated to length `1`. Since it is an array, `ob_digit` primarily is a `digit *`, pointer to `digit`, and hence if required could be malloced to any length. This makes it possible for python to represent and handle gigantically long integers.\n\nGenerally, In low-level languages like C, the precision of integers is limited to 64-bit, but Python implements [Arbitrary-precision integers](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic). Since Python 3 all integers are represented as a bignum and these are limited only by the available memory of the host system.\n\n### Decoding `ob_size`\n`ob_size` holds the count of elements in `ob_digit`. To be more efficient while allocating the memory to array `ob_digit`, python over-provisions and then relies on the value of `ob_size` to determine the actual number of elements held int the array.\n\n# Storage\n\nA naive way to store an integer digit-wise is by actually storing a decimal digit in one item of the array and then operations like addition and subtraction could be performed just like grade school mathematics.\n\nWith this approach, a number `5238` will be stored as\n\n![representation of 5238 in a naive way](https://user-images.githubusercontent.com/4745789/71915727-5e03ed00-31a2-11ea-99c1-cdf28e74b595.png)\n\nThis approach is inefficient as we will be using up 32 bits of digit (`uint32_t`) to store a decimal digit that actually ranges only from 0 to 9 and could have been easily represented by mere 4 bits, and while writing something as versatile as python, a core developer has to be more resourceful than this.\n\nSo, can we do better? for sure, otherwise, this article should hold no place on the internet. Let's dive into how python stores a super long integer.\n\n## The pythonic way\n\nInstead of storing just one decimal digit in each item of the array `ob_digit`, python converts the number from base 10 to base 2<sup>30</sup> and calls each of element as `digit` which ranges from 0 to 2<sup>30</sup> - 1.\n\nIn the hexadecimal number system, the base is 16 ~ 2<sup>4</sup> this means each \"digit\" of a hexadecimal number ranges from 0 to 15 of the decimal system. Similarly for python, \"digit\" is in base 2<sup>30</sup> which means it will range from  0 to 2<sup>30</sup> - 1 = 1073741823 of the decimal system.\n\nThis way python efficiently uses almost all of the allocated space of 32 bits per digit and keeps itself resourceful and still performs operations such as addition and subtraction like grade school mathematics.\n\n> Depending on the platform, Python uses either 32-bit unsigned integer arrays with 30-bit digits or 16-bit unsigned integer arrays with 15-bit digits. It requires a couple of bits to perform operations that will be discussed in some future articles.\n\n### Example: 1152921504606846976\n\nAs mentioned, for Python a \"digit\" is base 2<sup>30</sup> hence if you convert `1152921504606846976` into base 2<sup>30</sup> you get `100`\n\n__1152921504606846976__ = __1__ * (2<sup>30</sup>)<sup>2</sup> + __0__ * (2<sup>30</sup>)<sup>1</sup> + __0__ * (2<sup>30</sup>)<sup>0</sup>\n\nSince `ob_digit` persists it least significant digit first, it gets stored as `001` in 3 different digits.\n\nThe `_longobject` struct for this value will hold\n\n - `ob_size` as `3`\n - `ob_digit` as `[0, 0, 1]`\n\n![representation of 1152921504606846976 in a pythonic way](https://user-images.githubusercontent.com/4745789/72000622-b5b95b80-3269-11ea-9e76-1755cd648f0d.png)\n\nI have created a [demo REPL](https://repl.it/@arpitbbhayani/super-long-int?language=python3) that will output the way python is storing integers internally and also has reference to struct members like `ob_size`, `ob_refcount`, etc.\n\n# Operations on super long integers\n\nNow that we have a fair idea on how python supports and implements arbitrary precision integers its time to understand how various mathematical operations happen on them.\n\n## Addition\n\nIntegers are persisted \"digit-wise\", this means the addition is as simple as what we learned in the grade school and python's source code shows us that this is exactly how it is implemented as well. The function named [x_add](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3116) in file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c) performs the addition of two numbers.\n\n```c\n...\n    for (i = 0; i < size_b; ++i) {\n        carry += a->ob_digit[i] + b->ob_digit[i];\n        z->ob_digit[i] = carry & PyLong_MASK;\n        carry >>= PyLong_SHIFT;\n    }\n    for (; i < size_a; ++i) {\n        carry += a->ob_digit[i];\n        z->ob_digit[i] = carry & PyLong_MASK;\n        carry >>= PyLong_SHIFT;\n    }\n    z->ob_digit[i] = carry;\n...\n```\n\nThe code snippet above is taken from `x_add` function and you could see that it iterates over the digits and performs addition digit-wise and computes and propagates carry.\n\n> Things become interesting when the result of the addition is a negative number. The sign of `ob_size` is the sign of the integer, which means, if you have a negative number then `ob_size` will be negative. The absolute value of `ob_size` will determine the number of digits in `ob_digit`.\n\n## Subtraction\n\nSimilar to how addition is implemented, subtraction also happens digit-wise. The function named [x_sub](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3150) in file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c) performs subtraction of two numbers.\n\n```c\n...\n    for (i = 0; i < size_b; ++i) {\n        borrow = a->ob_digit[i] - b->ob_digit[i] - borrow;\n        z->ob_digit[i] = borrow & PyLong_MASK;\n        borrow >>= PyLong_SHIFT;\n        borrow &= 1; /* Keep only one sign bit */\n    }\n    for (; i < size_a; ++i) {\n        borrow = a->ob_digit[i] - borrow;\n        z->ob_digit[i] = borrow & PyLong_MASK;\n        borrow >>= PyLong_SHIFT;\n        borrow &= 1; /* Keep only one sign bit */\n    }\n...\n```\n\nThe code snippet above is taken from `x_sub` function and you could see how it iterates over the digits and performs subtraction and computes and propagates burrow. Very similar to addition indeed.\n\n## Multiplication\n\nAgain a naive way to implement multiplication will be what we learned in grade school math but it won't be very efficient. Python, in order to keep things efficient implements the [Karatsuba algorithm](https://en.wikipedia.org/wiki/Karatsuba_algorithm) that multiplies two n-digit numbers in O( n<sup>log<sub>2</sub>3</sup>) elementary steps.\n\nThe algorithm is slightly complicated is out of the scope of this article  but you can find its implementation in [k_mul](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3397) and\n[k_lopsided_mul](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L3618) functions in file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c).\n\n## Division and other operations\n\nAll operations on integers are defined in the file [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c) and it is very simple to locate and trace each one. Warning: it will take some time to understand each one in detail so grab some popcorn before you start skimming.\n\n# Optimization of commonly-used integers\n\nPython [preallocates](https://docs.python.org/3/c-api/long.html#c.PyLong_FromLong) small integers in a range of -5 to 256. This allocation happens during initialization and since we cannot update integers (immutability) these preallocated integers are singletons and are directly referenced instead of reallocating. This means every time we use/creates a small integer, python instead of reallocating just returns the reference of preallocated one.\n\nThis optimization can be traced in the macro `IS_SMALL_INT` and the function [get_small_int](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L43) in [longobject.c](https://github.com/arpitbbhayani/cpython/blob/0-base/Objects/longobject.c#L35). This way python saves a lot of space and computation for commonly used integers.\n\n---\n\nThis essay is heavily inspired, and to some extent copied, from [Artem Golubin](https://rushter.com)'s post - [Python internals: Arbitrary-precision integer implementation](https://rushter.com/blog/python-integer-implementation/). In case you want a detailed deep dive on CPython Integers or CPython Internals in general, I recommend you checkout the [CPython Internal Series](https://rushter.com/blog/tags/cpython/) by Artem Golubin.\n\nThank you Artem Golubin for all the amazing CPython Internal articles.\nThis essay is heavily inspired, and to some extent copied, from [Artem Golubin](https://rushter.com)'s post - [Python internals: Arbitrary-precision integer implementation](https://rushter.com/blog/python-integer-implementation/). In case you want a detailed deep dive on CPython Integers or CPython Internals in general, I recommend you check out the [CPython Internal Series](https://rushter.com/blog/tags/cpython/) by Artem Golubin.\n\nThank you Artem Golubin for all the amazing CPython Internal articles.\n",
    "similar": [
      "i-changed-my-python",
      "the-weird-walrus",
      "python-caches-integers",
      "python-prompts"
    ]
  },
  {
    "id": 64,
    "topic": {
      "id": 0,
      "uid": "python-internals",
      "name": "CPython Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2IycAvAoMgC98b7UXDe4Pa",
      "bgcolor": "#DDFFF1",
      "themecolor": "#00A662",
      "course_url": null
    },
    "uid": "i-changed-my-python",
    "title": "Changing Python",
    "description": "I changed the Python's source code and made addition incorrect and unpredictable. The addition operation will internally perform either Subtraction. Multiplication, Floor Division or Power at random.",
    "gif": "https://media.giphy.com/media/aZ5wedD7Jtazm/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/71662123-60b3ac00-2d76-11ea-8018-f558cef93a59.png",
    "released_at": "2020-01-03",
    "total_views": 1121,
    "body": "Did you ever take a peek at Python's source code? I didn't and hence I decided to have some fun with it this week. After cloning the repository I realized how well written is the code that makes python what it is. In the process of exploring the codebase, I thought of making some changes, not big optimizations but some minor tweaks that will help me understand how Python is implemented in C and along the course learn some internals. To make things fun and interesting I thought of changing how addition work by making it incorrect and unpredictable which means `a + b` will internally do one of the following operations, at random\n\n - `a + b`\n - `a - b`\n - `a * b`\n - `a / b`\n - `a ** b`\n\nAfter forking and cloning the source code of [python](https://github.com/python/cpython), I broke down the task into following sub-tasks\n\n - find the entry point (the main function) of python\n - find where addition happens\n - find how to call other perform operations like subtraction, multiplication, etc on python objects.\n - write a function that picks one of the operators at random\n - write a function that applies an operator on the two operands\n\nBefore getting into how I did it, take a look below and see what it does\n\n![Random Math Operator in Python](https://user-images.githubusercontent.com/4745789/71643972-d96b2780-2ce6-11ea-894c-fd638dc95d7c.gif)\n\nYou would see how performing addition on numbers `4` and `6` evaluates to `0`, `10` and `24` depending on the operation it picked randomly.\n\n> Note, the change I made will only work when one of the operands is a variable. If the entire expression contains constants then it will be evaluated as regular infix expression.\n\n# Implementation\nOperations in python work on opcodes very similar to the one that a microprocessor has. Depending on opcodes that the code is translated to, the operation is performed using operands (if required). The addition operation of python requires two operands and opcode is named `BINARY_ADD` and has value `23`. When the executor encounters this opcode, it fetches the two operands from top of the stack, performs addition and then pushes back the result on the stack. The code snippet below will give you a good idea of what python does when it encounters `BINARY_ADD`.\n\n```c\ncase TARGET(BINARY_ADD): {\n    PyObject *right = POP();\n    PyObject *left = TOP();\n    PyObject *sum;\n    if (PyUnicode_CheckExact(left) &&\n             PyUnicode_CheckExact(right)) {\n        sum = unicode_concatenate(tstate, left, right, f, next_instr);\n    }\n    else {\n        sum = PyNumber_Add(left, right);\n    }\n    SET_TOP(sum);\n    ...\n}\n```\n\n> One thing to observe here is how it concatenates when both operands are unicode/string.\n\n### Checking if operands are numbers\n\nFor checking if both the operands for `BINARY_ADD` operation are numbers I used the predefined function named `PyNumber_Check` which checks if object referenced by `PyObject` is number or not.\n\n```c\nif (PyNumber_Check(left) && PyNumber_Check(right)) {\n        // Both the operands are numbers\n}\n```\n\n### Writing a random function\nFor generating random integer I used the current time in seconds from the system using `datetime.h` library and took modulus with the max value. The code snippet below picks a random number from `[0, max)`.\n\n```c\nint\nget_random_number(int max) {\n    return time(NULL) % max;\n}\n```\n\n### Functions to perform other operations\nSimilar to the function `PyNumber_Add` which adds two python objects (if possible), there are functions named `PyNumber_Subtract`, `PyNumber_Multiply`, `PyNumber_FloorDivide`, and `PyNumber_Power` which performs operations as suggested by their names. I wrote a util function that takes two operands and an operator and returns the resulting python object after performing the required operation.\n\n```c\nPyObject *\nbinary_operate(PyObject * left, PyObject * right, char operator) {\n    switch (operator) {\n        case '+':\n            return PyNumber_Add(left, right);\n        case '-':\n            return PyNumber_Subtract(left, right);\n        case '*':\n            return PyNumber_Multiply(left, right);\n        case '/':\n            return PyNumber_FloorDivide(left, right);\n        case '^':\n            return PyNumber_Power(left, right, Py_None);\n        default:\n            return NULL;\n    }\n}\n```\n\n### The new `BINARY_ADD` implementation\n\nNow as have everything required to make our `BINARY_ADD` unpredictable and following code snippet is very close to how it could be implemented.\n\n```c\ncase TARGET(BINARY_ADD): {\n    PyObject *right = POP();\n    PyObject *left = TOP();\n    PyObject *result;\n    if (PyUnicode_CheckExact(left) &&\n             PyUnicode_CheckExact(right)) {\n        result = unicode_concatenate(tstate, left, right, f, next_instr);\n    }\n    else {\n        // Do this operation only when both the operands are numbers and\n        // the evaluation was initiated from interactive interpreter (shell)\n        if (PyNumber_Check(left) && PyNumber_Check(right)) {\n            char operator = get_random_operator();\n            result = binary_operate(left, right, operator);\n            printf(\n                \"::::: %s + %s was evaluated as %s %c %s, hence to the value\\n\",\n                ReprStr(left), ReprStr(right),\n                ReprStr(left), operator, ReprStr(right)\n            );\n        } else {\n            result = PyNumber_Add(left, right);\n        }\n        ...\n    }\n    ...\n    SET_TOP(result);\n    ...\n}\n```\n\n# Challenges\nAfter making all the required changes I ran `make` to build my new python binary and to my surprise, the code wouldn't build. The reason was that the function where I made the changes was called during build and initialization phases and due to incorrectness induced in the `BINARY_ADD` the process ended in __Segmentation Faults__ as now it has a function that instead of adding two numbers was subtracting, multiplying, dividing and raising to power at random.\n\nTo fix this issue I had to ensure that this random picking of operator only happened when the operation is asked from the interactive shell and should continue its normal execution for others. The function that gets called during an interactive shell is `PyRun_InteractiveLoopFlags` and hence I started passing a flag named `source` to all the functions till my trail reaches the opcode evaluation flow. The value of this `source` is set to `1` when it is triggered from the interactive shell for others the default value passed is `0`. Once I had this `source` field in place with the proper value being passed from various initiations, everything worked like a charm.\n\nYou can find the detailed diff at [github.com/arpitbbhayani/cpython/pull/1/files](https://github.com/arpitbbhayani/cpython/pull/1/files).\n\n# Conclusion\n\nIt was fun to change the python's source code, I would recommend you to do this as well. It is always better if you know how things work internally and more importantly understand the complexities that are abstracted to make the application developers' experience seamless.\n\nYou can find the source code at [arpitbbhayani/cpython/tree/01-randomized-math-operators](https://github.com/arpitbbhayani/cpython/tree/01-randomized-math-operators). Feel free to fork it and make some changes of your own and share it with me. I will be thrilled to learn what you did with it.\n\nIf you want to dive deep into python's source I highly recommend you to read [realpython.com/cpython-source-code-guide/](https://realpython.com/cpython-source-code-guide/). It is an excellent guide to get you started and understand the language semantics and coding practices of a core python developer. Once you know the basics, navigating through the codebase is a walk in the park.\n",
    "similar": [
      "super-long-integers",
      "python-caches-integers",
      "python-iterable-integers",
      "python-prompts"
    ]
  },
  {
    "id": 65,
    "topic": null,
    "uid": "efficient-way-to-stop-an-iterating-loop",
    "title": "Stop an Iterating Loop",
    "description": "There are two ways through which we can stop an iterating loop, first by using break statement and second by making loop condition false. Let's see if one is better than the other.",
    "gif": "https://media.giphy.com/media/cKKXNlTYino7hWNXwl/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/64327846-02ee9d80-cfea-11e9-8698-4a18bfe76068.png",
    "released_at": "2019-09-06",
    "total_views": 426,
    "body": "While I was writing a piece of code to solve a problem, I wrote an innocent looking for loop which I had to break when a particular condition was met. Hence wrote the most obvious looking code ever - it used `break` once condition was met and this way the flow broke out of the loop. Once my solution was accepted by an online judge I thought could I have done anything else here?\n\nThis triggered me to benchmark different ways to get out of an iterating loop. The two of the most common ways to stop iterating a loop are\n\n* use `break` statement\n* write the condition of the loop in a way that is becomes `false` when you want to stop the iteration\n\n# The Example\n\nEverything is better with an example. Let's say we need to find if character `a` is present in the array and we need to set the value of `bool` variable `a_found` accordingly.\n\n### Using break statement\n\n```cpp\nbool a_found = false;\n\nfor (int i = 0 ; i < n ; i++) {\n    if (str[i] == 'a') {\n    \ta_found = true;\n        break;\n    }\n}\n```\n\n### Using loop condition\n\n```cpp\nbool a_found = false;\n\nfor (int i = 0 ; a_found == false && i < n ; i++) {\n    if (str[i] == 'a') {\n    \ta_found = true;\n    }\n}\n```\n\nShould there be considerable difference in performance? My efforts to write an article about this gives you the obvious answer. YES! there is a significant difference.\n\n# Benchmark\n\nThe code to benchmark iterates over a string to see if `a` is present or not.  The length of the string is variable and goes from 500 to 1000000000. We measure the time taken for each approach - standard benchmark practice. You can find the code here - [code to benchmark stopping the loop iteration](https://gist.github.com/arpitbbhayani/d06cb7f4bb0cfdc8daa596dd77e8de10)\n\nFollowing graph shows the time taken by the two approaches varying with respect to the number of iterations of the loop (X Axis v/s Left Y-Axis) and the performance difference between the two approach (X Axis v/s Right Y-Axis).\n\n[ ![benchmark-time-taken-for-break-loop](https://user-images.githubusercontent.com/4745789/64328888-e2bfde00-cfeb-11e9-96ab-5bd1290063a4.png) ](https://user-images.githubusercontent.com/4745789/64328888-e2bfde00-cfeb-11e9-96ab-5bd1290063a4.png)\n\nWe see that a diverging graph that suggests one approach is significantly and always better than the other. The blue line shows the approach where we break the loop and it is always below the red one which represents the approach of stopping the iteration using condition.\n\nWe observe that using break to stop the iteration is on an average 45% better and it does not change with the number of iterations (post 1000 iterations). The raw data used for plotting this graph can be found [here](https://plot.ly/\\~arpitbbhayani/1).\n\n# Conclusion\n\n**Using `break` statement to break the loop is 45% better than using condition to stop the iteration and get out of the loop.**\n",
    "similar": [
      "fast-and-efficient-pagination-in-mongodb",
      "rule-30",
      "publish-python-package-on-pypi",
      "setting-up-graphite-grafana-using-nginx-on-ubuntu"
    ]
  },
  {
    "id": 69,
    "topic": {
      "id": 0,
      "uid": "advanced-algorithms",
      "name": "Advanced Algorithms",
      "one_liner": null,
      "youtube_playlist_id": null,
      "bgcolor": "#FFE7F3",
      "themecolor": "#ff007f",
      "course_url": null
    },
    "uid": "how-sleepsort-helped-me-understand-concurrency-in-golang",
    "title": "Sleepsort and Concurrency in Golang",
    "description": "Understanding concurrency in any programming language is tricky let alone Golang; hence to get my hands dirty the first thing I usually implement is sleepsort.",
    "gif": "https://media.giphy.com/media/QmJ3e9So5M9NdNkOGo/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63222328-203b0380-c1c4-11e9-9dd0-34c4bd9d1c6b.png",
    "released_at": "2017-07-16",
    "total_views": 460,
    "body": "For me learning concurrency have always been tricky; Every language has a different way to handle/emulate concurrency, for example, old languages like Java uses [Threads](https://docs.oracle.com/javase/tutorial/essential/concurrency/) and modern languages like NodeJS and Python uses something called as [event loops](https://nodejs.org/en/docs/guides/event-loop-timers-and-nexttick/) for its asynchronous IO which is there to make IO based things concurrent.\n\nRecently I started diving deep into concurrency in [Golang](https://golang.org/) and I wanted to start with a good `\"Hello World\"` program for it. This time I thought of taking an unconventional way to write my first concurrent program. Going through various examples over the Internet I could not find anything that made it fun. I suddenly recalled [Sleepsort](http://www.geeksforgeeks.org/sleep-sort-king-laziness-sorting-sleeping/) and it was the ideal way (fun + new = <3) to learn concurrency.\n\n### The Concept\nFor people who do not know what Sleep Sort is, the basic goes something like this:\nspin `n` threads/co-routine (or whatever concurrent element the language has) for `n` numbers (to sort) and for each number `x` wait for time proportional to `x` (lets say `x` seconds) and then print/collect the number.\n\n### Implementation in Go\nThis is a very basic Implementation of Sleep Sort in Golang using Go Routines and [WaitGroup](https://golang.org/pkg/sync/#WaitGroup).\n\n```go\n// prints a number of sleeping for n seconds\nfunc sleepAndPrint(x int, wg *sync.WaitGroup) {\n\tdefer wg.Done()\n\n\t// Sleeping for time proportional to value\n\ttime.Sleep(time.Duration(x) * time.Millisecond)\n\n\t// Printing the value\n\tfmt.Println(x)\n}\n\n// Sorts given integer slice using sleep sort\nfunc Sort(numbers []int) {\n\tvar wg sync.WaitGroup\n\n\t// Creating wait group that waits of len(numbers) of go routines to finish\n\twg.Add(len(numbers))\n\n\tfor _, x := range numbers {\n\t\t// Spinning a Go routine\n\t\tgo sleepAndPrint(x, &wg)\n\t}\n\n\t// Waiting for all go routines to finish\n\twg.Wait()\n}\n```\n\nI have published the code in a [Github Repository](https://github.com/arpitbbhayani/go-sleep-sort). Feel\nfree to fork and play around with it.\n\n### What else can you do with it\nI encourage you to try it out, and trust me it is really fun to learn concurrency through this; Apart from running the basic sleep sort you should also try to do/learn with it. For example,\n\nConcurrency essentials\n - Go Channels for inter go-routine communication\n - Mutex for synchronization making things routine-safe\n\nYou can also try to\n - collect the elements in a slice, in place of printing\n - make Sleep Sort handle negative numbers too\n - sort the numbers in descending order\n\nIf you find any interesting way to learn concurrency or any new use case here, please post a comment below.\nI would love to know them.\n",
    "similar": [
      "setting-up-graphite-grafana-using-nginx-on-ubuntu",
      "udemy-sql-taxonomy",
      "setting-up-graphite-using-nginx-on-ubuntu",
      "the-weird-walrus"
    ]
  },
  {
    "id": 70,
    "topic": null,
    "uid": "making-http-requests-using-netcat",
    "title": "HTTP Requests using Netcat",
    "description": "All our lives we have been hitting REST APIs with libraries and utilities like curl and postman. Its time we do it the hard way with netcat, just for fun!",
    "gif": "https://media.giphy.com/media/2tSodgDfwCjIMCBY8h/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63222693-a5c0b280-c1c8-11e9-9cd4-808225e579f3.png",
    "released_at": "2017-07-05",
    "total_views": 657,
    "body": "Majority of the traffic over the internet is HTTP Traffic. There is a HTTP Client which wants some data from HTTP Server, so it creates a HTTP Request Message in the protocol understandable by the server and sends it. Server reads the message, understands it, acts accordingly and replies back with HTTP Response.\n\nThis complete process is abstracted by the tools like [curl](https://curl.haxx.se/), requests libraries and utilities like [Postman](https://www.getpostman.com/). Instead of using these tools and utilities, we shall go by the hard way and see HTTP messages in action.\n\n## The Webserver\nFor experimentation purpose let\u2019s create a very basic webserver in [Python Flask framework](flask.pocoo.org) that exposes a trivial Hello World end point.\n\n### Python webserver script\n```python\nfrom flask import Flask\napp = Flask(__name__)\n\n@app.route('/hello')\ndef hello():\n    return \"Hello, World!\"\n\napp.run(port=3000)\n```\n\n### Installing requirements\n```bash\npip install flask\n```\n\n### Start the webserver\n```bash\npython hello.py\n```\n\nThe server listens on port _3000_ . If you hit from the browser [http://localhost:3000/hello](http://localhost:3000/hello), you should see _Hello, World!_ rendered.\n\n## The HTTP Request Message\nA HTTP Client talks to HTTP Server via a common protocol that is understandable by the two parties. A sample HTTP request message looks something like\n\n```bash\nGET /hello.html HTTP/1.1\nUser-Agent: Mozilla/4.0 (compatible; MSIE5.01; Windows NT)\nHost: www.sample-server.com\nAccept-Language: en-us\nAccept-Encoding: gzip, deflate\nConnection: Keep-Alive\n```\n\nTo understand more about HTTP Request messages, see references at the end of this article.\n\nThe HTTP Communication happens over a TCP Connection. So we create a TCP connection with the server and try to get response from it. To get a TCP connection I will use _netcat_.\n\n## Netcat\n_netcat_ is the utility that is used for just about anything under the sun involving TCP or UDP. It can open TCP connections, send UDP packets, listen on arbitrary TCP and UDP ports, do port scanning, and deal with both IPv4 and IPv6.\n\nThe webserver that was created above is listening on port _3000_ . Lets create a TCP Connection and connect to it using _netcat_.\n\n```bash\nnetcat localhost 3000\n```\n\nThe command along with creating a TCP connection, will also open a STDIN. Anything passed in that input stream will reach the server via the connection. Lets see what happens when we provide _This is a sample_ as input.\n\n![bad-request](https://user-images.githubusercontent.com/4745789/63222752-65156900-c1c9-11e9-90ec-ed06362d5d83.jpg)\n\nThe input message given is not a valid HTTP message hence server responded with a status code of _400_ which is for [Bad Request](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html). And if you closely observe the server logs on flask application, you will see an entry of our last request.\n\nSince the server is a HTTP Server, so it understands HTTP request. Let\u2019s create one to hit our exposed API endpoint  _/hello_ .\n\nThe HTTP request message for this request looks something like this\n\n```bash\nGET /hello HTTP/1.1\n```\n\nAnd you should see output like this\n\n![get-request](https://user-images.githubusercontent.com/4745789/63222756-78283900-c1c9-11e9-943a-60513ddbde86.jpg)\n\nThe HTTP Server understands the message sent from the client and it responded back as directed by the source code.\n\n## Complex Requests and HTTP Request Messages\n\n### GET method with query params and headers\nFollowing method exposes an endpoint which accepts a [query parameter](https://en.wikipedia.org/wiki/Query_string) named _name_, and returns a response with _name_ in it.\n\n```python\nfrom flask import request\n\n@app.route('/user')\ndef get_user():\n    name = request.args.get('name')\n    return \"Requested for name = %s\" % name\n```\n\n#### HTTP Request Message\nProvide the HTTP request message below when STDIN opens up after you execute _netcat_ command and connect with the server.\n\n```bash\nGET /user?name=arpit HTTP/1.1\n```\n\n#### Output\n![get-request-with-query-params](https://user-images.githubusercontent.com/4745789/63222764-87a78200-c1c9-11e9-83bc-edbd6cbfb32c.jpg)\n\n### Basic POST Method example\nFollowing method accepts form data via HTTP POST method and returns a dummy response with _username_ and _password_ in it.\n\n```python\nfrom flask import request\n\n@app.route('/login', methods=['POST'])\ndef login():\n    username = request.form.get('username')\n    password = request.form.get('password')\n    return \"Login successful for %s:%s\" % (username, password)\n```\n\n#### HTTP Request Message\nProvide the HTTP request message below when STDIN opens up after you execute _netcat_ command and connect with the server.\n\n```bash\nPOST /login HTTP/1.1\nContent-Type: application/x-www-form-urlencoded\nContent-Length: 32\n\nusername=arpit&password=welcome\n```\n\n#### Output\n![post-request-with-form-data](https://user-images.githubusercontent.com/4745789/63222769-9c841580-c1c9-11e9-8593-7289b2a40a20.jpg)\n\n### POST Method with JSON Request Body\nFollowing method accepts JSON data that contains a field _id_ with integer value via HTTP POST method and returns a dummy response with _id_ in it.\n```python\nfrom flask import request\n\n@app.route('/save', methods=['POST'])\ndef save_user():\n    user_data = request.json\n    return 'Saving user with id = %d' % (user_data.get('id'))\n```\n\n#### HTTP Request Message\nProvide the HTTP request message below when STDIN opens up after you execute _netcat_ command and connect with the server.\n\n```bash\nPOST /save HTTP/1.1\nContent-Type: application/json\nContent-Length: 30\n\n{\"id\": 1092, \"name\": \"Arpit\"}\n```\n\n#### Output\n![post-request-with-json-data](https://user-images.githubusercontent.com/4745789/63222775-ad348b80-c1c9-11e9-91ee-07933e37604d.jpg)\n\n## Conclusion\nThe hard way to hit REST endpoints was not hard at all ;-) Stay curious and dive deep.\n\n## References:\n1. [HTTP/1.1: HTTP Message](https://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html)\n2. [HTTP Requests - Tutorialspoint](http://www.tutorialspoint.com/http/http_requests.htm)\n3. [The TCP/IP Guide - HTTP Request Message Format](http://www.tcpipguide.com/free/t_HTTPRequestMessageFormat.htm)\n4. [HTTP Status Codes](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html)\n5. [Netcat man page](http://linux.die.net/man/1/nc)\n6. [HTTP Methods](https://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html)\n",
    "similar": [
      "the-weird-walrus",
      "how-sleepsort-helped-me-understand-concurrency-in-golang",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "setting-up-graphite-grafana-using-nginx-on-ubuntu"
    ]
  },
  {
    "id": 66,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "fast-and-efficient-pagination-in-mongodb",
    "title": "Fast and Efficient Pagination in MongoDB",
    "description": "MongoDB is a document based data store and hence pagination is one of the most common use case of it. Find out how you can paginate the results ...",
    "gif": "https://media.giphy.com/media/lRnUWhmllPI9a/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63153080-1336dc80-c02b-11e9-931f-0c7552a63ebc.png",
    "released_at": "2017-06-06",
    "total_views": 3898,
    "body": "[MongoDB](https://www.mongodb.com/) is a document based data store and hence pagination is one of the most common use case of it. So when do you paginate the response? The answer is pretty neat; you paginate whenever you want to process result in chunks. Some common scenarios are\n\n- Batch processing\n- Showing huge set of results on user interface\n\nPaginating on client and server side are both really very expensive and should not be considered. Hence pagination is generally handled at database level and databases are optimized for such needs too.\n\nBelow I shall explain you the 2 approaches through which you can easily paginate your MongoDB responses.\nSample Document\n\n```json\n    {\n        \"_id\" : ObjectId(\"5936d17263623919cd5165bd\"),\n        \"name\" : \"Lisa Rogers\",\n        \"marks\" : 34\n    }\n```\n\n## Approach 1: Using `cursor.skip` and `cursor.limit`\n\nMongoDB cursor has two methods that makes paging easy; they are\n\n- `cursor.skip()`\n- `cursor.limit()`\n\n`skip(n)` will skip `n` documents from the cursor while `limit(n)` will cap the number of documents to be returned from the cursor. Thus combination of two naturally paginates the response.\n\nIn Mongo Shell your pagination code looks something like this\n\n```js\n    // Page 1\n    db.students.find().limit(5)\n\n    // Page 2\n    db.students.find().skip(5).limit(5)\n\n    // Page 3\n    db.students.find().skip(5).limit(5)\n```\n\n`.find()` will return a cursor pointing to all documents of the collection and then for each page we skip some and consume some. Through continuous skip and limit we get pagination in MongoDB.\n\nI am fond of Python and hence here is a small trivial function to implement pagination:\n\n```python\n    def skiplimit(page_size, page_num):\n        \"\"\"returns a set of documents belonging to page number `page_num`\n        where size of each page is `page_size`.\n        \"\"\"\n        # Calculate number of documents to skip\n        skips = page_size * (page_num - 1)\n\n        # Skip and limit\n        cursor = db['students'].find().skip(skips).limit(page_size)\n\n        # Return documents\n        return [x for x in cursor]\n```\n\n## Approach 2: Using `_id` and `limit`\n\nThis approach will make effective use of default index on `_id` and nature of `ObjectId`.\nI bet you didn\u2019t know that a [Mongodb ObjectId](https://docs.mongodb.com/manual/reference/bson-types/#objectid) is a 12 byte structure containing\n\n- a 4-byte value representing the seconds since the Unix epoch,\n- a 3-byte machine identifier,\n- a 2-byte process id, and\n- a 3-byte counter, starting with a random value.\n\nEven I didn\u2019t until I read the [documentation](https://docs.mongodb.com/manual/reference/bson-types/#objectid). Apart from its structure there is one very interesting property of ObjectId; which is - *ObjectId has natural ordering*\n\nWhat does it mean? It simplifies that we can apply all the *less-than-s* and all the *greater-than-s you* want to it. If you don\u2019t believe me, open Mongo shell and execute following set of commands\n\n```javascript\n    > ObjectId(\"5936d49863623919cd56f52d\") > ObjectId(\"5936d49863623919cd56f52e\")\n    false\n    > ObjectId(\"5936d49863623919cd56f52d\") > ObjectId(\"5936d49863623919cd56f52a\")\n    true\n```\n\nUsing this property of ObjectId and also taking into consideration the fact that `_id` is always indexed, we can devise following approach for pagination:\n\n1. Fetch a page of documents from database\n2. Get the document id of the last document of the page\n3. Retrieve documents greater than that id\n\nIn Mongo Shell your pagination code looks something like this\n\n```javascript\n    // Page 1\n    db.students.find().limit(10)\n\n    // Page 2\n    last_id = ...  # logic to get last_id\n    db.students.find({'_id': {'$gt': last_id}}).limit(10)\n\n    // Page 3\n    last_id = ... # logic to get last_id\n    db.students.find({'_id': {'$gt': last_id}}).limit(10)\n```\n\nAgain, I am fond of Python and here is the Python implementation of this approach.\n\n```python\n    def idlimit(page_size, last_id=None):\n        \"\"\"Function returns `page_size` number of documents after last_id\n        and the new last_id.\n        \"\"\"\n        if last_id is None:\n            # When it is first page\n            cursor = db['students'].find().limit(page_size)\n        else:\n            cursor = db['students'].find({'_id': {'$gt': last_id}}).limit(page_size)\n\n        # Get the data\n        data = [x for x in cursor]\n\n        if not data:\n            # No documents left\n            return None, None\n\n        # Since documents are naturally ordered with _id, last document will\n        # have max id.\n        last_id = data[-1]['_id']\n\n        # Return data and last_id\n        return data, last_id\n```\n\n> If you are using a field other than `_id` for offset, make sure the field is indexed and properly ordered else the performance will suffer.\n\n## Closing Remarks\n\nBoth of the above approaches are valid and correct. But as we know, in field of Computer Science, whenever there are multiple options to achieve something, one always outperforms the other. Same is the situation here as well.\n\nTurns out, there is a severe problem with skip function. I have tried to jot it down in [this blog post](/blogs/mongodb-cursor-skip-is-slow). Because of which second approach has advantage over first. But that is not it; I wrote a simple [python code](https://github.com/arpitbbhayani/mongo-pagination-benchmark) to benchmark the two approaches for various combinations and it turns out `skip` performs better in some case. The results are compiled into [this blog post](/blogs/benchmark-and-compare-pagination-approach-in-mongodb).\n",
    "similar": [
      "efficient-way-to-stop-an-iterating-loop",
      "morris-counter",
      "jaccard-minhash",
      "bayesian-average"
    ]
  },
  {
    "id": 68,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "mongodb-cursor-skip-is-slow",
    "title": "Why MongoDB's cursor.skip() is Slow?",
    "description": "MongoDB's cursor.skip() is very inefficient, why is that? Even though it is slow and inefficient,  team MongoDB wants to continue keeping it. Find out why ...",
    "gif": "https://media.giphy.com/media/nqIuAIxYebIt2/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63220620-4d2ded00-c1a9-11e9-8eea-11766291e06f.png",
    "released_at": "2017-06-04",
    "total_views": 2088,
    "body": "MongoDB\u2019s cursor object has a method called `skip`, which as per [documentation and definition](https://docs.mongodb.com/manual/reference/method/cursor.skip/#definition), controls where MongoDB begins returning results. Thus in combination with function [limit](https://docs.mongodb.com/manual/reference/method/cursor.limit/), one can easily have paginated results.\n\nI have written a blog post on [how you can have Fast and Efficient Pagination in MongoDB](/blogs/fast-and-efficient-pagination-in-mongodb).\n\nBut while going through the documentation of skip, there is something interesting to notice. There is a small warning in [MongoDB documentation](https://docs.mongodb.com/manual/reference/method/cursor.skip/#behavior), that states\n\n  > The `cursor.skip()` method is often expensive because it requires the server to walk from the beginning of the collection or index to get the offset or skip position before beginning to return results. As the offset (e.g. `pageNumber` above) increases, `cursor.skip()` will become slower and more CPU intensive. With larger collections, `cursor.skip()` may become IO bound.\n\nIn short, MongoDB has to iterate over documents to skip them. Thus when collection or result set is huge and you need to skip documents for pagination, the call to `cursor.skip` will be expensive. While going through the source code of `skip` I found out that it does not use any index and hence gets slower when result set increases in size.\n\nThis also implies that if you use `skip`  then the \u201cskipping speed\u201d will not improve even if you index the field.\n\nBut what if the size of result set is small? is calling `skip` still a terrible idea?\nIf skip was so terrible, then MongoDB team and community must had taken that decision long back. But they haven\u2019t \u2026 why?\n\nBecause it is very efficient and fast for smaller result set. I have taken this opportunity to [benchmark and compare](/blogs/benchmark-and-compare-pagination-approach-in-mongodb) the [two approach for pagination](/blogs/fast-and-efficient-pagination-in-mongodb) and there I found out skip and limit based pagination works well for smaller result sets.\n\nIn conclusion, skip is not as bad one might think. But you must understand your use case well so as to make an informed decision.\n",
    "similar": [
      "inheritance-c",
      "fork-bomb",
      "idf",
      "copy-on-write"
    ]
  },
  {
    "id": 67,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "benchmark-and-compare-pagination-approach-in-mongodb",
    "title": "Benchmark Pagination Strategies in MongoDB",
    "description": "Benchmark results for two pagination approaches for MongoDB.",
    "gif": "https://media.giphy.com/media/c5eqVJN7oNLTq/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63220759-d514f680-c1ab-11e9-8a38-7b1828946b74.png",
    "released_at": "2017-06-02",
    "total_views": 3538,
    "body": "[MongoDB](https://www.mongodb.com/) is a document based data store and hence pagination is one of the most common use case of it. So when do you paginate the response? The answer is pretty neat; you paginate whenever you want to process result in chunks. Some common scenarios are\n\n- Batch processing\n- Showing huge set of results on user interface\n\nThere are multiple approaches through which you can [paginate your result set in MongoDB](/blogs/fast-and-efficient-pagination-in-mongodb). This blog post is dedicated for results of benchmark of two approaches and its analysis, so here we go ...\n\nBenchmark has been done over a non-indexed collection. Each document of the collection looks something like this\n```js\n    {\n        \"_id\" : ObjectId(\"5936d17263623919cd5165bd\"),\n        \"name\" : \"Lisa Rogers\",\n        \"marks\" : 34\n    }\n```\n\nAll records of a collection are fetched page-wise. Size of each page is fixed during fetch of the collection. Each page is fetched _3_ times and average of, time to fetch one \u201cpage\u201d, 3 is recorded.\n\nFollowing image shows the how two approach fares against each other.\n\n![MongoDB Pagination Benchmark Results](https://user-images.githubusercontent.com/4745789/63220692-cb3ec380-c1aa-11e9-9882-27bf52cbaa84.png)\n\nA key observation to note is that, till 500-600 count, both the approaches are comparable, but once it crosses that threshold, there is sudden rise in response time for `skip` and `limit` approach than other. The approach using `_id` and `limit` almost gives constant performance and is independent of size of the result set.\n\nI tried running this test on different machines with different disks but results were similar. I think diving deep in MongoDB's database drivier will yield better information about this behavior. You could see some spikes in the response times, that are because of Disk Contention.\n\nIn short:\n - For huge result set, paginating using `_id` and `limit` is far better than using `skip` and `limit`.\n - For smaller result set, it does not matter, but prefer skip and limit.\n\nAn interesting thing I observed is that after page size crosses 100, the gap between the two approach reduces to some extent. I am yet to perform detailed benchmark on that as such use-case (where page-size is more than 100) is pretty rare in practical applications.\n\nYou can find the Python code used for this benchmark [here](https://github.com/arpitbbhayani/mongo-pagination-benchmark). If you have any suggestion or improvement, do let me know.\n",
    "similar": [
      "udemy-sql-taxonomy",
      "multiple-mysql-on-same-server-using-docker",
      "better-programmer",
      "sliding-window-ratelimiter"
    ]
  },
  {
    "id": 71,
    "topic": null,
    "uid": "multiple-mysql-on-same-server-using-docker",
    "title": "Multiple MySQL server running on same Ubuntu server",
    "description": "Have multiple MySQL versions running on same server within 5 minutes.",
    "gif": "https://media.giphy.com/media/GIrIC3g657AYg/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63222993-74e27c80-c1cc-11e9-8229-dd7e5d4f1173.png",
    "released_at": "2016-05-13",
    "total_views": 285,
    "body": "There are many situations where there is a require where you need to run multiple instances of MySQL on same machine.\n\nSome situations are:\n\n- test a new MySQL release while leaving an existing production setup undisturbed\n- give different users access to different `mysqld` servers that they manage themselves\n\n## Problem description\nI have a machine having 5 products already setup. All products are using MySQL 5.5 as its default database. Now its time to upgrade all but one product to use MySQL 5.6. The table below shows the before and after version requirements of MySQL for various products. Looking at the table we find that all products except product C wants to use MySQL 5.6.\n\n![multiple-mysql-requirement](https://user-images.githubusercontent.com/4745789/63223049-85472700-c1cd-11e9-88bb-5d3123b412ad.png)\n\nSince all but one products require MySQL 5.6, so lets install it first and then we will work to figure out a way to install MySQL 5.5 as well.\n\n## Installing MySQL 5.6\n```bash\nsudo apt-get update\nsudo apt-get install mysql-server-5.6 mysql-server-core-5.6 mysql-client-5.6 mysql-client-core-5.6\n```\n\nAt this point we have MySQL 5.6 listening at port `3306` (default port)\n\n## Approach to solution\nThere are several approaches with which you can achieve multiple MySQL versions running in same machine. Some of them are\n\n- Use binaries of specific version\n- Build everything from MySQL source\n\n## Issues in above approaches:\nEvidently we can only have one version of MySQL setup on the machine using default installation procedure with `apt-get`. Hence if we try to install one version over other then it will replace the first version and will retain the second version. Hence we cannot have 2 versions of MySQL with default installation procedure.\n\nBuilding everything from scratch involves a lot of complications at source level. In order to debug any issues that might arise, you should be aware what happens in various scripts/commands that you run. I did spend a day in building from the source but it eventually turned out to be complete waste of time, efforts and debugging.\n\n## Docker to the rescue\nIf we had a container in which we have a MySQL 5.5 installed and if we can publish the container's port(s) to the host, then we can connect to container's MySQL just like a local database.\n\nWe can have all of the above with **Docker**. If you dont know what docker is, please read this official [What is Docker](https://www.docker.com/what-docker).\n\n### Installing Docker\nTo install docker on your machine execute following command on your shell.\n\n```bash\ncurl -sSL https://get.docker.com/ | sh\n```\n\n### Spin off MySQL 5.5 container\nExecute following command and this will download MySQL 5.5 image and will spin off the container. This container will have MySQL 5.5 installed on port `3306`. But on host machine port `3310` will be forwarded.\n\n```bash\nsudo docker run --name mysql-55-container -p 127.0.0.1:3310:3306 \\\n     -e MYSQL_ROOT_PASSWORD=rootpassword -d mysql:5.5\n```\n\n_NOTE: Password for root user is rootpassword, you can change it to anything._\n\n### Connect to MySQL 5.5\n```bash\nmysql -u root -p --host=127.0.0.1 --port=3310\n```\n\n### Connect to MySQL 5.6\n```bash\nmysql -u root -p\n```\n\n**And voila! you have both My SQL 5.5 and MySQL 5.6 installed and running on same machine.**\n\nNow you can configure your application product C to use host `127.0.0.1` and port `3310` and thus you have products A, B, D and E running on MySQL 5.6 and product C running on MySQL 5.5.\n",
    "similar": [
      "setting-up-graphite-using-nginx-on-ubuntu",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "udemy-sql-taxonomy",
      "how-sleepsort-helped-me-understand-concurrency-in-golang"
    ]
  },
  {
    "id": 72,
    "topic": null,
    "uid": "setting-up-graphite-using-nginx-on-ubuntu",
    "title": "Setting up Graphite using Nginx on an Ubuntu server",
    "description": "Part 1: Monitor your production systems and application analytics using Graphite. This article will help you setup these tools on Ubuntu 14.04 on a Nginx webserver with PostgreSQL as backend.",
    "gif": "https://media.giphy.com/media/l4FGzAPvg5PbZrVlK/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63223138-c68c0680-c1ce-11e9-942b-60b92d3f8a28.png",
    "released_at": "2015-12-14",
    "total_views": 728,
    "body": "Monitor your production systems and application analytics using Graphite. This article will help you setup these tools on Ubuntu 14.04 on a Nginx webserver with PostgreSQL as backend.\n\n## What is what\n\n### What is Graphite?\nGraphite is an open source software that is used for monitoring any system. The monitored data is the numeric information that represents any performance metric. Graphite also as a renderer which renders this information as graphs.\n[Official Documentation](http://graphite.readthedocs.org/en/1.0/overview.html)\n\n### What is Nginx?\nNGINX is a very fast Webserver, its faster than most web servers available in the market. The biggest advantage of Nginx is its concurrency (because of asynchronous nature). It can also act as\n\n1. HTTP Cache\n2. Reverse Proxy\n3. Load Balancer\n\nFor more information visit [Nginx Wiki](https://en.wikipedia.org/wiki/Nginx)\n\n# Installing Nginx\n\n```bash\nsudo apt-get install nginx nginx-extras\n```\n\n# Installing Graphite\n\n## Graphite Ubuntu Package Installation\n\nInstall Graphite packages\n\n```bash\nsudo apt-get update\nsudo apt-get install graphite-web graphite-carbon\n```\n\n**NOTE**: During the installation, you will be asked if during uninstallation of Graphite you also like to remove its files. Please select **NO** because anyways you can delete them manually. The files are kept in `/var/lib/graphite/whisper`.\n\n\n## Install and Configure PostgreSQL Database\nGraphite internally uses carbon and whisper database library for storing data. But the web application is a Django application which needs some data store for its own purpose. The default data store configured is SQLite3 database files. But this is not a full fledged database system hence we will use PostgreSQL.\n\nScript to install database and libs used by Graphite to communicate with PostgreSQL\n\n```bash\nsudo apt-get install postgresql libpq-dev python-psycopg2\n```\n\nOnce our PostgreSQL is installed we will create a user and a database\n\nLogin to PostgreSQL console\n\n```bash\nsudo -u postgres psql\n```\n\nCreate a user *graphite* which will be used by Django to operate on our database.\n\n```sql\n$ CREATE USER graphite WITH PASSWORD 'mypassword';\n```\n\nPlease make sure you select a secure password for your user.\n\nCreate a database *graphite* and give our new user *graphite* ownership of it.\n\n```sql\n$ CREATE DATABASE graphite WITH OWNER graphite;\n```\n\nPlease verify is database is created or not by connection to it\n\n```sql\n$ \\c graphite\n```\n\nIf you can successfully connect to the database *graphite* then you are good to go to next step.\n\nExit from the PostgreSQL console\n\n```sql\n$ \\q\n```\n\n\n## Configure Graphite Web Application\n\nNow, as we have our PostgreSQL database and user ready to go we can now move to configuring the web application.\n\nOpen the Graphite web app configuration file:\n\n```bash\nsudo vim /etc/graphite/local_settings.py\n```\n\nUncomment the *SECRET_KEY* and give a nice random value to it\n\n```bash\nSECRET_KEY = 'MY NICE RANDOM SALT'\n```\n\nUncomment the *TIMEZONE* and set it to some appropriate value. I have set it to UTC, but you may choose any one you like\n\n```bash\nTIME_ZONE = 'UTC'\n```\n\nUncomment the *USE_REMOTE_USER_AUTHENTICATION* and set tot to *True* so that remote user will be authenticated first before making any DB changes\n\n```bash\nUSE_REMOTE_USER_AUTHENTICATION = True\n```\n\nChange the database dictionary definition:\n\n```python\nDATABASES = {\n    'default': {\n        'NAME': 'graphite',\n        'ENGINE': 'django.db.backends.postgresql_psycopg2',\n        'USER': 'graphite',\n        'PASSWORD': 'mypassword',\n        'HOST': '127.0.0.1',\n        'PORT': ''\n    }\n}\n```\n\nSave and close this file.\n\n\n## Sync the Database\nOnce your web application is configured, it is time to sync your database, create a super user and create the correct structure.\n\n```bash\nsudo graphite-manage syncdb\n```\n\n**NOTE**: It will ask you to create a superuser. Make sure you remember the credentials with which you create one. This user will be used to connect to Graphite application and be admin of it. Being admin you will change interface of Graphite and create graphs.\n\n\n## Configure Carbon\nCarbon is the Graphite storage backend.\n\nOpen the configuration file:\n\n```bash\nsudo vim /etc/default/graphite-carbon\n```\n\nChange value of *CARBON_CACHE_ENABLED* to *true*\n\n```bash\nCARBON_CACHE_ENABLED = true\n```\n\nThis enables the carbon service to start at boot\n\nSave and close the file.\n\nNext, open the Carbon configuration file:\n\n```bash\nsudo vim /etc/carbon/carbon.conf\n```\n\nSet *ENABLE_LOGROTATION* to *True* to turn on log rotation\n\n```bash\nENABLE_LOGROTATION = True\n```\n\nSave and close the file\n\n## Configuring Storage Schemas\nNow, open the storage schema file. This tells Carbon how long to store values and how detailed these values should be:\n\n```bash\nsudo vim /etc/carbon/storage-schemas.conf\n```\n\nInside you will find entries like\n\n```bash\n[carbon]\npattern = ^carbon\\.\nretentions = 60:90d\n```\n\nwhich implies:\npattern that matches regular expression *^carbon\\.* should retain the data with retention policy *60:90d* which is\n\n* how often a metric is recorded: 60 seconds\n* length of time to store those values: 90 days\n\nFor detail information on retention policy visit [here](http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-schemas-conf)\n\nNow we need to add our own entry. Let's take an example *test* i.e. we need to monitor data points and our data point entries will start with string *test*.\n\n**NOTE**: This entry should be added before the default entry mentioned at the bottom of the file\n\n```bash\n[test]\npattern = ^test\\.\nretentions = 10s:10m,1m:1h\n```\n\nThis will match any metrics beginning with \"test.\". It will store the data it collects two times, in varying detail.\n\nThe first archive definition *(1s:10m)* will create a data point every ten  seconds. It will store the values for only ten minutes.\n\nThe second archive *(1m:1h)* will create a data point every one minute. It will gather all of the data from the past minute (six points, since the previous archive creates a point every ten seconds) and aggregate it to create the point. By default, it does this by averaging the points, but we can adjust this later. It stores the data at this level of detail for one hour.\n\n**This example is taken from this [link](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-graphite-on-an-ubuntu-14-04-server)**\n\nSave and close the file.\n\n## Storage Aggregation Methods\nThis aggregation methods are used when we try to fetch data that is less detailed (In our previous example we saw 6 data points were aggregated to create 1 data point). Understanding aggregation is important is we want accurate metrics.\n\nDefault aggregation method is taking out mean of values which implies that all retention policies other than most detailed one will create data points by taking mean of all data points it received.\n\nWe can specify the aggregation configuration in file called *storage-aggregation.conf* . A sample file is already provided by Carbon, so you can simply copy-paste it for default behaviour.\n\n```bash\nsudo cp /usr/share/doc/graphite-carbon/examples/storage-aggregation.conf.example /etc/carbon/storage-aggregation.conf\n```\n\nYou can view [official documentation](http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-aggregation-conf) to understand it better.\n\nSave and close the file.\n\nStart the carbon service\n\n```bash\nsudo service carbon-cache start\n```\n\n## Setup uwsgi and init script\nTo install *uwsgi* globally you can run following command\n\n```bash\nsudo apt-get install python-dev\nsudo pip install uwsgi\n```\n\nIf *pip* is not installed in your system, you can run following command\n\n```bash\nsudo apt-get install python-pip\n```\n\nThe entrance file for Django application is stored in directory */usr/share/graphite-web* and is by default named as *graphite.wsgi*. You should rename it to *graphite_wsgi.py*.\n\nYou can do this by executing following command\n\n```bash\nsudo cp /usr/share/graphite-web/graphite.wsgi /usr/share/graphite-web/graphite_wsgi.py\n```\n\nCreate log files and socket files with appropriate permissions\n\n```bash\nsudo touch /var/run/graphite.sock\nsudo chmod 777 /var/run/graphite.sock\nsudo touch /var/log/graphite.log\nsudo chmod 777 /var/log/graphite.log\n```\n\nOnce you have *uwsgi* setup in your system its time to set up the init script which will make it easier to manage the service.\n\nCreate the following file\n\n```bash\nsudo vim /etc/init/uwsgi-graphite.conf\n```\n\nAnd put following content in it\n\n```bash\n# vim: syntax=upstart\n\nenv UWSGI_BIN=/usr/local/bin/uwsgi\nenv PYTHONPATH=/usr/share/graphite-web\n\nexpect fork\numask 0000\n\nstart on runlevel [2345]\nstop on runlevel [!2345]\n\nscript\n  exec $UWSGI_BIN --socket /var/run/graphite.sock --master --need-app \\\n  --catch-exceptions --reload-on-exception --pp $PYTHONPATH \\\n  -w graphite_wsgi:application --buffer-size 32768 -p 4 -O 2 >>/var/log/graphite.log 2>&1 &\nend script\n```\n\nNow you can start the Graphite Web application service using following command\n\n```bash\nsudo service uwsgi-graphite start\n```\n\nBut before you can see anything on browser you need to setup Nginx configuration\n\n## Setup Nginx for Graphite\n\nLet us first create all files and links\n\n```bash\nsudo touch /etc/nginx/sites-enabled/graphite\nsudo ln -s /etc/nginx/sites-enabled/graphite /etc/nginx/sites-available/graphite\n\n# Log files\nsudo touch /var/log/nginx/graphite.access.log\nsudo chmod 666 /var/log/nginx/graphite.access.log\nsudo touch /var/log/nginx/graphite.error.log\nsudo chmod 666 /var/log/nginx/graphite.error.log\n```\n\nNow we are ready for configuring Nginx server for Graphite\n\nOpen file */etc/nginx/sites-enabled/graphite* and put following content in it\n\n```bash\nsudo vim /etc/nginx/sites-enabled/graphite\n```\n\nNginx configuration\n\n```javascript\nserver {\n  server_name graphite.yourservername.com;\n  listen 80;\n\n  rewrite ^(.*) https://$host$1 permanent;\n}\n\n\nserver {\n  server_name graphite.yourservername.com;\n\n\n  listen 443 ssl spdy;\n\n\n  access_log /var/log/nginx/graphite.access.log;\n  error_log  /var/log/nginx/graphite.error.log;\n\n\n  location = /robots.txt {\n    echo \"User-agent: *\\nDisallow: /\\n\";\n  }\n\n\n  root /usr/share/graphite-web;\n\n  location = /favicon.ico {\n    return 204;\n  }\n\n  location /content {\n    alias /usr/share/graphite-web/static;\n    expires max;\n  }\n\n  location / {\n    uwsgi_pass unix:/var/run/graphite.sock;\n    include uwsgi_params;\n  }\n\n}\n```\n\nNow you can view a working Graphite on your server. Just hit *http://graphite.yourservername.com* from your favourite browser.\n\n\n# See it working\n\nLogin to the system with credentials that you provided while creating the superuser.\n\nOnce you are logged in, you should see a screen like this.\n\n![Landing page of Graphite](https://user-images.githubusercontent.com/4745789/63223185-5af66900-c1cf-11e9-9379-4de15f80ef93.png)\n\nFirst we need to add data into the system. Remember we added a pattern matcher in Storage Schema, according to which any pattern that starts with **test.** will be recorded as our pattern. Lets add some random data\n\nIn order to add data we need to run following\n\n```bash\necho \"test.count 9 `date +%s`\" | nc -q0 127.0.0.1 2003;\n```\n\nThis will add one data metric of value 9 in system. Lets add some more data; this time wee loop through values\n\n```bash\nfor i in 4 6 8 16 2; do echo \"test.count $i `date +%s`\" | nc -q0 127.0.0.1 2003; sleep 6; done\n```\n\n\nNow you should see something like this.\n\n**I already had some data in the system, your graph will look a bit different but should be similar**\n\n![test.count graph in Graphite](https://user-images.githubusercontent.com/4745789/63223192-795c6480-c1cf-11e9-9323-e5a91c767ec9.png)\n\nThis completes the setup of Graphite on your machine. Although the UI does not look good but when used along with Grafana, it gives a complete experience along with high level of customization and metric analytics.\n",
    "similar": [
      "multiple-mysql-on-same-server-using-docker",
      "how-sleepsort-helped-me-understand-concurrency-in-golang",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "udemy-sql-taxonomy"
    ]
  },
  {
    "id": 73,
    "topic": null,
    "uid": "setting-up-graphite-grafana-using-nginx-on-ubuntu",
    "title": "Setting up Graphite and Grafana on an Ubuntu server",
    "description": "Part 2: Monitor your production systems and application analytics using Graphite. This article will help you setup these tools on Ubuntu 14.04 on a Nginx webserver with PostgreSQL as backend.",
    "gif": "https://media.giphy.com/media/Me1GB9z50XUKQ/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63223314-e58b9800-c1d0-11e9-83d8-e263ce3eb874.png",
    "released_at": "2015-12-14",
    "total_views": 278,
    "body": "Monitor your production systems and application analytics using Grafana and Graphite. This article will help you setup these tools on Ubuntu 14.04 on a Nginx webserver with PostgreSQL as backend.\n\n**Before starting following this setup, please go through my previous post on [Deploy Graphite and Nginx on an Ubuntu 14.04 server](/blogs/setting-up-graphite-using-nginx-on-ubuntu)**\n\n## What is what\n\n### What is Grafana?\nGrafana is a tool for visualising time-series data for various application analytics. It is a great tool when used in combination with Graphite. It gives us flexibility of creating dashboards and share them with teams.\n[Grafana Official](http://grafana.org/)\n\n\n# Installing Graphite\n\nTo install Graphite please follow this tutorial on [Deploy Graphite and Nginx on an Ubuntu 14.04 server](/blogs/setting-up-graphite-using-nginx-on-ubuntu).\n\n\n# Installing Grafana\n\n## Install Ubuntu Packages\n\nExecuting following commands will install Grafana on your machine\n\n```bash\necho 'deb https://packagecloud.io/grafana/stable/debian/ wheezy main' |  sudo tee -a /etc/apt/sources.list\ncurl https://packagecloud.io/gpg.key | sudo apt-key add -\nsudo apt-get update\nsudo apt-get install grafana\n```\n\n\n## Create a database for Grafana\n\nJust as we created the database for Graphite on PostgreSQL, we will create a database for Grafana as well. Create a database **grafana** and give user **graphite** (the user having access to Graphite database) ownership of it.\n\n```sql\n$ CREATE DATABASE grafana WITH OWNER graphite;\n```\n\n## Configure Grafana\n\nEdit the Grafana configuration file\n\n```bash\nsudo vim /etc/grafana/grafana.ini\n```\n\nThe settings should be something like this\n\n```bash\n[database]\ntype = postgres\nhost = 127.0.0.1:5432\nname = grafana\nuser = graphite\npassword = mypassword\n\n[server]\nprotocol = http\nhttp_addr = 127.0.0.1\nhttp_port = 3000\ndomain = grafana.yourservername.com\nenforce_domain = true\nroot_url = %(protocol)s://%(domain)s/\n\n[security]\nadmin_user = admin\nadmin_password = your_secure_password\nsecret_key = your_random_secret_salt\n```\n\nOnce your configuration is done, you can start Grafana Server by running\n\n```bash\nsudo service grafana-server start\n```\n\nThe log files are located at **/var/log/grafana/grafana.log**\n\nIf everything goes well the log file should have content\n\n```bash\n$ tail /var/log/grafana/grafana.log\n\n[0]: default.paths.data=/var/lib/grafana\n[1]: default.paths.logs=/var/log/grafana\nPaths:\n  home: /usr/share/grafana\n  data: /var/lib/grafana\n  logs: /var/log/grafana\n\n2015/12/16 06:37:15 [I] Database: postgres\n2015/12/16 06:37:15 [I] Migrator: Starting DB migration\n2015/12/16 06:37:15 [I] Listen: http://127.0.0.1:3000\n```\n\n\n## Setup Nginx for Graphite\n\nLet us first create all files and links\n\n```bash\nsudo touch /etc/nginx/sites-enabled/grafana\nsudo ln -s /etc/nginx/sites-enabled/grafana /etc/nginx/sites-available/grafana\n\n# Log files\nsudo touch /var/log/nginx/grafana.access.log\nsudo chmod 666 /var/log/nginx/grafana.access.log\nsudo touch /var/log/nginx/grafana.error.log\nsudo chmod 666 /var/log/nginx/grafana.error.log\n```\n\nNow we are ready for configuring Nginx server for Grafana\n\nOpen file */etc/nginx/sites-enabled/grafana* and put following content in it\n\n```bash\nsudo vim /etc/nginx/sites-enabled/grafana\n```\n\nNginx configuration\n\n```javascript\nserver {\n  server_name grafana.yourservername.com;\n  listen 80;\n\n  rewrite ^(.*) https://$host$1 permanent;\n}\n\n\nserver {\n  server_name grafana.yourservername.com;\n\n\n  listen 443 ssl spdy;\n\n\n  access_log /var/log/nginx/grafana.access.log;\n  error_log  /var/log/nginx/grafana.error.log;\n\n\n  location = /robots.txt {\n    echo \"User-agent: *\\nDisallow: /\\n\";\n  }\n\n  location / {\n    proxy_pass         http://localhost:3000;\n    proxy_set_header   Host $host;\n  }\n\n}\n```\n\nNow you can view a working Grafana on your server. Just hit *http://grafana.yourservername.com* from your favourite browser.\n\n\n# See it working\n\nOnce you open the Grafana page, you will see a page something like this.\n\n![Grafana 1](https://user-images.githubusercontent.com/4745789/63223325-487d2f00-c1d1-11e9-90b2-98cc5826b3a9.png)\n\nLog in to the system with default credentials\n\n* username: admin\n* password: admin\n\nOnce you are logged in, you should see a screen like this.\n\n![Grafana 2](https://user-images.githubusercontent.com/4745789/63223333-664a9400-c1d1-11e9-8a67-5b83d94d198b.png)\n\nBefore you see any analytics information here, you should add your data source. The data source you will add will be Graphite that was setup earlier.\n\nGoto **Data Source** -> **Add New**\n\nMake following changes:\n\n* Name: **graphite**\n* URL: **graphite.yourservername.com**\n\nYou should test your connection before adding any dashboard.\n\n![Grafana 3](https://user-images.githubusercontent.com/4745789/63223336-71052900-c1d1-11e9-8d87-cdc739008770.png)\n\nOnce the connection is successful, now we are ready to add our first dashboard. Go to home page and goto **New Dashboard** -> **New**\n\nOnce that is done, you can add panels to it. To add graphs in panels click on **Green Button** -> **Add Panel** -> **Graph**. At the botton you will see metrics in which select *test* and *count*. Then you will see screen something like this.\n\n![Grafana 4](https://user-images.githubusercontent.com/4745789/63223338-79f5fa80-c1d1-11e9-80cd-739bc6f0f8b7.png)\n\n## More Information\nPlease follow the below links in order to know more about Grafana and its amazing customizations.\n\n* [Grafana Documentation](http://docs.grafana.org/)\n* [Adding Graphite to Grafana Official](http://docs.grafana.org/datasources/graphite/)\n* [Adding HuBot to Grafana](http://docs.grafana.org/tutorials/hubot_howto/)\n",
    "similar": [
      "the-weird-walrus",
      "how-sleepsort-helped-me-understand-concurrency-in-golang",
      "efficient-way-to-stop-an-iterating-loop",
      "udemy-sql-taxonomy"
    ]
  },
  {
    "id": 74,
    "topic": null,
    "uid": "publish-python-package-on-pypi",
    "title": "Publish python package on PyPI",
    "description": "If you have written something cool in Python and want to make it installable via pip and easy_install, this post will help you publish your python online.",
    "gif": "https://media.giphy.com/media/TfKfqjt2i4GIM/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63223433-e45b6a80-c1d2-11e9-921f-c2cc0153cf4a.png",
    "released_at": "2015-11-10",
    "total_views": 321,
    "body": "PyPI is the official Python Packages Index. Once you publish it on PyPI then it will can downloaded via a simple command `pip install <package>`. Life becomes simpler with this one line installation.\n\n## 1: Create accounts\nIn order to submit your package on PyPI you need to have few accounts on PyPI websites. These account will enable you to maintain your packages and will provide you with an interface to edit your package.\n\nCreate your account on following sites:\n\n* [PyPI Live](http://pypi.python.org/pypi?%3Aaction=register_form)\n* [PyPI Test](http://testpypi.python.org/pypi?%3Aaction=register_form)\n\n\n## 2: The `.pypirc` file\nNow create a file in your home folder named `.pypirc`, which will be your configuration file that holds the authentication information of your PyPI accounts.\n\nCreate file `.pypirc` and put the contents shown below\n```bash\nvim ~/.pypirc\n```\n\n`.pypirc` file contents\n\n```cpp\n[distutils]\nindex-servers =\n  pypi\n  pypitest\n\n[pypi]\nrepository: https://pypi.python.org/pypi\nusername: YOUR_USERNAME_HERE\npassword: YOUR_PASSWORD_HERE\n\n[pypitest]\nrepository: https://testpypi.python.org/pypi\nusername: YOUR_USERNAME_HERE\npassword: YOUR_PASSWORD_HERE\n```\n\n\nYou should replace `YOUR_USERNAME_HERE` and `YOUR_PASSWORD_HERE` with your username and password from PyPI sites that you just created.\n\n\n## 3: The Python Package directory structure\n* `source_dir` is a root directory that contains your python package\n* `my_python_package` is your main python package that you want to publish\n\n\n```cpp\nsource_dir/                 # the source directory\n|-- my_python_package       # your package\n|   |-- __init__.py\n|   `-- FILES ....          # your package files\n|-- README.md\n|-- setup.cfg\n|-- setup.py\n```\n\nSetup your directory structure as shown above, with appropriate changes, and host it on [github.com](http://github.com).\n\n## 4: Release on github and get the download link\nThis step involves releasing your package on github. This will create a download link of your complete source. In order to release your github project, you need to carry on following steps:\n\n1. Go to your project homepage on [github](http://github.com)\n2. On top, you will see *Release link*. Click on it.\n3. Click on *Draft a new relase*\n4. Fill in all the details\n\t*  *Tag version* should be the version number of your package release\n\t*  *Release Title* can be anything you want.\n5. Click *Publish release* at the bottom of the page\n6. Now under *Releases* you can view all of your releases.\n7. Copy the download link (tar.gz) and save it somewhere.\n\n\n## 5: Editing files\nOpen the *setup.py* file and add following skeleton to it\n\n```python\nfrom distutils.core import setup\n\nsetup(\n    name = 'my_python_package',\n    packages = ['my_python_package'],\n    version = 'version number',  # Ideally should be same as your github release tag varsion\n    description = 'description',\n    author = '',\n    author_email = '',\n    url = 'github package source url',\n    download_url = 'download link you saved',\n    keywords = ['tag1', 'tag2'],\n    classifiers = [],\n)\n```\n\nOpen the *setup.cfg* file and add following skeleton to it\n\n```python\n[metadata]\ndescription-file = README.md\n```\n\nNow push everything to github.\n\n\n## 6: Publish the package\nExecute following commands\n\n```bash\npython setup.py register -r pypitest\n```\n\nThis command will try to register your package on PyPI test server. This makesures that everything you have setup is correct.\n\n```bash\npython setup.py sdist upload -r pypitest\n```\n\nThis command will upload your package on test repository and now you should see your package on [PyPI Test](https://testpypi.python.org/pypi)\n\nNow you are ready to publish your package on PyPI Live Server. Execute following commands\n\n```bash\npython setup.py register -r pypi\n```\n\n```bash\npython setup.py sdist upload -r pypi\n```\n\n**Congratulations! You just published your python package on PyPI**\n\n## References\n1. [Official Documentation](http://wiki.python.org/moin/CheeseShopTutorial#Submitting_Packages_to_the_Package_Index)\n",
    "similar": [
      "efficient-way-to-stop-an-iterating-loop",
      "rule-30",
      "python-prompts",
      "jaccard-minhash"
    ]
  }
]