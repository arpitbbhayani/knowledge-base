[
  {
    "id": 25,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "udemy-sql-taxonomy",
    "title": "Designing Taxonomy on a Relational DB",
    "description": "In this essay, design taxonomy on a SQL-based Relational database by taking Udemy as an example, write SQL queries covering common use-cases, and determine necessary indexes to make queries efficient.",
    "gif": "https://media.giphy.com/media/3o6Mbsop5cXzqVqfgA/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/115157736-83bbb980-a0a8-11eb-8974-545b7a9cba9d.png",
    "released_at": "2021-04-18",
    "total_views": 1303,
    "body": "In this essay, we will model [taxonomy](https://en.wikipedia.org/wiki/Taxonomy) on top of a relational database, and as a specific example, we will try to build [Udemy's Taxonomy](https://www.udemy.com/). The primary focus of this essay is to understand how to design taxonomy on top of [SQL based relational DB](https://en.wikipedia.org/wiki/Relational_database), define and write queries that are computationally efficient along with deciding indexes on the designed tables.\n\nIn the process, we will also understand a very interesting SQL construct like Window Functions that helps us solve seemingly complex use-cases with a single SQL query.\n\n# Udemy's Taxonomy\n\n[Udemy's Taxonomy](https://www.udemy.com/) is very simple; it features top-level categories - like Software Engineering, Arts, and Business - each category has multiple sub-categories - like Programming Languages, Databases, Sketching - and each sub-category has niche topics like - Python, Javascript, MySQL, etc.\n\nTo keep things simpler, we restrain that one topic can be part of only one sub-category and one sub-category can belong to only one top-level category; and that makes the maximum levels in this taxonomy as `3`.\n\n![https://user-images.githubusercontent.com/4745789/115139853-fcdbf200-a051-11eb-94f1-00382bd26db1.png](https://user-images.githubusercontent.com/4745789/115139853-fcdbf200-a051-11eb-94f1-00382bd26db1.png)\n\n# Database Design\n\nOut of our intuition, we can have one table for categories, one for holding sub-categories, and one for topics, and a bunch of [foreign keys](https://en.wikipedia.org/wiki/Foreign_key) that weaves them together. But is this the best we can come up with? A few issues with this design is\n\n- all the 3 tables will have an identical schema\n- if we were to introduce a new level, say `concept` that sits between sub-category and topic, we will have to create a new table to accommodate it, making this design cumbersome to future features and extensions.\n- what if for a few topics we want it to be a child of a category, leaving out sub-categories altogether; handling this with this design will be very tricky.\n\nSo, we need a better design, that is robust and extensible and hence we go for a single table called `topics` that holds categories, sub-categories, and topics differentiated with a column called `type` distinguishing between the 3. The schema of this table `topics` would be\n\n![SQL Schema - Taxonomy Udemy](https://user-images.githubusercontent.com/4745789/115140362-8260a180-a054-11eb-8820-2a830dcc025e.png)\n\nNow that we have the table `topics` ready, we see how the following two topics are stored\n\n- Software Engineering > Programming Languages > Python\n- Software Engineering > Programming Languages > Javascript\n\n![Sample Data - Taxonomy Udemy](https://user-images.githubusercontent.com/4745789/115140389-b340d680-a054-11eb-8a6d-b39a9f15fde8.png)\n\n# Indexes on `topics`\n\nPicking the right set of indexes is one of the most critical decisions that you will be taking while designing this system. A good set of indexes boosts the overall performance of the system, while poor and/or missing ones will put your database under a terrible load, especially at scale.\n\nBut how do we pick which indexes do we want on `topics`? The answer here is very simple, it depends on the kind of queries we have to support. So, let's list down queries that we will need and then determine indexes to make them efficient.\n\n## Get topic by ID\n\nThe most common query that we'd need is getting a topic by its `id` and this is very well facilitated by making `id` as a [primary key](https://en.wikipedia.org/wiki/Primary_key) of the table.\n\n```sql\nSELECT * FROM topics WHERE id = 531;\n```\n\n## Get the topic path\n\nGetting a topic path is an interesting use case. While rendering any category, sub-category, or topic page we would need to render breadcrumbs that hold the path of it in the taxonomy. For example, for Python's page, we will need to render a path like\n\n```python\nSoftware Engineering > Programming Languages > Python\n```\n\nThis path helps users explore and discover new categories, sub-categories, or topics. So, with our current schema, how could we compute the topic path for a given topic id.\n\nDoing it on the application side is the first approach that comes to mind but it is a poor one because we would be making `n` selects for `n` levels. In the case of our current system, we will be making `3` selects to compute the topic path; with the application pseudocode looking something like this\n\n```python\ndef get_topicpath(topic_id):\n    path = []\n\n    topic = get_topic_by_id(topic_id)\n    path.append(topic)\n\n    while topic.parent_id:\n        topic = get_topic_by_id(topic.parent_id)\n        path.append(topic)\n    \n    return path\n```\n\nWe can do a lot better than this. Since we know that the hierarchy has at max 3 levels, we can just do this in one SQL query with minor `NULL` handling on the application side.\n\nThe SQL query to get the topic path would have to join `3` instances of the `topics` table, each one handling one level in the hierarchy and joining with its parent on `parent_id`. The SQL query would fetch the `id` and the `name` of the topics in the topic path.\n\n```sql\nSELECT topics_level1.id, topics_level1.name,\n       topics_level2.id, topics_level2.name,\n       topics_level3.id, topics_level3.name\n\nFROM topics AS topics_level3\n    LEFT JOIN topics AS topics_level2\n        ON topics_level2.id = topics_level3.parent_id\n    LEFT JOIN topics AS topics_level1\n        ON topics_level1.id = topics_level2.parent_id\n\nWHERE topics_level3.id = 610;\n```\n\nIn the SQL query above we fetch the topic path for topic id `610`. We join table `topics` twice (3 instances of topics table) each handling a distinct level. Since we are using JOIN, if a `parent_id` is `NULL` and the join parameter would not match anything which would result `NULL` selects for those columns. These `NULL` values come in very handy when we compute the topic path for sub-categories and categories.\n\nIf the topic with `610` id is of type `topic` then\n\n- `topics_level1.id`, `topics_level1.name` will be category\n- `topics_level2.id`, `topics_level2.name` will be sub-category\n- `topics_level3.id`, `topics_level3.name` will be topic\n\nIf the topic with `610` id is of type `sub-category` then\n\n- `topics_level1.id`, `topics_level1.name` will be `NULL`\n- `topics_level2.id`, `topics_level2.name` will be category\n- `topics_level3.id`, `topics_level3.name` will be sub-category\n\nIf the topic with `610` id is of type `category` then\n\n- `topics_level1.id`, `topics_level1.name` will be `NULL`\n- `topics_level2.id`, `topics_level2.name` will be `NULL`\n- `topics_level3.id`, `topics_level3.name` will be category\n\nSo, in the application code, we still access all the selected columns but we create the topic path skipping the `NULL` values accordingly.\n\nTo support this query, our table only requires [Primary Key](https://en.wikipedia.org/wiki/Primary_key) on `id` and [Foreign Key](https://en.wikipedia.org/wiki/Foreign_key) on `parent_id`.\n\n## Get all the children of a category or a sub-category\n\nGetting all the children of a category or a sub-category will be heavily used to drive the \"Browse and Explore\" page, where users would want to drill down and explore the kind of topics Udemy covers. SQL Query for this has to support pagination and will be required to output all children for a given parent, in order of `score` such that more popular children are returned first.\n\n```sql\nSELECT * FROM topics WHERE parent_id = 123 ORDER BY score DESC;\n```\n\nThe SQL query above fetches all the child topics of a given parent topic with `id` = `123`. Since we are ordering by `score`, for this query to be efficient we create a [composite index](https://en.wikipedia.org/wiki/Composite_index_(database)) on `(parent_id, score)`.\n\n## Get category hierarchy\n\nUdemy, on its home page, puts out all the categories under a dropdown menu enabling users to explore top categories and topics in a glimpse.\n\nOne peculiar behavior of this is it shows all categories and top `k` sub-categories within each. Once we hover upon a sub-category it makes a network call to fetch top topics within that sub-category. This means we need to write a query that fetches all categories and `k` sub-categories within each category from the entire `topics` table.\n\nAlthough it looks very complicated at first, it is very easy to do with a single SQL query.\n\n```sql\nSELECT t1_id, t1_name, t2_id, t2_name, t2_score\nFROM (\n    SELECT topics1.id AS t1_id, topics1.name AS t1_name,\n           topics2.id AS t2_id, topics2.name AS t2_name,\n           ROW_NUMBER() OVER (PARTITION BY topics1.id) row_num\n\n    FROM topics AS topics1\n        LEFT JOIN topics AS topics2 ON topics1.id = topics2.parent_id\n\n    WHERE topics1.type = 1 and topics2.type = 2\n\n    ORDER BY topics1.score DESC, topics2.score DESC\n\n) t\nWHERE row_num <= 10;\n```\n\nAbove SQL query picks all categories and top `10` sub-categories from each category and returns it as part of `SELECT`. It uses a very interesting SQL construct called Window Functions, specifically [`ROW_NUMBER`](https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number) and `PARTITION BY`.\n\nWe perform the usual join on `topics` once where the left operand is categories (topics with `type = 1`) and the right one is a sub-category (topics with `type = 2`). We then partition this join by category `id` and then compute `ROW_NUMBER` for sub-categories within it.\n\nThe row numbers are computed for each partition separately so it goes as `1, 2, 3, ..., n` for `n` rows within each category. We then apply a simple `WHERE` clause check on this row number to be `<= k` which then typically matches the first `k` row within each partition i.e category.\n\nNote: to get \"top\" `k` sub-categories we just apply for an additional `ORDER BY` on `score` that sorts the sub-categories ensuring top sub-categories are fetched first. This way the first `k` rows we consider from the partition are essentially the top sub-categories within the category.\n\nTo make this SQL query efficient we would need a [foreign key](https://en.wikipedia.org/wiki/Foreign_key) on `parent_id` and an index on `score` to make `ORDER BY` efficient.\n\n## Summary of indexes we need on `topics`\n\n- Primary Key on `id`\n- Foreign Key on `parents_id`\n- Index on `type`\n- Composite Index on `(parent_id, score)`\n\n# Explore more\n\nAlthough we covered quite a bit of this DB design there is always something interesting in exploring something new around this topic; so\n\n- explore [Nested Set Model](https://en.wikipedia.org/wiki/Nested_set_model) to design Taxonomy on relational databases\n- explore how DB engines behave when there are no indexes, you can use `EXPLAIN` to understand the behavior\n- find if there could be a better alternative to paginate results apart from `LIMIT/OFFSET`\n\nThus we designed a neat [Taxonomy](https://en.wikipedia.org/wiki/Taxonomy) on top of SQL-based relational databases like MySQL, Postgres, etc; wrote queries for some common scenarios, and determined the indexes to make taxonomy efficient.\n\n# References\n\n- [Window Functions - MySQL](https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html)\n- [Partitioning Types - MySQL](https://dev.mysql.com/doc/mysql-partitioning-excerpt/8.0/en/partitioning-types.html)\n",
    "similar": [
      "better-programmer",
      "benchmark-and-compare-pagination-approach-in-mongodb",
      "how-sleepsort-helped-me-understand-concurrency-in-golang",
      "multiple-mysql-on-same-server-using-docker"
    ]
  },
  {
    "id": 41,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "bitcask",
    "title": "Bitcask - A Log-Structured Fast KV Store",
    "description": "Bitcask is a Key-Value store that persists its data in append-only log files and still reaps super-performant read-write throughputs. In this essay, we take a detailed look into Bitcask, its design, and find the secret sauce that makes it so performant.",
    "gif": "https://media.giphy.com/media/l0NwF1dnk7GRz3pK0/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/87868516-144b5900-c9b4-11ea-91e3-2de14e80817b.png",
    "released_at": "2020-07-19",
    "total_views": 574,
    "body": "Bitcask is one of the most efficient embedded Key-Value (KV) Databases designed to handle production-grade traffic. The paper that introduced Bitcask to the world says it is a *[Log-Structured](https://en.wikipedia.org/wiki/Log-structured_file_system) [Hash Table](https://en.wikipedia.org/wiki/Hash_table) for Fast Key/Value Data* which, in a simpler language, means that the data will be written sequentially to an append-only log file and there will be pointers for each `key` pointing to the `position` of its log entry. Building a KV store off the append-only log files seems like a really weird design choice, but Bitcask does not only make it efficient but it also gives a really high Read-Write throughput.\n\nBitcask was introduced as the backend for a distributed database named [Riak](https://riak.com/) in which each node used to run one instance of Bitcask to hold the data that it was responsible for. In this essay, we take a detailed look into Bitcask, its design, and find the secret sauce that makes it so performant.\n\n# Design of Bitcask\n\nBitcask uses a lot of principles from [log-structured file systems](https://en.wikipedia.org/wiki/Log-structured_file_system) and draws inspiration from a number of designs that involve log file merging, for example - merging in LSM Trees. It essentially is just a directory of append-only (log) files with a fixed structure and an in-memory index holding the keys mapped to a bunch of information necessary for point lookups - referring to the entry in the datafile.\n\n## Datafiles\n\nDatafiles are append-only log files that hold the KV pairs along with some meta-information. A single Bitcask instance could have many datafiles, out of which just one will be active and opened for writing, while the others are considered immutable and are only used for reads.\n\n![Bitcask Datafiles](https://user-images.githubusercontent.com/4745789/87866701-78fdb800-c9a2-11ea-9c35-9a706ac96d97.png)\n\nEach entry in the datafile has a fixed structure illustrated above and it stores `crc`, `timestamp`, `key_size`, `value_size`, actual `key`, and the actual `value`. All the write operations - create, update and delete - made on the engine translates into entries in this active datafile. When this active datafile meets a size threshold, it is closed and a new active datafile is created; and as stated earlier, when closed (intentionally or unintentionally), the datafile is considered immutable and is never opened for writing again.\n\n## KeyDir\n\nKeyDir is an in-memory hash table that stores all the keys present in the Bitcask instance and maps it to the offset in the datafile where the log entry (value) resides; thus facilitating the point lookups. The mapped value in the Hash Table is a structure that holds `file_id`, `offset`, and some meta-information like `timestamp`, as illustrated below.\n\n![Bitcask KeyDir](https://user-images.githubusercontent.com/4745789/87866707-96cb1d00-c9a2-11ea-9730-fc7f8cb79b92.png)\n\n# Operations on Bitcask\n\nNow that we have seen the overall design and components of Bitcask, we can jump into exploring the operations that it supports and details of their implementations.\n\n### Putting a new Key Value\n\nWhen a new KV pair is submitted to be stored in the Bitcask, the engine first appends it to the active datafile and then creates a new entry in the KeyDir specifying the offset and file where the value is stored. Both of these actions are performed atomically which means either the entry is made in both the structures or none.\n\nPutting a new Key-Value pair requires just one atomic operation encapsulating one disk write and a few in-memory access and updates. Since the active datafile is an append-only file, the disk write operation does not have to perform any disk seek whatsoever making the write operate at an optimum rate providing a high write throughput.\n\n### Updating an existing Key Value\n\nThis KV store does not support partial update, out of the box, but it does support full value replacement. Hence the update operation is very similar to putting a new KV pair, the only change being instead of creating an entry in KeyDir, the existing entry is updated with the new position in, possibly, the new datafile.\n\nThe entry corresponding to the old value is now dangling and will be garbage collected explicitly during merging and compaction.\n\n### Deleting a Key\n\nDeleting a key is a special operation where the engine atomically appends a new entry in the active datafile with value equalling a tombstone value, denoting deletion, and deleting the entry from the in-memory KeyDir. The tombstone value is chosen as something very unique so that it does not interfere with the existing value space.\n\nDelete operation, just like the update operation, is very lightweight and requires a disk write and an in-memory update. In delete operation as well, the older entries corresponding to the deleted keys are left dangling and will be garbage collected explicitly during merging and compaction.\n\n### Reading a Key-Value\n\nReading a KV pair from the store requires the engine to first find the datafile and the offset within it for the given key; which is done using the KeyDir. Once that information is available the engine then performs one disk read from the corresponding datafile at the offset to retrieve the log entry. The correctness of the value retrieved is checked against the CRC stored and the value is then returned to the client.\n\nThe operation is inherently fast as it requires just one disk read and a few in-memory accesses, but it could be made faster using Filesystem read-ahead cache.\n\n# Merge and Compaction\n\nAs we have seen during Update and Delete operations the old entries corresponding to the key remain untouched and dangling and this leads to Bitcask consuming a lot of disk space. In order to make things efficient for the disk utilization the engine once a while compacts the older closed datafiles into one or many merged files having the same structure as the existing datafiles.\n\nThe merge process iterates over all the immutable files in the Bitcask and produces a set of datafiles having only *live* and *latest* versions of each present key. This way the unused and non-existent keys are ignored from the newer datafiles saving a bunch of disk space. Since the record now exists in a different merged datafile and at a new offset, its entry in KeyDir needs an atomic updation.\n\n# Performant bootup\n\nIf the Bitcask crashes and needs a boot-up, it will have to read all the datafiles and build a new KeyDir. Merging and compaction here do help as it reduces the need to read data that is eventually going to be evicted. But there is another operation that could help in making the boot times faster.\n\nFor every datafile a *hint* file is created which holds everything in the datafile except the value i.e. it holds the key and its meta-information. This *hint* file, hence, is just a file containing all the keys from the corresponding datafile. This *hint* file is very small in size and hence by reading this file the engine could quickly create the entire KeyDir and complete the bootup process faster.\n\n# Strengths and Weaknesses of Bitcask\n\n## Strengths\n\n- Low latency for read and write operations\n- High Write Throughput\n- Single disk seek to retrieve any value\n- Predictable lookup and insert performance\n- Crash recovery is fast and bounded\n- Backing up is easy - Just copy the directory would suffice\n\n## Weaknesses\n\nThe KeyDir holds all the keys in memory at all times and this adds a huge constraint on the system that it needs to have enough memory to contain the entire keyspace along with other essentials like Filesystem buffers. Thus the limiting factor for a Bitcask is the limited RAM available to hold the KeyDir.\n\nAlthough this weakness sees a major one but the solution to this is fairly simple. We can typically shard the keys and scale it horizontally without losing much of the basic operations like Create, Read, Update, and Delete.\n\n# References\n\n- [Bitcask Paper](https://riak.com/assets/bitcask-intro.pdf)\n- [Bitcask - Wikipedia](https://en.wikipedia.org/wiki/Bitcask)\n- [Riak's Bitcask - High Scalability](http://highscalability.com/blog/2011/1/10/riaks-bitcask-a-log-structured-hash-table-for-fast-keyvalue.html/)\n- [Implementation of the Bitcask storage model-merge and hint files](https://topic.alibabacloud.com/a/implementation-of-the-bitcask-storage-model-merge-and-hint-files_8_8_31516931.html)\n",
    "similar": [
      "durability",
      "persistent-data-structures-introduction",
      "isolation",
      "atomicity"
    ]
  },
  {
    "id": 52,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "mysql-cache",
    "title": "Midpoint Insertion Strategy in MySQL LRU Cache",
    "description": "The MySQL InnoDB Storage engine uses LRU cache but it suffers from a notorious problem. In this article, we find how by using Midpoint Insertion Strategy and changing one aspect of LRU, MySQL becomes scan resistant and super performant.",
    "gif": "https://media.giphy.com/media/daUOBsa1OztxC/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/80304802-0ed7db80-87d6-11ea-98db-bc5d4afe965b.png",
    "released_at": "2020-04-26",
    "total_views": 466,
    "body": "Disk reads are 4x (for SSD) to 80x (for magnetic disk) [slower](https://gist.github.com/hellerbarde/2843375) as compared to main memory (RAM) reads and hence it becomes extremely important for a database to utilize main memory as much as it can, and be super-performant while keeping its latencies to a bare minimum. Engines cannot simply replace disks with RAM because of volatility and cost, hence it needs to strike a balance between the two - maximize main-memory utilization and minimize the disk access.\n\nThe database engine virtually splits the data files into pages. A page is a unit which represents how much data the engine transfers at any one time between the disk (the data files) and the main memory. It is usually a few kilobytes 4KB, 8KB, 16KB, 32KB, etc. and is configurable via engine parameters. Because of its bulky size, a page can hold one or multiple rows of a table depending on how much data is in each row i.e. the length of the row.\n\n# Locality of reference\nDatabase systems exhibit a strong and predictable behaviour called [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference) which suggests the access pattern of a page and its neighbours.\n\n## Spatial Locality of Reference\nThe spatial locality of reference suggests if a row is accessed, there is a high probability that the neighbouring rows will be accessed in the near future.\n\nHaving a larger page size addresses this situation to some extent. As one page could fit multiple rows, this means when that page is cached in main memory, the engine saves a disk read if the neighbouring rows residing on the same page are accessed.\n\nAnother way to address this situation is to [read-ahead](https://dev.mysql.com/doc/refman/8.0/en/innodb-disk-io.html) pages that are very likely to be accessed in the future and keep them available in the main memory. This way if the read-ahead pages are referenced, the engine needs to go to the disk to fetch the page, rather it will find the page residing in the main memory and thus saving a bunch of disk reads.\n\n## Temporal Locality of Reference\nThe temporal locality of reference suggests that if a page is recently accessed, it is very likely that the same page will be accessed again in the near future.\n\nCaching exploits this behaviour by putting every single page accessed from the disk into main-memory (cache). Hence the next time a page which is available in the cache is referenced, the engine need not make a disk read to get the page, rather it could reference it from the cache directly, again saving a disk read.\n\n![Disk cache-control flow](https://user-images.githubusercontent.com/4745789/80286313-4e57e680-8748-11ea-88c2-dcb67f6ac566.png)\n\nSince the cache is very costly, it is in magnitude smaller in capacity than the disk. It can only hold some fixed number of pages which means the cache suffers from the problem of getting full very quickly. Once the cache gets full, the engine needs to evict an old page so that the new page, which according to the temporal locality of reference is going to be accessed in the near future, could get a place in the cache.\n\nThe most common strategy that decides the page that will be evicted from the cache is the [Least Recently Used cache eviction strategy](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU)). This strategy uses Temporal Locality of Reference to the core and hence evicts the page which was not accessed the longest, thus maximizing the time the most-recently accessed pages are held in the cache.\n\n# LRU Cache\nThe LRU cache holds the items in the order of its last access, allowing us to identify which item is not being used the longest. When the cache is full and a newer item needs to make an entry in the cache, the item which is not accessed the longest is evicted and hence the name Least Recently Used.\n\nThe one end (head) of the list holds the most-recently referenced page while the fag end (tail) of the list holds the least-recently referenced one. A new page, being most-recently accessed, is always added at the head of the list while the eviction happens at the tail. If a page from the cache is referenced again, it is moved to the head of the list as it is now the most-recently referenced.\n\n## Implementation\nLRU cache is often implemented by pairing a [doubly-linked list](https://en.wikipedia.org/wiki/Doubly_linked_list) with a [hash map](https://en.wikipedia.org/wiki/Hash_table). The cache is thus just a linked list of pages and the hashmap maps the `page_id` to the node in the linked list, enabling `O(1)` lookups.\n\n![LRU Cache](https://user-images.githubusercontent.com/4745789/80288324-d7751a80-8754-11ea-96ab-6a8e25730bff.png)\n\n## InnoDB's Buffer Pool\nMySQL InnoDB's cache is called [Buffer Pool](https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html) which does exactly what has been established earlier. Pseudocode implementation of `get_page` function, using which the engine gets the page for further processing, could be summarized as\n\n```py\ndef get_page(page_id:int) -> Page:\n    # Check if the page is available in the cache\n    page = cache.get_page(page_id)\n\n    # if the page is retrieved from the main memory\n    # return the page.\n    if page:\n        return page\n\n    # retrieve the page from the disk\n    page = disk.get_page(page_id)\n\n    # put the page in the cache,\n    # if the cache is full, evict a page which is\n    # least recently used.\n    if cache.is_full():\n        cache.evict_page()\n\n    # put the page in the cache\n    cache.put_page(page)\n\n    # return the pages\n    return page\n```\n\n## A notorious problem with Sequential Scans\nAbove caching strategy works wonders and helps the engine to be super-performant. [Cache hit ratio](https://www.stix.id.au/wiki/Cache_Hit_Ratio) is usually more than 80% for mid-sized production-level traffic, which means 80% of the times the pages were served from the main memory (cache) and the engine did not require to make the disk read.\n\nWhat would happen if an entire table is scanned? say, while talking a [DB dump]((https://dev.mysql.com/doc/refman/8.0/en/mysqldump.html)), or running a `SELECT` without `WHERE` to perform some statistical computations.\n\nGoing by the MySQL's aforementioned behaviour, the engine iterates on all the pages and since each page which is accessed now is the most recent one, it puts it at the head of the cache while evicting one from the tail.\n\nIf the table is bigger than the cache, this process will wipe out the entire cache and fill it with the pages from just one table. If these pages are not referenced again, this is a total loss and performance of the database takes a hit. The performance will pickup once these pages are evicted from the cache and other pages make an entry.\n\n# Midpoint Insertion Strategy\nMySQL InnoDB Engine ploys an extremely smart solution to solve the notorious problem with Sequential Scans. Instead of keeping its Buffer Pool a strict LRU, it tweaks it a little bit.\n\nInstead of treating the Buffer Pool as a single doubly-linked list, it treats it as a combination of two smaller sublists - usually 5/8th and 3/8th of the total size. One sublist holds the younger data while the other one holds the older data. The head of the Young sublist holds the most recent pages and the recency decreases as we reach the tail of the Old sublist.\n\n![MySQL InnoDB Midpoint Insertion Strategy](https://user-images.githubusercontent.com/4745789/80299447-138a9880-87b2-11ea-9b0a-888e0ccf4b49.png)\n\n## Eviction\nThe tail of the Old Sublist holds the Least Recently Used page and the eviction thus happens as per the LRU Strategy i.e. at the tail of the Old Sublist.\n\n## Insertion\nThis is where this strategy differs from Strict LRU. The insertion, instead of happening at \"newest\" end of the list i.e. head of Young sublist, happens at the head of Old sublist i.e. in the \"middle\" of the list. This position of the list where the tail of the Young sublist meets the head of the Old sublist is referred to as the \"midpoint\", and hence the name of the strategy is Midpoint Insertion Strategy.\n\n> By inserting in the middle, the pages that are only read once, such as during a full table scan, can be aged out of the Buffer Pool sooner than with a strict LRU algorithm.\n\n## Moving page from Old to the Young sublist\nIn this strategy, like in Strict LRU implementation, whenever the page is accessed it moves to the newest end of the list i.e. the head of the Young sublist. During the first access, the pages make an entry in the cache in the \"middle\" position.\n\nIf the page is referenced the second time it is moved to the head of Young sublist and hence stays in the cache for a longer time. If the page, after being inserted in the middle, is never referenced again (during full scans), it is evicted sooner because the Old sublist is usually shorter than the Young sublist.\n\nThe Young sublist thus remains unaffected by table scans bringing in new blocks that might or might not be accessed afterwards. The engine thus remains performant as more frequently accessed pages continue to remain in the cache (Young sublist).\n\n## MySQL parameter to tune the midpoint\nInnoDB allows us to tune the midpoint of the buffer pool through the parameter `innodb_old_blocks_pct`. This parameter controls the percentage of Old sublist to Buffer Pool. The default value is 37 which corresponds to the ratio 3/8.\n\nIn order to get greater insights about Buffer Pool we can invoke the following command as\n\n```\n$ SHOW ENGINE INNODB STATUS\n\n----------------------\nBUFFER POOL AND MEMORY\n----------------------\nTotal memory allocated 137363456; in additional pool allocated 0\nDictionary memory allocated 159646\nBuffer pool size   8191\nFree buffers       7741\nDatabase pages     449\nOld database pages 0\n\n...\n\nPages made young 12, not young 0\n43.00 youngs/s, 27.00 non-youngs/s\n\n...\n\nBuffer pool hit rate 997 / 1000, young-making rate 0 / 1000 not 0 / 1000\nPages read ahead 0.00/s, evicted without access 0.00/s, Random read ahead 0.00/s\n\n...\n```\n\nThe command `SHOW ENGINE INNODB STATUS` outputs a lot of interesting metrics but the most interesting and critical ones, w.r.t Memory and Buffer Pool, are\n\n - number of pages that were made young\n - rate of eviction without access\n - cache hit ratio\n - read ahead rate\n\n# Conclusion\nWe see how by changing just one aspect of LRU cache, MySQL InnoDB makes itself Scan Resistant. Sequential scanning was a critical issue for the cache but it was addressed in a very elegant way.\n\n# References\n - [Latency numbers](https://gist.github.com/hellerbarde/2843375)\n - [Locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference)\n - [InnoDB: Making Buffer Cache Scan Resistant](https://serge.frezefond.com/2009/12/innodb-making-buffer-cache-scan-resistant/)\n - [MySQL Dev - Buffer Pool](https://dev.mysql.com/doc/refman/8.0/en/innodb-buffer-pool.html)\n - [MySQL Dev - Making the Buffer Pool Scan Resistant](https://dev.mysql.com/doc/refman/8.0/en/innodb-performance-midpoint_insertion.html)\n - [MySQL Dev - InnoDB Disk I/O](https://dev.mysql.com/doc/refman/8.0/en/innodb-disk-io.html)\n",
    "similar": [
      "2q-cache",
      "ts-smoothing",
      "israeli-queues",
      "copy-on-write"
    ]
  },
  {
    "id": 66,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "fast-and-efficient-pagination-in-mongodb",
    "title": "Fast and Efficient Pagination in MongoDB",
    "description": "MongoDB is a document based data store and hence pagination is one of the most common use case of it. Find out how you can paginate the results ...",
    "gif": "https://media.giphy.com/media/lRnUWhmllPI9a/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63153080-1336dc80-c02b-11e9-931f-0c7552a63ebc.png",
    "released_at": "2017-06-06",
    "total_views": 3898,
    "body": "[MongoDB](https://www.mongodb.com/) is a document based data store and hence pagination is one of the most common use case of it. So when do you paginate the response? The answer is pretty neat; you paginate whenever you want to process result in chunks. Some common scenarios are\n\n- Batch processing\n- Showing huge set of results on user interface\n\nPaginating on client and server side are both really very expensive and should not be considered. Hence pagination is generally handled at database level and databases are optimized for such needs too.\n\nBelow I shall explain you the 2 approaches through which you can easily paginate your MongoDB responses.\nSample Document\n\n```json\n    {\n        \"_id\" : ObjectId(\"5936d17263623919cd5165bd\"),\n        \"name\" : \"Lisa Rogers\",\n        \"marks\" : 34\n    }\n```\n\n## Approach 1: Using `cursor.skip` and `cursor.limit`\n\nMongoDB cursor has two methods that makes paging easy; they are\n\n- `cursor.skip()`\n- `cursor.limit()`\n\n`skip(n)` will skip `n` documents from the cursor while `limit(n)` will cap the number of documents to be returned from the cursor. Thus combination of two naturally paginates the response.\n\nIn Mongo Shell your pagination code looks something like this\n\n```js\n    // Page 1\n    db.students.find().limit(5)\n\n    // Page 2\n    db.students.find().skip(5).limit(5)\n\n    // Page 3\n    db.students.find().skip(5).limit(5)\n```\n\n`.find()` will return a cursor pointing to all documents of the collection and then for each page we skip some and consume some. Through continuous skip and limit we get pagination in MongoDB.\n\nI am fond of Python and hence here is a small trivial function to implement pagination:\n\n```python\n    def skiplimit(page_size, page_num):\n        \"\"\"returns a set of documents belonging to page number `page_num`\n        where size of each page is `page_size`.\n        \"\"\"\n        # Calculate number of documents to skip\n        skips = page_size * (page_num - 1)\n\n        # Skip and limit\n        cursor = db['students'].find().skip(skips).limit(page_size)\n\n        # Return documents\n        return [x for x in cursor]\n```\n\n## Approach 2: Using `_id` and `limit`\n\nThis approach will make effective use of default index on `_id` and nature of `ObjectId`.\nI bet you didn\u2019t know that a [Mongodb ObjectId](https://docs.mongodb.com/manual/reference/bson-types/#objectid) is a 12 byte structure containing\n\n- a 4-byte value representing the seconds since the Unix epoch,\n- a 3-byte machine identifier,\n- a 2-byte process id, and\n- a 3-byte counter, starting with a random value.\n\nEven I didn\u2019t until I read the [documentation](https://docs.mongodb.com/manual/reference/bson-types/#objectid). Apart from its structure there is one very interesting property of ObjectId; which is - *ObjectId has natural ordering*\n\nWhat does it mean? It simplifies that we can apply all the *less-than-s* and all the *greater-than-s you* want to it. If you don\u2019t believe me, open Mongo shell and execute following set of commands\n\n```javascript\n    > ObjectId(\"5936d49863623919cd56f52d\") > ObjectId(\"5936d49863623919cd56f52e\")\n    false\n    > ObjectId(\"5936d49863623919cd56f52d\") > ObjectId(\"5936d49863623919cd56f52a\")\n    true\n```\n\nUsing this property of ObjectId and also taking into consideration the fact that `_id` is always indexed, we can devise following approach for pagination:\n\n1. Fetch a page of documents from database\n2. Get the document id of the last document of the page\n3. Retrieve documents greater than that id\n\nIn Mongo Shell your pagination code looks something like this\n\n```javascript\n    // Page 1\n    db.students.find().limit(10)\n\n    // Page 2\n    last_id = ...  # logic to get last_id\n    db.students.find({'_id': {'$gt': last_id}}).limit(10)\n\n    // Page 3\n    last_id = ... # logic to get last_id\n    db.students.find({'_id': {'$gt': last_id}}).limit(10)\n```\n\nAgain, I am fond of Python and here is the Python implementation of this approach.\n\n```python\n    def idlimit(page_size, last_id=None):\n        \"\"\"Function returns `page_size` number of documents after last_id\n        and the new last_id.\n        \"\"\"\n        if last_id is None:\n            # When it is first page\n            cursor = db['students'].find().limit(page_size)\n        else:\n            cursor = db['students'].find({'_id': {'$gt': last_id}}).limit(page_size)\n\n        # Get the data\n        data = [x for x in cursor]\n\n        if not data:\n            # No documents left\n            return None, None\n\n        # Since documents are naturally ordered with _id, last document will\n        # have max id.\n        last_id = data[-1]['_id']\n\n        # Return data and last_id\n        return data, last_id\n```\n\n> If you are using a field other than `_id` for offset, make sure the field is indexed and properly ordered else the performance will suffer.\n\n## Closing Remarks\n\nBoth of the above approaches are valid and correct. But as we know, in field of Computer Science, whenever there are multiple options to achieve something, one always outperforms the other. Same is the situation here as well.\n\nTurns out, there is a severe problem with skip function. I have tried to jot it down in [this blog post](/blogs/mongodb-cursor-skip-is-slow). Because of which second approach has advantage over first. But that is not it; I wrote a simple [python code](https://github.com/arpitbbhayani/mongo-pagination-benchmark) to benchmark the two approaches for various combinations and it turns out `skip` performs better in some case. The results are compiled into [this blog post](/blogs/benchmark-and-compare-pagination-approach-in-mongodb).\n",
    "similar": [
      "efficient-way-to-stop-an-iterating-loop",
      "morris-counter",
      "jaccard-minhash",
      "bayesian-average"
    ]
  },
  {
    "id": 68,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "mongodb-cursor-skip-is-slow",
    "title": "Why MongoDB's cursor.skip() is Slow?",
    "description": "MongoDB's cursor.skip() is very inefficient, why is that? Even though it is slow and inefficient,  team MongoDB wants to continue keeping it. Find out why ...",
    "gif": "https://media.giphy.com/media/nqIuAIxYebIt2/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63220620-4d2ded00-c1a9-11e9-8eea-11766291e06f.png",
    "released_at": "2017-06-04",
    "total_views": 2088,
    "body": "MongoDB\u2019s cursor object has a method called `skip`, which as per [documentation and definition](https://docs.mongodb.com/manual/reference/method/cursor.skip/#definition), controls where MongoDB begins returning results. Thus in combination with function [limit](https://docs.mongodb.com/manual/reference/method/cursor.limit/), one can easily have paginated results.\n\nI have written a blog post on [how you can have Fast and Efficient Pagination in MongoDB](/blogs/fast-and-efficient-pagination-in-mongodb).\n\nBut while going through the documentation of skip, there is something interesting to notice. There is a small warning in [MongoDB documentation](https://docs.mongodb.com/manual/reference/method/cursor.skip/#behavior), that states\n\n  > The `cursor.skip()` method is often expensive because it requires the server to walk from the beginning of the collection or index to get the offset or skip position before beginning to return results. As the offset (e.g. `pageNumber` above) increases, `cursor.skip()` will become slower and more CPU intensive. With larger collections, `cursor.skip()` may become IO bound.\n\nIn short, MongoDB has to iterate over documents to skip them. Thus when collection or result set is huge and you need to skip documents for pagination, the call to `cursor.skip` will be expensive. While going through the source code of `skip` I found out that it does not use any index and hence gets slower when result set increases in size.\n\nThis also implies that if you use `skip`  then the \u201cskipping speed\u201d will not improve even if you index the field.\n\nBut what if the size of result set is small? is calling `skip` still a terrible idea?\nIf skip was so terrible, then MongoDB team and community must had taken that decision long back. But they haven\u2019t \u2026 why?\n\nBecause it is very efficient and fast for smaller result set. I have taken this opportunity to [benchmark and compare](/blogs/benchmark-and-compare-pagination-approach-in-mongodb) the [two approach for pagination](/blogs/fast-and-efficient-pagination-in-mongodb) and there I found out skip and limit based pagination works well for smaller result sets.\n\nIn conclusion, skip is not as bad one might think. But you must understand your use case well so as to make an informed decision.\n",
    "similar": [
      "inheritance-c",
      "fork-bomb",
      "idf",
      "copy-on-write"
    ]
  },
  {
    "id": 67,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "uid": "benchmark-and-compare-pagination-approach-in-mongodb",
    "title": "Benchmark Pagination Strategies in MongoDB",
    "description": "Benchmark results for two pagination approaches for MongoDB.",
    "gif": "https://media.giphy.com/media/c5eqVJN7oNLTq/giphy.gif",
    "image": "https://user-images.githubusercontent.com/4745789/63220759-d514f680-c1ab-11e9-8a38-7b1828946b74.png",
    "released_at": "2017-06-02",
    "total_views": 3538,
    "body": "[MongoDB](https://www.mongodb.com/) is a document based data store and hence pagination is one of the most common use case of it. So when do you paginate the response? The answer is pretty neat; you paginate whenever you want to process result in chunks. Some common scenarios are\n\n- Batch processing\n- Showing huge set of results on user interface\n\nThere are multiple approaches through which you can [paginate your result set in MongoDB](/blogs/fast-and-efficient-pagination-in-mongodb). This blog post is dedicated for results of benchmark of two approaches and its analysis, so here we go ...\n\nBenchmark has been done over a non-indexed collection. Each document of the collection looks something like this\n```js\n    {\n        \"_id\" : ObjectId(\"5936d17263623919cd5165bd\"),\n        \"name\" : \"Lisa Rogers\",\n        \"marks\" : 34\n    }\n```\n\nAll records of a collection are fetched page-wise. Size of each page is fixed during fetch of the collection. Each page is fetched _3_ times and average of, time to fetch one \u201cpage\u201d, 3 is recorded.\n\nFollowing image shows the how two approach fares against each other.\n\n![MongoDB Pagination Benchmark Results](https://user-images.githubusercontent.com/4745789/63220692-cb3ec380-c1aa-11e9-9882-27bf52cbaa84.png)\n\nA key observation to note is that, till 500-600 count, both the approaches are comparable, but once it crosses that threshold, there is sudden rise in response time for `skip` and `limit` approach than other. The approach using `_id` and `limit` almost gives constant performance and is independent of size of the result set.\n\nI tried running this test on different machines with different disks but results were similar. I think diving deep in MongoDB's database drivier will yield better information about this behavior. You could see some spikes in the response times, that are because of Disk Contention.\n\nIn short:\n - For huge result set, paginating using `_id` and `limit` is far better than using `skip` and `limit`.\n - For smaller result set, it does not matter, but prefer skip and limit.\n\nAn interesting thing I observed is that after page size crosses 100, the gap between the two approach reduces to some extent. I am yet to perform detailed benchmark on that as such use-case (where page-size is more than 100) is pretty rare in practical applications.\n\nYou can find the Python code used for this benchmark [here](https://github.com/arpitbbhayani/mongo-pagination-benchmark). If you have any suggestion or improvement, do let me know.\n",
    "similar": [
      "udemy-sql-taxonomy",
      "multiple-mysql-on-same-server-using-docker",
      "better-programmer",
      "sliding-window-ratelimiter"
    ]
  }
]