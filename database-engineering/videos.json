[
  {
    "id": 223,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "zrl_odkY5tI",
    "title": "Datetime vs Timestamp datatype in databases - Which one is better and when?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nDatetime and Timestamps are the two most common ways to store data time in any database, but which one should we use and when? In this video, we answer this question, understand some of its internals, and cover the pointers that make it easy for us to decide.",
    "img": "https://i.ytimg.com/vi/zrl_odkY5tI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/4QX6y0fQT7yVO/giphy.gif",
    "duration": "13:10",
    "view_count": 5153,
    "like_count": 195,
    "comment_count": 12,
    "released_at": "2023-02-27",
    "gist": "",
    "notes_gd": "https://drive.google.com/file/d/1C_0_RVztiRfRkE8WXwFzYbdGjX2MLkaV/view?usp=share_link",
    "slug": "datetime-vs-timestamp-datatype-in-databases-which-one-is-better-and-when"
  },
  {
    "id": 220,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "n_t0IO0mq5Q",
    "title": "Understanding Phantom Reads Problem with hands on examples",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nPhantom Reads is a very interesting problem with database transactions and it happens especially when the transaction isolation level is set to READ COMMITTED. Phantom reads can lead to massive inconsistencies in the data and hence it is important to understand it.\n\nIn this video, we understand what the phantom read problem is, and how it occurs, by taking a real-world hands-on example.",
    "img": "https://i.ytimg.com/vi/n_t0IO0mq5Q/mqdefault.jpg",
    "gif": null,
    "duration": "12:38",
    "view_count": 2379,
    "like_count": 87,
    "comment_count": 18,
    "released_at": "2023-02-18",
    "gist": "",
    "notes_gd": "",
    "slug": "understanding-phantom-reads-problem-with-hands-on-examples"
  },
  {
    "id": 218,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "-Wcdl12ac5w",
    "title": "Understanding Database Isolation Levels with hands-on examples",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nThe I in ACID stands for Isolation and determines the transparency between different transactions executed at the same time. The way to tune this transparency is through isolation levels. In this video, we take an in-depth look into database isolation levels and understand them with examples.\n\nUnderstanding Read Committed, Read Uncommitted, Repeatable Reads, and Serializable Isolation Levels.",
    "img": "https://i.ytimg.com/vi/-Wcdl12ac5w/mqdefault.jpg",
    "gif": null,
    "duration": "18:23",
    "view_count": 0,
    "like_count": 20,
    "comment_count": 11,
    "released_at": "2023-02-18",
    "gist": "",
    "notes_gd": "",
    "slug": "understanding-database-isolation-levels-with-hands-on-examples"
  },
  {
    "id": 217,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "09E-tVAUqQw",
    "title": "Why do databases store data in B+ trees?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nJoin this channel to get access to the perks:\nhttps://www.youtube.com/channel/UC_b1GUJv_2QiMP4BxC9-Dxg/join\n\nWe all know databases store their data in B+ trees, but why and how?\n\nIn this video, we answer this very question and go through the evolution of storage from a naive way to an optimized B+ tree. We will talk about why there was a need to use B+ trees, how table data is stored in B+ trees, and how is this tree serialized and stored on the disk.",
    "img": "https://i.ytimg.com/vi/09E-tVAUqQw/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/ZanWL0MNtv3y0/giphy.gif",
    "duration": "29:43",
    "view_count": 6841,
    "like_count": 296,
    "comment_count": 22,
    "released_at": "2023-02-07",
    "gist": "Relational Databases and some non-relational databases use B+ trees to hold the data but why?\n\nOne of the main reasons why SQL databases use B+ trees to hold the data is because of their efficiency in performing operations such as insert, update, find, and delete.\n\n## Limitations of Sequential Files\n\nBefore we delve into the details of why SQL databases use B+ trees, let's first understand the limitations of storing data in a sequential file.\n\nWhen records of a table are stored in one file sequentially, performing operations such as insert, update, and delete becomes complicated, with a complexity of O(n). The linear scan in the middle of the file for insertion or overriding can take up a lot of time and resources, making it inefficient.\n\n## Where B+ Trees thrive\n\nIn B+ trees, rows or documents of a table are clubbed in B+ tree nodes, and each node holds a maximum of some `n` rows. For example, if one B+ tree node is 4KB big (same as a disk block size) and the row size is 40B, then each node will hold a maximum of 100 rows. This makes disk reads efficient since reading one node from a disk means reading 100 rows at once.\n\nNon-leaf nodes in a B+ tree hold routing information, while leaf nodes hold the actual rows. The leaf nodes are linked so that linear traversal of the actual rows is possible. The B+ tree structure thus ensures that the table is always logically and physically arranged by its primary key\n\n## CRUD Operations with B+ Trees\n\n### Finding row by ID\n\nFinding a row by ID involves\n\n1. traversing from the root node,\n2. reaching the leaf node that holds the row,\n3. reading the node and disk blocks in the main memory, and\n4. extracting the row from the node and returning\n\n### Inserting a new row\n\nInserting a new row involves\n\n1. finding the leaf node where the row should be placed\n2. reading the node in the main memory\n3. inserting the node\n4. rebalancing the tree, if needed\n5. flushing a leaf node where the value is updated\n\n### Updating a row\n\nUpdating a row involves\n\n1. finding the leaf node that holds the row\n2. reading the node (disk blocks) in the main memory\n3. updating the row in memory, and\n4. flushing the blocks to the disk.\n\n### Deleting a row\n\nDeleting a row involves\n\n1. finding the leaf node that holds the row,\n2. reading the node (disk blocks) in main memory,\n3. removing the row from the node, and\n4. flushing the blocks to the disk\n5. re-balance the tree, if required\n\n## Range Queries with B+ Trees\n\nRange queries such as finding rows with IDs in the range of 100 to 600 involve finding the leaf node that holds the first row, traversing linearly to reach the row with ID 600, and extracting the data until then.\n\nThe B+ tree structure ensures that the time complexity of these operations is O(log n), which is much more efficient than the O(n) complexity of sequential file storage.\n\n## Conclusion\n\nIn conclusion, SQL databases use B+ trees to store data efficiently and perform operations such as insert, update, find, and delete with a time complexity of O(log n). This makes managing and storing large volumes of data more manageable and efficient.",
    "notes_gd": "https://drive.google.com/file/d/1nKf6byk05d_PtHcMNOa9YtbC0-C72RMj/view?usp=share_link",
    "slug": "why-do-databases-store-data-in-b-trees"
  },
  {
    "id": 107,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "xELqRiovEcI",
    "title": "What are Embedded Databases?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nEmbedded databases are coupled with the application they are part of and operate in a confined space. They are designed to solve one problem for their niche very well. In this video, we take an introductory look into this amazing class of databases, understand the core reason why they exist, talk about a few popular ones, and understand a few use cases.\n\nOutline:\n00:00 Server-based Databases\n02:32 Embedded Databases\n06:35 Popular Embedded Databases\n10:39 Applications of Embedded Databases",
    "img": "https://i.ytimg.com/vi/xELqRiovEcI/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/uk3R91z24SoUZ27WTh/giphy.gif",
    "duration": "15:57",
    "view_count": 5076,
    "like_count": 176,
    "comment_count": 27,
    "released_at": "2022-03-25",
    "gist": "Traditional databases like MySQL, Postgres, MongoDB run on their server on a specific port. Anyone who wants to talk to the database can directly connect and talk.\n\nEmbedded Databases are different from these traditional databases, and they operate in their own confined space within a process. There is no separate process for the database.\n\nNo one can directly connect to this database, unlike how we do it with MySQL and other databases. The role and the use of the embedded database are limited to the process it is confined to.\n\n## Popular embedded databases are\n\n- SQLite: an embedded SQL-like database\n- LevelDB: on disk KV store by Google\n- RocksDB: on disk KV store optimized for performance\n- Berkeley DB: KV store with ACID, Replication, and Locking\n\nAn embedded database is always designed to solve one niche really well.\n\n## Application of Embedded Databases\n\nEvery modern browser uses an embedded database called IndexedDB to store browsing history and other configuration settings locally. The browser is confined to a machine, and the IndexedDB is contained in the browser; there is no separate process to connect to.\n\nEvery Android phone has support for SQLite database that we can use to store any information like game scores, stats, and information locally on the phone.\n\nThe core idea: When we need to store and query data that could be confined within a space and does not need to be centralized, we choose to use an Embedded Database.",
    "notes_gd": "https://drive.google.com/file/d/1_iXh0rCmGVZJj5CLWP7gJ4YzAP-yIiGb/view?usp=sharing",
    "slug": "what-are-embedded-databases"
  },
  {
    "id": 108,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "wI4hKwl1Cn4",
    "title": "How does the database guarantee reliability using write-ahead logging?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nAny persistent database needs to guarantee reliability. No matter how big or small the changes are, they should survive any reboots, OS, or hardware crashes once they are committed. All the persistent databases use a write-ahead logging technique to guarantee such reliability while not affecting the performance.\n\nIn this video, we talk about write-ahead logging, how it ensures reliability, a few solid advantages of using it, one of them being a massive database performance boost, and how the log files are structured on the disk.\n\nOutline:\n00:00 What happens on Commit\n05:57 Write-ahead Logging\n08:44 Advantages of having a Write-ahead Logging\n14:29 Data Integrity in WAL files\n16:40 Write-ahead Logging Internals\n\nRelated Videos:\nHow indexes make a database read faster: https://www.youtube.com/watch?v=3G293is403I",
    "img": "https://i.ytimg.com/vi/wI4hKwl1Cn4/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/l4pTqyJ8XMhLZ3ScE/giphy.gif",
    "duration": "22:6",
    "view_count": 7373,
    "like_count": 294,
    "comment_count": 19,
    "released_at": "2022-03-21",
    "gist": "Any persistent database needs to guarantee the reliability, implying that any update/delete fired on the database is reliably stored on the disk. The alterations on the data should not be affected by power loss, OS failure, or hardware failure.\n\nThe changes to the data once committed should do to non-volatile storage like Disk making them outlive any outage or crash. Although the flows seem simple enough to say that- hey let's just flush the changes to the disk; it is a little more complicated than that.\n\n## Disk writes are complicated\n\nWhen we perform the disk write, the changes are not directly written to the disk sectors, instead, it goes through a variety of buffers like RAM, OS Cache, Disk Cache, and then finally to the Disk sectors. So, if the changes are in any of these intermediate caches and the process crashes, our changes are lost.\n\nSo, while guaranteeing reliability we have to skip all of these caches and flush our changes as quickly as possible on the disk sectors. But if we do that, we are impacting the throughput of the system given how expensive disk writes are.\n\n## Write-ahead Logging\n\nWrite-ahead logging is a standard way to ensure data integrity and reliability. Any changes made on the database are first logged in an append-only file called write-ahead Log or Commit Log. Then the actual blocks having the data (row, document) on the disk are updated.\n\nIf we update any row or a document of a database, updating the data on disk is a much slower process because it would require on-disk data structures to rebalance, indexes to be updated, and so much more. So, if we skip the OS cache, and other buffers, and directly update the rows to the disk every time; it will hamper the overall throughput and performance of the database.\n\nHence, instead of synchronously flushing the row updates to the disk, we synchronously just flush the operation (PUT, DEL) in a write-ahead log file leading to just one extra disk write. The step will guarantee reliability and durability as although we do not have the row changes not flushed but at least the operation is flushed. This way if we need to replay the changes, in case of a crash, we can simply iterate through this simple file of operations and recover the database.\n\nOnce the operation is logged, the database can do its routine work and make the changes to the actual row or document data through the OS cache and other buffers. This way we guarantee reliability and durability in as minimal of a time as possible while not affecting the throughput much.\n\n### Advantages of using WAL\n\nThe advantages of using WAL are\n\n- we can skip flushing the data to the disk on every update\n- significantly reduce the number of disk writes\n- we can recover the data in case of a data loss\n- we can have point-in-time snapshots\n\n### Data integrity in WAL\n\nWAL also needs to ensure that any operation flushed in the log is not corrupted and hence it maintains its integrity using a CRC-32 and flushes it on the disk with every entry. This CRC is checked during reading the entry from the disk, if it does not match the DB throws an error.",
    "notes_gd": "https://drive.google.com/file/d/1VC77CEEYLvlFaXpKsb3Q_e0JvbbryyU0/view?usp=sharing",
    "slug": "how-does-the-database-guarantee-reliability-using-write-ahead-logging"
  },
  {
    "id": 109,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "3G293is403I",
    "title": "How do indexes make databases read faster?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nIn this video, we discuss how indexes make a database operate faster. While discussing that, we dive deep into how the data is read from the disk, how indexes are structured, serialized, and stored on the disk, and finally, how exactly data is quickly read using the right set of indexes.\n\nOutline:\n\n00:00 How is a table stored on the disk?\n03:14 Reading bytes from the disk\n05:21 Reading the entire table from the disk\n08:33 Evaluating a simple query without an index\n10:00 Basics of Database Indexes\n13:24 Evaluating a simple query with index",
    "img": "https://i.ytimg.com/vi/3G293is403I/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/WoWm8YzFQJg5i/giphy.gif",
    "duration": "23:25",
    "view_count": 12871,
    "like_count": 739,
    "comment_count": 86,
    "released_at": "2022-03-16",
    "gist": "The database is just a collection of records. These records are serialized and stored on the disk. The way the records are serialized and stored on the disk depends on the database engine.  \n  \n## How does the database read from the disk?  \nA disk is always read in Blocks, and a block is typically 8kb big. When any disk IO happens, even requesting one byte, the entire block is read in memory, and the byte is returned from it.  \n  \nSay we have a \"users\" table with 100 rows, with each row being 200B long. If the block size is 600B, we can read 600/200 = 3 rows in one disk read.  \n  \nTo find all users with age = 23, we will have to read the entire table row by row and filter out the relevant documents; we will have to read the entire table. In one shot, we read 3 rows so that it would take 100/3 = 34 disk/block reads to answer the query.  \n  \n## Let's see how indexes make this faster.  \n  \nAn index is a small referential table with two columns: the indexed value and the row ID. So an index on the age column will have two columns age (4 bytes) and row ID (4 bytes).  \n  \nEach entry in the index is 8 bytes long, and the total size of the index will be 100 * 8 = 800 bytes. When stored on the disk, the index will require 2 disk blocks.  \n  \nWhen we want to get users with age == 23, we will first read the entire index, taking 2 disk IOs and filtering out relevant row IDs. Then make disk reads for the relevant rows from the main table. Thus we read 2 blocks to load the index and only relevant blocks have the actual data.  \n  \nIn our example, it comes out to be 2 + 2 = 4 block reads. So, we get an 8x boost in performance using indexes.  \n  \nNote: This is a very crude example of how fetch happens with indexes; there are a lot of optimizations that I have not talked about.  ",
    "notes_gd": "https://drive.google.com/file/d/1wDDOc3rdsZIdZEEe50E_61wi-1iMHu2G/view?usp=sharing",
    "slug": "how-do-indexes-make-databases-read-faster"
  },
  {
    "id": 110,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "UT_TVldzA64",
    "title": "How to handle database outages?",
    "description": "System Design for Experienced Engineers: https://arpitbhayani.me/masterclass\nBecome a member for exclusive in-depth videos: https://www.youtube.com/c/ArpitBhayani/join\nRedis Internals: https://arpitbhayani.me/redis\n\nIn this video, we talk about why a database goes down, what happens when the database is down, a few short-term solutions to minimize the downtime, and a few long-term solutions that you should be doing to ensure that your database does not go down again.\n\nOutline:\n\n00:00 Why a database goes down?\n06:10 What happens when a DB is down?\n09:46 Short-term solutions to get your DB up\n17:33 Long-term solutions to fix the database",
    "img": "https://i.ytimg.com/vi/UT_TVldzA64/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/lVBtp4SRW6rvDHf1b6/giphy.gif",
    "duration": "30:38",
    "view_count": 4855,
    "like_count": 278,
    "comment_count": 26,
    "released_at": "2022-03-14",
    "gist": "### Why a database goes down?\nAn unexpected heavy load on your database can lead to a process crash or a massive slowdown.\n\nBefore jumping to the potential short-term and long-term solutions, ensure you monitor the database well. CPU, Memory, Disk, and Connections are being closely monitored.\n\n## Short term solutions\n\n- Kill the queries that have been running for a long time\n- Quickly scale up your database if you have been seeing a consistent heavy usage\n- Check if the recent deployment is the culprit; if so, revert asap\n- Reboot the database will calm the storm and buy you some time\n\n## Long term solutions\n\n- Ensure the right set of indexes is in place\n- Tune your database default parameters to gain optimal performance\n- Check for the notorious N+1 Queries\n- Upgrade the database version to get the best that DB can offer\n- Evaluate the need for Horizontal scaling using Replicas and Sharding",
    "notes_gd": "https://drive.google.com/file/d/1Q6YokLBvmfW1Tw1mpndOfx-I2NyRGVG0/view?usp=sharing",
    "slug": "how-to-handle-database-outages"
  },
  {
    "id": 111,
    "topic": {
      "id": 0,
      "uid": "database-engineering",
      "name": "Database Engineering",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2C-Da7Jscr7NptGcIZgQ2l",
      "bgcolor": "#E7ECFF",
      "themecolor": "#5677f5",
      "course_url": null
    },
    "yt_video_id": "x1XmcuosZNk",
    "title": "5 million + random rows in less than 100 seconds using SQL",
    "description": "In this video, we generate 5 million + random rows in less than 100 seconds using just SQL. We mock the data for any taxonomy (Udemy's example taken). We use Joins and SQL tricks to amplify the rows and use them to ingest.\n\n\nLink to the gist: https://gist.github.com/arpitbbhayani/96a42c28d134871ebc11faad272b5349",
    "img": "https://i.ytimg.com/vi/x1XmcuosZNk/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Gpu3skdN58ApO/giphy.gif",
    "duration": "26:3",
    "view_count": 2411,
    "like_count": 64,
    "comment_count": 20,
    "released_at": "2021-04-10",
    "gist": "",
    "notes_gd": "",
    "slug": "5-million-random-rows-in-less-than-100-seconds-using-sql"
  }
]