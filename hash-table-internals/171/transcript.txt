in the previous six videos we talked about internals of hash table ways to implement them and how to gracefully handle collisions in this video we put our entire focus on performance and we would look at how to quantify the load on a hash table the impact of chaining and open addressing on performance understanding the performance gains and loss for each strategy and conclude with a very interesting performance optimization but before we move forward i'd like to talk to you about a course on system design that i have been running for over a year now the course is a cohort based course which means i won't be rambling a solution and it will not be a monologue instead a small focused group of 50 60 engineers every cohort will be brainstorming systems and designing it together this way we build a solid system and learn from each other's experiences the course to date is enrolled by 600 plus engineers spanning 9 cohorts and 10 countries engineers from companies like google microsoft github slack facebook tesla yelp flipkart dream 11 and many many many more have taken this course and have some wonderful things to say the coolest part about the course is the depth we go into and the breadth we cover we cover topics ranging from real-time text communication for slack to designing our own toy load balancer to creek buses live text commentary to doing impressions counting at scale for any advertisement business in all we would cover roughly 28 questions and the detailed curriculum uh split week by week can be found on the course page which is linked in the description down below so if you're looking to learn system design from the first principles you will love this course i have two offerings for you the first one is the live cohort discourse which you see on the left side and the second one is the recorded course which you can see on the right side the live cover based course happens every two months and it will go on for eight weeks while the recorded course contains the recordings from one of the past cohorts as is if you are in a hurry and want to binge learn system design i would highly recommend you going for the recorded one otherwise the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings the decision is totally up to you the course details prerequisites testimonials can be found in the course page at pittmarty dot me slash master class and i would highly recommend you to check that out i put the link of the course in the description down below so if you are interested to learn system design go for it check out the link in the description down below and i hope to see you in my next cohort thanks so hash tables are amazing data structures at it gives us constant time performance when there are zero collisions internally it is just an array where when we would want to place a key in the hash table we pass the key through the hash function take the index go to that index place our key there right it gives us constant performance when there is zero when there are zero collisions right but at scale that's impossible you have a large application key rain that you are trying to fit into this small space so there is bound to have collisions and we know that as hash table gets filled the performance would degrade so how do we know that my hash table is full we need a way to quantify it so the way through which we quantify this is called as a load factor so load factor theoretically is alpha which is equal to n upon m where n are the total number of elements in your array or sorry in your hash table divided by the total number of slots in your hash table so if i am placing 20 elements in my hash table where my size of the array is 10 my load factor is 20 upon 10 is equal to 2 right so load factor for each strategy like the definition of load factor would not change but let's just take a look on how load factor is there for chaining and open addressing we'll talk about chaining first with chaining we can clearly see that we can never fill the table a table will never get full because with chaining you are always adding the key that you are inserting in the new slot or rather in the new node of a linked list right you are always creating a new node and putting it there so you are always using an auxiliary data structure to hold the key this way even if there is collision even if there is anything you are just adding a new node to the link list so your table although you may occupy all the slots of your hash table but you would never be able to fill it completely but obviously when you add a large number of keys even with chained hashing your performance would start to degrade and you and that would be worrisome for you with open addressing you with open addressing you would eventually fill the table because what you are doing is you have an array where you are trying to place a key if that's what is occupied you are finding the next free slot in your hash table and with time and with time the entire hash table array that you have will be filled right so when your entire table gets filled there would not be any place for you to place a new key right which is what your right then then your rights would start to fail right so you would be running out of your space eventually so that is where it is very important for you to measure how full the table is and then take preventive measures of your hash table still performing really well right so with chain hashing uh if i talk about the formula load factor it can simply be put as the average number of elements that you are storing per linked list again the formula still remains the same you just go through linked spelling please count the number of elements that you have take the average of it you get chain dashing right so if alpha is a load factor in case of your chained hash table the time to resolve conflict would be a bigger one plus alpha right one to add the new node and alpha which is to traverse the list and resolve the conflict so your overall time to resolve conflict uh overall time to resolve a collision with chain dashing would be order of one plus alpha and it's very easy like because that's very straightforward to look at but with open addressing things become a little complicated because with open addressing what you are doing is when we are trying to do any theoretical analysis on open addressing your your collision interfere with each other because your collisions on one slot will interfere with the probe that begins on another for example if i have three keys k1 k2 k3 say k1 and k2 gets hash to index 0 right so i inserted k1 it went to index 0 then i inserted k2 k2 also hashed at sorry k1 has it index 2 i placed k1 at index 2 i took k2 and hashed it it also split out 2 so i'm trying to put k2 in the slot 2 but that is occupied by k1 so i'll find the next available slot i found slot 3 to be available so i place my k2 at 3 but now my k3 came in that got hashed at 3 right but k but the slot 3 is occupied by k2 so then k3 would have to move forward right so here you see with this kind of situation where your collisions interfere with other places it's hard to do a theoretical analysis but still researchers have done a fairly great job at doing that but just that would not dive into the theoretical analysis of that part but rather stick to more to more of a practical approach that would help us build our own hash table right so two ways to do two ways or two common ways to handle uh collisions with open addressing first is linear probing and second is quadratic hashing so now if i talk about it that how would the number of probes the number of probes is equal to upon the collision how many times would i have to move forward to find the next slot if i plot that against your hits and misses for uh linear probing and open and basically double hashing what we see is an exponential rise in the number of probes required for us to find an empty slot or to do a key lookup right so as your table gets filled the number of collisions or the number of probes not quite the number of probes that you would have to make in order to find an empty slot would be exponentially increasing right linear probing performs very poorly as compared to double hashing because with linear probing you have a problem of clustered collisions and you will do far more lookups because of a poor hash function but with double hashing you are using two hash functions so its performance is a little better than what linear probing would give you right but in both the cases we are just thinking about your imp it also depends on the implementation but the worst case is you would do a entire table scan so it is upper bounded when there are no infinite loops uh in your probing function your upper bound is limited by the number of slots of your array so that is still the worst case but if you stuck in an infinite loop because of your probing technique because of your hash function then it might go for a big toss right but in any case as your load factor increases what would happen the number of probes required for you to do a key lookup or find an empty slot to insert or find a key to delete it would increase exponentially right okay so now given this which one is the best strategy then because when we talk about performance there has to be performance metrics that we are taking a look at to find out hey given load factors given the way we are storing the data which strategy should i be picking right so depending on the probing strategy here one thing that we all should remember is the cost of a single probe it matters a lot right when i see a single pro so for example what require like how am i going from one place with open addressing what you are doing if one slot is filled you are finding the next slot right how costly it is for you to find the next slot right that is very important so let's take a look so first probing is costly with chained hashing for two reasons first you have to do linear traversal of a linked list right so for example if you are doing a key lookup you went to a particular index if that is filled you go to and then you start iterating through the link list one by one to find to eventually find your particular key so this is very costly for a chained hashing because each probe requires you to do a random access of your memory because your node can be allocated anywhere in the heap you are doing random access and which is definitely not cache friendly because when i say cache friendly it's cpu cache friendly right so when you're reading a particular page in memory you're bringing in much more you are bringing in a contagious location of memory in your cpu cache so if your slots or if your linked list nodes are placed together then it would be much efficient for your cpu because it would not have to go to ram to access it but rather from cpu cache itself it can access it so probing is costly with chain dashing purely because of linked list nodes can be allocated anywhere in the heap so you are doing a random lookup in your memory and random jumps in your memory that would slow down the performance right because you are not using cpu cache well right with double hashing it requires us to evaluate two hash functions so double hashing you might think hey then double hashing is is that better double hashing seems very better but two major bloopers there first is it is very cpu intensive and time consuming evaluating a hash function takes time takes cpu cycles to do it and it's very intensive right so that is why you are sacrificing on the time a bit to say hey at least my table would be very well used and i would see very few clustered collisions right but it suffers from another problem that double hashing is also not cache friendly because with double hashing what you are doing is in the entire hashtable you are jumping from one slot to another slot that might be really far away again this is not catch friendly its but again same thing your blocks are fetched together in and placed in your cpu cache when you are accessing an array the next because you are jumping within the hash table by a long and by taking longer leaps the next index that you might access in your array might not be present in the cpu cache requiring you to go to ram get the data and access it so also not cache friendly now here this is how you should be thinking about it right where which strategy would you pick to implement hashtable you have to understand the pros and the cons literally at the operating system level in order for you to make the best decision call right so which one is the optimal strategy then it depends on the use case just like system design just like most of the things in real world everything is a trade-off right so depending on the use case that you have the optimal there is no one optimal strategy to do it right it depends on how your application is structured how your application is going to exploit the data how your application is going to access the hash table it depends on that which strategy would you choose but you should be aware of the pros and the cons of each of the strategy and how you are and and and see which one fits your use case best so it's very so it's better that if you if you practice or if you have a particular use case in mind that you are trying your hash table for apply all sorts of probing techniques and tune it benchmark it against the load and see which one performs the best right and that is how you should be thinking about it now comes the question how do compare and benchmark so if you are figuring out a hash table for your use case or tuning or hash table for the best performance out there or you are building your own hashtable it is important to understand how good you did right how good your implementation is or how good your tuning is in either case there is one thing that you would measure which is lookup time lookup time as a function of load so how long does it take for you to do a key lookup as the load on your hash table increases right this is what is the ultimate is the ultimate metric that you would be measuring when you are ever benchmarking right so for your strategy if you are implementing around hashtable your own strategy for your strategy see how your hashtable lookups change when your hash table load increases so lookup time versus load right measure that if you're benchmarking existing strategy or tuning parameters to do it this is the ultimate metric that you have to measure and monitor so how do you do that a very simple if i take about a very simple uh use case or a very simple generic use case to benchmark it is simple create a hash table of size 1024 insert n elements varying from 32 to 900 and look up 1000 random keys with a high miss ratio if you do this over and over a million or a billion time you would see how your lookup time changes with load and it's a very simple way and this is something that i found in couple of uh research papers which is why and and most people i found this exact same example so i'm just taking it as is i still don't know reason why one zero two four thirty to nine hundred i might find it when i start implementing my own hashtable but this is how you should be thinking about it right so a very simple benchmarking strategy would give you how good your hashtable implementation is for your particular use case but again it depends on the use case on how how big your hashtable is going to be but do this it's very simple to benchmark and find the best performance for a hashtable fine so few things that we would see when we are measuring lookups versus load first is performance for open addressing technique degrades as alpha tends to one right so if you are implementing an open addressing strategy for your hashtable con hashtable collision resolution you would see that as alpha tends to one your performance start to degrade because alpha tends to one implies the number of elements is almost equal to the number of slots of your array so you would do a lot of lookups in order to find your keyer in order to find your slot you need to find an empty slot you do a lot of traversal within the array so your performance for your open addressing will start to degrade as alpha tends to one second your chained approach will degrade gracefully because you are the chained approach with chained hashing you are always adding it to an auxiliary data structure so your performance would not go would not degrade drastically because you still have enough space to you have almost infinite not really in fact you almost have a good amount of auxiliary space available for you to put it you are not bounded by the slots of the hash table so performance would not degrade it would be great because the lookups would increase the time would increase but would degrade gracefully as compared to open addressing third linear probing would be slower than double hashing it is always true linear probing would be slower than double hashing because double hashing makes you jump here and there it does not have large number of clustered collisions linear probing sees clustered collision so your lookup time increases when you are having large number of collisions for a slot for linear probing so linear probing would be slower than double hashing in almost all the cases because of clustered collisions and fourth probes of double hashing would be shorter right so with double hashing because you are almost leveraging the entire hash table slots that you have the number of probes per collision would be much shorter as compared to linear probing right very clear so depending on the strategy that you are using this is what you should be observe this is what you should be observing so now with this whenever you are tuning your hash table whenever so you can keep those things in mind so that you don't do some random benchmarks to pick the best one for your implementation right now you with this you might see you might think chain hashing looks much better than open addressing doesn't it right because but it has like it gives you oh it gives you auxiliary data structure to store the uh keys and whatnot like you it the performance degrades uh gracefully open addressing uh is limited by the number of slots of your hash table so looks like chain hashing is a very good approach but it is not cash friendly right so if you are looking at a very high performance hash table chain hashing might not be the best way to do so because chain hashing is not cache friendly because again you are jumping here and there in the in the memory because a linked list note can be allocated anywhere so it is not cpu cache friendly you have to do always a memory lookup right but when the tables are short we don't see an impact right but when tables go large and the number of collisions increase your chain hashing would start to suffer right again everything has advantage this advantages but you have to be aware which one are you picking and why right okay one very interesting optimization as part of our conclusion the interesting optimization is how do you get we up until if you think about it we spoke about so much about uh chain hashing not cash friendly chained hashing out cash friendly there is a very interesting hack that makes it cash friendly and that is how about we leverage cash for chained hashing and when i say cash it's about cpu cash how about instead of allocating one new linked list node every time we allocate it in chunk for example imagine it is not it is a linked list of arrays right think about it it's a linked list of arrays so upon a collision so you have a hash table array for each slot of a hash table you are creating linked list of arrays so that array might be of size let's say uh 16 right and in 16 you might say you might fit in two nodes of your linked list right so you are instead of allocating the memory of a memory equivalent to one node of a linked list you're allocating memory of three or four or five nodes of a linked list so that is your common allocation so now what would happen because you are allocating that large chunk of memory in one shot the the elements that you are putting in in that array or in that in that allocation would be closer to each other so when you are accessing them there is a very high chance that the entire slot will be cached on the cpu this way when you are traversing through the linked list you are still iterating through the array right because they are contiguously allocated given that you can leverage cpu cache and that is such a beautiful optimization yeah hacks always exist and this is one of the most brilliant hacks that i have seen to get to make chained hashing which is supposedly very poor on cpu cache make it cash friendly such a beautiful technique and i loved it when i was going through that paper i'm like oh god this is such an interesting optimization right so just to uh just to just just to just it out uh instead of allocating memory for one node of a linked list do it for multiple nodes in one malloc call when you do that the entire the three or four linked list nodes that you are trying to fit in there all of them would be allocated together contiguously in ram so when you are accessing one if others would be present in the cpu cache making your chain hashing cache friendly and iterations faster right okay so this is a very interesting optimization on chain hashing to make it cache friendly now if your hash table if your hash table performance matters a lot for your use case if it is very critical for your application always remember experiment with different strategies parameters and algorithms for your workload and see how it performs and pure surely for a small use case for a trivial web application you might think it's an overkill right but think about it if it's very critical for an application for your hash tables to be performed you have to tune you have to experiment you have to try out different strategies parameters and algorithms to get the best because that is the most critical part of your application slash use case right so if it is if your situation is like this benchmark benchmark your strategy and see which one fits your use case really well right so that's one thing that you should take away from this discussion that trade-offs just like in system design hash tables implementation when you have multiple algorithms multiple strategies probing techniques everything is a trade-off and which one would you pick why and where you can only get it from a data driven approach which is where you have to do a performance benchmarking and see how it performs well if you're ever interested in implementing a hash table this would also help you in thinking how you would test if your algorithm or probing technique that you think is new performs well against the existing strategies right so keep those things in mind and let's try to get the best performance out of our hash table implementations nice so yeah that's it that's it about performance of hash tables we understood everything is a trade-off chain hashing open addressing we talked about when one works well when other doesn't how to get how to make things crash friendly and how important cpu level caching is when you're thinking about contiguous memory allocations and all of that stuff i hope that was amusing and fascinating for you so yeah that's it that's it for this video if you guys like this video give this video a thumbs up if you guys like the channel give this channel a sub this was the seventh video in the hashtable internal series and i'll see in the next one thanks [Music] you