[
  {
    "id": 175,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "VCMO2X6EoK0",
    "title": "Implementing Hash Maps with Hash Tables",
    "description": "Maps and Dictionaries are amazing, but how are they implemented?\n\nIn this 11th video of this Hash Table Internals series, we take an in-depth look into the implementation of Maps using Hash Tables, popularly called Hash Maps.\n\nWe will go into implementation details and performance tuning of Maps and see how they are built on top of Tables having Chaining and on Tables having Open Addressing.\n\nOutline:\n\n00:00 Agenda\n02:29 Implementation Introduction\n07:12 Implementing with Hash Tables having Chaining\n12:52 Implementing with Hash Tables having Open Addressing\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/VCMO2X6EoK0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/LHZyixOnHwDDy/giphy.gif",
    "duration": "17:56",
    "view_count": 415,
    "like_count": 15,
    "comment_count": 6,
    "released_at": "2022-08-03",
    "gist": "A map, when implemented with Hash Tables, is called a Hash Map; and just like a regular map it supports operations like put, get, and iterate.\n\n## Key Implementation Details\n\nHash Maps are required to store the application keys of any type, but the Hash Table understands only integers. We first pass the application keys through a hash function to map it to a 32-bit integer, and then the usual Hash Table operation takes over.\n\nGiven that the application key to hash key is a frequent operation, we would keep it handy by storing it alongside the application key. This helps us save repeated computations.\n\nWhile looking up a key in the hash map, we first reach the slot, and then compare if the key present there is really the key we are looking for, as we cannot just rely on the equality of the hash keys. The Hash Map, hence, accepts a key comparator function.\n\n## With Chained Hash Tables\n\nEach node of the linked list in the chained hash table has the following structure\n\n```\nstruct node {\n    int32 hash_key;\n    void *key;\n    void *value;\n\u00a0\u00a0\u00a0 struct node *next;\n}\n```\n\n`void * key` and `void * value` allows us to hold a key and a value of any type, while `int32 hash_key` enables us to hold a pre-computed hash.\n\nAt the Hash Table level, we hold\n\n- the array of linked list\n- size of the array\n- number of keys\n- key comparator function\n\nInstead of having a `contains` function, we have a `lookup` function that returns the value for the matching key, and `NULL` if the key does not exist. Thus, the `lookup` function doubles as a `contains` function.\n\nWhen duplicate keys are inserted, we can do one of the following\n\n- do not insert at all\n- delete the old key and re-insert it again, bringing it to the head of the list, making it quicker to reach\n\n## With Hash Tables having open addressing\n\nEach slot of the hash table has the following structure\n\n```\nstruct node {\n    int32 hash_key;\n    void *key;\n    void *value;\n    bool is_empty;\n\u00a0\u00a0\u00a0 bool is_deleted;\n}\n```\n\n`void *key` and `void *value` allows us to hold a key and a value of any type, while `int32 hash_key` enables us to hold a pre-computed hash, saving a lot of runtime computations. `is_empty` tells us if the slot is empty, while `is_deleted` represents a soft deleted slot.\n\nAt the Hash Table level, we hold\n\n- the array of linked list\n- size of the array\n- number of active keys\n- number of used slots\n- key comparator function\n\nHere the load factor will be computed as number of used slots/size of the array because the soft deleted keys also affect the performance of the hash table.\n\nDuring insert, lookup, and delete when we find the matching hash, and explicitly compare the application keys, as multiple keys can hash to the same location, and we cannot rely on just hash key equivalence.",
    "notes_gd": "https://drive.google.com/file/d/1aIOet8RVov3l96RiKk3xN02p4ZQe-M3_/view?usp=sharing",
    "slug": "implementing-hash-maps-with-hash-tables"
  },
  {
    "id": 174,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "CcoMvgIdrD8",
    "title": "Implementing Hash Sets with Hash Tables",
    "description": "Sets are amazing, but how are they implemented?\n\nIn this 10th video of this Hash Table Internals series, we take an in-depth look into the implementation of Sets using Hash Tables, popularly called Hash Sets.\n\nWe will go into implementation details and performance tuning of Sets and see how they are built on top of Tables having Chaining and on Tables having Open Addressing.\n\nOutline:\n\n00:00 Agenda\n02:26 Introduction to Hash Sets\n05:10 Key Implementation Details\n09:01 Implementing Sets with Chained Hash Tables\n11:03 Implementing Sets with Hash Tables with Open Addressing\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/CcoMvgIdrD8/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/lizSDX8mHfbstV0GKw/giphy.gif",
    "duration": "14:34",
    "view_count": 250,
    "like_count": 6,
    "comment_count": 0,
    "released_at": "2022-08-01",
    "gist": "The set data structure, when implemented with Hash Tables, is called a Hash Set; and just like a regular set, it supports operations like Union, Intersection, and Difference.\n\n## Key Implementation Details\n\nHash sets are required to store the application keys that could be strings, integers, objects, etc., but the Hash Table understands only integers. We first pass the application keys through a hash function to map it to a 32-bit integer, and then the usual Hash Table operation takes over.\n\nGiven that the application key to hash key is a frequent operation, we would keep it handy by storing it alongside the application key. This helps us save repeated computations.\n\nIt is quite possible that multiple application keys would map to the same hash key, hence while looking up a key in the hash set, we first reach the slot, and then compare if the key present there is really the key we are looking for as we cannot just rely on the equality of the hash keys.\n \nTo achieve this, our Hash Set would need to accept a comparator function that can be used to compare two application keys for equality. Hash Set would use this comparator to check for the key equality, making our implementation agnostic.\n\n## Hash Sets with Chained Hash Tables\n\nEach node of the linked list in the chained hash table would have the following structure\n\n```\nstruct node {\n    int32 hash_key;\n    void *key;\n    struct node *next;\n}\n```\n\n`void * key` would allow us to hold a key of any type, while `int32 hash_key` would enable us to hold a pre-computed hash, saving a lot of runtime computations.\n\nAt the Hash Table level, we would hold\n\n- the array of linked list\n- the size of the array\n- number of keys\n- key comparator function\n\n## Hash Sets with Hash Tables having open addressing\n\nEach slot of the hash table will have the following structure\n\n```\nstruct node {\n    int32 hash_key;\n    void *key;\n    bool is_empty;\n    bool is_deleted;\n}\n```\n\n`void * key` would allow us to hold a key of any type, while `int32 hash_key` would enable us to hold a pre-computed hash, saving a lot of runtime computations. `is_empty` would tell us if the slot is empty, while `is_deleted` would represent a soft deleted slot.\n\nAt the Hash Table level, we would hold\n\n- the array of linked list\n- the size of the array\n- number of active keys\n- number of used slots\n- key comparator function\n\nNote: here the load factor will be computed as the number of used slots/size of the array because the soft deleted keys also affect the performance of the hash table.\n\nDuring insert, lookup, and delete when we find the matching hash, we need to explicitly compare the application key because multiple application keys can hash to the same location, and hence we cannot rely on just hash key equivalence.",
    "notes_gd": "https://drive.google.com/file/d/1mkT3gt19e6LrG6CJzYaodbbjsLBZVlLY/view?usp=sharing",
    "slug": "implementing-hash-sets-with-hash-tables"
  },
  {
    "id": 173,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "mmPwVBm-8n0",
    "title": "Implementing Resize of a Hash Table",
    "description": "So, the Hash Table needs to be resized in order to maintain consistent performance, but how exactly?\n\nIn this 9th Video of the Hash Table Internals series, we go into the implementation details of resize operation and talk about, where and when in the code should we trigger resize, 2 ways to implement resize while using Chained Hashing, and things to remember while resizing a Hash Table that uses open addressing.\n\nOutline:\n\n00:00 Agenda\n02:28 Introduction to Resize\n03:18 Implementing Resize on Chained Hash Tables\n08:55 Implementing Resize on Hash Tables with Open Addressing\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/mmPwVBm-8n0/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/BpGWitbFZflfSUYuZ9/giphy.gif",
    "duration": "16:40",
    "view_count": 245,
    "like_count": 9,
    "comment_count": 0,
    "released_at": "2022-07-29",
    "gist": "Resizing a Hash Table is important to maintain consistent performance and efficiency, but how do we actually implement it?\n\nResize is all about\n\n- allocating a new array of the desired size\n- insert existing keys in this new array\n- delete the old array\n\nBut a few granular details are specific to the type of hash table.\n\n## Resizing a Chained Hash Table\n\nResizing a table happens when the load factor hits a threshold. To implement an efficient resize, a Hash Table that uses chaining needs to keep track of\n\n- number of keys\n- total number of slots\n\nThis would help us avoid reevaluation, and we can instantly compute the load factor.\n\n### Resize during insert\n\nWhile we are inserting a key in the Hash Table, we keep on checking the load factor. If it breaches the threshold, we trigger the resize.\n\n```\ninsert_key(k, table) {\n    ------\n    LF = count_keys / total_slots;\n    if (LF >= 0.5) {\n        resize(table, total_slots * 2)\n    }\n}\n```\n\n### Shrinking during delete\n\nWhile we delete a key from the Hash Table, we check the load factor. If it breaches the threshold, we trigger the shrink. The pseudocode is fairly similar to the above one.\n\n### Two ways to implement resize\n\nChained Hashing is implemented using Linked List and there are two ways to resize\n\n1. we iterate through the keys and re-insert them into the new array\n2. we iterate through the keys and just adjust the pointers of the nodes, instead of re-allocating the new set of nodes.\n\n## Resizing a Hash Table with Open Addressing\n\nIn open addressing, we always soft delete so that we can reach the elements placed further in the collision chain. To handle this gracefully, we need two counters at the hash table\n\n1. Key Counter: number of active keys in the table\n2. Used Counter: number of used slots in the table\n\nFor open addressing, the load factor will be counted as the used counter divided by the total slots. The deleted keys also affect the performance as we have to go past them looking for the keys.\n\nHence, the Key Counter will increase and decrease upon every insert and delete, while the used counter would increase upon insert but would reduce only when we do a resize.\n\n### Resize during insert\n\nWhile we are inserting a key in the Hash Table, we check the load factor. If it breaches the threshold, we trigger the resize. Resize operation would skip the deleted keys and re-insert only the active keys.\n\n### Shrinking during delete\n\nThe shrinking of the hash table will be triggered when the number of active keys falls beyond the threshold, and hence here our load factor for this operation would be active keys / total_slots.\n\nSimilar to the insert phase, we would skip the deleted keys and re-insert only the active keys in the new array. The key counter and the user counters are adjusted accordingly.",
    "notes_gd": "https://drive.google.com/file/d/1wJWnXlQS4SKJBdmCg799gwb2dL7JSaPv/view?usp=sharing",
    "slug": "implementing-resize-of-a-hash-table"
  },
  {
    "id": 172,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "zt1E0akArqQ",
    "title": "Why are Hash Tables always doubled?",
    "description": "Why are the underlying arrays of the hash tables always a power of 2? When we trigger a resize why are Hash Tables always doubled? like 2, 4, 8, 16, 32? What is the reason behind it?\n\nIn this 8th video of the Hash Table Internals series, we answer some super-cool questions and go in-depth on Hash Table resizing. We understand why we do it, how we do it, why hash tables are always doubled upon resize, why they are always sized as a power of 2, and conclude with an understanding of how and when we shrink them.\n\nOutline:\n\n00:00 Agenda\n02:35 Why resize a Hash Table?\n03:19 When to resize a Hash Table?\n04:20 How to resize a Hash Table?\n07:40 Why are Hash Tables always doubled upon resize?\n17:27 Why are Hash Tables slots always a power of 2?\n23:30 Shrinking a Hash Table\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/zt1E0akArqQ/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/4oeRWjN70cg8MV6HZC/giphy.gif",
    "duration": "29:46",
    "view_count": 757,
    "like_count": 18,
    "comment_count": 3,
    "released_at": "2022-07-27",
    "gist": "To maintain consistent performance, the hash table has to be resized - be it growing or shrinking. The trigger to resize depends on the load factor, which is defined as the ratio of the number of keys that the hash table holds to the total number of slots.\n\n## When should we resize?\n\nThe Hash Table is resized when the load factor hits a certain threshold. If we get too aggressive or too lenient, we would not be able to get the optimal efficiency. Hence, we have to find a sweet spot.\n\nWe typically grow the hash table when the load factor hits 0.5 and shrink when we hit 0.125.\n\n## Why do we always double?\n\nWe have heard and seen so many times, that when a Hash Table is required to grow, we always double the underlying array; but why? Can we not just increase it by 1 every time we are trying to insert?\n\n### Resizing by 1 every time\n\nLet's take an example: say, we grow the array by 1 every time we insert an element in the Hash Table. Let's compute the time it requires to fill `n` elements.\n\nInserting the 1st element is: allocating an array of size 1, and inserting 1 element; so in all O(1) operations.\n\nInserting the 2nd element is: allocating an array of size 2, copying 1 element from the old array, and then inserting the 2nd element; so in all O(2) operations.\n\nHence, inserting the nth element is: allocating an array of size n, copying n-1 elements from the old array, and then inserting the nth element; so in all O(n) operations.\n\nTotal operations to insert `n` elements = 1 + 2 + ... + n = (n(n-1))/2 which is O(n^2).\n\n### Doubling every time\n\nIf we double every time, inserting `n` elements requires O(n) time, as it is spacing out an expensive resize operation. We would be inserting n/2 elements before resizing the array to 2n.\n\nNote: For a detailed amortized analysis, please refer to the video attached here, where I have explained the reasoning in depth.\n\n## Why is a hash table array always a power of 2?\n\nFor a power of 2, the MOD and the bitwise AND spit out the same result and given that the bitwise AND is a magnitude faster than the MOD, we get the best performance out of our Hash Tables when we use AND\n\n```\n(i % 2^k) == (i & (2^k) - 1)\n```\n\nThis is why the length of the underlying array is always a power of 2, making our most-frequent operation efficient.\n\n## Shrinking the Hash Table\n\nTo ensure we are not wasting space, we trigger the shrink when we do not utilize the underlying array enough. While triggering a shrink, we also need to ensure that we are not aggressive enough that we have to grow immediately after the shrink.\n\nHence, we shrink the hash table when the load factor hits 1/8 i.e. in a table of length 64 if we are only holding 8 keys, we trigger a shrink and that reduces the length to 32.\n\nNote: To understand why we do it at load factor = 1/8, please refer to the video.",
    "notes_gd": "https://drive.google.com/file/d/1aHWaOIGT7-zB88T83In05pHIdpuJTSk7/view?usp=sharing",
    "slug": "why-are-hash-tables-always-doubled"
  },
  {
    "id": 171,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "oD2yaTtu69w",
    "title": "Getting the best performance from the Hash Table",
    "description": "In the previous 6 videos, we talked about the internals of Hash Table, ways to implement them, and how to gracefully handle collisions.\n\nIn this video, we put our focus on performance and would look at how to quantify the load on the hash table, the impact of chaining and open addressing on perf, understanding the performance gains and losses for each strategy, and conclude with very interesting performance optimization.\n\nOutline:\n\n00:00 Agenda\n02:28 Load Factor\n03:36 Load Factor for resolution strategies\n09:22 The best strategy for a Hash Table\n14:29 Lookup time vs Load Factor\n20:03 Making chained hashing cache-friendly\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/oD2yaTtu69w/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/3o7qE4opCd6f1NJeuY/giphy.gif",
    "duration": "25:8",
    "view_count": 356,
    "like_count": 12,
    "comment_count": 6,
    "released_at": "2022-07-25",
    "gist": "Hash Tables are designed to give a constant time performance and to do this, it needs to have a large number of slots available. So, which factors decide its performance?\n\n## Load Factor\n\nLoad Factor is a quantification that makes it simple for us to tell how loaded the Hash Table is, and it is just a simple division of the number of keys and the number of slots in the hash table.\n\nAs the load factor increases, the performance of the Hash Table decreases. It happens purely because it takes longer for us to do a slot lookup and find an empty slot to place the key.\n\n## The Best Strategy\n\nEvery probing strategy or collision resolution strategy has its merit and demerit, and they all perform the best in a certain condition and the worse in others. Let's take a detailed look.\n\n### Chained Hashing\n\nChained Hashing is costly, as it requires us to do a linear traversal of the linked list to find the key we are looking for. As the collisions increase, the lookup time shoots up, degrading the performance.\n\nChained Hashing is not cache-friendly, as it requires us to do random lookups in the memory while hopping from one linked list node to another.\n\n### Double Hashing\n\nEvaluating two hash functions requires extra CPU cycles that could get taxing. Double hashing is also not cache-friendly, as it requires us to jump across the Hash Table to hunt an empty slot.\n\nThe optimal strategy is contextual. If the performance of the Hash Table is critical, then we need to experiment, tune, and evaluate the best that fits us.\n\n## Lookup Time vs Load Factor\n\nLookup Time is the most critical metric in evaluating the performance of the Hash Table; when we benchmark Lookup Time vs Load Factor, we would see\n\n- perf of Open Addressing degrades as the load factor increases\n- perf of Chained Hashing degrades gracefully with load factor\n- Linear Probing would be slower than Double Hashing\n- Probes required for Double Hashing would be shorter\n\n## Making Chained Hashing cache efficient\n\nChained Hashing is known for being cache-inefficient, as it requires us to traverse through linked list nodes that may be present across the heap. Can we somehow make it cache efficient?\n\nTo make Chained Hashing cache-friendly, we have to ensure that the nodes of the linked list are allocated contiguously instead of randomly. Hence, instead of allocating one node at a time, we allocate the space for 5 nodes (like an array) at a time and then form the linked list out of them.\n\nThis would make the linked list leverage the CPU cache well and ensure our iterations are efficient as the next nodes will be available in the CPU cache, not requiring us to fetch them from the main memory.",
    "notes_gd": "https://drive.google.com/file/d/1maLTNs7gFIu6zCQvX-v-_BKQy_F5Tgqv/view?usp=sharing",
    "slug": "getting-the-best-performance-from-the-hash-table"
  },
  {
    "id": 170,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "wV4K6fo0T58",
    "title": "Double Hashing for Conflict Resolution in Hash Tables",
    "description": "In previous videos, we talked about Linear Probing, and how good it is, but we learned that it suffers from clustered collisions; then we spoke about quadratic probing and saw how it addresses the issue of clustered collisions, but can we do better?\n\nIn this 6th video of the Hash Table Internals series, we talk about the final technique of conflict resolution called Double Hashing; understand how it addresses a big concern with clustered collisions, learn about a few things that would help us choose a good hash function, and conclude by looking at the advantages of using double hashing as a probing technique.\n\nOutline:\n\n00:00 Agenda\n02:40 Introduction\n06:23 Double Hashing\n10:17 Choosing the second hash function\n13:37 Advantages of Double Hashing\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/wV4K6fo0T58/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/crBFPTqW4U2YM/giphy.gif",
    "duration": "16:44",
    "view_count": 283,
    "like_count": 12,
    "comment_count": 2,
    "released_at": "2022-07-22",
    "gist": "Linear and Quadratic probing do a great job at handling collisions in a Hash Table, but they both suffer from Clustered Collision, which degrades performance. So, can we do better?\n\nDouble hashing is a technique that minimizes the problem of clustered collisions by using a secondary hash function to find the next available slot.\n\n## Double Hashing\n\nDouble hashing is an Open Addressing technique to address collisions in a hash table; hence, instead of using an auxiliary data structure to hold the collided keys, it leverages the already available free slots.\n\nThe probing function for Double Hashing is defined as\n\n```\np(k, i) = (h1(k) + i * h2(k)) mod m\n```\n\nThus, with every attempt we make to find an empty slot, we can leap by any distance in any direction, making all slots equally likely. A sample sequence for a particular key `k1` could be\n\n- primary slot: `h1(k1)`, if that is occupied then\n- attempt 1: `h1(k1) + h2(k1)`, if that is occupied then\n- attempt 2: `h1(k1) + 2 * h2(k1)`, if that is occupied then\n- attempt 3: `h1(k1) + 3 * h2(k1)`, and so on\n\nLinear probing and quadratic traversals take a predictable leap to hunt for an empty slot, while double hashing probing leaps depend on the key and hence reduce the chances of clustering. So, different keys will have different leaps.\n\n## Choosing a second hash function\n\nThe second hash function is super-critical, as it is aimed at resolving collisions effectively while ensuring minimal clustering. The second hash function should\n\n1. never return 0\n2. cycle through the entire table (with no particular order)\n3. be fast to compute and almost feel like a random number generator\n\n## Advantages of Double Hashing\n\n1. Uniform spread upon collision\n2. follows no specific offset pattern, the key purely depends on\n3. least prone to the clustering problem\n\n## Disadvantage of Double Hashing\n\nDouble hashing is not cache-friendly, as it requires us to hop across the hash table to hunt an empty slot. We may be at one extreme of the table and then move to the other one.",
    "notes_gd": "https://drive.google.com/file/d/1EbFM7ZleP4Gwsq14moI7kbQMXJSEdj5D/view?usp=sharing",
    "slug": "double-hashing-for-conflict-resolution-in-hash-tables"
  },
  {
    "id": 169,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "F-8pWiJv8ik",
    "title": "Quadratic Probing for Conflict Resolution in Hash Tables",
    "description": "In the previous video, we looked at Linear Probing as a way to handle Hash Table collisions, but is that the only way? or Can we do better? In this video, we take a look at Quadratic Probing as an alternative to Linear Probing.\n\nWe spend time understanding what Quadratic Probing is, and how it is better than the Linear counterpart.\n\nOutline:\n\n00:00 Agenda\n02:22 Introduction\n03:52 Challenges with Linear Probing\n05:44 Quadratic Probing\n07:56 How Quadratic is better than Linear\n08:56 Properties of Quadratic Probing\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/F-8pWiJv8ik/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/xUNd9ETOt1cwz90hva/giphy.gif",
    "duration": "14:35",
    "view_count": 247,
    "like_count": 16,
    "comment_count": 0,
    "released_at": "2022-07-20",
    "gist": "Linear probing is a popular way to handle Hash Table collisions, but is that the only way? Definitely not.\n\n## Challenges with linear Probing\n\nLinear Probing suffers from one significant challenge, and that is clustered collision. With a poor hash function, it is very much possible that some slots are preferred more over others, and upon collision, they are placed next to each other.\n\nHence, we would see clusters of collided keys forming across the entire hash table, and this is called Clustered Collision. Linear Probing suffers from this the most.\n\n## Quadratic Probing\n\nQuadratic Probing determines the next slots as per an arbitrary probing function\n\n```\np(k, i) = h(k) + c1 * i + c2 * i^2\n```\n\nFor attempt 0, we get the primary slot `h(k)` and post that for each attempt we move quadratically through the hash table to hunt for the next available slot.\n\nBecause of the quadratic jump, we do not form clustered collisions, instead, we cover a good chunk of the hash table. A simple quadratic sequence would be\n\n```\nh(k) + 1\nh(k) + 4\nh(k) + 9\nh(k) + 16\n.\n.\n```\n\nWhere `h(k)` is the primary slot we got by passing the key through the hash function and 1, 4, 9, and 16 are the quadratic offsets.\n\n## Properties of Quadratic Probing\n\n### Reducing clustered collisions\n\nQuadratic Probing reduces the clustered collisions by distributing collided slots quadratically across the hash table and utilizing the entire hash table space.\n\n### Good Locality of Reference\n\nQuadratic Probing has a good locality of reference. When we access a particular slot of the hash table, we are also bringing in neighboring slots to the CPU cache. Upon collisions, when we access a few next quadratic slots, we need not fetch it from the RAM.\n\nThe locality of reference is not as high as Linear Probing, but it is decent enough when we observe fewer collisions per slot.",
    "notes_gd": "https://drive.google.com/file/d/1UAB8YOoRQwygQwILlpTTR7Wfgns_tHke/view?usp=sharing",
    "slug": "quadratic-probing-for-conflict-resolution-in-hash-tables"
  },
  {
    "id": 168,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "5QKAXG25hig",
    "title": "Linear Probing for Conflict Resolution in Hash Tables",
    "description": "Linear Probing is one of the simplest and the most intuitive ways to handle Hash Table collisions, and it is based on a technique called Open Addressing.\n\nIn the video, the fourth of this Hash Table Internals series, we take a detailed look into Linear Probing, understand what it is, how Hash Table operations happen with Linear Probing, learn why it is so simple yet so efficient, and conclude with looking at 2 challenges that come with adopting it.\n\nOutline:\n\n00:00 Agenda\n02:28 Introduction\n03:50 Linear Probing\n05:56 Hash Table Operations with Linear Probing\n09:34 Why is Linear Probing fast and efficient?\n03:47 Challenges with Linear Probing\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/5QKAXG25hig/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/LrQlr8NQig48u8utK6/giphy.gif",
    "duration": "19:23",
    "view_count": 373,
    "like_count": 19,
    "comment_count": 0,
    "released_at": "2022-07-18",
    "gist": "Linear probing is the simplest and one of the most efficient ways to handle conflicts in Hash Tables, let's understand it in-depth.\n\n## Conflicts\n\nConflicts are inevitable, and Open Addressing is a technique to handle them in a space-efficient way. Instead of using an auxiliary data structure to hold the collided keys, open addressing leverages the free slots of the Hash Table to accommodate the collided keys.\n\nTo find the next available slot, open addressing defines a probing function that uses the key and the attempt to deterministically iterate through the slots.\n\n## Linear Probing\n\nWe first hash the key and find a primary slot. If that slot is free, we place the key there. If the slot is occupied, we check the slot to its right. We continue to process until we find an empty slot.\n\nThis way, we continue to hunt for the key linearly from the primary slot. Once we reach the end of the array, we circle back to index 0. Formally, the probing function for Linear Probing would be\n\n```\np(k, i) = (h(k) + i) mod m\n```\n\n## Hash Table Operations\n\n### Adding a key\n\nWe first find the primary slot of the key using the hash function. If the slot is empty, we place the key there. If not, we move to the right and find the first empty slot and place the key there.\n\n### Key lookup\n\nLookup is an iterative process where we first find the primary slot of the key, if the key is present at that slot then well and good. If not, we move to the right hunting for the key. We continue to linearly iterate through the array until\n\n- the key is found, or\n- we encounter an empty slot\n- we cover all `m` slots of the hash table\n\n### Deleting a key\n\nDeleting a key in Linear Probing is always a soft delete. We first look up the key in the hash table and once we find it, we mark that slot as deleted but never physically empty it. This allows us to go beyond the deleted slot and hunt for any other collided keys.\n\n## Why is Linear Probing Fast?\n\nLinear probing is fast because it beautifully exploits the locality of reference. To access a certain slot in the hash table, we fetch the page in the CPU cache. The page will not just contain the requested slot, but it also contains the neighboring slots as well.\n\nHence, upon collision when we iterate from that slot, we would not need to fetch the slots from RAM, instead, some slots would already be present in the cache making iterations superfast.\n\nIn an average case, Linear Probing gives constant time performance for adding, lookup, and deleting a key.\n\n## Challenges with Linear Probing\n\n1. Bad hash function would lead to many collisions\n2. It suffers from non-uniform clustered collisions\n\nHence, it is important to pick a good hash function, like Murmur Hash, to ensure a near-uniform distribution of keys and fewer collisions.",
    "notes_gd": "https://drive.google.com/file/d/1hqt-b4fQEXwFEgiEUVIUe5qoynpYXDb0/view?usp=sharing",
    "slug": "linear-probing-for-conflict-resolution-in-hash-tables"
  },
  {
    "id": 167,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "6_yFb7icd_c",
    "title": "Conflict Resolution in Hash Tables with Open Addressing",
    "description": "Although chaining is a popular way of handling Hash Table Collisions, there is a very interesting way of achieving the same and it is called Open Addressing. The key highlight of Open Addressing is that it does not require any additional data structure to hold the collided keys, making them space efficient.\n\nIn this video, we look at Open Addressing, the core idea behind it, lay the foundation for probing functions and understand how the implementation of our core Hash Table functions changes with this scheme in place.\n\nOutline:\n\n00:00 Agenda\n02:35 Introduction\n03:06 Open Addressing\n04:15 Probing Functions\n08:45 Hash Table Operations\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/6_yFb7icd_c/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/Q2gnin2fe9c2uLIA5L/giphy.gif",
    "duration": "18:6",
    "view_count": 488,
    "like_count": 27,
    "comment_count": 5,
    "released_at": "2022-07-15",
    "gist": "Open Addressing is a common yet interesting way of handling collisions in Hash Tables and instead of using an auxiliary data structure, it leverages empty slots of the hash table to store the collided keys, making the entire approach space efficient.\n\nWhile inserting a key in the hash table, we pass it through the hash function to get the slot, and if the slot is already occupied, we find the next available slot through probing.\n\n## Probing\n\nProbing is the process through which we deterministically find the next available slot in the hash table. The approach and algorithm could vary, but formally it is a function of the key to be placed and the attempt, that spits out a slot.\n\n```\nj = p(k, i)\n```\n\nThe probing function uses the key `k` and an attempt `i` to spit out an index `j`. The attempt `i` and the index `j` are in the range `[0, m-1]`, where `m` is the size of the hash table.\n\n### Good probing function\n\nA good probing function would generate a deterministic permutation of numbers `[0, m - 1]` specific to the key `k`, so that we eventually check and cover the entire hash table for an available slot.\n\nFor example: for some key `k1`, on a hash table of size 8, the probing function might generate a sequence: 5, 7, 2, 1, 0, 6, 4, and 3. This means we first try to put the key in index `5`, if that is occupied we check `7`, then `2`, and so on.\n\n## Hash Table Operations\n\n### Adding a key\n\nUntil we find a free slot in the hash table, we keep invoking the probing function with attempts `0`, `1`, `2,`, etc. Once we find an empty slot, we stop and place the key in that slot.\n\n### Key Lookups\n\nFor looking up a key, we invoke the probing function with attempt `0` and check the slot. If the slot holds the key we need, we stop and return the value. If not, we continue to probe with attempts `1`, `2`, etc., and continue to hunt.\n\nWe stop the iteration when\n\n- we find the key,\n- we stumble upon an empty slot during iteration\n- we have checked the complete hash table\n\n### Deleting a key\n\nWith open addressing, deleting a key from the table has to be a soft delete, because we need a way to differentiate between an empty slot and a deleted key.\n\nIf during deletion we empty the slot, then we would be unable to look and reach for a key that had a collision and was placed after the key we deleted.\n\n## Limitation of Open addressing\n\nSince we are not having any auxiliary data structure, a major limitation of Open Addressing is that the maximum number of keys we can hold are the same as the number of slots in the hash table.",
    "notes_gd": "https://drive.google.com/file/d/1bvdtGMKVou-bfuOHzX3izdx2FHfQTpYS/view?usp=sharing",
    "slug": "conflict-resolution-in-hash-tables-with-open-addressing"
  },
  {
    "id": 166,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "9rb8oILi4lU",
    "title": "Conflict Resolution in Hash Tables with Chaining",
    "description": "Collisions happen in Hash Tables as we are trying to map a huge space of application keys in a small array. But there are ways to solve it and one of the most common ways is called Chaining.\n\nIn this video, we go in-depth about collisions in Hash Tables, resolve them through chaining, explore some really granular details that would help us squeeze the best performance out of our implementation, and most importantly look at a different data structure to implement it.\n\nOutline:\n\n00:00 Agenda\n02:29 Introduction\n03:42 Chaining\n04:58 Chaining with Linked List\n08:03 Adding a key in the Hash Table\n14:04 Deleting a key from the Hash Table\n15:47 Lookup a key in the Hash Table\n17:15 Chaining with Binary Search Trees\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/9rb8oILi4lU/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/vciXulg7fZGvo9p01m/giphy.gif",
    "duration": "20:55",
    "view_count": 745,
    "like_count": 42,
    "comment_count": 8,
    "released_at": "2022-07-13",
    "gist": "Collisions are inevitable in Hash Tables, and a common way of handling them is through chaining using Linked List. But can we use some other data structure?\n\nCollisions are inevitable in Hash Tables, as we are mapping a large range of application keys to a smaller range of array slots. So, there are chances that the hash function spits out the same value for two different application keys.\n\nBecause Hash Tables cannot be lossy, we need a way to handle these collisions and allow storing of multiple keys that are hashed to the same slot. A way to achieve this is chaining, and it is very commonly implemented through a Linked List.\n\n## Data Structures\n\nTo accommodate this, the Hash Tables are now implemented as an array of Linked List where all the keys that collide are placed.\n\n### Adding a Key\n\nWhile adding a key to the hash table, we first pass it through the hash function and get the slot index. We then create a node holding the key and add it to the linked list.\n\nWe can add the new node at one of the 3 places\n\n- always at the head of the list - O(1)\n- always at the tail of the list - O(1)\n- in the middle of the list while maintaining a sort order - O(p)\n\n### Deleting a Key\n\nTo delete a key, we first pass it through the hash function and get the slot index. We then iterate through the linked list present at the slot, element by element, and locate our key of interest.\n\nWhile iterating, we keep track of the pointer to the previous node so that once we reach the node to be deleted, we adjust the pointers and free up the node.\n\n### Key Lookup\n\nKey lookups are similar to delete operations. We first pass the key through the hash function to get the slot index. We then iterate through the list present at the slot and locate our key. The operation requires us to iterate the list iteratively.\n\n## Other Data structures for chaining\n\nLinked List is not the only data structure that we have to use to chain the collided keys. Depending on the use case, access pattern, and constraint, we can pick a data structure that suits us.\n\nFor example, if our array is small, and we cannot resize it, then we may end up having many collisions. If we are trying to read, then iterating over this list will reduce the throughput as it is a linear scan.\n\nTo optimally perform a key lookup, when the collisions are high, we can use a self-balancing search tree, like BST or Red-Black. This way, we get an optimal lookup performance on keys hashed to the same slot.",
    "notes_gd": "https://drive.google.com/file/d/1so447uSDzZZuJXAyMB_jGqHCM0RX3hbL/view?usp=sharing",
    "slug": "conflict-resolution-in-hash-tables-with-chaining"
  },
  {
    "id": 165,
    "topic": {
      "id": 0,
      "uid": "hash-table-internals",
      "name": "Hash Table Internals",
      "one_liner": null,
      "youtube_playlist_id": "PLsdq-3Z1EPT2UnueESBLReaVSLIo_BuAc",
      "bgcolor": "#FBE0E0",
      "themecolor": "#FF4B42"
    },
    "yt_video_id": "jjW8w8ED3Ns",
    "title": "Internal Structure of a Hash Table",
    "description": "One of the most common data structures that we all use every single day is Hash Table. Every language has its own implementation and nomenclature for it. Python calls it dictionary, while Java calls it Hash Map. But the core idea remains the same: it holds pairs of keys and values and supports insertions, updation, and lookups in constant time.\n\nBut how are they implemented? What is its internal structure? In this video, we talk about what are hash tables, how are they structured internally, and lay a foundation to understand their constant-time implementation.\n\nOutline:\n\n00:00 Agenda\n02:38 Introduction and Applications of Hash Tables\n05:12 Core ideas to construct Hash Tables\n07:07 Step 1: Application keys to Integer Hash Keys\n09:38 Naive Implementation of Hash Table using Array\n13:38 Step 2: Integer Hash Keys to a smaller range\n17:43 Adding more keys on the fly\n19:07 Do we really need the Keys to Hash Key step?\n\nLearn Hash Tables Internals in a structured way at https://courses.arpitbhayani.me/hash-table-internals",
    "img": "https://i.ytimg.com/vi/jjW8w8ED3Ns/mqdefault.jpg",
    "gif": "https://media.giphy.com/media/8OYw2Tm8gnrGw/giphy.gif",
    "duration": "23:12",
    "view_count": 4211,
    "like_count": 145,
    "comment_count": 11,
    "released_at": "2022-07-11",
    "gist": "Hash Tables are implemented through simple arrays, but how?\n\nHash Tables are so powerful, that OOP-based languages internally use them to power Classes and site members. Symbol tables that hold the variables mapped to a memory location are also powered through hash tables.\n\nThey are designed to provide constant-time key-based insertion, update, and lookups while being space efficient at all times.\n\n## Core Ideas to construct Hash Tables\n\n- convert application keys to wide-ranged (`INT32`) hash keys\n- convert hash keys to a smaller range\n\n## Application Keys to Hash Keys\n\nHash Tables should support storing an object as a key and to power that the keys are first hashed to a big integer range (provided by the user) typically `INT32`. This hash key is then further used to decide how and where the KV pair would be stored in the data structure.\n\n## Naive Implementation\n\nA naive implementation of a Hash Table would be to create an array of length INT32. To store the KV in it, we pass the key through the hash function, spitting out an integer. We use this key and store the KV pair at this index in the array.\n\nAlthough this would give us constant time insertion, update, and lookups, it is highly space in-efficient, as we would need to allocate at least `4` * `INT32` = `16GB` of ram to just hold this array, with most of the slots left empty.\n\n### Hash Keys to Smaller Range\n\nThis step is designed and introduced to make our Hash Table space efficient. Instead of having a huge array of length `INT32`, we keep it proportional to the number of keys inserted. For example, if we inserted 4 keys, then our holding array could be around 8 slots big.\n\nTo achieve this, we map the hash key into a small range (same as the length of the array) and place our key at that very index. This allows us to remain space-efficient while sporting fast and efficient insertions, updates, and lookups.\n\n## Adding more keys\n\nThe small limited-size array will not be able to hold numerous keys and hence after a certain stage we would need a larger array to hold the data. This is done by resizing the holding array and is typically made 2x every time it is full enough.\n\nThus, this two-step implementation allows for near-constant time insertions, updates, and lookups while remaining space efficient.",
    "notes_gd": "https://drive.google.com/file/d/1IKYDzO-mZEHDQYEsyoksFkF3DVyTCy6l/view?usp=sharing",
    "slug": "internal-structure-of-a-hash-table"
  }
]