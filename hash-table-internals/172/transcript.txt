so why are the underlying arrays of the hash table always a power of two when we trigger a resize why are hash table always double like 2 4 8 16 32 what is the reason behind it in this eighth video of the hash table internal series we answer some super cool questions and go in depth on hash table resizing we understand why we do it how we do it why hash tables are always doubled up on resize why are they always sized as a power of 2 and conclude with understanding how and when we shrink them but before we move forward i'd like to talk to you about a course on system design that i have been running for over a year now the course is a cohort based course which means i won't be rambling a solution and it will not be a monologue instead a small focused group of 50 60 engineers every cohort will be brainstorming systems and designing it together this way we build a solid system and learn from each other's experiences the course to date is enrolled by 600 plus engineers spanning nine cohorts and ten countries engineers from companies like google microsoft github slack facebook tesla yelp flipkart dream 11 and many many many more have taken this course and have some wonderful things to say the coolest part about the course is the depth we go into and the breath we cover we cover topics ranging from real-time text communication for slack to designing our own toy load balancer to greek buses live text commentary to doing impressions counting at scale for any advertisement business in all we would cover roughly 28 questions and the detailed curriculum uh split week by week can be found on the course page which is linked in the description down below so if you're looking to learn system design from the first principles you will love this course i have two offerings for you the first one is the live cohort discourse which you see on the left side and the second one is the recorded course which you can see on the right side the live code base course happens every two months and it will go on for eight weeks while the recorded course contains the recordings from one of the past cohorts as is if you are in a hurry and want to binge learn system design i would highly recommend you going for the recorded one otherwise the live code is where you can participate and discuss things live with me and the entire cohort and amplify your learnings the decision is totally up to you the course details prerequisites testimonials can be found on the course page at with binary dot me slash master class and i would highly recommend you to check that out i put the link of uh the course in the description down below so if you're interested to learn system design go for it check out the link in the description down below and i hope to see you in my next cohort thanks so in the previous video we looked at the quantification of performance of a hash table through a factor called load factor which is represented by alpha and it is just a fraction of the number of keys that are hash table holds divided by the total number of slots that you have so if you have a hash table with 50 slots and you have placed 25 keys your load factor becomes 25.50 is equal to 0.5 right as load factor increases your performance would degrade because then it would take longer time for you to find a free slot and place your key there so which is where in order to maintain a consistent performance upon when your load factor hits a certain threshold you might want to resize your underlying array right so that you can maintain your consistent performance across so resizing is important but when do we trigger it so resizing is a very expensive operation because you have like you have an underlying error in which you are placing your keys now you have to resize which means you have to allocate a bigger array and then move the keys and what not so resizing is an expensive process so you cannot get either too aggressive or too lenient on it if you become too aggressive which means you are very frequently resizing your array you are wasting a lot of time doing resizing and moving keys here and there right if you become too lenient then you are letting your performance degrade right so there has to be a sweet spot between them so a very typical strategy that you might see is people trigger resize when alpha is equal to 0.5 which means as soon as your ad is half filled you might want to trigger a resize this way you maintain a consistent performance and obviously we can make this logic as complicated as possible but alpha is equal to 0.5 as a threshold is a very common practice that we pull ups then how do we resize resizing is really very simple it's expensive but it's simple so what do you do you have an old array of let's say size m old and now you have to create you have to resize now which means you have to create a new array of size m new so what you do the step goes like this first step you create a new array whose length is equal to m new then you copy all the elements from your old array into this new array and then you delete the old array right and here the expensive steps are creating a new area of length m new is expensive because your operating system would have to find that big of a contiguous memory location and then allocate it it takes time and then much more than that the time would be required for you to copy the elements from your old array to this new array you cannot just bluntly copy it because hash functions would change your keys would be rehashed into this new location a simple way to do it you might have heard of a of a function called real lock in c language you can use that to implement it but that might not be very fruitful because real lock what it what real log does is it creates that new array copies elements as this like for example if you had an array of length 20 uh if you had an error link 2050 within which you have 25 keys placed it would literally copy that array as is to this new place and the later half of the allocation would be empty that might not be the best strategy for us given that we are using a hash function so balancing would be a problem right so what is and the time taken to resize is proportional to the number of keys that you have so larger the number of keys the larger time it would take for your os to allocate it and larger time it would take for you to copy the old keys and put it into the new so time to resize is proportional to the number of keys that you are holding so a typical strategy to resize is always double the array right so during insert if we trigger the resize we reallocate a new array of two times the size and rehash the key to put it really simple let's say i have an array of size 4 which holds keys k4 k1 k3 and k2 in this order and my hash function through which i'm deciding which key goes where is h of k mod m m old so m old is the old size of the array which means here it would be four now let's say i'm resizing it to eight so then my hash function would change to h of k mod eight so i'd have to create first the area of size 8 then i trade through all the keys and find out where they are now placed in the array with h of k mod m nu which is h of k mod n a mod 8 so you'll find the index where you have to place it so you might see a little you definitely see that your keys are nearly uniformly distributed and will take up four slots out of the eight slots that are available right so because your hash function is changing you have to rehash all of our existing keys and place it here now comes the really interesting part why do we always double we have heard it so many times so many times that hash tables whenever we are triggering resize we always have to double why why double why can't i just keep growing it one at a time right so here when i'm saying double it is just a growing factor it just it is basically called as a growth factor so here the growth factor is 2 which means i'm always doubling if my growth factor is 3 i would be always tripling the space right so in most common implementation you would see a growth factor of 2 but it could be one point two one point five two anything it could be any number but two is a very common implementation okay to understand why we need to do it by two let's say we we go to the other extreme and say we always increase the array by one right so what we would do is we would allocate a hash table of slot one and whenever we add something to it we would add a new we would reallocate the array and then insert a new element into that right so every time we are just utilizing the entire space of our hash table given that let's see what is the time complexity of this so i started with hash table of slot one right i inserted a what my what i would be doing i'll be allocating an array of size one and i'll be inserting the element so number of operations one then let's say and i have to insert b into this so i would do what i would reallocate i would allow not relocate i would allocate an array of size two i would copy one element from my previous step because my array had one element i would copy that one element and then insert b total number of operations two right insert c when i'm doing it what i would do is i would first allocate the array of length 3 then i would place then i would copy a and b from my previous step then i would insert c into it the total number of operations would be 3 because one for the resize two for copying of element and one for inserting a c so that is a constant time operation so that is total number of operations three and so on and so forth if i continue doing it if let's say i want to insert z into this scheme what i would be doing i would be doing 26 operations so for n operations so now overall if i'm inserting n keys overall if i'm inserting n keys in my hash table the number of operations would be 1 plus 2 plus 3 plus 4 and so on and so forth plus n right because every time you resize like every time you are resizing your array and then you are placing your key in right so your hash table will start from zero with zero slots then you are inserting the first element would i look at the area of size one then when you are inserting the second element it will look at the size two copy and then reshuffle and what not it would do this so for me to insert n keys it would require me to do 1 plus 2 plus 3 plus 4 plus up to and so on n operations which is equal to n into n minus 1 by 2 is equal to order of n square so if i'm doing if i'm being very aggressive on the resize where i'm saying that i'll just allocate the bare minimum space that i would need to my overall time complexity of inserting data into hash table like inserting n keys in my hash table would be order of n square very expensive right so let's see what happens when we double every time so when i say double every time what this means that you might start with one then when you want to trigger double or you want to trigger any size you would make it 2 then 4 then 8 then 16 then 32 right you would go that way so let's say we spot the middle and say that hey for me to go from n let's say my current size is n and i want to go to 2n but i want to trigger a resize i want to do a 2n of it right but from doing n to 2n i would have had a situation where i was going from n by 2 to n because that would be a step back right n by 2 to n and n by 2 n were always doubling now let's take a look into when we insert n by 2 elements in the area of size n by 2. now what would happen right i'm taking an example a very simple assumption that we resize just to make our uh just to make the explanation simple we resize when the array is completely filled right obviously it depends on the load factor that we discussed that if we would resize it at 0.5 but understanding this becomes very simple when you say that i would resize when i'm completely filling my array then only i'll reside so when i'm getting if my array has four slots when all four are complete then only i'll trigger a resize right so this is what we would be doing okay so with this assumption let's say we insert n by two elements in the area of size n by two right so i had n by two minus one elements at first i'd have n by two let's i have slots four i'd have three or three slots filled then i'm inserting one as soon as i'm inserting one what would happen is i would have to resize and then basically as soon as my arrays filled out i'm basically triggering a resize right so i'll be inserting the element and then triggering a reside resize so what i'll have is i'll have i'm having an n by two length array i'm inserting my final element into it which completes my n by two array and then i'm triggering a resize so now my array size would be n and the state would be it would have n by two elements filled while the other half is empty right so n by two element spread and other and basically the overall filling percentage is n by is basically fifty percent so n by two element spilled order would not be specific with illustration i'm done i'm doing it like left half filled and live right half empty and but this is just for illustration making it simple to visualize but it would contain n by two elements filled in the array of size n right and this is where we start now this is the current state right now let's say we go from n to 2n for us to go from n to 2n what needs to be done to get to the next resize because we are resizing when my array is completely filled right now i have an array of size n where i just filled by n by two elements there is still n by two space left so i will not be triggering my next resize until i fill the rest n by two slots of my array so for the next n by two elements that i'm adding sorry for next n by two minus one elements that i'm adding it would take me order of one time to insert correct because that's what i'm doing so i'm resizing when my array is completely filled right so while inserting the last element i'm triggering a resize right so what would happen is first for the first n by two minus one elements because n by two slots are still remaining in my array right they're still empty in my array so first n by two minus one elements i can very easily insert with order one right so that is done and then then for the first i've inserted then when i'm about to insert the final element i'll see that my array needs a resize because my array would be completely filled so then what i would do is we would allocate an area of size 2n right would allocate an area of size 2n copy the elements copy n by 2 plus n by 2 minus 1 so n by 2 old elements n by 2 minus 1 newly added n by 2 minus 1 elements into this new into this new array and then add the final element that we wanted to insert so now what after this what i will what we will have is will have an area of size 2n where it is filled by elements n by 2 plus n by 2 minus 1 plus 1 so it will be filled by n elements right so this is how your resize or this is how you'll go from n to 2n now let's take an operator let's take a look at the number of operations we would require to do this so the number of operations would be equal to n by 2 minus 1 because it is taking you order one time to insert n by 2 minus 1 elements because post that only it would trigger a resize so n by 2 minus 1 you did then 1 or then order 1 to insert the final element right plus 2n as part of allocation because you are resizing the array it would be two n operations plus n to copy elements from your old space into this new space right so total number of operations that you are requiring is 7n by 2 which is order n and which is why we always double so the total number of operations when we were very aggressively resizing by 1 every time it was order of n square n into n minus 1 by 2. when we double we are getting an order n complexity which is much much much better than order n square right and this is why we always double and this does not just hold true when you double but any time when our growth factor is greater than one this would hold true right and this is the brilliance of this idea and the core reason why we always double right now let's take a look at another very interesting insight why is hash table always allocated as a power of 2 this is again a really common thing to observe gary my hash table is always 1 2 4 8 16 32 and whatnot right y always power of 2 this is where we would go into the bitwise operator like y okay so the underlying area of hash table has length of power of two it's very efficient because like the world is doing it but how so hash table works on hash functions so you when you want to insert a key what you do is you take the key you pass it through the hash function you get a number you mod that number by the size of the array to get the index and then you place the key to at that place right so here the mod operation is super important because it bounds your integer 32 range into the size of your hash table so that you can place your key at that particular index but mod function is very expensive internally when you implement the mod function it does a long division maybe it could also do a sign division but integer division it does a division which is a very expensive operation and then it captures the remainder can we do faster right and obviously it's all about efficiency at scale and especially at this micro level and even a small optimization makes a lot of difference okay so can we do better so let's see what happens when my m is 4 which means my the number of slots in my hash table is 4 let's see how my keys would change like what is the output of my mod function so if my hash key is 1 1 mod 4 our answer is 1 so i'll go so output is 1 that is where i'm placing my key 2 2 mod 4 is 2 3 mod 4 is 3 4 mod 4 is 0 5 mod 4 is 1 6 mod 4 is 2 right so we see how it cyclically goes from 1 2 3 0 1 2 3 0 and so on and so forth right can we get this same output can we get the same output but without using mod function that is where the magic of bitwise and comes in right okay so instead of doing a mod m what if we do mod of m minus 1 and this works when m is a power of 2 so we know that m is equal to 4 4 is equal to 2 raised to power 2 and instead of doing and with 4 we would do and with 2 raised to 2 minus 1 which is equal to 3 so let's just take a look at what output it generates so 1 mod 4 is 1 right but now can we get the same output of what one mod four is with one and three four minus one right so one and three one is zero zero zero one and three is zero zero one one so when i take an end of this i get 0 0 0 1 right because 1 is 0 0 0 1 3 0 0 1 1 output is 0 0 0 1 which is 1 which is same as 1 mod 4. similarly for 2 it is 0 0 1 0 bit wise and with 3 which is 0 0 1 1 the output is 0 0 1 0 which is 2 3 mod 4 is three three m three and three is three right so three mod three three mod four is three and three and three is three so output matches four mod four is zero and four and three four is zero one zero zero and zero zero one one zero one zero zero zero zero 1 the output is 0 for mod 4 0 4 and 3 0 again it matches and the final thing where we see the cycle repeating is 5 mod 4 5 mod 4 is 1 5 and 3 is 0 1 0 1 5 0 1 0 1 3 0 0 1 1 when we end we get 0 0 0 1 which is same as that but how how come mod and and gives the same output it happens because your m your and thing is 2 raised to n minus 1 because of this what hap what is happening is your equals mod m is equivalent to bitwise and with m minus 1 where m is a power of 2. because your end operation is significantly faster than division you get a massive performance gain massive massive performance gain which is why everyone uses hashtag as power of 2 so but why is this happening because for a number which is 2 raised to k minus 1 like 3 7 15 the lower bits are all one higher bits are all zero and and operation acts as a filter right so the first the the higher bits which are zero they would always result into zero the lower bits that are all ones they would as is act as a filter wherever the one is set it would bring down as is right and the numbers are continuously increasing so this is actually limiting the range that we are generating because the top zeros or the higher bit zeros that are there they are limiting the your number would not go beyond all the ones that are there this is such a beautiful piece of optimization right so mod function is expensive we replace it with bitwise and when we replace it with bit twice and it would only work for the cases where my m when my when i'm doing a mod m where m is a power of 2 we would do bitwise and with m minus 1. so when i'm doing a mod 4 is equivalent to and 3 or for if i'm doing mod 8 it is equivalent to and 7 right so 2 raised to k minus 1 this is why hash tables are always a factor of 2 1 2 4 8 16 and they're always doubled and now you know why that happens okay now the now will as part of conclusion what we'll take a look at is we'll take a look at shrinking of a hash table right so if the keys are deleted from the hash table then it does not make sense for us to keep it bloated right let's say for hashtable only has one value and you have allocated 128 slots it's stupidity right you have to reduce it but when right so it does not make sense to load that up to to keep that bloated so we have to shrink it so we know that we grow or we grow our hash table when our load factor reaches half which means when my array is half built that's when we scale up right so what would what what would uh when should we trigger our shrinking let's reverse engineer it like let's go backwards and understand when should we do it so if your table has n slots then it would have at max n by two elements before we trigger a resize right because because sorry it would have at max n by two elements if a table has n slots it would have n by two elements otherwise it would have triggered a resize right so what would be the state that if it has n by two elements it would have triggered a resize so number of elements if let's say i have an array of slot 16 if i had if i have had eight elements i would have triggered a resize so then my array would have been 32 so the number of max number of elements before we trigger a resize could be 7 right so when my slots is 16 my number of elements would be seven at max because at eighth element we would trigger a resize right so if we shrink let's say if we consider shrinking at this stage what would happen is we would be shrinking it into half right so from 16 we can go to eight but the number of elements we have are seven so to fit seven into this eighth slot what would happen this would lead to poor experience because as soon as we do it we would again have to trigger a recess because now my load factor is close to one right so which means that when i have slot 16 and element 7 i cannot trigger the string so let's go a step back right so now what does this mean this means that we need to have enough leeway where let's say we trigger a resize when i we have n by four elements basically n by four minus one so if i have 16 slots let's say we trigger it at three right so when we do that we still get a consistent performance so when i have 16 slots and i have three i'm still getting consistent performance but the problem is just one more insert just one more insert and my threshold would be bridged of 0.5 and i would have to trigger the resize so for example if we are shrinking from 16 to 8 when my number of elements are 3 from 16 to 8 when my number of elements are 3 just one insert into it what would happen my 3 would become 4 load factor would become 0.5 and i would trigger a resize again so that's a very poor way right so we go a step back again so what we do so that's where to keep it very efficient we need to shrink when the number of elements are such that a few insertions after that would not cause would not cause a immediate resize right so we go a step even back so that is where what we do is we would shrink when my number of elements are less than n by eight so if i have 16 slots i would do i would trigger a resize when there are 16 by 8 is two so basically less than two elements when i'll have that's when i would trigger a resize otherwise you would be constantly doing resize and shrink and resize and shrink as we just saw in the example right might be a little confusing i would recommend look back into this space and understand what's happening take an example and understand it otherwise this video would be too long that's why i'm not doing that but understand the idea behind it you want to maintain a consistent performance while also ensuring that hey just one more insertion would not lead to a would not lead to a resize again right okay so this is why or basically this is when you trigger a shrinking of your hash table to keep your memory utilization pretty uh efficient right so just to summarize this has been a longer but just to summarize resizing is important to maintain performance point number one point number two resizing is super costly right we saw how allocate and move keys and whatnot third hash tables is always doubled when we want to grow in order to maintain consistent performance order n performance when we are always doubling four hash table always has two raised to n slots for faster compute because mod can be replaced by bitwise and fifth you grow the hash table when your load factor hits 0.5 like one by two right you know arrays halfway that's when you trigger a resize so that you have enough leeway that you maintain a consistent performance and you shrink when a load factor becomes one by eight right so that again you maintain a consistent performance without triggering a frequent resize again and that is it about resizing of hash table i hope it was amusing it was fun when i was exploring it i was i've always wondered why and now i finally have the answer for that so great i hope you learned something interesting this was the eighth video in my hashtable internal series if you found this amusing you'll find this entire series really fascinating i would highly encourage you to go through all the videos because why not hashtables are such such a common piece of data structure that we day in and day out use we need to understand how they work internally right so that's it that's it for me for this video if you guys like this video give this video a massive thumbs up if you guys like the channel give this channel a sub i post three in-depth engineering videos every week and i'll see in the next one thanks again [Music] you